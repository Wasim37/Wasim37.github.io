<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>星空str</title><meta description="ai,机器学习,深度学习,算法,leetcode,java"><meta property="og:type" content="blog"><meta property="og:title" content="Wasim37"><meta property="og:url" content="https://wangxin123.com/"><meta property="og:site_name" content="Wasim37"><meta property="og:description" content="ai,机器学习,深度学习,算法,leetcode,java"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"><meta property="article:author" content="Wasim37"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://wangxin123.com/"},"headline":"Wasim37","image":["https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"],"author":{"@type":"Person","name":"Wasim37"},"description":"ai,机器学习,深度学习,算法,leetcode,java"}</script><link rel="alternative" href="/atom.xml" title="星空str" type="application/atom+xml"><link rel="icon" href="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/wu.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="/css/style.css"><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="/js/globalUtils.js"></script></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/logo.svg" alt="星空str" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/wasim37"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-26T14:22:00.000Z">2017-10-26</time><a class="commentCountImg" href="/2017/10/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/#comment-container"><span class="display-none-class">f756fd2293917cf1253a6d78ea7adf31</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="f756fd2293917cf1253a6d78ea7adf31"> 0</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/">经典模型</a></span><span class="level-item">11 分钟 读完 (大约 1714 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/26/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/">贝叶斯算法</a></h1><div class="content"><p>持续更新中。。。</p>
<ul>
<li><a href="https://github.com/Wasim37/machine_learning_code/tree/master/06%20%E8%B4%9D%E5%8F%B6%E6%96%AF/notebook">示例代码</a></li>
<li><a href="#贝叶斯定理相关公式">贝叶斯定理相关公式</a></li>
<li><a href="https://www.zhihu.com/question/22905989">条件概率和后验概率区别</a></li>
<li><a href="#朴素贝叶斯">朴素贝叶斯</a></li>
<li><a href="#高斯朴素贝叶斯">高斯朴素贝叶斯</a></li>
<li><a href="#伯努利朴素贝叶斯">伯努利朴素贝叶斯</a></li>
<li><a href="#多项式朴素贝叶斯">多项式朴素贝叶斯</a></li>
<li><a href="#贝叶斯网络">贝叶斯网络</a></li>
<li><a href="https://www.zhihu.com/question/28006799/answer/38996563">怎么通俗易懂地解释贝叶斯网络和它的应用</a></li>
<li><a href="#用贝叶斯机率说明Dropout的原理">用贝叶斯机率说明Dropout的原理</a></li>
<li><a href="#如何用贝叶斯算法实现垃圾邮件检测">如何用贝叶斯算法实现垃圾邮件检测</a></li>
<li><a href="#当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑">当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑</a></li>
</ul>
<hr>
<h3 id="贝叶斯定理相关公式"><a href="#贝叶斯定理相关公式" class="headerlink" title="贝叶斯定理相关公式"></a><h2 id="贝叶斯定理相关公式">贝叶斯定理相关公式</h2></h3><ul>
<li>先验概率P(A)：在不考虑任何情况下，A事件发生的概率</li>
<li>条件概率P(B|A)：A事件发生的情况下，B事件发生的概率</li>
<li>后验概率P(A|B)：在B事件发生之后，对A事件发生的概率的重新评估。<a href="https://www.zhihu.com/question/22905989">条件概率和后验概率区别</a></li>
<li>全概率：如果B和B’构成样本空间的一个划分，那么事件A的概率为：B和B’的概率分别乘以A对这两个事件的概率之和。</li>
<li><strong>贝叶斯定理：</strong> <script type="math/tex">P(B_i|A)=\frac{P(AB_i)}{P(A)}=\frac{P(B_i)P(A|B_i)}{\sum_j P(B_j)P(A|B_j)}</script></li>
</ul>
<p>贝叶斯不同于SVM、逻辑回归与决策树等判别式模型，它属于生成式模型（LDA、HMM等）。<br><strong>贝叶斯思想可以概括为先验概率+数据=后验概率，后验概率就是我们要求的。</strong></p>
<hr>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a><h2 id="朴素贝叶斯">朴素贝叶斯</h2></h3><p>朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是很简单地<strong>假设样本特征彼此独立</strong>. 这个假设<strong>现实中基本上不存在</strong>， 但特征相关性很小的实际情况还是很多， 所以很多时候这个模型仍然能够工作得很好。</p>
<p><strong>如果样本特征属性关联很大，朴素贝叶斯就没法很好解决这类问题，可以考虑使用贝叶斯网络。</strong></p>
<hr>
<h3 id="高斯朴素贝叶斯"><a href="#高斯朴素贝叶斯" class="headerlink" title="高斯朴素贝叶斯"></a><h2 id="高斯朴素贝叶斯">高斯朴素贝叶斯</h2></h3><p>Gaussian Naive Bayes是指当 <strong>特征属性为连续值时，而且分布服从高斯分布</strong>，<br>那么在计算 P(x|y) 的时候可以直接使用高斯分布的概率公式：</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514104856.png" alt=""></p>
<p>因此只需要计算出各个类别中此特征项划分的各个均值和标准差</p>
<hr>
<h3 id="伯努利朴素贝叶斯"><a href="#伯努利朴素贝叶斯" class="headerlink" title="伯努利朴素贝叶斯"></a><h2 id="伯努利朴素贝叶斯">伯努利朴素贝叶斯</h2></h3><p>Bernoulli Naive Bayes是指当 <strong>特征属性为连续值时，而且分布服从伯努利分布，</strong><br>那么在计算 P(x|y) 的时候可以直接使用伯努利分布的概率公式：</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514104905.png" alt=""></p>
<p>伯努利分布是一种离散分布，只有两种可能的结果。1表示成功，出现的概率为p；<br>0表示失败，出现的概率为q=1-p；其中均值为E(x)=p，方差为Var(X)=p(1-p)</p>
<hr>
<h3 id="多项式朴素贝叶斯"><a href="#多项式朴素贝叶斯" class="headerlink" title="多项式朴素贝叶斯"></a><h2 id="多项式朴素贝叶斯">多项式朴素贝叶斯</h2></h3><p>Multinomial Naive Bayes是指当 <strong>特征属性服从多项分布</strong>，从而对于每个类别y，<br>参数为θy =(θy1, θy2,…,θyn)，其中n为特征属性数目，那么P(xi|y)的概率为θyi</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514104934.png" alt=""></p>
<hr>
<h3 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a><h2 id="贝叶斯网络">贝叶斯网络</h2></h3><p><strong>当多个特征属性之间存在着某种相关关系的时候，使用朴素贝叶斯算法就没法解决这类问题，那么贝叶斯网络就是解决这类应用场景的一个非常好的算法。</strong></p>
<p>把某个研究系统中涉及到的随机变量，根据是否条件独立绘制在一个有向图中，就形成了贝叶斯网络。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514111959.png" alt=""></p>
<p>贝叶斯网络(BN)，又称有向无环图模型，是一种概率图模型，根据概率图的拓扑结构，<strong>考察一组随机变量{X1, X2,…,Xn}及其N组条件概率分布的性质。</strong></p>
<p>一般而言，贝叶斯网络的有向无环图中的节点表示随机变量，可以是可观察到的变量，或隐变量，未知参数等等。<strong>连接两个节点之间的箭头代表两个随机变量之间的因果关系</strong>(也就是这两个随机变量之间非条件独立)，如果两个节点间以一个单箭头连接在一起，表示其中一个节点是“因”，另外一个是“果”，从而两节点之间就会产生一个条件概率值。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514111945.png" alt=""></p>
<p><strong>注意：每个节点在给定其直接前驱的时候，条件独立于其后继。</strong></p>
<p>更多细节见我整理的PDF文件。</p>
<hr>
<h3 id="如何用贝叶斯算法实现垃圾邮件检测"><a href="#如何用贝叶斯算法实现垃圾邮件检测" class="headerlink" title="如何用贝叶斯算法实现垃圾邮件检测"></a><h2 id="如何用贝叶斯算法实现垃圾邮件检测">如何用贝叶斯算法实现垃圾邮件检测</h2></h3><p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E8%B4%9D%E5%8F%B6%E6%96%AF.png" alt=""></p>
<p>首先通过相除消去分子，大于1即正样本，小于1即负样本。<br>然后将难以计算的联合分布概率P(y=T|x)和P(y=F|x)进行转换即可。</p>
<p>下面是另外两种解释：</p>
<p>原文：<a href="https://github.com/SunnyMarkLiu/NaiveBayesSpamFilter">https://github.com/SunnyMarkLiu/NaiveBayesSpamFilter</a></p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514110848.png" alt=""></p>
<p>原文：<a href="https://blog.csdn.net/Gane_Cheng/article/details/53219332">https://blog.csdn.net/Gane_Cheng/article/details/53219332</a></p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514110919.png" alt=""></p>
<hr>
<h3 id="当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑"><a href="#当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑" class="headerlink" title="当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑"></a><h2 id="当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑">当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑</h2></h3><p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/55.png" alt=""></p>
<hr>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-25T14:22:00.000Z">2017-10-25</time><a class="commentCountImg" href="/2017/10/25/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/#comment-container"><span class="display-none-class">4bf7bf6c585216595e9cce01d69be18b</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="4bf7bf6c585216595e9cce01d69be18b"> 0</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/">经典模型</a></span><span class="level-item">21 分钟 读完 (大约 3128 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/25/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/">聚类算法</a></h1><div class="content"><ul>
<li><a href="https://github.com/Wasim37/machine_learning_code/tree/master/05%20%E8%81%9A%E7%B1%BB%20Clustering">示例代码</a></li>
<li><a href="https://www.processon.com/view/link/5ad81e5be4b046910642bc4b">思维导图</a></li>
<li><a href="#聚类简介">聚类简介</a></li>
<li><a href="#kmeans">kmeans</a></li>
<li><a href="#Mini Batch K-Means">Mini Batch K-Means</a></li>
<li><a href="#二分KMeans算法">二分KMeans算法</a></li>
<li><a href="#Kmeans++">Kmeans++</a></li>
<li><a href="#层次聚类">层次聚类</a></li>
<li><a href="#密度聚类">密度聚类</a></li>
<li><a href="#谱聚类">谱聚类</a></li>
</ul>
<hr>
<h3 id="聚类简介"><a href="#聚类简介" class="headerlink" title="聚类简介"></a>聚类简介</h3><p>聚类就是对大量未知标注的数据集，按照数据 <strong>内部存在的数据特征</strong> 将数据集划分为 <strong>多个不同的类别</strong>，使 <strong>类别内的数据比较相似，类别之间的数据相似度比较小</strong>；</p>
<p>聚类算法的<font color="red">重点是计算样本项之间的相似度，有时候也称为样本间的距离。</font></p>
<p>聚类属于无监督学习，和分类算法的区别：<br>（1）分类算法是有监督学习，基于有标注的历史数据进行算法模型构建<br>（2）聚类算法是无监督学习，数据集中的数据是没有标注的</p>
<p><strong>聚类算法的衡量指标:</strong></p>
<ul>
<li>混淆矩阵</li>
<li>均一性</li>
<li>完整性</li>
<li>V-measure</li>
<li>调整兰德系数(ARI)</li>
<li>调整互信息(AMI)</li>
<li>轮廓系数(Silhouette)</li>
</ul>
<hr>
<h3 id="kmeans"><a href="#kmeans" class="headerlink" title="kmeans"></a>kmeans</h3><p>K-means算法，也称为K-平均或者K-均值，是一种使用广泛的最基础的聚类算法，一般作为接触聚类算法的第一个算法。</p>
<p>假设输入样本为 T=X1, X2, …, Xm; 则 <strong>算法步骤为</strong>（使用欧几里得距离公式）：<br>（1）随机初始化 k 个类别中心 a1, a2,…ak;<br>（2）对于每个样本 Xi，将其标记为距离类别中心 aj 最近的类别 j<br>（3）更新每个类别的中心点 aj 为隶属该类别的所有样本的均值<br>（4）重复上面两步操作，直到达到某个中止条件</p>
<p><strong>中止条件</strong>：迭代次数、最小平方误差MSE、簇中心点变化率</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%81%9A%E7%B1%BB/20180515105259.png" alt=""></p>
<p><strong>K-means算法在迭代的过程中使用所有点的均值作为新的质点(中心点)，如果簇中存在异常点，将导致均值偏差比较严重。K-means算法是初值敏感的，选择不同的初始值可能导致不同的簇划分规则。</strong></p>
<p><strong>缺点：</strong></p>
<ul>
<li>K值是用户给定的，在进行数据处理前，K值是未知的，不同的K值得到的结果也不一样；</li>
<li>对初始簇中心点是敏感的</li>
<li>不适合发现非凸形状的簇或者大小差别较大的簇</li>
<li>特殊值(离群值)对模型的影响比较大</li>
</ul>
<p><strong>优点：</strong></p>
<ul>
<li>理解容易，聚类效果不错</li>
<li>处理大数据集的时候，该算法可以保证较好的伸缩性和高效率</li>
<li>当簇近似高斯分布的时候，效果非常不错</li>
</ul>
<hr>
<h3 id="Mini-Batch-K-Means"><a href="#Mini-Batch-K-Means" class="headerlink" title="Mini Batch K-Means"></a>Mini Batch K-Means</h3><p><strong>当需要聚类的数据量非常大的时候，效率是最重要的诉求。</strong></p>
<p>Mini Batch K-Means算法是K-Means算法的一种优化变种，采用<font color="red">小规模的数据子集</font>(每次训练使用的数据集是在训练算法的时候随机抽取的数据子集)<font color="red">减少计算时间</font>，同时试图优化目标函数；<strong>Mini Batch K-Means算法可以减少K-Means算法的收敛时间，而且产生的结果效果只是略差于标准K-Means算法。</strong></p>
<p>算法步骤如下：</p>
<ul>
<li>首先 <strong>抽取部分数据集</strong>，使用K-Means算法构建出K个聚簇点的模型</li>
<li>继续抽取训练数据集中的部分数据集样本数据，并将其添加到模型中，分配给距离最近的聚簇中心点</li>
<li>更新聚簇的中心点值</li>
<li>循环迭代第二步和第三步操作，直到中心点稳定或者达到迭代次数，停止计算操作</li>
</ul>
<hr>
<h3 id="二分KMeans算法"><a href="#二分KMeans算法" class="headerlink" title="二分KMeans算法"></a>二分KMeans算法</h3><p><strong>解决K-Means算法对初始簇心比较敏感的问题</strong>，二分K-Means算法是一种弱化初始质心的一种算法。</p>
<p>具体思路步骤如下：</p>
<ul>
<li><strong>将所有样本数据作为一个簇放到一个队列中</strong></li>
<li><strong>从队列中选择一个簇进行K-means算法划分，划分为两个子簇，并将子簇添加到队列中</strong></li>
<li>循环迭代第二步操作，直到中止条件达到(聚簇数量、最小平方误差、迭代次数等)</li>
<li>队列中的簇就是最终的分类簇集合</li>
</ul>
<p>从队列中 <strong>选择划分聚簇</strong> 的规则一般有两种方式；</p>
<ul>
<li>对所有簇计算误差和SSE(SSE也可以认为是距离函数的一种变种)，选择SSE最大的聚簇进行划分<br>操作(优选这种策略)</li>
<li>选择样本数据量最多的簇进行划分操作</li>
</ul>
<font color="red">由于计算量大，二分KMeans优化算法使用的比较少</font>

<hr>
<h3 id="Kmeans"><a href="#Kmeans" class="headerlink" title="Kmeans++"></a>Kmeans++</h3><p><strong>解决K-Means算法对初始簇心比较敏感的问题，使用非常广泛。</strong></p>
<p>K-Means++算法和K-Means算法的区别主要在于K的选择上。K-Means算法使用随机给定的方式。K-means++的思想则是：<font color="red">初始的聚类中心之间的相互距离要尽可能的远</font>。</p>
<p>主要步骤如下：</p>
<ul>
<li>从输入的数据点集合中 <strong>随机选择一个点作为第一个聚类中心</strong></li>
<li><strong>对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)</strong></li>
<li><strong>选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大</strong></li>
<li>重复2和3直到k个聚类中心被选出来</li>
<li>利用这k个初始的聚类中心来运行标准的k-means算法</li>
</ul>
<p>从上面的算法描述上可以看到，<strong>算法的关键是第3步</strong>，如何将D(x)反映到点被选择的概率上，一种算法如下(详见此地)：</p>
<ul>
<li>先从我们的数据库随机挑个随机点当“种子点”</li>
<li>对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。</li>
<li>然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。</li>
<li>重复2和3直到k个聚类中心被选出来</li>
<li>利用这k个初始的聚类中心来运行标准的k-means算法</li>
</ul>
<p>可以看到算法的第三步选取新中心的方法，这样就能保证距离D(x)较大的点，会被选出来作为聚类中心了。至于为什么原因很简单，如下图 所示：<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%81%9A%E7%B1%BB/20180515111218.png" alt=""></p>
<p>假设A、B、C、D的D(x)如上图所示，当算法取值Sum(D(x))*random时，该值会以较大的概率落入D(x)较大的区间内，所以对应的点会以较大的概率被选中作为新的聚类中心。</p>
<p><strong>缺点</strong>：由于聚类中心点选择过程中的内在有序性，在扩展方面存在着性能方面的问题(第k个聚类中心点的选择依赖前k-1个聚类中心点的值)</p>
<hr>
<h3 id="层次聚类-Hierarchical-Clustering"><a href="#层次聚类-Hierarchical-Clustering" class="headerlink" title="层次聚类 Hierarchical Clustering"></a>层次聚类 Hierarchical Clustering</h3><p><strong>k-means聚类算法的一个变体，主要是为了改进k-means算法随机选择初始质心的随机性造成聚类结果不确定性的问题。</strong></p>
<p>层次聚类算法主要分为两大类算法：</p>
<ul>
<li>凝聚的层次聚类：<strong>AGNES</strong> 算法(AGglomerative NESting)==&gt;采用 <strong>自底向上</strong> 的策略。<strong>最初将每个对象作为一个簇，然后这些簇根据某些准则被一步一步合并</strong>，两个簇间的距离可以由这两个不同簇中距离最近的数据点的相似度来确定；聚类的合并过程反复进行直到所有的对象满足簇数目。</li>
<li>分裂的层次聚类：<strong>DIANA</strong> 算法(DIvisive ANALysis)==&gt;采用 <strong>自顶向下</strong> 的策略。<strong>首先将所有对象置于一个簇中，然后按照某种既定的规则逐渐细分为越来越小的簇</strong>(比如最大的欧式距离)，直到达到某个终结条件(簇数目或者簇距离达到阈值)。</li>
</ul>
<p><strong>优缺点</strong></p>
<ul>
<li>简单，理解容易</li>
<li>合并点/分裂点选择不太容易</li>
<li>合并/分类的操作不能进行撤销</li>
<li>大数据集不太适合</li>
<li>执行效率较低O(t*n 2 )，t为迭代次数，n为样本点数</li>
</ul>
<hr>
<h3 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h3><p><strong>解决K-Means算法只能发现凸聚类的缺点。</strong></p>
<p><strong>密度聚类可以发现任意形状的聚类，而且对噪声数据不敏感。但是计算复杂度高，计算量大。常用算法为 DBSCAN 和 密度最大值算法。</strong></p>
<hr>
<h1 id="kmeans的复杂度"><a href="#kmeans的复杂度" class="headerlink" title="kmeans的复杂度"></a>kmeans的复杂度</h1><p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/34.png" alt=""></p>
<p>时间复杂度：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数<br>空间复杂度：O((m+K)n)，其中，K为簇的数目，m为记录数，n为维数</p>
<hr>
<h1 id="优化Kmeans"><a href="#优化Kmeans" class="headerlink" title="优化Kmeans"></a>优化Kmeans</h1><p>Kmeans总结：<br><a href="https://www.processon.com/view/link/5ad81e5be4b046910642bc4b">https://www.processon.com/view/link/5ad81e5be4b046910642bc4b</a></p>
<p>使用kd树或者ball tree<br>将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。</p>
<hr>
<h1 id="KMeans初始类簇中心点的选取"><a href="#KMeans初始类簇中心点的选取" class="headerlink" title="KMeans初始类簇中心点的选取"></a>KMeans初始类簇中心点的选取</h1><p>k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。</p>
<ol>
<li>从输入的数据点集合中随机选择一个点作为第一个聚类中心</li>
<li>对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)</li>
<li>选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大</li>
<li>重复2和3直到k个聚类中心被选出来</li>
<li>利用这k个初始的聚类中心来运行标准的k-means算法</li>
</ol>
<hr>
<h1 id="在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离"><a href="#在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离" class="headerlink" title="在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离"></a>在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离</h1><p>闵可夫斯基距离(Minkowski)<br>（1）当p为1的时候是<strong>曼哈顿距离</strong>(Manhattan)（城市街区距离）<br>（2）当p为2的时候是<strong>欧式距离</strong>(Euclidean)（又称欧几里得度量，最常见的两点之间或多点之间的距离表示法）<br>（3）当p为无穷大的时候是<strong>切比雪夫距离</strong>(Chebyshev)</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/20180419192857.png" alt=""></p>
<p><strong>不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制</strong>。而欧氏距离可用于任何空间的距离计算。因为数据点可以存在于任何空间，所以欧氏距离是更可行的选择。</p>
<p>但<strong>欧氏距离也有明显的缺点。它将样本不同属性（即各指标或各变量量纲）之间的差别等同看待</strong>，这一点有时不能满足实际要求。</p>
<p>曼哈顿距离和欧式距离一般用途不同，无相互替代性。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-22T14:22:00.000Z">2017-10-22</time><a class="commentCountImg" href="/2017/10/22/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/SVM/#comment-container"><span class="display-none-class">764fa7077ee5e1d8fc4c2870ac868244</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="764fa7077ee5e1d8fc4c2870ac868244"> 0</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/">经典模型</a></span><span class="level-item">25 分钟 读完 (大约 3772 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/22/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/SVM/">SVM</a></h1><div class="content"><p>持续更新中。。。</p></div><div class="level is-mobile is-flex"><div class="level-start"><div class="is-uppercase article-more button is-small size-small"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/SVM/">SVM</a></div></div><div class="level-start"><div class="level-item"><a class="article-more button is-small size-small link-muted" href="/2017/10/22/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/SVM/#more"><i class="fas fa-book-reader has-text-grey"> </i>阅读更多&gt;&gt;</a></div></div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-22T14:22:00.000Z">2017-10-22</time><a class="commentCountImg" href="/2017/10/22/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/KNN/#comment-container"><span class="display-none-class">657991e6af2590a0acf220ca03183d3e</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="657991e6af2590a0acf220ca03183d3e"> 0</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/">经典模型</a></span><span class="level-item">5 分钟 读完 (大约 764 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/22/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/KNN/">KNN</a></h1><div class="content"><h3 id="简述KNN算法过程"><a href="#简述KNN算法过程" class="headerlink" title="简述KNN算法过程"></a>简述KNN算法过程</h3><p>谓k最近邻，就是k个最近的邻居的意思，说的是<strong>每个样本都可以用它最接近的k个邻居来代表</strong>。</p>
<ol>
<li>计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；</li>
<li>对上面所有的距离值进行排序；</li>
<li>选前k个最小距离的样本；</li>
<li>根据这k个样本的标签进行投票，得到最后的分类类别；</li>
</ol>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/20180514120948.png" alt=""></p>
<p>KNN算法不仅可以用于分类，<strong>还可以用于回归。</strong><br>通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。<br>更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成反比。</p>
<hr>
<h3 id="KNN中的K如何选取的"><a href="#KNN中的K如何选取的" class="headerlink" title="KNN中的K如何选取的"></a>KNN中的K如何选取的</h3><p>关于什么是KNN，可以查看此文：《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》（链接：<a href="http://blog.csdn.net/v_july_v/article/details/8203674）。KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说：">http://blog.csdn.net/v_july_v/article/details/8203674）。KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说：</a></p>
<p>如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，<strong>K值的减小就意味着整体模型变得复杂，容易发生过拟合；</strong></p>
<p>如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且<strong>K值的增大就意味着整体的模型变得简单。</strong></p>
<p>K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</p>
<p><strong>在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</strong></p>
<hr>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-21T14:22:00.000Z">2017-10-21</time><a class="commentCountImg" href="/2017/10/21/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/#comment-container"><span class="display-none-class">641bd497a93aa25611f6280e906e542e</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="641bd497a93aa25611f6280e906e542e"> 0</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/">经典模型</a></span><span class="level-item">36 分钟 读完 (大约 5360 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/21/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/">决策树、随机森林和提升算法</a></h1><div class="content"><p>持续更新中。。。</p></div><div class="level is-mobile is-flex"><div class="level-start"><div class="is-uppercase article-more button is-small size-small"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a><a class="link-muted mr-2" rel="tag" href="/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/">随机森林</a><a class="link-muted mr-2" rel="tag" href="/tags/%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/">提升算法</a></div></div><div class="level-start"><div class="level-item"><a class="article-more button is-small size-small link-muted" href="/2017/10/21/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/#more"><i class="fas fa-book-reader has-text-grey"> </i>阅读更多&gt;&gt;</a></div></div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-19T14:22:00.000Z">2017-10-19</time><a class="commentCountImg" href="/2017/10/19/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/#comment-container"><span class="display-none-class">7ed7ac9bfcd25b77db45d0d4747bd696</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="7ed7ac9bfcd25b77db45d0d4747bd696"> 0</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/">经典模型</a></span><span class="level-item">1 小时 读完 (大约 7589 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/19/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/">回归算法</a></h1><div class="content"><p>持续更新中。。。</p></div><div class="level is-mobile is-flex"><div class="level-start"><div class="is-uppercase article-more button is-small size-small"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">线性回归</a><a class="link-muted mr-2" rel="tag" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">逻辑回归</a></div></div><div class="level-start"><div class="level-item"><a class="article-more button is-small size-small link-muted" href="/2017/10/19/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/#more"><i class="fas fa-book-reader has-text-grey"> </i>阅读更多&gt;&gt;</a></div></div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-18T14:22:00.000Z">2017-10-18</time><a class="commentCountImg" href="/2017/10/18/%E5%85%B6%E4%BB%96/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/#comment-container"><span class="display-none-class">813dc5c0e202f19ea6c07dd0b5f01f28</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="813dc5c0e202f19ea6c07dd0b5f01f28"> 0</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></span><span class="level-item">2 小时 读完 (大约 20969 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/18/%E5%85%B6%E4%BB%96/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/">机器学习选择题集锦</a></h1><div class="content"><h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><p>1、对于线性回归，我们应该有以下哪些假设？</p>
<ol>
<li>找到离群点很重要, 因为线性回归对离群点很敏感</li>
<li>线性回归要求所有变量必须符合正态分布</li>
<li>线性回归假设数据没有多重线性相关性<br>A 1 和 2<br>B 2 和 3<br>C 1,2 和 3<br>D 以上都不是</li>
</ol></div><div class="level is-mobile is-flex"><div class="level-start"><div class="is-uppercase article-more button is-small size-small"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div></div><div class="level-start"><div class="level-item"><a class="article-more button is-small size-small link-muted" href="/2017/10/18/%E5%85%B6%E4%BB%96/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/#more"><i class="fas fa-book-reader has-text-grey"> </i>阅读更多&gt;&gt;</a></div></div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-09-24T16:00:00.000Z">2017-09-25</time><a class="commentCountImg" href="/2017/09/25/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/#comment-container"><span class="display-none-class">34632dedb1a9fddcc731817f62fa2bd8</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="34632dedb1a9fddcc731817f62fa2bd8"> 0</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></span><span class="level-item">3 分钟 读完 (大约 428 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/09/25/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/">数学基础</a></h1><div class="content"><h1 id="知识要点"><a href="#知识要点" class="headerlink" title="知识要点"></a>知识要点</h1><p><img src='https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/基础数学/数学基础.pdf_P6.jpg' width="70%" ><br></div><div class="level is-mobile is-flex"><div class="level-start"><div class="level-item"><a class="article-more button is-small size-small link-muted" href="/2017/09/25/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/#more"><i class="fas fa-book-reader has-text-grey"> </i>阅读更多&gt;&gt;</a></div></div></div></article></div><nav class="pagination is-centered mt-4" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/2/">上一页</a></div><div class="pagination-next is-invisible is-hidden-mobile"><a href="/page/4/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><a class="pagination-link is-current" href="/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png" alt="星空str"></figure><p class="title is-size-4 is-block line-height-inherit">星空str</p><p class="is-size-6 is-block">Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>GuangZhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">28</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">23</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/wasim37" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/wasim37"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Weibo" href="/"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Email" href="/"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Next" href="/"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><hr></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"><span class="level-start"><span class="level-item">特征工程</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"><span class="level-start"><span class="level-item">知识总结</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="level-start"><span class="level-item">神经网络</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">经典模型</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"><span class="level-start"><span class="level-item">计算机视觉</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="level-start"><span class="level-item">论文阅读</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5/"><span class="level-start"><span class="level-item">项目实践</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">最新评论</h3><span class="body_hot_comment">加载中，最新评论有1分钟延迟...</span></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2021-01-01T16:00:00.000Z">2021-01-02</time></p><p class="title is-6"><a class="link-muted" href="/2021/01/02/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-01-01T16:00:00.000Z">2021-01-02</time></p><p class="title is-6"><a class="link-muted" href="/2021/01/02/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">其他</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-01-01T14:22:00.000Z">2021-01-01</time></p><p class="title is-6"><a class="link-muted" href="/2021/01/01/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">机器学习笔记</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-04-10T16:00:00.000Z">2020-04-11</time></p><p class="title is-6"><a class="link-muted" href="/2020/04/11/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%BB%8ENB%E5%88%B0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">从NB到语言模型</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-04-10T16:00:00.000Z">2020-04-11</time></p><p class="title is-6"><a class="link-muted" href="/2020/04/11/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">朴素贝叶斯</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/">经典模型</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/04/"><span class="level-start"><span class="level-item">四月 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/02/"><span class="level-start"><span class="level-item">二月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"><span class="tag">贝叶斯</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/"><span class="tag">提升算法</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/StyleTransfer/"><span class="tag">StyleTransfer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/YOLO/"><span class="tag">YOLO</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/word2vec/"><span class="tag">word2vec</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"><span class="tag">人脸识别</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"><span class="tag">决策树</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8D%B7%E7%A7%AF/"><span class="tag">卷积</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"><span class="tag">隐马尔科夫模型</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Attention-Model/"><span class="tag">Attention Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"><span class="tag">特征工程</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA/"><span class="tag">知识追踪</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><span class="tag">线性回归</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%81%9A%E7%B1%BB/"><span class="tag">聚类</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SVM/"><span class="tag">SVM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"><span class="tag">边缘检测</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/logo.svg" alt="星空str" height="28"></a><p class="size-small"><span>&copy; 2020 wasim37</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/wasim37"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'http://wangxin123.com',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back-to-top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script src="/js/gallery.js" defer></script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            MathJax.Hub.Config({
                'HTML-CSS': {
                    matchFontHeight: false
                },
                SVG: {
                    matchFontHeight: false
                },
                CommonHTML: {
                    matchFontHeight: false
                },
                tex2jax: {
                    inlineMath: [
                        ['$','$'],
                        ['\\(','\\)']
                    ]
                }
            });
        });</script><!--!--><script src="/js/main.js" defer></script><script src="/js/comment-issue-data.js" defer></script><link rel="stylesheet" href="/css/insight.css"><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>(function (window) {
            var INSIGHT_CONFIG = {
                TRANSLATION: {
                    POSTS: '文章',
                    PAGES: '页面',
                    CATEGORIES: '分类',
                    TAGS: '标签',
                    UNTITLED: '(无标题)',
                },
                CONTENT_URL: '/content.json',
            };
            window.INSIGHT_CONFIG = INSIGHT_CONFIG;
        })(window);</script><script src="/js/insight.js" defer></script></body></html>