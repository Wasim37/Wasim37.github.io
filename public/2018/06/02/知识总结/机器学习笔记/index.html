<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>机器学习笔记 - 星空str</title><meta description="ai,机器学习,深度学习,算法,leetcode,java"><meta property="og:type" content="blog"><meta property="og:title" content="Wasim37"><meta property="og:url" content="https://wangxin123.com/"><meta property="og:site_name" content="Wasim37"><meta property="og:description" content="ai,机器学习,深度学习,算法,leetcode,java"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"><meta property="article:published_time" content="2018-06-02T14:22:00.000Z"><meta property="article:modified_time" content="2020-05-27T08:24:13.885Z"><meta property="article:author" content="Wasim37"><meta property="article:tag" content="机器学习"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://wangxin123.com/"},"headline":"Wasim37","image":["https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"],"datePublished":"2018-06-02T14:22:00.000Z","dateModified":"2020-05-27T08:24:13.885Z","author":{"@type":"Person","name":"Wasim37"},"description":"ai,机器学习,深度学习,算法,leetcode,java"}</script><link rel="alternative" href="/atom.xml" title="星空str" type="application/atom+xml"><link rel="icon" href="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/wu.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="/css/style.css"><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="/js/globalUtils.js"></script></head><body class="is-2-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/logo.svg" alt="星空str" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/wasim37"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2018-06-02T14:22:00.000Z">2018-06-02</time><a class="commentCountImg" href="/2018/06/02/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#comment-container"><span class="display-none-class">441572d694a28adc232a1d6d8c45da2e</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="441572d694a28adc232a1d6d8c45da2e"> 0</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></span><span class="level-item">42 分钟 读完 (大约 6274 个字)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">机器学习笔记</h1><div class="content"><ul>
<li><a href="http://www.ai-start.com/ml2014/html/week8.html#header-n186">PCA_吴恩达</a></li>
<li><a href="http://www.ai-start.com/ml2014/html/week9.html#header-n5">异常值检测_吴恩达</a></li>
<li><a href="http://www.ai-start.com/ml2014/html/week9.html#header-n279">推荐系统_吴恩达</a></li>
<li><a href="https://www.zhihu.com/question/19971859">协同过滤和基于内容推荐有什么区别</a></li>
</ul>
<ul>
<li><a href="https://www.zhihu.com/question/23259302">如何准备机器学习工程师的面试</a></li>
<li><a href="https://www.zhihu.com/question/62482926">如何判断某个人的机器学习水平</a></li>
</ul>
<hr>
<a id="more"></a>
<h1 id="机器学习项目开发流程"><a href="#机器学习项目开发流程" class="headerlink" title="机器学习项目开发流程"></a>机器学习项目开发流程</h1><p><strong>1 抽象成数学问题</strong><br>机器学习的训练过程很耗时，胡乱尝试时间成本太高，所以明确问题是进行机器学习的第一步。<br>抽象成数学问题，指的明确我们可以获得什么样的数据，目标问题属于分类、回归还是聚类，如果都不是，如何转换为其中的某类问题。</p>
<p><strong>2 获取数据</strong><br>数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。<br>数据要有代表性，否则必然会过拟合。而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。<br>对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。</p>
<p><strong>3 特征预处理与特征选择</strong><br>良好的数据要能够提取出良好的特征才能真正发挥效力。<br>特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。</p>
<p>筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。</p>
<p><strong>4 训练模型与调优</strong><br>直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。</p>
<p><strong>5 模型诊断</strong><br>如何确定模型调优的方向与思路呢这就需要对模型进行诊断的技术。<br>过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。</p>
<p>误差分析，也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……<br>诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。</p>
<p><strong>6 模型融合</strong><br>一般来说，模型融合后都能使得效果有一定提升。而且效果很好。<br>工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。</p>
<p><strong>7 上线运行</strong><br>这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。</p>
<hr>
<h1 id="有监督、无监督与半监督学习"><a href="#有监督、无监督与半监督学习" class="headerlink" title="有监督、无监督与半监督学习"></a>有监督、无监督与半监督学习</h1><ul>
<li><p><strong>有监督学习</strong>：<br>对有标记的训练样本进行学习，对训练样本外的数据进行预测。<br>如 LR（Logistic Regression），SVM（Support Vector Machine），RF（RandomForest），GBDT（Gradient Boosting Decision Tree），感知机（Perceptron）、BP神经网络（Back Propagation）等。</p>
</li>
<li><p><strong>无监督学习</strong>：<br>对未标记的样本进行训练学习，试图发现样本中的内在结构。<br>如聚类(KMeans)、降维、文本处理、DL。</p>
</li>
<li><p><strong>半监督学习(Semi-Supervised Learning，SSL)</strong>：<br>主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类，是有监督学习和无监督学习的结合。<br>半监督学习对于减少标注代价，提高学习机器性能具有重大实际意义。</p>
</li>
</ul>
<hr>
<h1 id="判别式模型与生成式模型"><a href="#判别式模型与生成式模型" class="headerlink" title="判别式模型与生成式模型"></a>判别式模型与生成式模型</h1><p><strong>判别式模型(Discriminative Model)</strong>：直接对<strong>条件概率p(y|x)</strong>进行建模，常见判别模型有：<br>线性回归、决策树、支持向量机SVM、k近邻、神经网络、集成学习、条件随机场CRF等；</p>
<p><strong>生成式模型(Generative Model)</strong>：对<strong>联合分布概率p(x,y)</strong>进行建模，常见生成式模型有：<br>隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM及其他混合模型、LDA和限制波兹曼机等；</p>
<p>生成式模型更普适，判别式模型更直接，目标性更强。<br>生成式模型关注数据是如何产生的，寻找的是数据分布模型。<br>判别式模型关注的数据的差异性，寻找的是分类面。<br>由生成式模型可以产生判别是模型，但是由判别式模式没法形成生成式模型</p>
<hr>
<h1 id="对所有优化问题来说，-有没有可能找到比現在已知算法更好的算法"><a href="#对所有优化问题来说，-有没有可能找到比現在已知算法更好的算法" class="headerlink" title="对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法"></a>对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法</h1><p>没有免费的午餐定理：</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/40.png" alt=""></p>
<ul>
<li>对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。</li>
<li>也就是说：对于所有问题，无论学习算法A多聪明，学习算法 B多笨拙，它们的期望性能相同。</li>
<li>但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，<strong>在优化算法时，针对具体问题进行分析，是算法优化的核心所在。</strong></li>
</ul>
<hr>
<h1 id="谈谈判别式模型和生成式模型"><a href="#谈谈判别式模型和生成式模型" class="headerlink" title="谈谈判别式模型和生成式模型"></a>谈谈判别式模型和生成式模型</h1><p>判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。<br>生成方法：由数据学习联合概率密度分布函数 P（X，Y），然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。<br>由生成模型可以得到判别模型，但由判别模型得不到生成模型。<br>常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场<br>常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机</p>
<hr>
<h1 id="线性分类器与非线性分类器的区别以及优劣"><a href="#线性分类器与非线性分类器的区别以及优劣" class="headerlink" title="线性分类器与非线性分类器的区别以及优劣"></a>线性分类器与非线性分类器的区别以及优劣</h1><p>线性和非线性是针对，模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2那么就是非线性模型，如果输入是x和X^2则模型是线性的。<br>线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。<br>非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。<br>常见的线性分类器有：LR，贝叶斯分类，单层感知机、线性回归<br>常见的非线性分类器：决策树、RF、GBDT、多层感知机<br>SVM两种都有（看线性核还是高斯核）</p>
<hr>
<h1 id="请大致对比下plsa和LDA的区别"><a href="#请大致对比下plsa和LDA的区别" class="headerlink" title="请大致对比下plsa和LDA的区别"></a>请大致对比下plsa和LDA的区别</h1><p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/56.png" alt=""><br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/57.png" alt=""><br>更多请参见：《通俗理解LDA主题模型》（链接：<a href="http://blog.csdn.net/v_july_v/article/details/41209515）。">http://blog.csdn.net/v_july_v/article/details/41209515）。</a></p>
<hr>
<h1 id="特征比数据量还大时，选择什么样的分类器"><a href="#特征比数据量还大时，选择什么样的分类器" class="headerlink" title="特征比数据量还大时，选择什么样的分类器"></a>特征比数据量还大时，选择什么样的分类器</h1><p>线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。</p>
<hr>
<h1 id="说说常见的优化算法及其优缺点"><a href="#说说常见的优化算法及其优缺点" class="headerlink" title="说说常见的优化算法及其优缺点"></a>说说常见的优化算法及其优缺点</h1><p>温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。<br>简言之<br>1）随机梯度下降<br>优点：可以一定程度上解决局部最优解的问题<br>缺点：收敛速度较慢<br>2）批量梯度下降<br>优点：容易陷入局部最优解<br>缺点：收敛速度较快<br>3）mini_batch梯度下降<br>综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。<br>4）牛顿法<br>牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算  Hessian矩阵比较困难。<br>5）拟牛顿法<br>拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。</p>
<p>具体而言<br>从每个batch的数据来区分<br>梯度下降：每次使用全部数据集进行训练<br>优点：得到的是最优解<br>缺点：运行速度慢，内存可能不够<br>随机梯度下降：每次使用一个数据进行训练<br>优点：训练速度快，无内存问题<br>缺点：容易震荡，可能达不到最优解<br>Mini-batch梯度下降<br>优点：训练速度快，无内存问题，震荡较少<br>缺点：可能达不到最优解</p>
<p>从优化方法上来分：<br>随机梯度下降（SGD）<br>缺点<br>选择合适的learning    rate比较难<br>对于所有的参数使用同样的learning rate<br>容易收敛到局部最优<br>可能困在saddle point<br>SGD+Momentum<br>优点：<br>积累动量，加速训练<br>局部极值附近震荡时，由于动量，跳出陷阱<br>梯度方向发生变化时，动量缓解动荡。<br>Nesterov Mementum<br>与Mementum类似，优点：<br>避免前进太快<br>提高灵敏度<br>AdaGrad<br>优点：<br>控制学习率，每一个分量有各自不同的学习率<br>适合稀疏数据<br>缺点<br>依赖一个全局学习率<br>学习率设置太大，其影响过于敏感<br>后期，调整学习率的分母积累的太大，导致学习率很低，提前结束训练。<br>RMSProp<br>优点：<br>解决了后期提前结束的问题。<br>缺点：<br>依然依赖全局学习率<br>Adam<br>Adagrad和RMSProp的合体<br>优点：<br>结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点<br>为不同的参数计算不同的自适应学习率<br>也适用于大多非凸优化 -    适用于大数据集和高维空间<br>牛顿法<br>牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难<br>拟牛顿法<br>拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。</p>
<hr>
<h1 id="试证明样本空间任意点x到超平面-w，b-的距离为-6-2"><a href="#试证明样本空间任意点x到超平面-w，b-的距离为-6-2" class="headerlink" title="试证明样本空间任意点x到超平面(w，b)的距离为(6.2)"></a>试证明样本空间任意点x到超平面(w，b)的距离为(6.2)</h1><p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/69.png" alt=""></p>
<hr>
<h1 id="对于维度极低的特征，选择线性还是非线性分类器"><a href="#对于维度极低的特征，选择线性还是非线性分类器" class="headerlink" title="对于维度极低的特征，选择线性还是非线性分类器"></a>对于维度极低的特征，选择线性还是非线性分类器</h1><p>非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。</p>
<ol>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。</li>
</ol>
<hr>
<h1 id="什么是ill-condition病态问题"><a href="#什么是ill-condition病态问题" class="headerlink" title="什么是ill-condition病态问题"></a>什么是ill-condition病态问题</h1><p>训练完的模型，测试样本稍作修改就会得到差别很大的结果，就是病态问题，模型对未知数据的预测能力很差，即泛化误差大。</p>
<hr>
<h1 id="常用的聚类划分方式有哪些列举代表算法"><a href="#常用的聚类划分方式有哪些列举代表算法" class="headerlink" title="常用的聚类划分方式有哪些列举代表算法"></a>常用的聚类划分方式有哪些列举代表算法</h1><ol>
<li>基于划分的聚类:K-means，k-medoids，CLARANS。</li>
<li>基于层次的聚类：AGNES（自底向上），DIANA（自上向下）。</li>
<li>基于密度的聚类：DBSACN，OPTICS，BIRCH(CF-Tree)，CURE。</li>
<li>基于网格的方法：STING，WaveCluster。</li>
<li>基于模型的聚类：EM，SOM，COBWEB。</li>
</ol>
<hr>
<h1 id="LR和SVM的联系与区别"><a href="#LR和SVM的联系与区别" class="headerlink" title="LR和SVM的联系与区别"></a>LR和SVM的联系与区别</h1><p>联系：<br>1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）<br>2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。<br>区别：<br>1、LR是参数模型，SVM是非参数模型。<br>2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。<br>3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。<br>4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算。<br>5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。</p>
<hr>
<h1 id="SVM、LR、决策树的对比"><a href="#SVM、LR、决策树的对比" class="headerlink" title="SVM、LR、决策树的对比"></a>SVM、LR、决策树的对比</h1><ul>
<li>模型复杂度：SVM支持核函数，可处理线性非线性问题; LR模型简单，训练速度快，适合处理线性问题; 决策树容易过拟合，需要进行剪枝.</li>
<li>损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失</li>
<li>数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感</li>
<li>数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核</li>
</ul>
<hr>
<h1 id="EM算法、HMM和CRF区别"><a href="#EM算法、HMM和CRF区别" class="headerlink" title="EM算法、HMM和CRF区别"></a>EM算法、HMM和CRF区别</h1><p>这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。<br>（1）EM算法<br>　　EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。<br>　　注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。<br>（2）HMM算法<br>　　隐马尔可夫模型是用于标注问题的生成模型。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。<br>马尔科夫三个基本问题：<br>概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法<br>学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。<br>预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）<br>（3）条件随机场CRF<br>　　给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。<br>　　之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。<br>（4）HMM和CRF对比<br>　　其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。</p>
<hr>
<h1 id="RF与GBDT之间的区别与联系"><a href="#RF与GBDT之间的区别与联系" class="headerlink" title="RF与GBDT之间的区别与联系"></a>RF与GBDT之间的区别与联系</h1><p>1）相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。<br>2）不同点：<br>a 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成<br>b 组成随机森林的树可以并行生成，而GBDT是串行生成<br>c 随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和<br>d 随机森林对异常值不敏感，而GBDT对异常值比较敏感<br>e 随机森林是减少模型的方差，而GBDT是减少模型的偏差<br>f 随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化</p>
<hr>
<h1 id="（决策树、Random-Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么"><a href="#（决策树、Random-Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么" class="headerlink" title="（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么"></a>（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么</h1><p>集成学习的集成对象是学习器. Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器， 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器， 迭代集成(平滑加权).<br>决策树属于最常用的学习器， 其学习过程是从根建立树， 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂， CART决策树用基尼指数计算最优分裂， xgboost决策树使用二阶泰勒展开系数计算最优分裂.<br>下面所提到的学习器都是决策树:<br>Bagging方法:<br>    学习器间不存在强依赖关系， 学习器可并行训练生成， 集成方式一般为投票;<br>    Random Forest属于Bagging的代表， 放回抽样， 每个学习器随机选择部分特征去优化;<br>Boosting方法:<br>   学习器之间存在强依赖关系、必须串行生成， 集成方式为加权和;<br>    Adaboost属于Boosting， 采用指数损失函数替代原本分类任务的0/1损失函数;<br>    GBDT属于Boosting的优秀代表， 对函数残差近似值进行梯度下降， 用CART回归树做学习器， 集成为回归模型;<br>    xgboost属于Boosting的集大成者， 对函数残差近似值进行梯度下降， 迭代时利用了二阶梯度信息， 集成模型可分类也可回归. 由于它可在特征粒度上并行计算， 结构风险和工程实现都做了很多优化， 泛化， 性能和扩展性都比GBDT要好。<br>关于决策树，这里有篇《决策树算法》（链接：<a href="http://blog.csdn.net/v_july_v/article/details/7577684）。而随机森林Random">http://blog.csdn.net/v_july_v/article/details/7577684）。而随机森林Random</a> Forest是一个包含多个决策树的分类器。至于AdaBoost，则是英文”Adaptive Boosting”（自适应增强）的缩写，关于AdaBoost可以看下这篇文章《Adaboost 算法的原理与推导》。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。</p>
<p>xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：<br>1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数<br>2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性<br>3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的</p>
<hr>
</div><div class="article-tags size-small is-uppercase mb-4"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/ali-zfb.png" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/weixin-pay.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2018/06/08/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">特征选择</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2018/05/19/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/%E5%8D%B7%E7%A7%AF%E5%B1%82%E3%80%81%E6%B1%A0%E5%8C%96%E5%B1%82%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E8%AF%A6%E8%A7%A3/"><span class="level-item">卷积层、池化层和全连接层详解</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.css"><script src="/js/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: '441572d694a28adc232a1d6d8c45da2e',
            repo: 'blog_comment',
            owner: 'Wasim37',
            clientID: '55fcbea044b44e047f89',
            clientSecret: '47addd42a9469d80bd508118ac73033915e3142c',
            admin: ["wasim37"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: 'last',
            
            
            enableHotKey: true,
            isLocked: false
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget toc-scroll" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex" id="toc-item-机器学习项目开发流程" href="#机器学习项目开发流程"><span>机器学习项目开发流程</span></a></li><li><a class="is-flex" id="toc-item-有监督、无监督与半监督学习" href="#有监督、无监督与半监督学习"><span>有监督、无监督与半监督学习</span></a></li><li><a class="is-flex" id="toc-item-判别式模型与生成式模型" href="#判别式模型与生成式模型"><span>判别式模型与生成式模型</span></a></li><li><a class="is-flex" id="toc-item-对所有优化问题来说，-有没有可能找到比現在已知算法更好的算法" href="#对所有优化问题来说，-有没有可能找到比現在已知算法更好的算法"><span>对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法</span></a></li><li><a class="is-flex" id="toc-item-谈谈判别式模型和生成式模型" href="#谈谈判别式模型和生成式模型"><span>谈谈判别式模型和生成式模型</span></a></li><li><a class="is-flex" id="toc-item-线性分类器与非线性分类器的区别以及优劣" href="#线性分类器与非线性分类器的区别以及优劣"><span>线性分类器与非线性分类器的区别以及优劣</span></a></li><li><a class="is-flex" id="toc-item-请大致对比下plsa和LDA的区别" href="#请大致对比下plsa和LDA的区别"><span>请大致对比下plsa和LDA的区别</span></a></li><li><a class="is-flex" id="toc-item-特征比数据量还大时，选择什么样的分类器" href="#特征比数据量还大时，选择什么样的分类器"><span>特征比数据量还大时，选择什么样的分类器</span></a></li><li><a class="is-flex" id="toc-item-说说常见的优化算法及其优缺点" href="#说说常见的优化算法及其优缺点"><span>说说常见的优化算法及其优缺点</span></a></li><li><a class="is-flex" id="toc-item-试证明样本空间任意点x到超平面-w，b-的距离为-6-2" href="#试证明样本空间任意点x到超平面-w，b-的距离为-6-2"><span>试证明样本空间任意点x到超平面(w，b)的距离为(6.2)</span></a></li><li><a class="is-flex" id="toc-item-对于维度极低的特征，选择线性还是非线性分类器" href="#对于维度极低的特征，选择线性还是非线性分类器"><span>对于维度极低的特征，选择线性还是非线性分类器</span></a></li><li><a class="is-flex" id="toc-item-什么是ill-condition病态问题" href="#什么是ill-condition病态问题"><span>什么是ill-condition病态问题</span></a></li><li><a class="is-flex" id="toc-item-常用的聚类划分方式有哪些列举代表算法" href="#常用的聚类划分方式有哪些列举代表算法"><span>常用的聚类划分方式有哪些列举代表算法</span></a></li><li><a class="is-flex" id="toc-item-LR和SVM的联系与区别" href="#LR和SVM的联系与区别"><span>LR和SVM的联系与区别</span></a></li><li><a class="is-flex" id="toc-item-SVM、LR、决策树的对比" href="#SVM、LR、决策树的对比"><span>SVM、LR、决策树的对比</span></a></li><li><a class="is-flex" id="toc-item-EM算法、HMM和CRF区别" href="#EM算法、HMM和CRF区别"><span>EM算法、HMM和CRF区别</span></a></li><li><a class="is-flex" id="toc-item-RF与GBDT之间的区别与联系" href="#RF与GBDT之间的区别与联系"><span>RF与GBDT之间的区别与联系</span></a></li><li><a class="is-flex" id="toc-item-（决策树、Random-Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么" href="#（决策树、Random-Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么"><span>（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么</span></a></li></ul></div></div><script type="text/javascript" async>
        $(document).ready(function () { //参考自 https://github.com/ppoffice/hexo-theme-icarus/pull/616/files
            var observerTopMargin;
            var scrollObserver;
            var headerElems = $(".headerlink");
            var activeTocItem;
        
            function initIntersectionObserver(docHeight) {
                observerTopMargin = docHeight;
                scrollObserver = new IntersectionObserver(scrollCallBack,
                    {
                        root: null,  // viewpoint
                        rootMargin: docHeight + "px 0px -80% 0px"  // cover top 30% of viewport to the top of document
                    })
            }
        
            function scrollCallBack(entries, observer) {
                if ($(window).scrollTop() > observerTopMargin * 0.7) { 
                    // User somehow scroll to 70% of observerTopMargin (which is inited as 200% document height)
                    // Observer top margin need to extend to cover all the space to the top of the document
                    initIntersectionObserver(observerTopMargin * 2)
                    observer.disconnect();
                    return;
                }
                let toActive;
                if (entries[0].intersectionRatio == 1) {  // enter viewed area
                    let entry = entries.reduce((u, v) => (u.target.toc_id > v.target.toc_id ? u : v));  // get the lowest item
                    toActive = $("#toc-item-" + $(entry.target).attr("href").substr(1));
                } else {
                    let entry = entries.reduce((u, v) => (u.target.toc_id < v.target.toc_id ? u : v));  // get the highest item
                    let idx = Math.max(entry.target.toc_id - 1, 0);
                    toActive = $("#toc-item-" + $(headerElems[idx]).attr("href").substr(1));
                }
                if (activeTocItem) activeTocItem.removeClass("is-current");
                activeTocItem = toActive
                activeTocItem.addClass("is-current");
            }
        
            initIntersectionObserver($(document).height() * 2);
            headerElems.each(function (index, obj) {
                obj.toc_id = index;
                scrollObserver.observe(obj);
            })
        });</script></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2019-05-21T14:22:00.000Z">2019-05-21</time></p><p class="title is-6"><a class="link-muted" href="/2019/05/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94/">知识追踪模型调研</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2019-05-01T14:22:00.000Z">2019-05-01</time></p><p class="title is-6"><a class="link-muted" href="/2019/05/01/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5/%E8%AF%95%E9%A2%98%E7%9F%A5%E8%AF%86%E7%82%B9%E6%99%BA%E8%83%BD%E6%A0%87%E6%B3%A8/">试题知识点智能标注</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5/">项目实践</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2019-04-10T16:00:00.000Z">2019-04-11</time></p><p class="title is-6"><a class="link-muted" href="/2019/04/11/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%BB%8ENB%E5%88%B0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">从NB到语言模型</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2019-04-05T16:00:00.000Z">2019-04-06</time></p><p class="title is-6"><a class="link-muted" href="/2019/04/06/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6/">贝叶斯与垃圾邮件</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2019-04-03T14:22:00.000Z">2019-04-03</time></p><p class="title is-6"><a class="link-muted" href="/2019/04/03/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5/">自然语言处理与词嵌入</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"><span class="level-start"><span class="level-item">特征工程</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/"><span class="level-start"><span class="level-item">理论基础</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"><span class="level-start"><span class="level-item">知识总结</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="level-start"><span class="level-item">神经网络</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">经典模型</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"><span class="level-start"><span class="level-item">计算机视觉</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="level-start"><span class="level-item">论文阅读</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5/"><span class="level-start"><span class="level-item">项目实践</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/06/"><span class="level-start"><span class="level-item">六月 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/logo.svg" alt="星空str" height="28"></a><p class="size-small"><span>&copy; 2020 wasim37</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/wasim37"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'http://wangxin123.com',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back-to-top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script src="/js/gallery.js" defer></script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            MathJax.Hub.Config({
                'HTML-CSS': {
                    matchFontHeight: false
                },
                SVG: {
                    matchFontHeight: false
                },
                CommonHTML: {
                    matchFontHeight: false
                },
                tex2jax: {
                    inlineMath: [
                        ['$','$'],
                        ['\\(','\\)']
                    ]
                }
            });
        });</script><!--!--><script src="/js/main.js" defer></script><script src="/js/comment-issue-data.js" defer></script><link rel="stylesheet" href="/css/insight.css"><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>(function (window) {
            var INSIGHT_CONFIG = {
                TRANSLATION: {
                    POSTS: '文章',
                    PAGES: '页面',
                    CATEGORIES: '分类',
                    TAGS: '标签',
                    UNTITLED: '(无标题)',
                },
                CONTENT_URL: '/content.json',
            };
            window.INSIGHT_CONFIG = INSIGHT_CONFIG;
        })(window);</script><script src="/js/insight.js" defer></script></body></html>