<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>从NB到语言模型 - 星空str</title><meta description="ai,机器学习,深度学习,算法,leetcode,java"><meta property="og:type" content="blog"><meta property="og:title" content="Wasim37"><meta property="og:url" content="https://wangxin123.com/"><meta property="og:site_name" content="Wasim37"><meta property="og:description" content="ai,机器学习,深度学习,算法,leetcode,java"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"><meta property="article:published_time" content="2020-04-10T16:00:00.000Z"><meta property="article:modified_time" content="2020-05-26T07:46:23.465Z"><meta property="article:author" content="Wasim37"><meta property="article:tag" content="贝叶斯"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://wangxin123.com/"},"headline":"Wasim37","image":["https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"],"datePublished":"2020-04-10T16:00:00.000Z","dateModified":"2020-05-26T07:46:23.465Z","author":{"@type":"Person","name":"Wasim37"},"description":"ai,机器学习,深度学习,算法,leetcode,java"}</script><link rel="alternative" href="/atom.xml" title="星空str" type="application/atom+xml"><link rel="icon" href="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/wu.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?060269356a8ca9046e6a13dcba6f9559";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="/css/style.css"><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="/js/globalUtils.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body class="is-2-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/logo.svg" alt="星空str" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/wasim37"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-04-10T16:00:00.000Z">2020-04-11</time><a class="commentCountImg" href="/2020/04/11/%E4%BB%8ENB%E5%88%B0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/#comment-container"><span class="display-none-class">9d954c459b332655cb9c9baf7f78cb0e</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="9d954c459b332655cb9c9baf7f78cb0e"> 0</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/NLP/">NLP</a></span><span class="level-item">42 分钟 读完 (大约 6354 个字)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">从NB到语言模型</h1><div class="content"><p><a name="n4jr6"></a></p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 id="1-引言：朴素贝叶斯的局限性"><a href="#1-引言：朴素贝叶斯的局限性" class="headerlink" title="1. 引言：朴素贝叶斯的局限性"></a>1. 引言：朴素贝叶斯的局限性</h2><p>我们知道朴素贝叶斯的局限性来源于其条件独立假设，它将文本看成是词袋子模型，不考虑词语之间的顺序信息，就会把“武松打死了老虎”与“老虎打死了武松”认作是一个意思。那么有没有一种方法提高其对词语顺序的识别能力呢？有，就是这里要提到的N-gram语言模型。<br><a name="jTi2S"></a></p>
<h2 id="2-N-gram语言模型是啥？"><a href="#2-N-gram语言模型是啥？" class="headerlink" title="2. N-gram语言模型是啥？"></a>2. N-gram语言模型是啥？</h2><p><a name="odXF2"></a></p>
<h3 id="2-1从假设性独立到联合概率链规则"><a href="#2-1从假设性独立到联合概率链规则" class="headerlink" title="2.1从假设性独立到联合概率链规则"></a>2.1从假设性独立到联合概率链规则</h3><p>照抄我们垃圾邮件识别中的条件独立假设，长这个样子：</p>
<blockquote>
<script type="math/tex; mode=display">P(（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)|S)</script><script type="math/tex; mode=display">
=P(“我”|S)×P(“司”|S)×P(“可”|S)×P(“办理”|S)×P(“正规发票”|S)</script><script type="math/tex; mode=display">×P(“保真”|S)×P(“增值税”|S)×P(“发票”|S)×P(“点数”|S)×P(“优惠”|S)</script></blockquote>
<p>为了简化起见，我们以字母<script type="math/tex">x_i</script>表示每一个词语，并且先不考虑条件“S”。于是上式就变成了下面的独立性公式。</p>
<blockquote>
<script type="math/tex; mode=display">P(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10})</script><script type="math/tex; mode=display">=P(x_1)P(x_2)P(x_3)P(x_4)P(x_5)P(x_6)P(x_7)P(x_8)P(x_9)P(x_{10})</script><script type="math/tex; mode=display">=P(“我”)P(“司”)P(“可”)P(“办理”)...P(“优惠”)</script></blockquote>
<a id="more"></a>
<p>上面的公式要求满足独立性假设，如果去掉独立性假设，我们应该有下面这个<strong>恒等式，即联合概率链规则(chain rule)</strong> ：</p>
<blockquote>
<script type="math/tex; mode=display">P(x_1,x_2,x_3,x_4,x_5,…,x_n)</script><script type="math/tex; mode=display">=P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_n|x_1,x_2,...,x_{n-1})</script></blockquote>
<p><a name="t8z8Z"></a></p>
<h3 id="2-2-从联合概率链规则到n-gram语言模型"><a href="#2-2-从联合概率链规则到n-gram语言模型" class="headerlink" title="2.2 从联合概率链规则到n-gram语言模型"></a>2.2 从联合概率链规则到n-gram语言模型</h3><p>上面的联合概率链规则公式考虑到词和词之间的依赖关系，但是比较复杂，在实际生活中几乎没办法使用，于是我们就想了很多办法去近似这个公式，比如我们要讲到的语言模型n-gram就是它的一个简化。<br />如果我们考虑一个词语对上一个词语的依赖关系，公式就简化了如下形式，我们把它叫做二元语法（bigram，2-gram）:</p>
<blockquote>
<script type="math/tex; mode=display">P(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10})</script><script type="math/tex; mode=display">=P(x_1)P(x_2|x_1)P(x_3|x_2)P(x_4|x_3)..P(x_{10}|x_9)</script><script type="math/tex; mode=display">=P(“我”)P(“司”|“我”)P(“可”|“司”)P(“办理”|“可”)...P(“优惠”|“点数”)</script></blockquote>
<p>如果把依赖词长度再拉长一点，考虑一个词对前两个词的依赖关系，就叫做三元语法（trigram，3-gram），公式如下:</p>
<blockquote>
<script type="math/tex; mode=display">P(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10})</script><script type="math/tex; mode=display">=P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)P(x_4|x_2,x_3)×...×P(x_{10}|x_8,x_9)</script><script type="math/tex; mode=display">=P(“我”)P(“司”|“我”)P(“可”|“我”,“司”)P(“办理”|“司”,“可”)...P(“优惠”|“发票”,“点数”)</script></blockquote>
<p>如果我们再考虑长一点，考虑n个词语之间的关系，恩恩，这就是n-gram的由来。歪果仁果然取名字简单粗暴又好记…<br />其实以上几个简化后的公式，就是著名的<strong>马尔科夫假设（Markov Assumption）：下一个词的出现仅依赖于它前面的一个或几个词</strong>。这相对于联合概率链规则，其实是一个有点粗糙的简化，不过很好地体现了就近思路，离得较远和关系比较弱的词语就被简化和省略了。实际应用中，这些简化后的n-gram语法比独立性假设还是强很多的。<br><a name="GgXpT"></a></p>
<h3 id="2-3-怎样选择依赖词的个数”n”？"><a href="#2-3-怎样选择依赖词的个数”n”？" class="headerlink" title="2.3 怎样选择依赖词的个数”n”？"></a>2.3 怎样选择依赖词的个数”n”？</h3><p>选择依赖词的个数”n”主要与计算条件概率有关。<strong>理论上，只要有足够大的语料，n越大越好，毕竟这样考虑的信息更多嘛</strong>。条件概率很好算，统计一下各个元组出现的次数就可以，比如:</p>
<blockquote>
<script type="math/tex; mode=display">P(“优惠”|“发票”,“点数”)=\frac{(“发票”,“点数”，“优惠”)出现的次数}{(“发票”,“点数”)出现的次数}</script></blockquote>
<p>但我们实际情况往往是<strong>训练语料很有限，很容易产生数据稀疏</strong>，不满足大数定律，算出来的概率失真。比如(“发票”,“点数”，“优惠”)在训练集中竟没有出现，就会导致零概率问题。<br />又比如在英文语料库IBM, Brown中，三四百兆的语料，其测试语料14.7%的trigram和2.2%的bigram在训练语料中竟未出现！<br />另一方面，如果n很大，<strong>参数空间过大，也无法实用</strong>。假设词表的大小为<script type="math/tex">100,000</script>，那么n-gram模型的参数数量为<script type="math/tex">100,000^n</script>。这么多的参数，估计内存就不够放了。<br />那么，如何选择依赖词的个数n呢？从前人的经验来看：</p>
<ul>
<li>经验上，trigram用的最多。尽管如此，原则上，能用bigram解决，绝不使用trigram。n取≥4的情况较少。</li>
<li>更大的n：对下一个词出现的约束信息更多，具有更大的<strong>辨别力</strong>；</li>
<li>更小的n：在训练语料库中出现的次数更多，具有更可靠的统计信息，具有更高的<strong>可靠性、实用性</strong>。<br><a name="sfWlp"></a><h2 id="3-N-gram实际应用举例"><a href="#3-N-gram实际应用举例" class="headerlink" title="3. N-gram实际应用举例"></a>3. N-gram实际应用举例</h2>说了这么N-gram语言模型的背景知识，咱们再来看看N-gram语言模型在自然语言处理中有哪些常见应用。 PS：此部分以原理介绍为多，具体的技术实现细节请参考文中链接或者google。<br><a name="J7Nm8"></a><h3 id="3-1-词性标注"><a href="#3-1-词性标注" class="headerlink" title="3.1 词性标注"></a>3.1 词性标注</h3><strong>词性标注是一个典型的多分类问题</strong>。常见的词性包括名词、动词、形容词、副词等。而一个词可能属于多种词性。如“爱”，可能是动词，可能是形容词，也可能是名词。但是一般来说，“爱”作为动词还是比较常见的。所以统一给“爱”分配为动词准确率也还足够高。这种最简单粗暴的思想非常好实现，如果准确率要求不高则也比较常用。<strong>它只需要基于词性标注语料库做一个统计就够了，连贝叶斯方法、最大似然法都不要用</strong>。词性标注语料库一般是由专业人员搜集好了的，长下面这个样子。其中斜线后面的字母表示一种词性，词性越多说明语料库分得越细：<br /><br><br />需要比较以下各概率的大小，选择概率最大的词性即可：<blockquote>
</blockquote>
</li>
</ul>
<blockquote>
<script type="math/tex; mode=display">P(词性_i|“爱")=\frac{“爱"作为“词性_i”的次数}{“爱"出现的次数}  ； i=1,2,3...</script></blockquote>
<p><strong>但这种方法没有考虑上下文的信息。而一般来说，形容词后面接名词居多，而不接动词，副词后面才接动词，而不接名词。</strong> 考虑到词性会受前面一两个词的词性的影响，<strong>可以引入2-gram模型提升匹配的精确度。</strong> 我们匹配以下这句话（已被空格分好词）中“爱”的词性：</p>
<blockquote>
<script type="math/tex; mode=display">"闷骚的 李雷 很 爱 韩梅梅"</script></blockquote>
<p>将公式进行以下改造，比较各概率的大小，选择概率最大的词性：</p>
<blockquote>
<script type="math/tex; mode=display">P(词性_i|“很”的词性（副词），“爱")=\frac{前面被“副词”修饰的“爱"作为“词性_i”的次数}{前面被“副词”修饰的“爱"出现的次数}  ； i=1,2,3...</script></blockquote>
<p>计算这个概率需要对语料库进行统计。但前提是你得先判断好“很”的词性，因为采用2-gram模型，进而就需要提前判断“李雷”的词性，需要判断“闷骚的”词性。但是“闷骚的”作为第一个词语，已经找不比它更靠前的词语了。这时就可以考虑用之前最简单粗暴的方法判断“闷骚的”的词性，统一判断为形容词即可。<br />PS:词性标注是自然语言处理中的一项基础性工作，有其细节实现远比我们介绍地更加丰富。感兴趣的同学可以看看这篇文章<a href="https://superangevil.wordpress.com/2009/10/20/nltk5/">《NLTK读书笔记 — 分类与标注》</a><br><a name="5sKbr"></a></p>
<h3 id="3-2-垃圾邮件识别"><a href="#3-2-垃圾邮件识别" class="headerlink" title="3.2 垃圾邮件识别"></a>3.2 垃圾邮件识别</h3><p>是的，亲，你！没！看！错！可以用N-gram进行垃圾邮件识别，而且是朴素贝叶斯方法的进化版。<strong>下面我们用直观的例子探讨一下其在分类问题上是怎么发挥作用的</strong>。一个可行的思路如下：</p>
<ul>
<li>先对邮件文本进行断句，以句尾标点符号（“。” “!” “？”等）为分隔符将邮件内容拆分成不同的句子。</li>
<li>用N-gram分类器(马上提到)判断每个句子是否为垃圾邮件中的敏感句子。</li>
<li>当被判断为敏感句子的数量超过一定数量（比如3个）的时候，认为整个邮件就是垃圾邮件。</li>
</ul>
<p>咳咳，有同学问N-gram分类器是什么鬼，这个分类器靠谱么。N-gram分类器是结合贝叶斯方法和语言模型的分类器。这里用<script type="math/tex">Y_1,Y_2</script>分别表示这垃圾邮件和正常邮件，用<script type="math/tex">X</script>表示被判断的邮件的句子。根据贝叶斯公式有：</p>
<blockquote>
<script type="math/tex; mode=display">P(Y_i|X)\propto P(X|Y_i)P(Y_i)  ； i=1,2</script></blockquote>
<p>比较i=1和2时两个概率值的大小即可得到<script type="math/tex">X</script>所属的分类。对于句子（“我”,“司”,“可”,“办理”,“正规发票”,“保真”,“增值税”,“发票”,“点数”,“优惠”)用字母<script type="math/tex">X</script>代表，每一个词语用字母<script type="math/tex">x_i</script>表示。<script type="math/tex">x</script>就可以写成一个<script type="math/tex">x_i</script>组成的向量，<script type="math/tex">x_i</script>就是这向量中某个维度的特征。<strong>对<script type="math/tex">P(X|Y_i)</script>套用2-gram模型。</strong> 则上式化简为：</p>
<blockquote>
<script type="math/tex; mode=display">P(Y_i|X)\propto P(X|Y_i)P(Y_i)  ； i=1,2</script><script type="math/tex; mode=display">\propto P(x_1|Y_i)P(x_2|x_1，Y_i)P(x_3|x_2，Y_i)...P(x_{10}|x_9，Y_i)P(Y_i)</script><script type="math/tex; mode=display">\propto P(“我”|Y_i)P(“司”|“我”，Y_i)P(“可”|“司”，Y_i)...P(“优惠”|“点数”，Y_i)P(Y_i)</script></blockquote>
<p>公式中的条件概率也比较好求，举个例子：</p>
<blockquote>
<script type="math/tex; mode=display">P(“优惠”|“点数”，Y_i)=\frac{在类别Y_i中，(“点数”，“优惠”)出现的次数}{在类别Y_i中，“点数”出现的次数}</script></blockquote>
<p>剩下的就需要在语料库中间做一个的统计就是了。<strong>因为这种方法考虑到了词语前面的一个词语的信息，同时也考虑到了部分语序信息，因此区分效果会比单纯用朴素贝叶斯方法更好。</strong><br />多提几句，N-gram方法在实际应用中有一些tricks需要注意：</p>
<ul>
<li>3-gram方法的公式与上面类似。此处省略。从区分度来看，3-gram方法更好些。</li>
<li>句子开头的词，比如本例中的“我”，<strong>因为要考虑其本身作为开头的特征，可以考虑在其前面再添加一个句子起始符号如”《S》”，这样我们就不必单独计算<script type="math/tex">P("我"|Y_i)</script>，而是替换为计算<script type="math/tex">P("我"|"《S》",Y_i)</script>。形式上与2-gram统一。</strong> 这样统计和预测起来都比较方便。</li>
<li>一般地，<strong>如果采用N-gram模型，可以在文本开头加入n-1个虚拟的开始符号</strong>，这样在所有情况下预测下一个词的可依赖词数都是一致的。</li>
<li>与朴素贝叶斯方法一样，<strong>N-gram模型也会发生零概率问题，也需要平滑技术。</strong> ，别着急，下面马上讨论到。<br><a name="4Zmc3"></a><h3 id="3-3-中文分词"><a href="#3-3-中文分词" class="headerlink" title="3.3 中文分词"></a>3.3 中文分词</h3>之前说过，中文分词技术是“中文NLP中，最最最重要的技术之一”，重要到某搜索引擎厂有专门的team在集中精力优化这一项工作，重要到能影响双语翻译10%的准确度，能影响某些query下搜索引擎几分之一的广告收入。不过简单的分词实现方式中，包含的原理其实也非常易懂。<br />说起来，<strong>中文分词也可以理解成一个多分类的问题。</strong> 这里用<script type="math/tex">X</script>表示被分词的句子“我司可办理正规发票”， <strong>用<script type="math/tex">Y_i</script>表示该句子的一个分词方案。</strong>，咱们继续套用贝叶斯公式：<blockquote>
<script type="math/tex; mode=display">P(Y_i|X)\propto P(X|Y_i)P(Y_i)  ； i=1,2,3...</script></blockquote>
</li>
</ul>
<p>比较这些概率的大小，找出使得<script type="math/tex">P(Y_i|X)</script>最大的<script type="math/tex">Y_i</script>即可得到<script type="math/tex">X</script> 所属的分类(分词方案)了。<br /><script type="math/tex">Y_i</script>作为分词方案，其实就是个词串，比如（“我司”，“可”，“办理”，“正规发票”）或者（“我”，“司可办”，“理正规”，“发票”），也<strong>就是一个向量</strong>了。<br />而上面贝叶斯公式中<script type="math/tex">P(X|Y_i)</script>项的意思就是在分类方案<script type="math/tex">Y_i</script>的前提下，其对应句子为XX 的概率。而无论分词方案是（“我司”，“可”，“办理”，“正规发票”）还是（“我”，“司可办”，“理正规”，“发票”），或者其他什么方案，其对应的句子都是“我司可办理正规发票”。也就是说<strong>任意假想的一种分词方式之下生成的句子总是唯一的</strong>（只需把分词之间的分界符号扔掉剩下的内容都一样）。于是<strong>可以将<script type="math/tex">P(X|Y_i)</script>看作是恒等于1</strong>的。这样贝叶斯公式又进一步化简成为：</p>
<blockquote>
<script type="math/tex; mode=display">P(Y_i|X)\propto P(Y_i)  ； i=1,2,3...</script></blockquote>
<p>也就是说我们只要取最大化的<script type="math/tex">P(Y_i)</script>就成了。而<script type="math/tex">Y_i</script>就是一个词串，也就是一个向量，可以直接套用我们上面的N-gram语言模型。这里采用2-gram。于是有：</p>
<blockquote>
<script type="math/tex; mode=display">P(Y_1)=P(“我司”，“可”，“办理”，“正规发票”)</script><script type="math/tex; mode=display">=P(“我司”)P(“可”|“我司”)P(“办理”|“可”)P(“正规发票”|“办理”）</script></blockquote>
<p>第二种分词方案的概率为：</p>
<blockquote>
<script type="math/tex; mode=display">P(Y_2)=P(“我”，“司可办”，“理正规”，“发票”)</script><script type="math/tex; mode=display">=P(“我”)P(“司可办”|“我”)P(“理正规”|“司可办”)P(“发票”|“理正规”）</script></blockquote>
<p>由于在语料库中“司可办”与“理正规”一起连续出现的概率为0，于是<script type="math/tex">P(Y_2)=0</script> ,<script type="math/tex">P(Y_1)</script> 的概率更高，优先选择<script type="math/tex">Y_1</script>的分词方案。<br><a name="aBhGV"></a></p>
<h3 id="3-4机器翻译与语音识别"><a href="#3-4机器翻译与语音识别" class="headerlink" title="3.4机器翻译与语音识别"></a>3.4机器翻译与语音识别</h3><p>除了上述说到的应用，N-gram语言模型在机器翻译和语音识别等顶级NLP应用中也有很大的用途。 当然，机器翻译和语音识别是非常复杂的过程，N-gram语言模型只是其中的一部分，但是缺少它整个过程却进行不下去。对于这两个应用我们不打算罗列大量的公式，而只是举些例子，让大家了解一下语言模型是怎么发挥作用的。 对于机器翻译而言，比如中译英，我们对于同一句话『李雷出现在电视上』，得到的三个译文：</p>
<ul>
<li>LiLei appeared in TV</li>
<li>In LiLei appeared TV</li>
<li>LiLei appeared on TV</li>
</ul>
<p>其对应短语的翻译概率是一致的，从短语翻译的角度我们无法评定哪句才是正确的翻译结果。这时候，如果我们再使用语言模型(比如机器翻译里面最常见的是3-gram)，我们计算会得到最后一句话<script type="math/tex">P(“LiLei”|“《S》”,“《S》”)P(“appeared”|“LiLei”,“《S》”)P(“on”|“LiLei”,“appeared”)P(“TV”|“appeared”,“on”)</script>的概率高于第一句<script type="math/tex">P(“LiLei”|“《S》”,“《S》”)P(“appeared”|“LiLei”,“《S》”)P(“in”|“LiLei”,“appeared”)P(“TV”|“appeared”,“in”)</script>和第二句<script type="math/tex">P(“in”|“《S》”,“《S》”)P(“LiLei”|“in”,“《S》”)P(“appeared”|“LiLei”,“in”)P(“TV”|“appeared”,“appeared”)</script>，因此我们选择第三句作为正确的答案。这也表明大量语料上的语言模型能够在一定程度上，体现出我们表达某种语言时候的说话习惯。<br />对应到语音识别问题中，我们也会遇到相同的问题，对于以下的2个句子：</p>
<ul>
<li>I went to a party</li>
<li>Eye went two a bar tea</li>
</ul>
<p>或者对应下述2个句子：</p>
<ul>
<li>你现在在干什么？</li>
<li>你西安载感什么？</li>
</ul>
<p>其对应的发音是完全一致的，这时如果我们借助于语言模型，我们会发现<script type="math/tex">P(“I”|“《S》”,“《S》”)P(“went”|“I”,“《S》”)P(“to”|“I”,“went”)P(“a”|“went”,“to”)P(“party”|“to”,“a”)</script>的概率大于<script type="math/tex">P(“Eye”|“《S》”,“《S》”)P(“went”|“Eye”,“《S》”)P(“two”|“Eye”,“went”)P(“a”|“went”,“two”)P(“bar”|“two”,“a”)P(“tea”|“a”,“bar”)</script>，而<script type="math/tex">P(“你”|“《S》”,“《S》”)P(“现在”|“你”,“《S》”)P(“在”|“你”,“现在”)P(“干什么”|“在”,“现在”)</script>概率远大于 <script type="math/tex">P(“你”|“《S》”,“《S》”)P(“西安”|“你”,“《S》”)P(“载”|“西安”,“你”)P(“感”|“西安”,“载”)P(“什么”|“感”,“载”)</script>，因此我们会选择<strong>I went to a party</strong> 和 <strong>你现在在干什么</strong>作为正确的语音识别结果。<br />上面只是简单的举例，但是大家应该看出来了，在机器翻译和语音识别中，N-gram语言模型有着至关重要的地位。同样在现在最顶级的计算机视觉任务『图片内容表述』中，语言模型也发挥着至关重要的作用。语言模型的重要性可见一斑。<br><a name="8Iv0D"></a></p>
<h2 id="4-平滑技术"><a href="#4-平滑技术" class="headerlink" title="4. 平滑技术"></a>4. 平滑技术</h2><p>现在我们可以比较专门探讨平滑技术了。为了解决零概率问题呢，我们需要给 <strong>“未出现的n-gram条件概率分布一个非零估计值，相应得需要降低已出现n-gram条件概率分布，且经数据平滑后一定保证概率和为1”</strong>。这就是平滑技术的基本思想。<br><a name="ng5Nw"></a></p>
<h3 id="4-1-拉普拉斯平滑"><a href="#4-1-拉普拉斯平滑" class="headerlink" title="4.1 拉普拉斯平滑"></a>4.1 拉普拉斯平滑</h3><p>这是最古老的一种平滑方法，又称加一平滑法，其<strong>保证每个n-gram在训练语料中至少出现1次</strong>。以计算概率<script type="math/tex">P(“优惠”|“发票”,“点数”)</script>为例，公式如下：</p>
<blockquote>
<script type="math/tex; mode=display">P(“优惠”|“发票”,“点数”)=\frac{(“发票”,“点数”，“优惠”)出现的次数+1}{(“发票”,“点数”)出现的次数+所有不重复的三元组的个数}</script></blockquote>
<p>在所有不重复的三元组的个数远大于(“发票”,“点数”)出现的次数时，即训练语料库中绝大部分n-gram都是未出现的情况（一般都是如此），<strong>拉普拉斯平滑有“喧宾夺主”的现象，效果不佳。</strong><br><a name="k498G"></a></p>
<h3 id="4-2-古德图灵-Good-Turing-平滑"><a href="#4-2-古德图灵-Good-Turing-平滑" class="headerlink" title="4.2 古德图灵(Good Turing)平滑"></a>4.2 古德图灵(Good Turing)平滑</h3><p>通过对语料库的统计，我们能够知道出现r次 （r&gt;0） 的n元组的个数为<script type="math/tex">N_r</script>。可以令从未出现的n元组的个数为<script type="math/tex">N_0</script>。古德图灵平滑的思想是:</p>
<ul>
<li><strong>出现0次的n元组也不能认为其是0次，应该给它一个比较小的估计值</strong>，比如为<script type="math/tex">d_0</script>次。</li>
<li>为了保证总共的（出现和未出现的）n元组的次数不变，<strong>其他所有已出现的n元组的次数r应该打一个折扣</strong>，比如为<script type="math/tex">d_r</script>次。</li>
<li>然后再<strong>用新的<script type="math/tex">d_r</script>去计算各个条件概率。</strong></li>
</ul>
<p>所以问题的关键是计算drdr。<strong>为了保证平滑前后n元组的总共出现次数不变</strong>，有：</p>
<blockquote>
<script type="math/tex; mode=display">\sum_{r=0}^\infty d_r×N_r=\sum_{r=0}^\infty (r+1)×N_{r+1}</script></blockquote>
<p>所以干脆令：</p>
<blockquote>
<script type="math/tex; mode=display">d_r×N_r=(r+1)×N_{r+1}； r=0,1,2...</script></blockquote>
<p>这样就可以求出<script type="math/tex">d_r</script>了。但是，当<script type="math/tex">N_r>N_{r+1}</script> 时，使得模型质量变差<br />直接的改进策略就是 <strong>“对出现次数超过某个阈值的n元组，不进行平滑，阈值一般取8~10”</strong>，其他方法请参见<a href="http://faculty.cs.byu.edu/~ringger/CS479/papers/Gale-SimpleGoodTuring.pdf">“Simple Good-Turing”</a>。<br><a name="o9Fx1"></a></p>
<h3 id="4-3-组合估计平滑"><a href="#4-3-组合估计平滑" class="headerlink" title="4.3 组合估计平滑"></a>4.3 组合估计平滑</h3><p><strong>不管是拉普拉斯平滑，还是古德图灵平滑技术，对于未出现的n元组都一视同仁，而这难免存在不合理。</strong> 因为哪怕是未发生的事件，相互之间真实的概率也会存在差别。<br />另一方面，一个n元组可能未出现，但是其(n-1)元组或者(n-2)元组是出现过的，这些信息如果不利用就直接浪费掉了。<strong>在没有足够的数据对高元n-gram模型进行概率估计时，低元n-gram模型通常可以提供有用的信息。</strong> 因此可以利用利用低元n-gram模型的信息对高元n-gram模型进行估计：</p>
<ul>
<li>如果低元n-gram模型的概率本来就很低，那么就给高元n-gram模型一个较低的估计值；</li>
<li>如果低元n-gram模型有一个中等的概率，那么就给高元n-gram模型一个较高的估计值。</li>
</ul>
<p>常用的组合估计算法有<strong>线性差值法</strong>和<strong>Katz回退法</strong>。具体公式比较复杂，这里就不列了。感兴趣的同学可参考 Christopher D. Manning 的<a href="http://book.douban.com/subject/1224802/">《统计自然语言处理基础》</a><br><a name="RwPTi"></a></p>
<h2 id="5-从N-gram谈回贝叶斯方法"><a href="#5-从N-gram谈回贝叶斯方法" class="headerlink" title="5. 从N-gram谈回贝叶斯方法"></a>5. 从N-gram谈回贝叶斯方法</h2><p>聊了这么多N-gram语言模型，我们再回到贝叶斯方法，从实际应用中看看他们的关联。 最原始的用贝叶斯方法进行分类的公式其实非常简单：</p>
<blockquote>
<script type="math/tex; mode=display">P(Y_i|X)\propto P(X|Y_i)P(Y_i)  ； i=1,2,3...</script></blockquote>
<p>具体到不同应用中，它就可以演化出多种玩法：</p>
<ul>
<li>对于<strong>拼写纠错（非词错误）</strong> ，<script type="math/tex">X</script>是错误的词语，<script type="math/tex">Y_i</script>是候选的改正词语，二者都是标量。</li>
<li>对于<strong>垃圾邮件识别</strong>，<script type="math/tex">X</script>是邮件中的句子，<script type="math/tex">Y_i</script>是备选的邮件类别。<script type="math/tex">X</script>可以处理成向量，<script type="math/tex">Y_i</script>还是标量。<ul>
<li>如果对向量<script type="math/tex">X</script>采用条件独立假设，就是朴素贝叶斯方法。</li>
<li>如果对向量<script type="math/tex">X</script>采用马尔科夫假设，就是N-gram语言模型。</li>
</ul>
</li>
<li>对于<strong>中文分词</strong>，XX是被分词的句子，<script type="math/tex">Y_i</script>是备选的分词方案（词串）。这里把XX看成是一个整体，所以可以理解成标量。而<script type="math/tex">Y_i</script>则是向量。这里对向量YiYi采用马尔科夫假设，也是N-gram语言模型。</li>
</ul>
<p>那么有没有一种模型处理的<script type="math/tex">X</script>和<script type="math/tex">Y_i</script>都是向量呢？有的，这就是传说中的隐马尔科夫模型(HMM)。以后的课会讲到。</p>
</div><div class="article-tags size-small is-uppercase mb-4"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/">贝叶斯</a></div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/ali-zfb.png" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/weixin-pay.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/01/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">机器学习笔记</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/04/11/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"><span class="level-item">朴素贝叶斯</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.css"><script src="/js/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: '9d954c459b332655cb9c9baf7f78cb0e',
            repo: 'blog_comment',
            owner: 'Wasim37',
            clientID: '55fcbea044b44e047f89',
            clientSecret: '47addd42a9469d80bd508118ac73033915e3142c',
            admin: ["wasim37"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: 'last',
            
            
            enableHotKey: true,
            isLocked: false
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget toc-scroll" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex" id="toc-item-1-引言：朴素贝叶斯的局限性" href="#1-引言：朴素贝叶斯的局限性"><span>1. 引言：朴素贝叶斯的局限性</span></a></li><li><a class="is-flex" id="toc-item-2-N-gram语言模型是啥？" href="#2-N-gram语言模型是啥？"><span>2. N-gram语言模型是啥？</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-2-1从假设性独立到联合概率链规则" href="#2-1从假设性独立到联合概率链规则"><span>2.1从假设性独立到联合概率链规则</span></a></li><li><a class="is-flex" id="toc-item-2-2-从联合概率链规则到n-gram语言模型" href="#2-2-从联合概率链规则到n-gram语言模型"><span>2.2 从联合概率链规则到n-gram语言模型</span></a></li><li><a class="is-flex" id="toc-item-2-3-怎样选择依赖词的个数”n”？" href="#2-3-怎样选择依赖词的个数”n”？"><span>2.3 怎样选择依赖词的个数”n”？</span></a></li></ul></li><li><a class="is-flex" id="toc-item-3-N-gram实际应用举例" href="#3-N-gram实际应用举例"><span>3. N-gram实际应用举例</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-3-1-词性标注" href="#3-1-词性标注"><span>3.1 词性标注</span></a></li><li><a class="is-flex" id="toc-item-3-2-垃圾邮件识别" href="#3-2-垃圾邮件识别"><span>3.2 垃圾邮件识别</span></a></li><li><a class="is-flex" id="toc-item-3-3-中文分词" href="#3-3-中文分词"><span>3.3 中文分词</span></a></li><li><a class="is-flex" id="toc-item-3-4机器翻译与语音识别" href="#3-4机器翻译与语音识别"><span>3.4机器翻译与语音识别</span></a></li></ul></li><li><a class="is-flex" id="toc-item-4-平滑技术" href="#4-平滑技术"><span>4. 平滑技术</span></a><ul class="menu-list"><li><a class="is-flex" id="toc-item-4-1-拉普拉斯平滑" href="#4-1-拉普拉斯平滑"><span>4.1 拉普拉斯平滑</span></a></li><li><a class="is-flex" id="toc-item-4-2-古德图灵-Good-Turing-平滑" href="#4-2-古德图灵-Good-Turing-平滑"><span>4.2 古德图灵(Good Turing)平滑</span></a></li><li><a class="is-flex" id="toc-item-4-3-组合估计平滑" href="#4-3-组合估计平滑"><span>4.3 组合估计平滑</span></a></li></ul></li><li><a class="is-flex" id="toc-item-5-从N-gram谈回贝叶斯方法" href="#5-从N-gram谈回贝叶斯方法"><span>5. 从N-gram谈回贝叶斯方法</span></a></li></ul></div></div><script type="text/javascript" async>
        $(document).ready(function () { //参考自 https://github.com/ppoffice/hexo-theme-icarus/pull/616/files
            var observerTopMargin;
            var scrollObserver;
            var headerElems = $(".headerlink");
            var activeTocItem;
        
            function initIntersectionObserver(docHeight) {
                observerTopMargin = docHeight;
                scrollObserver = new IntersectionObserver(scrollCallBack,
                    {
                        root: null,  // viewpoint
                        rootMargin: docHeight + "px 0px -80% 0px"  // cover top 30% of viewport to the top of document
                    })
            }
        
            function scrollCallBack(entries, observer) {
                if ($(window).scrollTop() > observerTopMargin * 0.7) { 
                    // User somehow scroll to 70% of observerTopMargin (which is inited as 200% document height)
                    // Observer top margin need to extend to cover all the space to the top of the document
                    initIntersectionObserver(observerTopMargin * 2)
                    observer.disconnect();
                    return;
                }
                let toActive;
                if (entries[0].intersectionRatio == 1) {  // enter viewed area
                    let entry = entries.reduce((u, v) => (u.target.toc_id > v.target.toc_id ? u : v));  // get the lowest item
                    toActive = $("#toc-item-" + $(entry.target).attr("href").substr(1));
                } else {
                    let entry = entries.reduce((u, v) => (u.target.toc_id < v.target.toc_id ? u : v));  // get the highest item
                    let idx = Math.max(entry.target.toc_id - 1, 0);
                    toActive = $("#toc-item-" + $(headerElems[idx]).attr("href").substr(1));
                }
                if (activeTocItem) activeTocItem.removeClass("is-current");
                activeTocItem = toActive
                activeTocItem.addClass("is-current");
            }
        
            initIntersectionObserver($(document).height() * 2);
            headerElems.each(function (index, obj) {
                obj.toc_id = index;
                scrollObserver.observe(obj);
            })
        });</script></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2021-01-02T14:22:00.000Z">2021-01-02</time></p><p class="title is-6"><a class="link-muted" href="/2021/01/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2021-01-01T14:22:00.000Z">2021-01-01</time></p><p class="title is-6"><a class="link-muted" href="/2021/01/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">机器学习笔记</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/">知识总结</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-04-10T16:00:00.000Z">2020-04-11</time></p><p class="title is-6"><a class="link-muted" href="/2020/04/11/%E4%BB%8ENB%E5%88%B0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">从NB到语言模型</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-04-10T16:00:00.000Z">2020-04-11</time></p><p class="title is-6"><a class="link-muted" href="/2020/04/11/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">朴素贝叶斯</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2018-07-10T14:22:00.000Z">2018-07-10</time></p><p class="title is-6"><a class="link-muted" href="/2018/07/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%AF%BE%E5%A0%82%E7%AC%94%E8%AE%B0/">吴恩达目标检测课堂笔记</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"><span class="level-start"><span class="level-item">知识总结</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/04/"><span class="level-start"><span class="level-item">四月 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/02/"><span class="level-start"><span class="level-item">二月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/logo.svg" alt="星空str" height="28"></a><p class="size-small"><span>&copy; 2020 wasim37</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/wasim37"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'http://wangxin123.com',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back-to-top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script src="/js/gallery.js" defer></script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            MathJax.Hub.Config({
                'HTML-CSS': {
                    matchFontHeight: false
                },
                SVG: {
                    matchFontHeight: false
                },
                CommonHTML: {
                    matchFontHeight: false
                },
                tex2jax: {
                    inlineMath: [
                        ['$','$'],
                        ['\\(','\\)']
                    ]
                }
            });
        });</script><!--!--><script src="/js/main.js" defer></script><script src="/js/comment-issue-data.js" defer></script><link rel="stylesheet" href="/css/insight.css"><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>(function (window) {
            var INSIGHT_CONFIG = {
                TRANSLATION: {
                    POSTS: '文章',
                    PAGES: '页面',
                    CATEGORIES: '分类',
                    TAGS: '标签',
                    UNTITLED: '(无标题)',
                },
                CONTENT_URL: '/content.json',
            };
            window.INSIGHT_CONFIG = INSIGHT_CONFIG;
        })(window);</script><script src="/js/insight.js" defer></script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>