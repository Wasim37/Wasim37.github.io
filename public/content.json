{"pages":[{"title":"About","text":"","link":"/about/index.html"}],"posts":[{"title":"卷积网络的边缘检测","text":"神经网络由浅层到深层，分别可以检测出图片的边缘特征、局部特征（例如眼睛、鼻子等），到后面的一层就可以根据前面检测的特征来识别整体面部轮廓。这些工作都是依托卷积神经网络来实现的。 卷积运算（Convolutional Operation）是卷积神经网络最基本的组成部分。我们以边缘检测为例，来解释卷积是怎样运算的。 边缘检测图片最常做的边缘检测有两类：垂直边缘（Vertical Edges）检测和水平边缘（Horizontal Edges）检测。 图片的边缘检测可以通过与相应滤波器进行卷积来实现。以垂直边缘检测为例，原始图片尺寸为 6x6，中间的矩阵被称作滤波器（filter），尺寸为 3x3，卷积后得到的图片尺寸为 4x4，得到结果如下（数值表示灰度，以左上角和右下角的值为例）： 可以看到，卷积运算的求解过程是从左到右，由上到下，每次在原始图片矩阵中取与滤波器同等大小的一部分，每一部分中的值与滤波器中的值对应相乘后求和，将结果组成一个矩阵。 下图对应一个垂直边缘检测的例子： 如果将最右边的矩阵当作图像，那么中间一段亮一些的区域对应最左边的图像中间的垂直边缘。 这里有另一个卷积运算的动态的例子，方便理解： 图中的*表示卷积运算符号。因为在计算机中这个符号表示一般的乘法，在不同的深度学习框架中，卷积操作的 API 定义可能不同： 在 Python 中，卷积用 conv_forward() 表示；在 Tensorflow 中，卷积用 tf.nn.conv2d() 表示；在 keras 中，卷积用 Conv2D() 表示。 更多边缘检测的例子如果将灰度图左右的颜色进行翻转，再与之前的滤波器进行卷积，得到的结果也有区别。实际应用中，这反映了由明变暗和由暗变明的两种渐变方式。可以对输出图片取绝对值操作，以得到同样的结果。 垂直边缘检测和水平边缘检测的滤波器如下所示： 其他常用的滤波器还有 Sobel 滤波器和 Scharr 滤波器。它们增加了中间行的权重，这样可能更加稳健。 滤波器中的值还可以设置为参数，通过模型训练来得到。这样，神经网络使用反向传播算法可以学习到一些低级特征，从而实现对图片所有边缘特征的检测，而不仅限于垂直边缘和水平边缘。","link":"/2017/12/18/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"},{"title":"卷积操作详解（填充、步长、高维卷积、卷积公式）","text":"对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。 填充假设输入图片的大小为 n×n，而滤波器的大小为 f×f，则卷积后的输出图片大小为 (n−f+1)×(n−f+1)。 这样就有两个问题： 每次卷积运算后，输出图片的尺寸缩小； 原始图片的角落、边缘区像素点在输出中采用较少，输出图片丢失边缘位置的很多信息。 为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行填充（Padding），以增加矩阵的大小。通常将 0 作为填充值。 设每个方向扩展像素点数量为 p，则填充后原始图片的大小为 **(n+2p)×(n+2p)$，滤波器大小保持 $f×f$不变，则输出图片大小为 $(n+2p−f+1)×(n+2p−f+1)$。 因此，在进行卷积运算时，我们有两种选择： Valid 卷积：不填充，直接卷积。结果大小为 $(n−f+1)×(n−f+1)$； Same 卷积：进行填充，并使得卷积后结果大小与输入一致，这样 $p = \\frac{f-1}{2}$。 在计算机视觉领域，f通常为奇数。原因包括 Same 卷积中 $p = \\frac{f-1}{2}$ 能得到自然数结果，并且滤波器有一个便于表示其所在位置的中心点。 卷积步长卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置步长（Stride）来压缩一部分信息。 步长表示滤波器在原始图片的水平方向和垂直方向上每次移动的距离。之前，步长被默认为 1。而如果我们设置步长为 2，则卷积过程如下图所示： 设步长为 s，填充长度为 p，输入图片大小为 n×n，滤波器大小为 f×f，则卷积后图片的尺寸为： \\biggl\\lfloor \\frac{n+2p-f}{s}+1 \\biggr\\rfloor \\times \\biggl\\lfloor \\frac{n+2p-f}{s}+1 \\biggr\\rfloor注意公式中有一个向下取整的符号，用于处理商不为整数的情况。向下取整反映着当取原始矩阵的图示蓝框完全包括在图像内部时，才对它进行运算。 目前为止我们学习的“卷积”实际上被称为互相关（cross-correlation），而非数学意义上的卷积。真正的卷积操作在做元素乘积求和之前，要将滤波器沿水平和垂直轴翻转（相当于旋转 180 度）。因为这种翻转对一般为水平或垂直对称的滤波器影响不大，按照机器学习的惯例，我们通常不进行翻转操作，在简化代码的同时使神经网络能够正常工作。 高维卷积如果我们想要对三通道的 RGB 图片进行卷积运算，那么其对应的滤波器组也同样是三通道的。过程是将每个单通道（R，G，B）与对应的滤波器进行卷积运算求和，然后再将三个通道的和相加，将 27 个乘积的和作为输出图片的一个像素值。 不同通道的滤波器可以不相同。例如只检测 R 通道的垂直边缘，G 通道和 B 通道不进行边缘检测，则 G 通道和 B 通道的滤波器全部置零。当输入有特定的高、宽和通道数时，滤波器可以有不同的高和宽，但通道数必须和输入一致。 如果想同时检测垂直和水平边缘，或者更多的边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。设输入图片的尺寸为 $n×n×nc$（nc为通道数），滤波器尺寸为 $f×f×nc$，则卷积后的输出图片尺寸为 $(n−f+1)×(n−f+1)×n′c$，$n′c$为滤波器组的个数。 单层卷积网络 与之前的卷积过程相比较，卷积神经网络的单层结构多了激活函数和偏移量；而与标准神经网络： Z^{[l]} = W^{[l]}A^{[l-1]}+bA^{[l]} = g^{[l]}(Z^{[l]})相比，滤波器的数值对应着权重 $W[l]$，卷积运算对应着 $W[l]$与 $A[l−1]$的乘积运算，所选的激活函数变为 ReLU。 对于一个 3x3x3 的滤波器，包括偏移量 b在内共有 28 个参数。不论输入的图片有多大，用这一个滤波器来提取特征时，参数始终都是 28 个，固定不变。即选定滤波器组后，参数的数目与输入图片的尺寸无关。因此，卷积神经网络的参数相较于标准神经网络来说要少得多。这是 CNN 的优点之一。 符号总结设 $l$ 层为卷积层： $f^{[l]}$：滤波器的高（或宽） $p^{[l]}$：填充长度 $s^{[l]}$：步长 $n^{[l]}_c$：滤波器组的数量 输入维度：$n^{[l-1]}_H \\times n^{[l-1]}_W \\times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_H$表示输入图片的高，$n^{[l-1]}_W$表示输入图片的宽。之前的示例中输入图片的高和宽都相同，但是实际中也可能不同，因此加上下标予以区分。 输出维度：$n^{[l]}_H \\times n^{[l]}_W \\times n^{[l]}_c$ 。其中 n^{[l]}_H = \\biggl\\lfloor \\frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \\biggr\\rfloorn^{[l]}_W = \\biggl\\lfloor \\frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \\biggr\\rfloor每个滤波器组的维度：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_c$ 为输入图片通道数（也称深度）。权重维度：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c \\times n^{[l]}_c$偏置维度：$1 \\times 1 \\times 1 \\times n^{[l]}_c$ 由于深度学习的相关文献并未对卷积标示法达成一致，因此不同的资料关于高度、宽度和通道数的顺序可能不同。有些作者会将通道数放在首位，需要根据标示自行分辨。","link":"/2018/01/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%A1%AB%E5%85%85%E3%80%81%E6%AD%A5%E9%95%BF%E3%80%81%E9%AB%98%E7%BB%B4%E5%8D%B7%E7%A7%AF%E3%80%81%E5%8D%B7%E7%A7%AF%E5%85%AC%E5%BC%8F%EF%BC%89/"},{"title":"神经风格转移","text":"简介神经风格迁移（Neural style transfer）将参考风格图像的风格“迁移”到另外一张内容图像中，生成具有其特色的图像。 相关链接： 代码示例 fast-style-transfer github项目 深度卷积网络在学什么？想要理解如何实现神经风格转换，首先要理解在输入图像数据后，一个深度卷积网络从中都学到了些什么。我们借助可视化来做到这一点。 我们通过遍历所有的训练样本，找出使该层激活函数输出最大的 9 块图像区域。可以看出，浅层的隐藏层通常检测出的是原始图像的边缘、颜色、阴影等简单信息。随着层数的增加，隐藏单元能捕捉的区域更大，学习到的特征也由从边缘到纹理再到具体物体，变得更加复杂。 相关论文：Zeiler and Fergus., 2013, Visualizing and understanding convolutional networks 代价函数神经风格迁移生成图片 G 的代价函数如下： J(G) = \\alpha \\cdot J_{content}(C, G) + \\beta \\cdot J_{style}(S, G)其中，$α$、$β$ 是用于控制相似度比重的超参数。 神经风格迁移的算法步骤如下： 随机生成图片 G 的所有像素点； 使用梯度下降算法使代价函数最小化，以不断修正 G 的所有像素点。 相关论文：Gatys al., 2015. A neural algorithm of artistic style 内容代价函数上述代价函数包含一个内容代价部分和风格代价部分。我们先来讨论内容代价函数 $J_{content}(C, G)$，它表示内容图片 C 和生成图片 G 之间的相似度。 $J_{content}(C, G)$ 的计算过程如下： 使用一个预训练好的 CNN（例如 VGG）； 选择一个隐藏层 l 来计算内容代价。l 太小则内容图片和生成图片像素级别相似，l 太大则可能只有具体物体级别的相似。因此，l 一般选一个中间层； 设 a(C)[l]、a(G)[l] 为 C 和 G 在 l 层的激活，则有： J_{content}(C, G) = \\frac{1}{2}||(a^{(C)[l]} - a^{(G)[l]})||^2$a^{(C)[l]}$ 和 $a^{(G)[l]}$ 越相似，则 $J_{content}(C, G)$ 越小。 风格代价函数 每个通道提取图片的特征不同，比如标为红色的通道提取的是图片的垂直纹理特征，标为黄色的通道提取的是图片的橙色背景特征。那么计算这两个通道的相关性，相关性的大小，即表示原始图片既包含了垂直纹理也包含了该橙色背景的可能性大小。通过 CNN，“风格”被定义为同一个隐藏层不同通道之间激活值的相关系数，因其反映了原始图片特征间的相互关系。 对于风格图像 S，选定网络中的第 l 层，则相关系数以一个 gram 矩阵的形式表示： 其中，$i$ 和 $j$ 为第 $l$ 层的高度和宽度；$k$ 和 $k′$ 为选定的通道，其范围为 1 到 $nC^{[l]}$；$a^{l}{ijk}$ 为激活。 同理，对于生成图像 G，有： 因此，第 $l$ 层的风格代价函数为： 如果对各层都使用风格代价函数，效果会更好。因此有： J_{style}(S, G) = \\sum_l \\lambda^{[l]} J^{[l]}_{style}(S, G)其中，$lambda$ 是用于设置不同层所占权重的超参数。 推广至一维和三维之前我们处理的都是二维图片，实际上卷积也可以延伸到一维和三维数据。我们举两个示例来说明。 EKG 数据（心电图）是由时间序列对应的每个瞬间的电压组成，是一维数据。一般来说我们会用 RNN（循环神经网络）来处理，不过如果用卷积处理，则有： 输入时间序列维度：14 x 1 滤波器尺寸：5 x 1，滤波器个数：16 输出时间序列维度：10 x 16 而对于三维图片的示例，有 输入 3D 图片维度：14 x 14 x 14 x 1 滤波器尺寸：5 x 5 x 5 x 1，滤波器个数：16 输出 3D 图片维度：10 x 10 x 10 x 16","link":"/2018/01/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E7%A7%BB/"},{"title":"人脸识别","text":"简介人脸验证（Face Verification）和人脸识别（Face Recognition）的区别： 人脸验证：一般指一个一对一问题，只需要验证输入的人脸图像是否与某个已知的身份信息对应； 人脸识别：一个更为复杂的一对多问题，需要验证输入的人脸图像是否与多个已知身份信息中的某一个匹配。 一般来说，由于需要匹配的身份信息更多导致错误率增加，人脸识别比人脸验证更难一些。 相关链接： 代码示例 A Face Detection Benchmark One-Shot 学习人脸识别所面临的一个挑战是要求系统只采集某人的一个面部样本，就能快速准确地识别出这个人，即只用一个训练样本来获得准确的预测结果。这被称为One-Shot 学习。 有一种方法是假设数据库中存有 N 个人的身份信息，对于每张输入图像，用 Softmax 输出 N+1 种标签，分别对应每个人以及都不是。然而这种方法的实际效果很差，因为过小的训练集不足以训练出一个稳健的神经网络；并且如果有新的身份信息入库，需要重新训练神经网络，不够灵活。 因此，我们通过学习一个 Similarity 函数来实现 One-Shot 学习过程。Similarity 函数定义了输入的两幅图像的差异度，其公式如下： Similarity = d(img1, img2)可以设置一个超参数 τ 作为阈值，作为判断两幅图片是否为同一个人的依据。 Siamese 孪生网络实现 Similarity 函数的一种方式是使用Siamese 网络，它是一种对两个不同输入运行相同的卷积网络，然后对它们的结果进行比较的神经网络。 如上图示例，将图片 $x^{(1)}$、$x^{(2)}$ 分别输入两个相同的卷积网络中，经过全连接层后不再进行 Softmax，而是得到特征向量 $f(x^{(1)})$、$f(x^{(2)})$。这时，Similarity 函数就被定义为两个特征向量之差的 L2 范数： d(x^{(1)}, x^{(2)}) = ||f(x^{(1)}) - f(x^{(2)})||^2_2孪生网络05年由Lecun提出，15年做了改进。2-branches networks ==&gt; 2-channel networks。siamese(孪生) 网络简介：http://blog.csdn.net/qq_15192373/article/details/78404761 相关论文：Taigman et al., 2014, DeepFace closing the gap to human level performance Triplet 损失Triplet 损失函数用于训练出合适的参数，以获得高质量的人脸图像编码。“Triplet”一词来源于训练这个神经网络需要大量包含 Anchor（靶目标）、Positive（正例）、Negative（反例）的图片组，其中 Anchor 和 Positive 需要是同一个人的人脸图像。 对于这三张图片，应该有： ||f(A) - f(P)||^2_2 + \\alpha \\le ||f(A) - f(N)||^2_2其中，α 被称为间隔（margin），用于确保 f() 不会总是输出零向量（或者一个恒定的值）。 Triplet 损失函数的定义： L(A, P, N) = max(||f(A) - f(P)||^2_2 - ||f(A) - f(N)||^2_2 + \\alpha, 0)其中，因为 $||f(A) - f(P)||^2_2 - ||f(A) - f(N)||^2_2 + \\alpha$ 的值需要小于等于 0，因此取它和 0 的更大值。 对于大小为 m 的训练集，代价函数为： J = \\sum^m_{i=1}L(A^{(i)}, P^{(i)}, N^{(i)})通过梯度下降最小化代价函数。 在选择训练样本时，随机选择容易使 Anchor 和 Positive 极为接近，而 Anchor 和 Negative 相差较大，以致训练出来的模型容易抓不到关键的区别。因此，最好的做法是人为增加 Anchor 和 Positive 的区别，缩小 Anchor 和 Negative 的区别，促使模型去学习不同人脸之间的关键差异。 相关论文：Schroff et al., 2015, FaceNet: A unified embedding for face recognition and clustering 二分类结构除了 Triplet 损失函数，二分类结构也可用于学习参数以解决人脸识别问题。其做法是输入一对图片，将两个 Siamese 网络产生的特征向量输入至同一个 Sigmoid 单元，输出 1 则表示是识别为同一人，输出 0 则表示识别为不同的人。 Sigmoid 单元对应的表达式为： \\hat y = \\sigma (\\sum^K_{k=1}w_k|f(x^{(i)})_{k} - x^{(j)})_{k}| + b)其中，wk 和 b 都是通过梯度下降算法迭代训练得到的参数。上述计算表达式也可以用另一种表达式代替： \\hat y = \\sigma (\\sum^K_{k=1}w_k \\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k} + b)其中，\\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k} 被称为 $χ$ 方相似度。 无论是对于使用 Triplet 损失函数的网络，还是二分类结构，为了减少计算量，可以提前计算好编码输出 f(x) 并保存。这样就不必存储原始图片，并且每次进行人脸识别时只需要计算测试图片的编码输出。","link":"/2018/01/21/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"},{"title":"贝叶斯算法","text":"持续更新中。。。 示例代码 贝叶斯定理相关公式 条件概率和后验概率区别 朴素贝叶斯 高斯朴素贝叶斯 伯努利朴素贝叶斯 多项式朴素贝叶斯 贝叶斯网络 怎么通俗易懂地解释贝叶斯网络和它的应用 用贝叶斯机率说明Dropout的原理 如何用贝叶斯算法实现垃圾邮件检测 当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑 贝叶斯定理相关公式 先验概率P(A)：在不考虑任何情况下，A事件发生的概率 条件概率P(B|A)：A事件发生的情况下，B事件发生的概率 后验概率P(A|B)：在B事件发生之后，对A事件发生的概率的重新评估。条件概率和后验概率区别 全概率：如果B和B’构成样本空间的一个划分，那么事件A的概率为：B和B’的概率分别乘以A对这两个事件的概率之和。 贝叶斯定理： P(B_i|A)=\\frac{P(AB_i)}{P(A)}=\\frac{P(B_i)P(A|B_i)}{\\sum_j P(B_j)P(A|B_j)} 贝叶斯不同于SVM、逻辑回归与决策树等判别式模型，它属于生成式模型（LDA、HMM等）。贝叶斯思想可以概括为先验概率+数据=后验概率，后验概率就是我们要求的。 朴素贝叶斯朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是很简单地假设样本特征彼此独立. 这个假设现实中基本上不存在， 但特征相关性很小的实际情况还是很多， 所以很多时候这个模型仍然能够工作得很好。 如果样本特征属性关联很大，朴素贝叶斯就没法很好解决这类问题，可以考虑使用贝叶斯网络。 高斯朴素贝叶斯Gaussian Naive Bayes是指当 特征属性为连续值时，而且分布服从高斯分布，那么在计算 P(x|y) 的时候可以直接使用高斯分布的概率公式： 因此只需要计算出各个类别中此特征项划分的各个均值和标准差 伯努利朴素贝叶斯Bernoulli Naive Bayes是指当 特征属性为连续值时，而且分布服从伯努利分布，那么在计算 P(x|y) 的时候可以直接使用伯努利分布的概率公式： 伯努利分布是一种离散分布，只有两种可能的结果。1表示成功，出现的概率为p；0表示失败，出现的概率为q=1-p；其中均值为E(x)=p，方差为Var(X)=p(1-p) 多项式朴素贝叶斯Multinomial Naive Bayes是指当 特征属性服从多项分布，从而对于每个类别y，参数为θy =(θy1, θy2,…,θyn)，其中n为特征属性数目，那么P(xi|y)的概率为θyi 贝叶斯网络当多个特征属性之间存在着某种相关关系的时候，使用朴素贝叶斯算法就没法解决这类问题，那么贝叶斯网络就是解决这类应用场景的一个非常好的算法。 把某个研究系统中涉及到的随机变量，根据是否条件独立绘制在一个有向图中，就形成了贝叶斯网络。 贝叶斯网络(BN)，又称有向无环图模型，是一种概率图模型，根据概率图的拓扑结构，考察一组随机变量{X1, X2,…,Xn}及其N组条件概率分布的性质。 一般而言，贝叶斯网络的有向无环图中的节点表示随机变量，可以是可观察到的变量，或隐变量，未知参数等等。连接两个节点之间的箭头代表两个随机变量之间的因果关系(也就是这两个随机变量之间非条件独立)，如果两个节点间以一个单箭头连接在一起，表示其中一个节点是“因”，另外一个是“果”，从而两节点之间就会产生一个条件概率值。 注意：每个节点在给定其直接前驱的时候，条件独立于其后继。 更多细节见我整理的PDF文件。 如何用贝叶斯算法实现垃圾邮件检测 首先通过相除消去分子，大于1即正样本，小于1即负样本。然后将难以计算的联合分布概率P(y=T|x)和P(y=F|x)进行转换即可。 下面是另外两种解释： 原文：https://github.com/SunnyMarkLiu/NaiveBayesSpamFilter 原文：https://blog.csdn.net/Gane_Cheng/article/details/53219332 当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑","link":"/2017/10/26/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"},{"title":"机器学习（八）：隐马尔科夫模型","text":"持续更新中。。。 示例代码 贝叶斯网络、马尔科夫网络、EM、条件随机场、最大熵模型的联系 贝叶斯网络、马尔科夫网络、EM、条件随机场、最大熵模型的联系1、将随机变量作为结点，若两个随机变量相关或者不独立，则将二者连接一条边；若给定若干随机变量，则形成一个有向图，即构成一个网络。2、如果该网络是有向无环图，则这个网络称为贝叶斯网络。3、如果这个有向图的拓扑结构退化成线性链的形式，则得到马尔科夫模型；因为每个结点都是随机变量，将其看成各个时刻(或空间)的相关变化，以随机过程的视角，则可以看成是马尔科夫过程。4、若上述网络是无向的，则称为概率无向图模型，又称马尔科夫随机场。5、如果在给定某些条件的前提下，研究这个马尔科夫随机场，则得到条件随机场。6、如果使用条件随机场解决标注问题，并且进一步将条件随机场中的网络拓扑变成线性的，则得到线性链条件随机场。 变量 + 依赖关系 —&gt; 概率图网络 概率有向图 贝叶斯网络 马尔科夫过程 一阶马尔科夫链 马尔科夫模型 概率无向图（马尔科夫随机场） 条件随机场 线性链条件随机场 生成式模型对联合分布概率p(x,y)进行建模，判别式模型直接对条件概率p(y|x)进行建模。 贝叶斯网络又称信度网络或信念网络，是一个有向无环图。它的理论基础是贝叶斯公式。 朴素贝叶斯基于贝叶斯定理与特征条件独立假设。 概率无向图模型又称马尔科夫随机场，是一个可以由无向图表示的联合概率分布 随机过程又称为随机函数，是随时间而随机变化的过程。 马尔科夫性质：可以简单理解为将来与现在有关，与过去无关 HMM的预测问题又称为解码问题，它的学习问题可以用EM算法解决。 最大熵模型：满足已知条件的所有概率模型中，熵最大的模型就是最好的模型。 条件随机场与最大熵模型都是指数模型，CRF的求解借鉴了最大熵的思想 只要可以转换为 序列标注 的问题，都可以用HMM、CRF和 MEMM（最大熵隐马尔科夫模型）解决","link":"/2017/10/29/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"},{"title":"数学基础","text":"知识要点 常数π 自然常数e 对数函数、指数函数 导数 导数的性质：单调性 凸集 导数的性质：凹凸性 凸函数 常见导函数 导数的应用 偏导数 方向导数 梯度 梯度下降 Taylor公式 Taylor公式应用 组合数 古典概率 概率公式 联合概率 条件概率 全概率公式 贝叶斯公式 事件的独立性 期望 方差 标准差 协方差 期望方差案例 协方差矩阵 Pearson相关系数 中心矩、原点矩 峰度 偏度 概率密度函数 正态分布 两点分布（0-1分布、伯努利分布） 二项分布 几何分布 泊松分布 均匀分布 指数分布 切比雪夫不等式/切比雪夫定理 大数定律 中心极限定理 最大似然法 向量 向量的运算 正交向量 线性代数 矩阵 矩阵的直观表示 方阵、零矩阵、单位矩阵 矩阵的加减法 矩阵与数的乘法 矩阵与向量的乘法 矩阵与矩阵的乘法 矩阵的转置 矩阵的秩 方阵行列式 代数余子式 伴随矩阵 方阵的逆 状态转移模型 概率转移矩阵 平稳分布 向量组 向量组等价 系数矩阵 正交矩阵 对称矩阵 特征值和特征向量 特征值和特征向量求解 特征值的性质 可对角化矩阵 正定矩阵 例题 奇异矩阵 QR分解 SVD 向量的导数 标量对向量的导数","link":"/2017/09/25/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"},{"title":"CNN经典网络总结","text":"LeNet-5LeNet诞生于1998年，网络结构比较完整，包括卷积层、pooling层、全连接层，这些都是现代CNN网络的基本组件，被认为是CNN的开端。 网络特点： LeNet-5 针对灰度图像而训练，因此输入图片的通道数为 1。 该模型总共包含了约 6 万个参数，远少于标准神经网络所需。 典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 【CONV layer-&gt;POOL layer-&gt;CONV layer-&gt;POOL layer-&gt;FC layer-&gt;FC layer-&gt;OUTPUT layer】。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用。在计算神经网络的层数时，通常只统计具有权重和参数的层，池化层没有需要训练的参数，所以和之前的卷积层共同计为一层。 当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数。 相关论文：LeCun et.al., 1998. Gradient-based learning applied to document recognition。吴恩达老师建议精读第二段，泛读第三段。 AlexNet-8 AlexNet-8有5个卷积层和3个全连接层，它与 LeNet-5 模型类似，但是更复杂，包含约 6000 万参数。当用于训练图像和数据集时，AlexNet 能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据。 相对于1998年的LeNet，2012年才出现的AlexNet有了历史性的突破，相对LeNet做了如下改进：（1）数据不够的情况下，采用了数据增强（2）用ReLU代替了传统的Tanh或者Logistic。（3）引入Dropout，防止过拟合，是AlexNet中一个很大的创新。（4）Local Response Normalization，局部响应归一化，简称LRN，实际就是利用临近的数据做归一化。这个策略贡献了1.2%的Top-5错误率。（5）多GPU训练。如图 AlexNet-2 所示，训练分布在两个GPU上，这是比一臂之力还大的洪荒之力。（6）Overlapping Pooling，Overlapping的意思是有重叠，即Pooling的步长比Pooling Kernel的对应边要小。这个策略贡献了0.3%的Top-5错误率。 相关论文：Krizhevsky et al.,2012. ImageNet classification with deep convolutional neural networks。这是一篇易于理解并且影响巨大的论文，计算机视觉群体自此开始重视深度学习。 VGG-16 特点： VGG 使用最广泛的网络为 VGG-16 网络。 超参数较少，只需要专注于构建卷积层。 结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。 VGG 需要训练的特征数量巨大，包含多达约 1.38 亿个参数。 相关论文：Simonvan &amp; Zisserman 2015. Very deep convolutional networks for large-scale image recognition。 1x1 卷积1x1 卷积（1x1 convolution，或称为 Network in Network）指滤波器的尺寸为 1。当通道数为 1 时，1x1 卷积意味着卷积操作等同于乘积操作。 而当通道数更多时，1x1 卷积的作用实际上类似全连接的神经网络结构，从而降低（或升高，取决于滤波器组数）数据的维度。 池化能压缩数据的高度 $n_H$ 及宽度 $n_W$，而 1×1 卷积能压缩数据的通道数 $n_C$。在如下图所示的例子中，用 32 个大小为 1×1×192 的滤波器进行卷积，就能使原先数据包含的 192 个通道压缩为 32 个。 虽然论文 Lin et al., 2013. Network in network 中关于架构的详细内容并没有得到广泛应用，但是 1x1 卷积的理念十分有影响力，许多神经网络架构（包括 Inception 网络）都受到它的影响。 GoogleLeNet-22 Inception 网络2012年AlexNet做出历史突破以来，直到GoogLeNet出来之前，主流的网络结构突破大致是网络更深（层数），网络更宽（神经元数）。所以大家调侃深度学习为“深度调参”，但是纯粹的增大网络有如下缺点： 参数太多，容易过拟合，若训练数据集有限； 网络越大计算复杂度越大，难以应用； 网络越深，梯度越往后穿越容易消失（梯度弥散），难以优化模型 那么如何增加网络深度和宽度的同时减少参数？Inception在这样的情况下应运而生。 在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 Inception 网络的作用 是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层。 如图，Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。 相关论文：Szegedy et al., 2014, Going Deeper with Convolutions 计算成本问题在提升性能的同时，Inception 网络有着较大的计算成本。下图是一个例子： 图中有 32 个滤波器，每个滤波器的大小为 5x5x192。输出大小为 28x28x32，所以需要计算 28x28x32 个数字，对于每个数，都要执行 5x5x192 次乘法运算。加法运算次数与乘法运算次数近似相等。因此，可以看作这一层的计算量为 28x28x32x5x5x192 = 1.2亿。 为了解决计算量大的问题，可以引入 1x1 卷积来减少其计算量。 对于同一个例子，我们使用 1x1 卷积把输入数据从 192 个通道减少到 16 个通道，然后对这个较小层运行 5x5 卷积，得到最终输出。这个 1x1 的卷积层通常被称作瓶颈层（Bottleneck layer）。 改进后的计算量为 28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。 只要合理构建瓶颈层，就可以既显著缩小计算规模，又不会降低网络性能。 完整的 Inception 网络 上图是引入 1x1 卷积后的 Inception 模块。值得注意的是，为了将所有的输出组合起来，红色的池化层使用 Same 类型的填充（padding）来池化使得输出的宽高不变，通道数也不变。 多个 Inception 模块组成一个完整的 Inception 网络（被称为 GoogLeNet，以向 LeNet 致敬），如下图所示： 注意黑色椭圆圈出的隐藏层，这些分支都是 Softmax 的输出层，可以用来参与特征的计算及结果预测，起到调整并防止发生过拟合的效果。 经过研究者们的不断发展，Inception 模型的 V2、V3、V4 以及引入残差网络的版本被提出，这些变体都基于 Inception V1 版本的基础思想上。顺便一提，Inception 模型的名字来自电影《盗梦空间》。 ResNet-152 残差网络因为存在梯度消失和梯度爆炸问题，网络越深，就越难以训练成功。**残差网络**（Residual Networks，简称为 ResNets）可以有效解决这个问题。 上图的结构被称为残差块（Residual block）。通过捷径（Short cut，或者称跳远连接，Skip connections）可以将 a[l]添加到第二个 ReLU 过程中，直接建立 a[l]与 a[l+2]之间的隔层联系。表达式如下： z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}a^{[l+1]} = g(z^{[l+1]})z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}a^{[l+2]} = g(z^{[l+2]} + a^{[l]})构建一个残差网络就是将许多残差块堆积在一起，形成一个深度网络。 为了便于区分，在 ResNets 的论文 He et al., 2015. Deep residual networks for image recognition中，非残差网络被称为普通网络（Plain Network）。将它变为残差网络的方法是加上所有的跳远连接。 在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，训练错误会先减少，然后开始增多。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。 残差网络有助于解决梯度消失和梯度爆炸问题，使得在训练更深的网络的同时，又能保证良好的性能。 残差网络有效的原因假设有一个大型神经网络，其输入为 $X$，输出为 $a^{[l]}$。给这个神经网络额外增加两层，输出为 $a^{[l+2]}$。将这两层看作一个具有跳远连接的残差块。为了方便说明，假设整个网络中都选用 ReLU 作为激活函数，因此输出的所有激活值都大于等于 0。 则有： \\begin{equation} \\begin{split} a^{[l+2]} &= g(z^{[l+2]}+a^{[l]}) \\\\ &= g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]}) \\end{split} \\end{equation}当发生梯度消失时，$W^{[l+2]}\\approx0$，$b^{[l+2]}\\approx0$，则有： a^{[l+2]} = g(a^{[l]}) = ReLU(a^{[l]}) = a^{[l]}因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。 注意，如果 $a^{[l]}$与 $a^{[l+2]}$ 的维度不同，需要引入矩阵 $W_s$ 与 $a^{[l]}$ 相乘，使得二者的维度相匹配。参数矩阵 $W_s$既可以通过模型训练得到，也可以作为固定值，仅使 $a^{[l]}$ 截断或者补零。 上图是论文提供的 CNN 中 ResNet 的一个典型结构。卷积层通常使用 Same 卷积以保持维度相同，而不同类型层之间的连接（例如卷积层和池化层），如果维度不同，则需要引入矩阵 $W_s$。 经典网络对比","link":"/2018/01/15/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/CNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C%E6%80%BB%E7%BB%93/"},{"title":"序列模型与注意力机制","text":"Seq2Seq 模型 $$ i\\hbar\\frac{\\partial}{\\partial t}\\psi=-\\frac{\\hbar^2}{2m}\\nabla^2\\psi+V\\psi $$ Seq2Seq（Sequence-to-Sequence）模型能够应用于机器翻译、语音识别等各种序列到序列的转换问题。一个 Seq2Seq 模型包含编码器（Encoder）和解码器（Decoder）两部分，它们通常是两个不同的 RNN。如下图所示，将编码器的输出作为解码器的输入，由解码器负责输出正确的翻译结果。 提出 Seq2Seq 模型的相关论文： Sutskever et al., 2014. Sequence to sequence learning with neural networks Cho et al., 2014. Learning phrase representaions using RNN encoder-decoder for statistical machine translation 这种编码器-解码器的结构也可以用于图像描述（Image captioning）。将 AlexNet 作为编码器，最后一层的 Softmax 换成一个 RNN 作为解码器，网络的输出序列就是对图像的一个描述。 图像描述的相关论文： Mao et. al., 2014. Deep captioning with multimodal recurrent neural networks Vinyals et. al., 2014. Show and tell: Neural image caption generator Karpathy and Fei Fei, 2015. Deep visual-semantic alignments for generating image descriptions 选择最可能的句子机器翻译用到的模型与语言模型相似，只是用编码器的输出作为解码器第一个时间步的输入（而非 0）。因此机器翻译的过程其实相当于建立一个条件语言模型。 由于解码器进行随机采样过程，输出的翻译结果可能有好有坏。因此需要找到能使条件概率最大化的翻译，即 arg \\ max_{y^{⟨1⟩}, ..., y^{⟨T_y⟩}}P(y^{⟨1⟩}, ..., y^{⟨T_y⟩} | x)鉴于贪心搜索算法得到的结果显然难以不符合上述要求，解决此问题最常使用的算法是 集束搜索（Beam Search）。 集束搜索集束搜索会考虑每个时间步多个可能的选择。设定一个 集束宽（Bean Width）B，代表了解码器中每个时间步的预选单词数量。例如 B=3，则将第一个时间步最可能的三个预选单词及其概率值 $P(\\hat y^{⟨1⟩}|x)$ 保存到计算机内存，以待后续使用。 第二步中，分别将三个预选词作为第二个时间步的输入，得到 $P(\\hat y^{⟨2⟩}|x, \\hat y^{⟨1⟩})$。 因为我们需要的其实是第一个和第二个单词对（而非只有第二个单词）有着最大概率，因此根据条件概率公式，有： $P(\\hat y^{⟨1⟩}, \\hat y^{⟨2⟩}|x) = P(\\hat y^{⟨1⟩}|x) P(\\hat y^{⟨2⟩}|x, \\hat y^{⟨1⟩})$ 设词典中有 $N$ 个词，则当 $B=3$ 时，有 $3*N$ 个 $P(\\hat y^{⟨1⟩}, \\hat y^{⟨2⟩}|x)$。仍然取其中概率值最大的 3 个，作为对应第一个词条件下的第二个词的预选词。以此类推，最后输出一个最优的结果，即结果符合公式： arg \\ max \\prod^{T_y}_{t=1} P(\\hat y^{⟨t⟩} | x, \\hat y^{⟨1⟩}, ..., \\hat y^{⟨t-1⟩})可以看到，当 $B=1$ 时，集束搜索就变为贪心搜索。 优化：长度标准化长度标准化（Length Normalization）是对集束搜索算法的优化方式。对于公式 arg \\ max \\prod^{T_y}_{t=1} P(\\hat y^{⟨t⟩} | x, \\hat y^{⟨1⟩}, ..., \\hat y^{⟨t-1⟩})当多个小于 1 的概率值相乘后，会造成 数值下溢（Numerical Underflow），即得到的结果将会是一个电脑不能精确表示的极小浮点数。因此，我们会取 log 值，并进行标准化： arg \\ max \\frac{1}{T_y^{\\alpha}} \\sum^{T_y}_{t=1} logP(\\hat y^{⟨t⟩} | x, \\hat y^{⟨1⟩}, ..., \\hat y^{⟨t-1⟩})其中，$T_y$ 是翻译结果的单词数量，$α$ 是一个需要根据实际情况进行调节的超参数。标准化用于减少对输出长的结果的惩罚（因为翻译结果一般没有长度限制）。 关于集束宽 B 的取值，较大的 B 值意味着可能更好的结果和巨大的计算成本；而较小的 B 值代表较小的计算成本和可能表现较差的结果。通常来说，B 可以取一个 10 以下的值。 和 BFS、DFS 等精确的查找算法相比，集束搜索算法运行速度更快，但是不能保证一定找到 arg max 准确的最大值。 误差分析集束搜索是一种启发式搜索算法，其输出结果不总为最优。当结合 Seq2Seq 模型和集束搜索算法所构建的系统出错（没有输出最佳翻译结果）时，我们通过误差分析来分析错误出现在 RNN 模型还是集束搜索算法中。 例如，对于下述两个由人工和算法得到的翻译结果： Human: Jane visits Africa in September. (y∗)Algorithm: Jane visited Africa last September. (y∗) 将翻译中没有太大差别的前三个单词作为解码器前三个时间步的输入，得到第四个时间步的条件概率 $P(y^* | x)$ 和 $P(\\hat y | x)$，比较其大小并分析： 如果 $P(y^ | x) &gt; P(\\hat y | x)$，说明是集束搜索算法出现错误，没有选择到概率最大的词；如果 $P(y^ | x) \\le P(\\hat y | x)$，说明是 RNN 模型的效果不佳，预测的第四个词为“in”的概率小于“last”。建立一个如下图所示的表格，记录对每一个错误的分析，有助于判断错误出现在 RNN 模型还是集束搜索算法中。如果错误出现在集束搜索算法中，可以考虑增大集束宽 B；否则，需要进一步分析，看是需要正则化、更多数据或是尝试一个不同的网络结构。 Bleu 得分Bleu（Bilingual Evaluation Understudy） 得分用于评估机器翻译的质量，其思想是机器翻译的结果越接近于人工翻译，则评分越高。 最原始的 Bleu 将机器翻译结果中每个单词在人工翻译中出现的次数作为分子，机器翻译结果总词数作为分母得到。但是容易出现错误，例如，机器翻译结果单纯为某个在人工翻译结果中出现的单词的重复，则按照上述方法得到的 Bleu 为 1，显然有误。改进的方法是将每个单词在人工翻译结果中出现的次数作为分子，在机器翻译结果中出现的次数作为分母。 上述方法是以单个词为单位进行统计，以单个词为单位的集合称为unigram（一元组）。而以成对的词为单位的集合称为bigram（二元组）。对每个二元组，可以统计其在机器翻译结果（$count$）和人工翻译结果（$count_{clip}$）出现的次数，计算 Bleu 得分。 以此类推，以 n 个单词为单位的集合称为n-gram（多元组），对应的 Blue（即翻译精确度）得分计算公式为： p_n = \\frac{\\sum_{\\text{n-gram} \\in \\hat y}count_{clip}(\\text{n-gram})}{\\sum_{\\text{n-gram} \\in \\hat y}count(\\text{n-gram})}对 N 个 pn 进行几何加权平均得到： p_{ave} = exp(\\frac{1}{N}\\sum^N_{i=1}log^{p_n})有一个问题是，当机器翻译结果短于人工翻译结果时，比较容易能得到更大的精确度分值，因为输出的大部分词可能都出现在人工翻译结果中。改进的方法是设置一个最佳匹配长度（Best Match Length），如果机器翻译的结果短于该最佳匹配长度，则需要接受 简短惩罚（Brevity Penalty，BP）： BP = \\begin{cases} 1, &MT\\_length \\ge BM\\_length \\\\ exp(1 - \\frac{MT\\_length}{BM\\_length}), &MT\\_length \\lt BM\\_length \\end{cases}因此，最后得到的 Bleu 得分为： Blue = BP \\times exp(\\frac{1}{N}\\sum^N_{i=1}log^{p_n})Bleu 得分的贡献是提出了一个表现不错的单一实数评估指标，因此加快了整个机器翻译领域以及其他文本生成领域的进程。 相关论文：neni et. al., 2002. A method for automatic evaluation of machine translation 注意力模型对于一大段文字，人工翻译一般每次阅读并翻译一小部分。因为难以记忆，很难每次将一大段文字一口气翻译完。同理，用 Seq2Seq 模型建立的翻译系统，对于长句子，Blue 得分会随着输入序列长度的增加而降低。 实际上，我们也并不希望神经网络每次去“记忆”很长一段文字，而是想让它像人工翻译一样工作。因此，注意力模型（Attention Model）被提出。目前，其思想已经成为深度学习领域中最有影响力的思想之一。 注意力模型的一个示例网络结构如上图所示。其中，底层是一个双向循环神经网络（BRNN），该网络中每个时间步的激活都包含前向传播和反向传播产生的激活： a^{\\langle t’ \\rangle} = ({\\overrightarrow a}^{\\langle t’ \\rangle}, {\\overleftarrow a}^{\\langle t’ \\rangle})顶层是一个“多对多”结构的循环神经网络，第 t 个时间步的输入包含该网络中前一个时间步的激活 $s^{\\langle t-1 \\rangle}$、输出 $y^{\\langle t-1 \\rangle}$ 以及底层的 BRNN 中多个时间步的激活 c，其中 c 有（注意分辨 α 和 a）： c^{\\langle t \\rangle} = \\sum_{t’}\\alpha^{\\langle t,t’ \\rangle}a^{\\langle t’ \\rangle}其中，参数 $\\alpha^{\\langle t,t’ \\rangle}$ 即代表着 $y^{\\langle t \\rangle}$ 对 $a^{\\langle t’ \\rangle}$ 的“注意力”，总有： \\sum_{t’}\\alpha^{\\langle t,t’ \\rangle} = 1我们使用 Softmax 来确保上式成立，因此有： \\alpha^{\\langle t,t’ \\rangle} = \\frac{exp(e^{\\langle t,t’ \\rangle})}{\\sum^{T_x}_{t'=1}exp(e^{\\langle t,t’ \\rangle})}而对于 $e^{\\langle t,t’ \\rangle}$，我们通过神经网络学习得到。输入为 $s^{\\langle t-1 \\rangle}$ 和 $a^{\\langle t’ \\rangle}$，如下图所示： 注意力模型的一个缺点是时间复杂度为 $O(n^3)$。 相关文章：带Attention机制的Seq2Seq框架梳理 相关论文： Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate Xu et. al., 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention：将注意力模型应用到图像标注中 语音识别在语音识别任务中，输入是一段以时间为横轴的音频片段，输出是文本。 音频数据的常见预处理步骤是运行音频片段来生成一个声谱图，并将其作为特征。以前的语音识别系统通过语言学家人工设计的音素（Phonemes）来构建，音素 指的是一种语言中能区别两个词的最小语音单位。现在的端到端系统中，用深度学习就可以实现输入音频，直接输出文本。 对于训练基于深度学习的语音识别系统，大规模的数据集是必要的。学术研究中通常使用 3000 小时长度的音频数据，而商业应用则需要超过一万小时的数据。 语音识别系统可以用注意力模型来构建，一个简单的图例如下： 用 CTC（Connectionist Temporal Classification）损失函数来做语音识别的效果也不错。由于输入是音频数据，使用 RNN 所建立的系统含有很多个时间步，且输出数量往往小于输入。因此，不是每一个时间步都有对应的输出。CTC 允许 RNN 生成下图红字所示的输出，并将两个空白符（blank）中重复的字符折叠起来，再将空白符去掉，得到最终的输出文本。 相关论文：Graves et al., 2006. Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks 触发词检测触发词检测（Trigger Word Detection）常用于各种智能设备，通过约定的触发词可以语音唤醒设备。 使用 RNN 来实现触发词检测时，可以将触发词对应的序列的标签设置为“1”，而将其他的标签设置为“0”。 代码示例序列模型和注意力机制","link":"/2018/03/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"},{"title":"循环训练模型","text":"前言自然语言和音频都是前后相互关联的数据，对于这些序列数据需要使用循环神经网络（Recurrent Neural Network，RNN）来进行处理。 使用 RNN 实现的应用包括下图中所示： 数学符号对于一个序列数据 $x$ ，用符号 $x^{⟨t⟩}$ 来表示这个数据中的第 $t$ 个元素，用 $y^{⟨t⟩}$ 来表示第 $t$ 个标签，用 $T_x$ 和 $T_y$ 来表示输入和输出的长度。对于一段音频，元素可能是其中的几帧；对于一句话，元素可能是一到多个单词。 第 $i$ 个序列数据的第 $t$ 个元素用符号 $x^{(i)⟨t⟩}$，第 $t$ 个标签即为 $y^{(i)⟨t⟩}$。对应即有 $T^{(i)}_x$ 和 $T^{(i)}_y$。 想要表示一个词语，需要先建立一个词汇表（Vocabulary），或者叫字典（Dictionary）。将需要表示的所有词语变为一个列向量，可以根据字母顺序排列，然后根据单词在向量中的位置，用 one-hot 向量（one-hot vector）来表示该单词的标签：将每个单词编码成一个 $R^{|V| \\times 1}$ 向量，其中 |V| 是词汇表中单词的数量。一个单词在词汇表中的索引在该向量对应的元素为 1，其余元素均为 0。 例如，’zebra’排在词汇表的最后一位，因此它的词向量表示为： w^{zebra} = \\left [ 0, 0, 0, ..., 1\\right ]^T补充：one-hot 向量是最简单的词向量。它的缺点是，由于每个单词被表示为完全独立的个体，因此单词间的相似度无法体现。例如单词 hotel 和 motel 意思相近，而与 cat 不相似，但是 (w^{hotel})^Tw^{motel} = (w^{hotel})^Tw^{cat} = 0 循环神经网络模型RNN 的目的是用来处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。 比如对于序列数据，使用标准神经网络存在以下问题： 对于不同的示例，输入和输出可能有不同的长度，因此输入层和输出层的神经元数量无法固定。 从输入文本的不同位置学到的同一特征无法共享。 模型中的参数太多，计算量太大。 为了解决这些问题，引入循环神经网络（Recurrent Neural Network，RNN）。RNN之所以称为循环神经网路，因为一个序列当前的输出与前面的输出也有关。 一种循环神经网络的结构如下图所示： 当元素 $x^{⟨t⟩}$ 输入对应时间步（Time Step）的隐藏层的同时，该隐藏层也会接收来自上一时间步的隐藏层的激活值 $a^{⟨t-1⟩}$，其中 $a^{⟨0⟩}$ 一般直接初始化为零向量。一个时间步输出一个对应的预测结果 $\\hat y^{⟨t⟩}$。 循环神经网络从左向右扫描数据，同时每个时间步的参数也是共享的，输入、激活、输出的参数对应为 $W{ax}$、$W{aa}$、$W_{ay}$。 目前我们看到的模型的问题是，只使用了这个序列中之前的信息来做出预测，即后文没有被使用。可以通过双向循环神经网络（Bidirectional RNN，BRNN）来解决这个问题。 前向传播过程的公式如下： 激活函数 $g_1$ 通常选择 tanh，有时也用 ReLU；g2可选 sigmoid 或 softmax，取决于需要的输出类型。 为了进一步简化公式以方便运算，可以将 $W{ax}$、$W{aa}$水平并列为一个矩阵 $W_{a}$，同时 $a^{⟨t-1⟩}$和 $x^{⟨t⟩}$ 堆叠成一个矩阵。则有： RNN反向传播为了计算反向传播过程，需要先定义一个损失函数。单个位置上（或者说单个时间步上）某个单词的预测值的损失函数采用 交叉熵损失函数，如下所示： L^{⟨t⟩}(\\hat y^{⟨t⟩}, y^{⟨t⟩}) = -y^{⟨t⟩}log\\hat y^{⟨t⟩} - (1 - y^{⟨t⟩})log(1-\\hat y^{⟨t⟩})将单个位置上的损失函数相加，得到整个序列的成本函数如下： J = L(\\hat y, y) = \\sum^{T_x}_{t=1} L^{⟨t⟩}(\\hat y^{⟨t⟩}, y^{⟨t⟩})循环神经网络的反向传播被称为 通过时间反向传播（Backpropagation through time），因为从右向左计算的过程就像是时间倒流。 更详细的计算公式如下： 附DNN前向反向传播公式： 不同结构某些情况下，输入长度和输出长度不一致。根据所需的输入及输出长度，循环神经网络可分为“一对一”、“多对一”、“多对多”等结构： 语言模型语言模型（Language Model）是根据语言客观事实而进行的语言抽象数学建模，能够估计某个序列中各元素出现的可能性。例如，在一个语音识别系统中，语言模型能够计算两个读音相近的句子为正确结果的概率，以此为依据作出准确判断。 建立语言模型所采用的训练集是一个大型的 语料库（Corpus），指数量众多的句子组成的文本。建立过程的第一步是 标记化（Tokenize），即建立字典；然后将语料库中的每个词表示为对应的 one-hot 向量。另外，需要增加一个额外的标记 EOS（End of Sentence）来表示一个句子的结尾。标点符号可以忽略，也可以加入字典后用 one-hot 向量表示。 对于语料库中部分特殊的、不包含在字典中的词汇，例如人名、地名，可以不必针对这些具体的词，而是在词典中加入一个 UNK（Unique Token）标记来表示。 将标志化后的训练集用于训练 RNN，过程如下图所示： 在第一个时间步中，输入的 $a^{⟨0⟩}$ 和 $x^{⟨1⟩}$ 都是零向量，$\\hat y^{⟨1⟩}$ 是通过 softmax 预测出的字典中每个词作为第一个词出现的概率；在第二个时间步中，输入的 x⟨2⟩是训练样本的标签中的第一个单词 $y^{⟨1⟩}$（即“cats”）和上一层的激活项 $a^{⟨1⟩}$，输出的 $y^{⟨2⟩}$ 表示的是通过 softmax 预测出的、单词“cats”后面出现字典中的其他每个词的条件概率。以此类推，最后就可以得到整个句子出现的概率。 定义损失函数为： L(\\hat y^{⟨t⟩}, y^{⟨t⟩}) = -\\sum_t y_i^{⟨t⟩} log \\hat y^{⟨t⟩}则成本函数为： J = \\sum_t L^{⟨t⟩}(\\hat y^{⟨t⟩}, y^{⟨t⟩}) 采样在训练好一个语言模型后，可以通过 采样（Sample） 新的序列来了解这个模型中都学习到了一些什么。 在第一个时间步输入 $a^{⟨0⟩}$ 和 $x^{⟨1⟩}$ 为零向量，输出预测出的字典中每个词作为第一个词出现的概率，根据 softmax 的分布进行随机采样（np.random.choice），将采样得到的 $\\hat y^{⟨1⟩}$ 作为第二个时间步的输入 $x^{⟨2⟩}$ 。以此类推，直到采样到 EOS，最后模型会自动生成一些句子，从这些句子中可以发现模型通过语料库学习到的知识。 这里建立的是基于词汇构建的语言模型。根据需要也可以构建基于字符的语言模型，其优点是不必担心出现未知标识（UNK），其缺点是得到的序列过多过长，并且训练成本高昂。因此，基于词汇构建的语言模型更为常用。 RNN 的梯度消失The cat,which already ate a bunch of food, was full.The cats,which already ate a bunch of food, were full. 对于以上两个句子，后面的动词单复数形式由前面的名词的单复数形式决定。但是基本的 RNN 不擅长捕获这种长期依赖关系。究其原因，由于梯度消失，在反向传播时，后面层的输出误差很难影响到较靠前层的计算，网络很难调整靠前的计算。 在反向传播时，随着层数的增多，梯度不仅可能指数型下降，也有可能指数型上升，即梯度爆炸。不过 梯度爆炸 比较容易发现，因为参数会急剧膨胀到数值溢出（可能显示为 NaN）。这时可以采用梯度修剪（Gradient Clipping）来解决：观察梯度向量，如果它大于某个阈值，则缩放梯度向量以保证其不会太大。相比之下，梯度消失问题更难解决。LSTM 和 GRU 的设计就是为了解决长期依赖问题，都可以作为缓解梯度消失问题的方案。 GRU（门控循环单元）GRU（Gated Recurrent Units, 门控循环单元）改善了 RNN 的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题。 The cat,which already ate a bunch of food, was full. 当我们从左到右读上面这个句子时，GRU 单元有一个新的变量称为 c，代表记忆细胞（Memory Cell），其作用是提供记忆的能力，记住例如前文主语是单数还是复数等信息。在时间 t，记忆细胞的值 $c^{⟨t⟩}$ 等于输出的激活值 $a^{⟨t⟩}$；$\\tilde c^{⟨t⟩}$ 代表下一个 c 的候选值。$Γ_u$ 代表 更新门（Update Gate），用于决定什么时候更新记忆细胞的值。以上结构的具体公式为： 当使用 sigmoid 作为激活函数 $σ$ 来得到 $Γ_u$时，Γu 的值在 0 到 1 的范围内，且大多数时间非常接近于 0 或 1。当 $Γ_u = 1$时，$c^{⟨t⟩}$被更新为 $\\tilde c^{⟨t⟩}$，否则保持为 $c^{⟨t-1⟩}$。因为 $Γ_u$ 可以很接近 0，因此 $c^{⟨t⟩}$ 几乎就等于 $c^{⟨t-1⟩}$。在经过很长的序列后，c 的值依然被维持，从而实现“记忆”的功能。 以上实际上是简化过的 GRU 单元，但是蕴涵了 GRU 最重要的思想。完整的 GRU 单元添加了一个新的相关门（Relevance Gate） $Γ_r$，表示 $\\tilde c^{⟨t⟩}$和 $c^{⟨t⟩}$ 的相关性。因此，表达式改为如下所示： 相关论文： ho et al., 2014. On the properties of neural machine translation: Encoder-decoder approaches Chung et al., 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling LSTM（长短期记忆）LSTM（Long Short Term Memory，长短期记忆）网络比 GRU 更加灵活和强大，它额外引入了遗忘门（Forget Gate） $Γ_f$和输出门（Output Gate） $Γ_o$。公式如下： 将多个 LSTM 单元按时间次序连接起来，就得到一个 LSTM 网络。 以上是简化版的 LSTM。在更为常用的版本中，几个门值不仅取决于 $a^{⟨t-1⟩}$ 和 $x^{⟨t⟩}$，有时也可以偷窥上一个记忆细胞输入的值 $c^{⟨t-1⟩}$，这被称为窥视孔连接（Peephole Connection)。这时，和 GRU 不同，$c^{⟨t-1⟩}$ 和门值是一对一的。 Tensorflow 官网推荐了一篇 [伟大的文章](#https://colah.github.io/posts/2015-08-Understanding-LSTMs/)， 特别介绍递归神经网络和LSTM 相关论文：Hochreiter &amp; Schmidhuber 1997. Long short-term memory 双向循环神经网络（BRNN）单向的循环神经网络在某一时刻的预测结果只能使用之前输入的序列信息。双向循环神经网络（Bidirectional RNN，BRNN）可以在序列的任意位置使用之前和之后的数据。其工作原理是增加一个反向循环层，结构如下图所示： 因此，有 y^{⟨t⟩} = g(W_y[\\overrightarrow a^{⟨t⟩}, \\overleftarrow a^{⟨t⟩}] + b_y)这个改进的方法不仅能用于基本的 RNN，也可以用于 GRU 或 LSTM。缺点 是需要完整的序列数据，才能预测任意位置的结果。例如构建语音识别系统，需要等待用户说完并获取整个语音表达，才能处理这段语音并进一步做语音识别。因此，实际应用会有更加复杂的模块。 深度循环神经网络（DRNN)循环神经网络的每个时间步上也可以包含多个隐藏层，形成深度循环神经网络（Deep RNN)。结构如下图所示： 以 $a^{[2]⟨3⟩}$ 为例，有 $a^{[2]⟨3⟩} = g(W_a^{[2]}[a^{[2]⟨2⟩}, a^{[1]⟨3⟩}] + b_a^{[2]})$ 代码示例循环序列模型","link":"/2018/02/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"},{"title":"自然语言处理与词嵌入","text":"词嵌入one-hot 向量将每个单词表示为完全独立的个体，不同词向量都是正交的，因此单词间的相似度无法体现。 换用特征化表示方法能够解决这一问题。我们可以通过用语义特征作为维度来表示一个词，因此语义相近的词，其词向量也相近。 将高维的词嵌入“嵌入”到一个二维空间里，就可以进行可视化。常用的一种可视化算法是 t-SNE 算法。在通过复杂而非线性的方法映射到二维空间后，每个词会根据语义和相关程度聚在一起。相关论文：van der Maaten and Hinton., 2008. Visualizing Data using t-SNE 词嵌入（Word Embedding）是 NLP 中语言模型与表征学习技术的统称，概念上而言，它是指把一个维数为所有词的数量的高维空间（one-hot 形式表示的词）“嵌入”到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。对大量词汇进行词嵌入后获得的词向量，可用于完成 命名实体识别（Named Entity Recognition） 等任务。 词嵌入与迁移学习用词嵌入做迁移学习可以降低学习成本，提高效率。其步骤如下： 从大量的文本集中学习词嵌入，或者下载网上开源的、预训练好的词嵌入模型； 将这些词嵌入模型迁移到新的、只有少量标注训练集的任务中； 可以选择是否微调词嵌入。当标记数据集不是很大时可以省下这一步。 词嵌入与类比推理词嵌入可用于类比推理。例如，给定对应关系“男性（Man）”对“女性（Woman）”，想要类比出“国王（King）”对应的词汇。则可以有 $e{man} - e{woman} \\approx e{king} - e? $ ，之后的目标就是找到词向量 w，来找到使相似度 $sim(ew, e{king} - e{man} + e{woman})$ 最大。 一个最常用的相似度计算函数是 余弦相似度（cosine similarity）。公式为： sim(u, v) = \\frac{u^T v}{|| u ||_2 || v ||_2}相关论文：Mikolov et. al., 2013, Linguistic regularities in continuous space word representations 嵌入矩阵 不同的词嵌入方法能够用不同的方式学习到一个嵌入矩阵（Embedding Matrix） E。将字典中位置为 i 的词的 one-hot 向量表示为 $o_i$，词嵌入后生成的词向量用 $e_i$ 表示，则有： E \\cdot o_i = e_i但在实际情况下一般不这么做。因为 one-hot 向量维度很高，且几乎所有元素都是 0，这样做的效率太低。因此，实践中直接用专门的函数查找矩阵 E 的特定列。 学习词嵌入神经概率语言模型神经概率语言模型（Neural Probabilistic Language Model） 构建了一个能够通过上下文来预测未知词的神经网络，在训练这个语言模型的同时学习词嵌入。 训练过程中，将语料库中的某些词作为目标词，以目标词的部分上下文作为输入，Softmax 输出的预测结果为目标词。嵌入矩阵 E 和 w、b 为需要通过训练得到的参数。这样，在得到嵌入矩阵后，就可以得到词嵌入后生成的词向量。 相关论文：Bengio et. al., 2003, A neural probabilistic language model Word2VecWord2Vec 是一种简单高效的词嵌入学习算法，包括 2 种模型： Skip-gram (SG)：根据词预测目标上下文 Continuous Bag of Words (CBOW)：根据上下文预测目标词 每种语言模型又包含负采样（Negative Sampling）和分级的 Softmax（Hierarchical Softmax）两种训练方法。 相关论文：Mikolov et. al., 2013. Efficient estimation of word representations in vector space. Skip-gramSkip-Gram 设某个词为 c，该词的一定词距内选取一些配对的目标上下文 t，则该网路仅有的一个 Softmax 单元输出条件概率： p(t|c) = \\frac{exp(\\theta_t^T e_c)}{\\sum^m_{j=1}exp(\\theta_j^T e_c)}$θ_t$ 是一个与输出 t 有关的参数，其中省略了用以纠正偏差的参数。损失函数仍选用交叉熵： L(\\hat y, y) = -\\sum^m_{i=1}y_ilog\\hat y_i在此 Softmax 分类中，每次计算条件概率时，需要对词典中所有词做求和操作，因此计算量很大。解决方案之一是使用一个分级的 Softmax 分类器（Hierarchical Softmax Classifier），形如二叉树。在实践中，一般采用霍夫曼树（Huffman Tree）而非平衡二叉树，常用词在顶部。 如果在语料库中随机均匀采样得到选定的词 c，则 ‘the’, ‘of’, ‘a’, ‘and’ 等出现频繁的词将影响到训练结果。因此，采用了一些策略来平衡选择。 CBOW CBOW 模型的工作方式与 Skip-gram 相反，通过采样上下文中的词来预测中间的词。 吴恩达老师没有深入去讲 CBOW。想要更深入了解的话，推荐资料 word2vec原理推导与代码分析-码农场（中文）以及 课程 cs224n 的 notes1（英文）。 负采样为了解决 Softmax 计算较慢的问题，Word2Vec 的作者后续提出了 负采样（Negative Sampling）模型。对于监督学习问题中的分类任务，在训练时同时需要正例和负例。在分级的 Softmax 中，负例放在二叉树的根节点上；而对于负采样，负例是随机采样得到的。 如上图所示，当输入的词为一对上下文-目标词时，标签设置为 1（这里的上下文也是一个词）。另外任意取 k 对非上下文-目标词作为负样本，标签设置为 0。对于小数据集，k 取 5~20 较为合适；而当有大量数据时，k 可以取 2~5。 改用多个 Sigmoid 输出上下文-目标词（c, t）为正样本的概率： P(y=1 | c, t) = \\sigma(\\theta_t^Te_c)其中，$\\theta_t$、$e_c$ 分别代表目标词和上下文的词向量。 之前训练中每次要更新 n 维的多分类 Softmax 单元（n 为词典中词的数量）。现在每次只需要更新 k+1 维的二分类 Sigmoid 单元，计算量大大降低。 关于计算选择某个词作为负样本的概率，作者推荐采用以下公式（而非经验频率或均匀分布）： p(w_i) = \\frac{f(w_i)^{\\frac{3}{4}}}{\\sum^m_{j=0}f(w_j)^{\\frac{3}{4}}}其中，$f(w_i)$ 代表语料库中单词 wi 出现的频率。上述公式更加平滑，能够增加低频词的选取可能。 相关论文：Mikolov et. al., 2013. Distributed representation of words and phrases and their compositionality GloveGloVe（Global Vectors）是另一种流行的词嵌入算法。Glove 模型基于语料库统计了词的 共现矩阵 X，X中的元素 $X_{ij}$ 表示单词 i 和单词 j “为上下文-目标词”的次数。之后，用梯度下降法最小化以下损失函数： J = \\sum^N_{i=1}\\sum^N_{j=1}f(X_{ij})(\\theta^t_ie_j + b_i + b_j - log(X_{ij}))^2其中，$θi$、$e_j$ 是单词 i 和单词 j 的词向量；$b_i$、$b_j$；$f()$ 是一个用来避免 $X{ij}=0$ 时 $log(X{ij})$ 为负无穷大、并在其他情况下调整权重的函数。$X{ij}=0$ 时，$f(X_{ij}) = 0$。 “为上下文-目标词”可以代表两个词出现在同一个窗口。在这种情况下，$θ_i$ 和 $e_j$ 是完全对称的。因此，在训练时可以一致地初始化二者，使用梯度下降法处理完以后取平均值作为二者共同的值。 相关论文：Pennington st. al., 2014. Glove: Global Vectors for Word Representation 最后，使用各种词嵌入算法学到的词向量实际上大多都超出了人类的理解范围，难以从某个值中看出与语义的相关程度。 情感分类情感分类是指分析一段文本对某个对象的情感是正面的还是负面的，实际应用包括舆情分析、民意调查、产品意见调查等等。情感分类的问题之一是标记好的训练数据不足。但是有了词嵌入得到的词向量，中等规模的标记训练数据也能构建出一个效果不错的情感分类器。 如上图所示，用词嵌入方法获得嵌入矩阵 E 后，计算出句中每个单词的词向量并取平均值，输入一个 Softmax 单元，输出预测结果。这种方法的优点是适用于任何长度的文本；缺点是没有考虑词的顺序，对于包含了多个正面评价词的负面评价，很容易预测到错误结果。 使用 RNN 能够实现一个效果更好的情感分类器： 词嵌入除偏语料库中可能存在性别歧视、种族歧视、性取向歧视等非预期形式偏见（Bias），这种偏见会直接反映到通过词嵌入获得的词向量。例如，使用未除偏的词嵌入结果进行类比推理时，”Man” 对 “Computer Programmer” 可能得到 “Woman” 对 “Housemaker” 等带有性别偏见的结果。词嵌入除偏的方法有以下几种： 1. 中和本身与性别无关词汇 对于“医生（doctor）”、“老师（teacher）”、“接待员（receptionist）”等本身与性别无关词汇，可以中和（Neutralize）其中的偏见。首先用“女性（woman）”的词向量减去“男性（man）”的词向量，得到的向量 $g=e{woman}−e{man}$ 就代表了“性别（gender）”。假设现有的词向量维数为 50，那么对某个词向量，将 50 维空间分成两个部分：与性别相关的方向 g 和与 g 正交的其他 49 个维度 $g_{\\perp}$。如下左图： 而除偏的步骤，是将要除偏的词向量（左图中的 $e{receptionist}$）在向量 g 方向上的值置为 0，变成右图所示的 $e^{debiased}{receptionist}$。 2. 均衡本身与性别有关词汇 对于“男演员（actor）”、“女演员（actress）”、“爷爷（grandfather）”等本身与性别有关词汇，中和“婴儿看护人（babysit）”中存在的性别偏见后，还是无法保证它到“女演员（actress）”与到“男演员（actor）”的距离相等。对这样一对性别有关的词，除偏的过程是均衡（Equalization）它们的性别属性。其核心思想是确保一对词（actor 和 actress）到 g⊥ 的距离相等。 相关论文：Bolukbasi et. al., 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings 代码示例自然语言处理与词嵌入","link":"/2018/03/02/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5/"},{"title":"吴恩达目标检测课堂笔记","text":"目标检测是计算机视觉领域中一个新兴的应用方向，其任务是对输入图像进行分类的同时，检测图像中是否包含某些目标，并对他们准确定位并标识。 本文所涉及的目标检测算法是 Ng 课堂上所讲的 YOLO，除此之外流行的还有 RCNN、Fast RCNN、Faster RCNN 和 SSD。 相关链接： 代码示例 YOLO: Real-Time Object Detection 目标检测 ppt 文件 密码：kt2h SSD github项目 目标定位定位分类问题不仅要求判断出图片中物体的种类，还要在图片中标记出它的具体位置，用边框（Bounding Box，或者称包围盒）把物体圈起来。一般来说，定位分类问题通常只有一个较大的对象位于图片中间位置；而在目标检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。 为了定位图片中汽车的位置，可以让神经网络多输出 4 个数字，标记为 $b_x$、$b_y$、$b_h$、$b_w$。将图片左上角标记为 (0, 0)，右下角标记为 (1, 1)，则有： 红色方框的中心点：($b_x$，$b_y$) 边界框的高度：$b_h$ 边界框的宽度：$b_w$ 因此，训练集不仅包含对象分类标签，还包含表示边界框的四个数字。定义目标标签 Y 如下： \\left[\\begin{matrix}P_c\\\\ b_x\\\\ b_y\\\\ b_h\\\\ b_w\\\\ c_1\\\\ c_2\\\\ c_3\\end{matrix}\\right]则有： P_c=1, Y = \\left[\\begin{matrix}1\\\\ b_x\\\\ b_y\\\\ b_h\\\\ b_w\\\\ c_1\\\\ c_2\\\\ c_3\\end{matrix}\\right]其中，$c_n$ 表示存在第 n个种类的概率；如果 $P_c=0$，表示没有检测到目标，则输出标签后面的 7 个参数都是无效的，可以忽略（用 ? 来表示）。 P_c=0, Y = \\left[\\begin{matrix}0\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\\\ ?\\end{matrix}\\right]损失函数可以表示为 $L(\\hat y, y)$，如果使用平方误差形式，对于不同的 $P_c$有不同的损失函数（注意下标 i指标签的第 i个值）： $P_c=1$，即$y_1=1$：$L(\\hat y,y)=(\\hat y_1-y_1)^2+(\\hat y_2-y_2)^2+\\cdots+(\\hat y_8-y_8)^2$ $P_c=0$，即$y_1=0$：$L(\\hat y,y)=(\\hat y_1-y_1)^2$ 除了使用平方误差，也可以使用逻辑回归损失函数，类标签 $c_1$,$c_2$,$c_3$ 也可以通过 softmax 输出。相比较而言，平方误差已经能够取得比较好的效果。 特征点检测神经网络可以像标识目标的中心点位置那样，通过输出图片上的特征点，来实现对目标特征的识别。在标签中，这些特征点以多个二维坐标的形式表示。 通过检测人脸特征点可以进行情绪分类与判断，或者应用于 AR 领域等等。也可以透过检测姿态特征点来进行人体姿态检测。 目标检测想要实现目标检测，可以采用基于滑动窗口的目标检测（Sliding Windows Detection）算法。该算法的步骤如下： 训练集上搜集相应的各种目标图片和非目标图片，样本图片要求尺寸较小，相应目标居于图片中心位置并基本占据整张图片。 使用训练集构建 CNN 模型，使得模型有较高的识别率。 选择大小适宜的窗口与合适的固定步幅，对测试图片进行从左到右、从上倒下的滑动遍历。每个窗口区域使用已经训练好的 CNN 模型进行识别判断。 可以选择更大的窗口，然后重复第三步的操作。 滑动窗口目标检测的优点是原理简单，且不需要人为选定目标区域；缺点是需要人为直观设定滑动窗口的大小和步幅。滑动窗口过小或过大，步幅过大均会降低目标检测的正确率。另外，每次滑动都要进行一次 CNN 网络计算，如果滑动窗口和步幅较小，计算成本往往很大。 所以，滑动窗口目标检测算法虽然简单，但是性能不佳，效率较低。 基于卷积的滑动窗口实现相比从较大图片多次截取，在卷积层上应用滑动窗口目标检测算法可以提高运行速度。所要做的仅是将全连接层换成卷积层，即使用与上一层尺寸一致的滤波器进行卷积运算。 如图，对于 16x16x3 的图片，步长为 2，CNN 网络得到的输出层为 2x2x4。其中，2x2 表示共有 4 个窗口结果。对于更复杂的 28x28x3 的图片，得到的输出层为 8x8x4，共 64 个窗口结果。最大池化层的宽高和步长相等。 运行速度提高的原理：在滑动窗口的过程中，需要重复进行 CNN 正向计算。因此，不需要将输入图片分割成多个子集，分别执行向前传播，而是将它们作为一张图片输入给卷积网络进行一次 CNN 正向计算。这样，公共区域的计算可以共享，以降低运算成本。 相关论文：Sermanet et al., 2014. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks 边框预测（YOLO）在上述算法中，边框的位置可能无法完美覆盖目标，或者大小不合适，或者最准确的边框并非正方形，而是长方形。 YOLO（You Only Look Once） 算法可以用于得到更精确的边框。YOLO 算法将原始图片划分为 n×n 网格，并将目标定位一节中提到的图像分类和目标定位算法，逐一应用在每个网格中，每个网格都有标签如： \\left[\\begin{matrix}P_c\\\\ b_x\\\\ b_y\\\\ b_h\\\\ b_w\\\\ c_1\\\\ c_2\\\\ c_3\\end{matrix}\\right]若某个目标的中点落在某个网格，则该网格负责检测该对象。 如上面的示例中，如果将输入的图片划分为 3×3 的网格、需要检测的目标有 3 类，则每一网格部分图片的标签会是一个 8 维的列矩阵，最终输出的就是大小为 3×3×8 的结果。要得到这个结果，就要训练一个输入大小为 100×100×3，输出大小为 3×3×8 的 CNN。在实践中，可能使用更为精细的 19×19 网格，则两个目标的中点在同一个网格的概率更小。 YOLO 算法的优点： 和图像分类和目标定位算法类似，显式输出边框坐标和大小，不会受到滑窗分类器的步长大小限制。 仍然只进行一次 CNN 正向计算，效率很高，甚至可以达到实时识别。 如何编码边框 $b_x$、$b_y$、$b_h$、$b_w$？YOLO 算法设 $b_x$、$b_y$、$b_h$、$b_w$ 的值是相对于网格长的比例。则 $b_x$、$b_y$ 在 0 到 1 之间，而 $b_h$、$b_w$ 可以大于 1。当然，也有其他参数化的形式，且效果可能更好。这里只是给出一个通用的表示方法。 相关论文：Redmon et al., 2015. You Only Look Once: Unified, Real-Time Object Detection。 Ng 认为该论文较难理解。 交互比交互比（IoU, Intersection Over Union）函数用于评价对象检测算法，它计算预测边框和实际边框交集（I）与并集（U）之比： IoU = \\frac{I}{U}IoU 的值在 0～1 之间，且越接近 1 表示目标的定位越准确。IoU 大于等于 0.5 时，一般可以认为预测边框是正确的，当然也可以更加严格地要求一个更高的阈值。 非极大值抑制YOLO 算法中，可能有很多网格检测到同一目标。非极大值抑制（Non-max Suppression）会通过清理检测结果，找到每个目标中点所位于的网格，确保算法对每个目标只检测一次。 进行非极大值抑制的步骤如下： 将包含目标中心坐标的可信度 P_c 小于阈值（例如 0.6）的网格丢弃； 选取拥有最大 P_c 的网格； 分别计算该网格和其他所有网格的 IoU，将 IoU 超过预设阈值的网格丢弃； 重复第 2~3 步，直到不存在未处理的网格。 上述步骤适用于单类别目标检测。进行多个类别目标检测时，对于每个类别，应该单独做一次非极大值抑制。 Anchor Boxes到目前为止，我们讨论的情况都是一个网格只检测一个对象。如果要将算法运用在多目标检测上，需要用到 Anchor Boxes。一个网格的标签中将包含多个 Anchor Box，相当于存在多个用以标识不同目标的边框。 在上图示例中，我们希望同时检测人和汽车。因此，每个网格的的标签中含有两个 Anchor Box。输出的标签结果大小从 3×3×8 变为 3×3×16。若两个 P_c 都大于预设阈值，则说明检测到了两个目标。 在单目标检测中，图像中的目标被分配给了包含该目标中点的那个网格；引入 Anchor Box 进行多目标检测时，图像中的目标则被分配到了包含该目标中点的那个网格以及具有最高 IoU 值的该网格的 Anchor Box。 Anchor Boxes 也有局限性，对于同一网格有三个及以上目标，或者两个目标的 Anchor Box 高度重合的情况处理不好。 Anchor Box 的形状一般通过人工选取。高级一点的方法是用 k-means 将两类对象形状聚类，选择最具代表性的 Anchor Box。 如果对以上内容不是很理解，在“3.9 YOLO 算法”一节中视频的第 5 分钟，有一个更为直观的示例。 R-CNN前面介绍的滑动窗口目标检测算法对一些明显没有目标的区域也进行了扫描，这降低了算法的运行效率。为了解决这个问题，R-CNN（Region CNN，带区域的 CNN）被提出。通过对输入图片运行图像分割算法，在不同的色块上找出候选区域（Region Proposal），就只需要在这些区域上运行分类器。 R-CNN 的缺点是运行速度很慢，所以有一系列后续研究工作改进。例如 Fast R-CNN（与基于卷积的滑动窗口实现相似，但得到候选区域的聚类步骤依然很慢）、Faster R-CNN（使用卷积对图片进行分割）。不过大多数时候还是比 YOLO 算法慢。 相关论文： R-CNN：Girshik et al., 2013. Rich feature hierarchies for accurate object detection and semantic segmentation Fast R-CNN：Girshik, 2015. Fast R-CNN Faster R-CNN：Ren et al., 2016. Faster R-CNN: Towards real-time object detection with region proposal networks","link":"/2018/07/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%AF%BE%E5%A0%82%E7%AC%94%E8%AE%B0/"},{"title":"SVM","text":"持续更新中。。。 示例代码 SVM目标函数推导 大边界的直观理解与数学解释 核函数 常用核函数及核函数的条件 逻辑回归、SVM和神经网络使用场景 吴恩达SVM视频笔记 支持向量机通俗导论（理解SVM的三层境界） 带核的SVM为什么能分类非线性问题 SVM常见问题 SVM目标函数推导SVM就是寻找一个超平面，将所有的数据点尽可能的分开，而且数据点离超平面距离越远越好。相对逻辑回归和神经网络，SVM在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。 SVM 模型可以由 LR 模型推导而来，下面是 LR 的直观理解： LR 单个样本的损失函数： 接着我们对 LR 的代价函数（所有样本）进行转换，首先去掉 1/m 这一项，这也会得出同样的 $\\theta$ 最优值，然后令 $C=1/\\lambda$ 得到代价函数： \\min_\\limits{\\theta}C\\sum_\\limits{i=1}^{m}\\left[y^{(i)}{\\cos}t_{1}\\left(\\theta^{T}x^{(i)}\\right)+\\left(1-y^{(i)}\\right){\\cos}t\\left(\\theta^{T}x^{(i)}\\right)\\right]+\\frac{1}{2}\\sum_\\limits{i=1}^{n}\\theta^{2}_{j}我们最小化这个代价函数，令第一项为0，获得包含参数 $\\theta$ 的第二项，SVM就是用第二项来直接预测值等于0还是1。其实支持向量机做的全部事情，就是极小化参数向量范数的平方，或者说长度的平方。学习参数 $\\theta$ 就是支持向量机假设函数的形式，这就是支持向量机数学上的定义。 根据逻辑回归 $h_\\theta \\left( x \\right)$ 的公式，我们知道当 $\\theta^Tx$ 大于0的话，模型代价函数值为1，类似地，如果你有一个负样本，则仅需要 $\\theta^Tx$ 小于0就会将负例正确分离 。 但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本，即不仅仅要求大于0，我们需要的是比0值大很多，比如大于等于1，我也想这个比0小很多，比如我希望它小于等于-1，这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说安全的间距因子。 所以最小化问题可以转换为： 这就是 SVM 的最终目标函数。 大边界的直观理解与数学解释SVM 不仅需要能分类，还需要较高的鲁棒性，需要努力寻找一个最大间距（下图中的黑色超平面）来分离样本。所以 SVM 有时被称为大间距分类器。 可是为什么 SVM 能得到最大间距分类器呢？我们仍然从 SVM 的目标函数进行分析。 在这之前首先说下 向量内积的相关知识， $\\left|| u |\\right|$表示 u 的范数，即 u 的长度，即向量 u 的欧几里得长度，并且 $\\left|| u \\right||=\\sqrt{u{1}^{2}+u{2}^{2}}$。我们将向量 v 投影到向量 u 上，做一个直角投影，接下来我度量这条红线的长度。我称这条红线的长度为 p ，因此内积 $u^Tv=p\\centerdot \\left|| u |\\right|$。注意，如果 u 和 v 之间的夹角大于90度，内积是个负数。 SVM 的目标函数为： 根据向量内积的知识对目标函数进行转换，同时假设只有两个样本，每个样本只有两个维度，令 $\\theta_0 = 0$，$n = 2$ 得到 现在我们假设有如上图所示的样本分布，那么 SVM 会选择怎样的决策边界呢？ SVM 在分类的时候，为了不让模型过于复杂，会让 $\\theta$ 的范数需要尽可能小，那么相应的，P也就是投影需要尽可能的大。我们知道 SVM 选择的参数 $\\theta$ 的方向是和决策界是90度正交的，所以很明显，下图中，右边的绿色决策边界更理想，因为此刻样本在 $\\theta$ 方向上的投影大很多。这就是为什么支持向量机最终会找到大间距分类器的原因。 \\min_\\limits{\\theta}C\\sum_\\limits{i=1}^{m}\\left[y^{(i)}{\\cos}t_{1}\\left(\\theta^{T}x^{(i)}\\right)+\\left(1-y^{(i)}\\right){\\cos}t\\left(\\theta^{T}x^{(i)}\\right)\\right]+\\frac{1}{2}\\sum_\\limits{i=1}^{n}\\theta^{2}_{j}但是最大间距分类只有当参数 C 是非常大的时候才起效，回顾 $C=1/\\lambda$ ，因此：（1）C 较大时，相当于 $\\lambda$ 较小，可能会导致过拟合，高方差。（2）C 较小时，相当于 $\\lambda$ 较大，可能会导致低拟合，高偏差。 比如你加了上图这个异常样本，为了将样本用最大间距分开，SVM 会将参数 C 设置的非常大，得到红色的决策边界，这是非常不明智的。但是 如果 C 设置的小一点，你最终会得到这条黑线。当 C 不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。甚至当你的数据不是线性可分的时候，支持向量机也可以给出好的结果。 核函数SVM 在处理非线性可分问题时，会使用核函数。核函数具体怎么来的，可以概括为以下三点：（1）实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去（2）但是如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小高到可怕，几乎不可计算。（3）此时，核函数隆重登场。核函数的价值在于它虽然也是将特征从低维映射到高维，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上。 下面进行详细说明。 假设有两个样本 X1 和 X2，它们是二维平面的两个坐标。样本分布如图所示 此刻为了线性可分，我们想到一个方法，把样本投射到高纬空间，如图所示 我们知道一条二次曲线（圆圈是二次曲线的一种特殊情况）的方程可以写作这样的形式： 那么对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，会得到五个维度，所以需要映射到五维空间，即R2→R5。假设新的空间的五个坐标的值分别为 Z1=X1, Z2=X1^2, Z3=X2, Z4=X2^2, Z5=X1X2，那么上面的方程在新的坐标系下可以写作 这个过程其实涉及了两个步骤：（1）首先使用一个非线性映射将数据变换到一个高纬特征空间F，（2）然后在高纬特征空间使用内积的公式进行计算，进行线性分类。 但是这里有一个很大的问题，我们对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；如果原始空间是三维，那么我们会得到 19 维的新空间，这个数目是呈爆炸性增长的，这给计算带来了非常大的困难，而且如果遇到无穷维的情况，就根本无从计算。 那么怎么办呢？我们首先来看下高纬空间的内积计算（尖括号代表内积计算，$ϕ$ 代表低纬到高纬的映）： 另外，我们又注意到： 二者有很多相似的地方，实际上，我们只要把某几个维度线性缩放一下，然后再加上一个常数维度，具体来说，上面这个式子的计算结果实际上和映射 之后的内积的结果是相等的，那么区别在于什么地方呢？1. 一个是映射到高维空间中，然后再根据内积的公式进行计算；2. 而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。 我们把这里的计算两个向量在隐式映射过后的空间中的内积的函数叫做核函数。核函数能简化映射空间中的内积运算——刚好“碰巧”的是，在我们的 SVM 里需要计算的地方数据向量总是以内积的形式出现的。 常用的核函数： 刚才所举例子用的就是多项式核（R = 1，d = 2），但是 高斯核 相对而言用的最广泛，注意高斯核与正态分布没什么实际上的关系，只是看上去像而已。 至于线性核，这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了(意思是说，我们有时候写代码，或写公式的时候，只要写个通用模板或表达式，然后再代入不同的核，不必要分别写一个线性的和一个非线性的)。 逻辑回归、SVM和神经网络使用场景由逻辑回归的目标函数可以近似推导出不带核函数的 SVM 的目标函数，所以逻辑回归和不带核函数的 SVM 是非常相似的算法，它们通常会做相似的事情，并给出相似的结果。但当模型的复杂度上升，比如当你有多达1万的样本时，也可能是5万，你的特征变量数量就会非常大。在这样一个非常常见的体系里，不带核函数的 SVM 就会表现得尤其突出。 那么如何在 LR 和 SVM 之间进行选择呢？下面是一些普遍使用的准则：n 为特征数，m 为训练样本数。（1）如果相较于 m 而言，n 要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。（2）如果 n 较小，而且 m 大小中等，例如 n 在 1-1000 之间，而 m 在10-10000之间，使用高斯核函数的支持向量机。（3）如果 n 较小，而 m 较大，例如 n 在1-1000之间，而 m 大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。 但是通常更加重要的是：你有多少数据，你有多熟练是否擅长做误差分析和排除学习算法，指出如何设定新的特征变量和找出其他能决定你学习算法的变量等方面，这些方面会比你使用 SVM，LR 还是神经网络更加重要。 支持向量机通俗导论（理解SVM的三层境界）在线阅读链接：http://blog.csdn.net/v_july_v/article/details/7624837网盘下载地址：https://pan.baidu.com/s/1htfvbzI 密码：qian建议下载网盘里的pdf阅读，文档附带完整书签。 常用核函数及核函数的条件推荐一篇文章： svm核函数的理解和选择 带核的SVM为什么能分类非线性问题核函数的本质是两个函数的內积，通过核函数，SVM将低维数据隐射到高维空间，在高维空间，非线性问题转化为线性问题，详见 核函数。","link":"/2017/10/22/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/SVM/"},{"title":"聚类算法","text":"持续更新中。。。 示例代码 思维导图 聚类简介 kmeans Mini Batch K-Means 二分KMeans算法 Kmeans++ 层次聚类 密度聚类 谱聚类 聚类简介聚类就是对大量未知标注的数据集，按照数据 内部存在的数据特征 将数据集划分为 多个不同的类别，使 类别内的数据比较相似，类别之间的数据相似度比较小； 聚类算法的重点是计算样本项之间的相似度，有时候也称为样本间的距离。 聚类属于无监督学习，和分类算法的区别：（1）分类算法是有监督学习，基于有标注的历史数据进行算法模型构建（2）聚类算法是无监督学习，数据集中的数据是没有标注的 聚类算法的衡量指标: 混淆矩阵 均一性 完整性 V-measure 调整兰德系数(ARI) 调整互信息(AMI) 轮廓系数(Silhouette) kmeansK-means算法，也称为K-平均或者K-均值，是一种使用广泛的最基础的聚类算法，一般作为接触聚类算法的第一个算法。 假设输入样本为 T=X1, X2, …, Xm; 则 算法步骤为（使用欧几里得距离公式）：（1）随机初始化 k 个类别中心 a1, a2,…ak;（2）对于每个样本 Xi，将其标记为距离类别中心 aj 最近的类别 j（3）更新每个类别的中心点 aj 为隶属该类别的所有样本的均值（4）重复上面两步操作，直到达到某个中止条件 中止条件：迭代次数、最小平方误差MSE、簇中心点变化率 K-means算法在迭代的过程中使用所有点的均值作为新的质点(中心点)，如果簇中存在异常点，将导致均值偏差比较严重。K-means算法是初值敏感的，选择不同的初始值可能导致不同的簇划分规则。 缺点： K值是用户给定的，在进行数据处理前，K值是未知的，不同的K值得到的结果也不一样； 对初始簇中心点是敏感的 不适合发现非凸形状的簇或者大小差别较大的簇 特殊值(离群值)对模型的影响比较大 优点： 理解容易，聚类效果不错 处理大数据集的时候，该算法可以保证较好的伸缩性和高效率 当簇近似高斯分布的时候，效果非常不错 Mini Batch K-Means当需要聚类的数据量非常大的时候，效率是最重要的诉求。 Mini Batch K-Means算法是K-Means算法的一种优化变种，采用小规模的数据子集(每次训练使用的数据集是在训练算法的时候随机抽取的数据子集)减少计算时间，同时试图优化目标函数；Mini Batch K-Means算法可以减少K-Means算法的收敛时间，而且产生的结果效果只是略差于标准K-Means算法。 算法步骤如下： 首先 抽取部分数据集，使用K-Means算法构建出K个聚簇点的模型 继续抽取训练数据集中的部分数据集样本数据，并将其添加到模型中，分配给距离最近的聚簇中心点 更新聚簇的中心点值 循环迭代第二步和第三步操作，直到中心点稳定或者达到迭代次数，停止计算操作 二分KMeans算法解决K-Means算法对初始簇心比较敏感的问题，二分K-Means算法是一种弱化初始质心的一种算法。 具体思路步骤如下： 将所有样本数据作为一个簇放到一个队列中 从队列中选择一个簇进行K-means算法划分，划分为两个子簇，并将子簇添加到队列中 循环迭代第二步操作，直到中止条件达到(聚簇数量、最小平方误差、迭代次数等) 队列中的簇就是最终的分类簇集合 从队列中 选择划分聚簇 的规则一般有两种方式； 对所有簇计算误差和SSE(SSE也可以认为是距离函数的一种变种)，选择SSE最大的聚簇进行划分操作(优选这种策略) 选择样本数据量最多的簇进行划分操作 由于计算量大，二分KMeans优化算法使用的比较少 Kmeans++解决K-Means算法对初始簇心比较敏感的问题，使用非常广泛。 K-Means++算法和K-Means算法的区别主要在于K的选择上。K-Means算法使用随机给定的方式。K-means++的思想则是：初始的聚类中心之间的相互距离要尽可能的远。 主要步骤如下： 从输入的数据点集合中 随机选择一个点作为第一个聚类中心 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大 重复2和3直到k个聚类中心被选出来 利用这k个初始的聚类中心来运行标准的k-means算法 从上面的算法描述上可以看到，算法的关键是第3步，如何将D(x)反映到点被选择的概率上，一种算法如下(详见此地)： 先从我们的数据库随机挑个随机点当“种子点” 对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。 然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。 重复2和3直到k个聚类中心被选出来 利用这k个初始的聚类中心来运行标准的k-means算法 可以看到算法的第三步选取新中心的方法，这样就能保证距离D(x)较大的点，会被选出来作为聚类中心了。至于为什么原因很简单，如下图 所示： 假设A、B、C、D的D(x)如上图所示，当算法取值Sum(D(x))*random时，该值会以较大的概率落入D(x)较大的区间内，所以对应的点会以较大的概率被选中作为新的聚类中心。 缺点：由于聚类中心点选择过程中的内在有序性，在扩展方面存在着性能方面的问题(第k个聚类中心点的选择依赖前k-1个聚类中心点的值) 层次聚类 Hierarchical Clusteringk-means聚类算法的一个变体，主要是为了改进k-means算法随机选择初始质心的随机性造成聚类结果不确定性的问题。 层次聚类算法主要分为两大类算法： 凝聚的层次聚类：AGNES 算法(AGglomerative NESting)==&gt;采用 自底向上 的策略。最初将每个对象作为一个簇，然后这些簇根据某些准则被一步一步合并，两个簇间的距离可以由这两个不同簇中距离最近的数据点的相似度来确定；聚类的合并过程反复进行直到所有的对象满足簇数目。 分裂的层次聚类：DIANA 算法(DIvisive ANALysis)==&gt;采用 自顶向下 的策略。首先将所有对象置于一个簇中，然后按照某种既定的规则逐渐细分为越来越小的簇(比如最大的欧式距离)，直到达到某个终结条件(簇数目或者簇距离达到阈值)。 优缺点 简单，理解容易 合并点/分裂点选择不太容易 合并/分类的操作不能进行撤销 大数据集不太适合 执行效率较低O(t*n 2 )，t为迭代次数，n为样本点数 密度聚类解决K-Means算法只能发现凸聚类的缺点。 密度聚类可以发现任意形状的聚类，而且对噪声数据不敏感。但是计算复杂度高，计算量大。常用算法为 DBSCAN 和 密度最大值算法。","link":"/2017/10/25/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"},{"title":"特征工程","text":"特征工程是什么数据和特征决定机器学习上限，而模型和算法只是逼近这个上限。特征工程目的：最大限度地从原始数据中提取特征以供算法和模型使用。 数据清洗数据清洗的结果直接关系到模型效果以及最终的结论。在实际的工作中，数据清洗通常占开发过程的 50%-80% 的时间。 在数据预处理过程主要考虑两个方面，如下： 选择数据处理工具：关系型数据库戒者Python 查看数据的元数据以及数据特征：一是查看元数据，包括字段解释、数据来源等一切可以描述数据的信息；另外是抽取一部分数据，通过人工查看的方式，对数据本身做一个比较直观的了解，并且初步发现一些问题，为之后的数据处理做准备。 缺省值清洗缺省值是数据中最常见的一个问题，处理缺省值有很多方式，主要包括以下四个步骤进行缺省值处理： 确定缺省值范围 去除不需要的字段 填充缺省值内容 重新获取数据 注意：最重要的是 缺省值内容填充。 在进行确定缺省值范围的时候，对每个字段都计算其缺失比例，然后按照缺失比例和字段重要性分别指定不同的策略。 在进行去除不需要的字段的时候，需要注意的是：删除操作最好不要直接操作不原始数据上，最好的是抽取部分数据进行删除字段后的模型构建，查看模型效果，如果效果不错，那么再到全量数据上进行删除字段操作。总而言之：该过程简单但是必须慎用，不过一般效果不错，删除一些丢失率高以及重要性低的数据可以降低模型的训练复杂度，同时又不会降低模型的效果。 填充缺省值很重要，常用方法如下： 以业务知识经验推测填充缺省值 以同一字段指标的计算结果(均值、中位数、众数等)填充缺省值 以不同字段指标的计算结果来推测性的填充缺省值，比如通过身仹证号码计算年龄、通过收货地址来推测家庭住址、通过访问的IP地址来推测家庭/公司/学校的家庭住址等等 如果某些指标非常重要，但是缺失率有比较高，而且通过其它字段没法比较精准的计算出指标值的情况下，那么就需要和数据产生方(业务人员、数据收集人员等)沟通协商，是否可以通过其它的渠道获取相关的数据，也就是进行重新获取数据的操作。 格式内容清洗一般情况下，数据是由用户/访客产生的，也就有很大的可能性存在格式和内容上不一致的情况，所以在进行模型构建之前需要先进行数据的格式内容清洗操作。格式内容问题主要有以下几类： 时间、日期、数值、半全角等显示格式不一致：直接将数据转换为一类格式即可，该问题一般出现在多个数据源整合的情况下。 内容中有不该存在的字符：最典型的就是在头部、中间、尾部的空格等问题，这种 情况下，需要以半自劢校验加半人工方式来找出问题，并去除不需要的字符。内容不该字段应有的内容不符：比如姓名写成了性别、身仹证号写成手机号等问题。 逻辑错误清洗主要是通过简单的逻辑推理发现数据中的问题数据，防止分析结果走偏，主要包含以下几个步骤： 数据去重 去除/替换不合理的值 去除/重构不可靠的字段值(修改矛盾的内容) 去除不需要的数据一般情况下，我们会尽可能多的收集数据，但是不是所有的字段数据都是可以应用到模型构建过程的，也不是说将所有的字段属性都放到构建模型中，最终模型的效果就一定会好，实际上来讲，字段属性越多，模型的构建就会越慢，所以有时候可以考虑将不要的字段进行删除操作。在进行该过程的时候，要注意备仹原始数据。 关联性验证如果数据有多个来源，那么有必要进行关联性验证，该过程常应用到多数据源合并的过程中，通过验证数据之间的关联性来选择比较正确的特征属性，比如：汽车的线下贩买信息和电话客服问卷信息，两者之间可以通过姓名和手机号进行关联操作，匹配两者之间的车辆信息是否是同一辆，如果不是，那么就需要进行数据调整。 特征转换特征转换主要指将原始数据中的字段数据进行转换操作，从而得到适合进行算法模型构建的输入数据(数值型数据)，在这个过程中主要包括但不限于以下几种数据的处理： 文本数据转换为数值型数据 缺省值填充 定性特征属性哑编码 定量特征属性二值化 特征标准化不归一化 文本特征属性转换机器学习的模型算法均要求输入的数据必须是数值型的，所以对于文本类型的特征属性，需要进行文本数据转换，也就是需要将文本数据转换为数值型数据。常用方式如下： 词袋法(BOW/TF) TF-IDF(Term frequency-inverse document frequency) HashTF Word2Vec 词袋法词袋法(Bag of words, BOW)是最早应用于NLP和IR领域的一种文本处理模型，该模型忽略文本的语法和语序，用一组无序的单词(words)来表达一段文字戒者一个文档，词袋法中使用单词在文档中出现的次数(频数)来表示文档。 词集法(Set of words, SOW)是词袋法的一种变种，应用的比较多，和词袋法的原理一样，是以文档中的单词来表示文档的一种的模型，区别在于：词袋法使用的是单词的频数，而在词集法中使用的是单词是否出现，如果出现赋值为1，否则为0。 TF-IDF词条的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降；也就是说词条在文本中出现的次数越多，表示该词条对该文本的重要性越高，词条在所有文本中出现的次数越少，说明这个词条对文本的重要性越高。TF(词频)指某个词条在文本中出现的次数，一般会将其进行归一化处理(该词条数量/该文档中所有词条数量)；IDF(逆向文件频率)指一个词条重要性的度量，一般计算方式为总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。TF-IDF实际上是：TF * IDF 假设单词用t表示，文档用d表示，语料库用D表示，那么N(t,D)表示包含单词t的文档数量，|D|表示文档数量，|d|表示文档d中的所有单词数量。N(t,d)表示在文档d中单词t出现的次数。 TF-IDF除了使用默认的tf和idf公式外，tf和idf公式还可以使用一些扩展之后公式来进行指标的计算，常用的公式有： 有两个文档，单词统计如下，请分别计算各个单词在文档中的TF-IDF值以及这些文档使用单词表示的特征向量。 HashTF-IDF不管是前面的词袋法还是TF-IDF，都避免不了计算文档中单词的词频，当文档数量比较少、单词数量比较少的时候，我们的计算量不会太大，但是当这个数量上升到一定程度的时候，程序的计算效率就会降低下去，这个时候可以通过HashTF的形式来解决该问题。 HashTF的计算规则是：在计算过程中，不计算词频，而是计算单词进行hash后的hash值的数量(有的模型中可能存在正则化操作)； HashTF的特点：运行速度快，但是无法获取高频词，有可能存在单词碰撞问题(hash值一样) 在scikit中，对于文本数据主要提供了三种方式将文本数据转换为数值型的特征向量，同时提供了一种对TF-IDF公式改版的公式。所有的转换方式均位于模块：sklearn.feature_extraction.text Word2VecWord2Vec是Google于2013年开源推出的一个用户获取wordvector的工具包，具有简单、高效的特性；Word2Vec通过对文档中所有单词进行分析，获得单词之间的关联程度，从而获取词向量，最终形成一个词向量矩阵。对词向量矩阵的分析：分类、聚类、相似性计算等等；是一种在NLP和大数据机器学习中应用比较多的一种文本转换数值型向量的方式。 吴恩达word2vec视频讲解吴恩达word2vec总结 无量纲化无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。 标准化标准化：基于特征属性的数据(也就是特征矩阵的列)，获取均值和方差，然后将特征值转换至服从标准正态分布。计算公式如下： 区间缩放法区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为： 归一化简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下： 对定性特征哑编码定性变量是反映”职业”、”教育程度”等现象属性特点的变量，只能反映现象的属性特点，而不能说明具体量的大小和差异。 哑编码(OneHotEncoder)：对于定性的数据(也就是分类的数据)，可以采用N位的状态寄存器来对N个状态进行编码，每个状态都有一个独立的寄存器位，并且在仸意状态下只有一位有效；是一种常用的将特征数字化的方式。比如有一个特征属性:[‘male’,’female’]，那么male使用向量[1,0]表示，female使用[0,1]表示。 对定量特征二值化定量变量是反映类似”天气温度”和”月收入”等属性的变量，可以用数值表示其观察结果，这些数值具有明确的数值含义，不仅能分类而且能测量出来具体大小和差异。这些变量就是定量变量也称数值变量，定量变量的观察结果成为定量数据。是说明事物数字特征的一个名称。 二值化(Binarizer)：对于定量的数据根据给定的阈值，将其进行转换，如果大于阈值，那么赋值为1；否则赋值为0 缺省值填充对于缺省的数据，在处理之前一定需要进行预处理操作，一般采用中位数、均值戒者众数来进行填充，在scikit中主要通过Imputer类来实现对缺省值的填充 数据多项式扩充变换多项式数据变换主要是指基于输入的特征数据按照既定的多项式规则构建更多的输出特征属性，比如输入特征属性为[a,b]，当设置degree为2的时候，那么输出的多项式特征为[1, a, b, a^2, ab, b^2] 特征选择当做完特征转换后，实际上可能会存在很多的特征属性，比如：多项式扩展转换、文本数据转换等等，但是太多的特征属性的存在可能会导致模型构建效率降低，同时模型的效果有可能会变的不好，那么这个时候就需要从这些特征属性中选择出影响最大的特征属性作为最后构建模型的特征属性列表。 在选择模型的过程中，通常从两方面来选择特征： 特征是否发散：如果一个特征不发散，比如方差解决于0，也就是说这样的特征对于样本的区分没有什么作用。 特征不目标的相关性：如果不目标相关性比较高，应当优先选择。 根据特征选择的形式又可以将特征选择方法分为3种： Filter：过滤法，按照发散性戒者相关性对各个特征进行评分，设定阈值戒者待选择阈值的个数，从而选择特征；常用方法包括方差选择法、相关系数法、卡方检验、互信息法等。 Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征戒者排除若干特征；常用方法主要是递归特征消除法。 Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权重系数，根据系数从大到小选择特征；常用方法主要是基于惩罚项的特征选择法。 Filter方差选择法方差选择法：先计算各个特征属性的方差值，然后根据阈值，获取方差大于阈值的特征。 相关系数法相关系数法：先计算各个特征属性对于目标值的相关系数以及相关系数的P值，然后获取大于阈值的特征属性。 卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量： 不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。 互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下： 为了处理定量数据，最大信息系数法被提出 Wrapper递归特征消除法递归特征消除法：使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 Embedded基于惩罚项的特征选择法在使用惩罚项的基模型，除了可以筛选出特征外，同时还可以进行降维操作。 基于树模型的特征选择法树模型中GBDT在构建的过程会对特征属性进行权重的给定，所以GBDT也可以应用在基模型中进行特征选择。 降维特征选择完成后，可以直接进行模型训练，但是可能由于特征矩阵过大，导致计算量大，为了节省训练时长，可以降低特征矩阵的维度。 常见的降维方法除了基于L1的惩罚模型外，还有主成分析法(PCA)和线性判别分析法(LDA)，这两种方法的本质都是将原始数据映射到维度更低的样本空间中；但是采用的方式不同，PCA是为了让映射后的样本具有更大的发散性，LDA是为了让映射后的样本有最好的分类性能 。 主成分分析法 PCA主成分析(PCA)：将高纬的特征向量合并成低纬的特征属性，是一种无监督的降维方法。吴恩达PCA算法讲解 线性判别分析法 LDA线性判断分析(LDA)：LDA是一种基于分类模型进行特征属性合并的操作，是一种有监督的降维方法。 异常检测吴恩达异常检测讲解","link":"/2017/11/02/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"title":"决策树、随机森林和提升算法","text":"持续更新中。。。 决策树 示例代码 信息熵、联合熵、条件熵、相对熵、互信息的定义 什么是最大熵 什么是决策树 决策树构建过程 决策树的纯度 ID3、C4.5、CART介绍 剪枝 决策树优化策略 决策树总结 相关链接 集成方法 bootstrap, boosting, bagging 简介 提升算法 Adboost、GBDT、Xgboost 简介 GBDT简介 从回归树到GBDT GBDT与随机森林 GBDT与XGBOOST 怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感 决策树如何处理不完整数据 怎么理解决策树、xgboost能处理缺失值而有的模型svm对缺失值比较敏感 随机森林如何处理缺失值 随机森林如何评估特征重要性 为什么xgboost要用泰勒展开，优势在哪里 xgboost如何寻找最优特征，是有放回还是无放回 信息熵、联合熵、条件熵、相对熵、互信息的定义下面的截图截自宗成庆的《统计自然语言处理 》： 什么是最大熵熵是随机变量不确定性的度量，不确定性越大，熵值越大；若随机变量退化成定值，熵为0。如果没有外界干扰，随机变量总是趋向于无序，在经过足够时间的稳定演化，它应该能够达到的最大程度的熵。 为了准确的估计随机变量的状态，我们一般习惯性最大化熵，认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型(泛化能力最好)。换言之，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，其原则是承认已知事物（知识），且对未知事物不做任何假设，没有任何偏见。 例如，投掷一个骰子，如果问”每个面朝上的概率分别是多少”，你会说是等概率，即各点出现的概率均为1/6。因为对这个”一无所知”的色子，什么都不确定，而假定它每一个朝上概率均等则是最合理的做法。从投资的角度来看，这是风险最小的做法，而从信息论的角度讲，就是保留了最大的不确定性，也就是说让熵达到最大。 什么是决策树决策树(Decision Tree)是在已知各种情况发生概率的基础上，通过构建决策树来进行分析的一种方式，是一种直观应用概率分析的一种图解法；决策树是一种预测模型，代表的是对象属性与对象值之间的映射关系；决策树是一种树形结构，其中每个内部节点表示一个属性的测试，每个分支表示一个测试输出，每个叶节点代表一种类别；决策树是一种非常常用的有监督的分类算法。 决策树的决策过程就是从根节点开始，测试待分类项中对应的特征属性，并按照其值选择输出分支，直到叶子节点，将叶子节点的存放的类别作为决策结果。 决策树分为两大类：分类树和回归树，前者用于分类标签值，后者用于预测连续值，常用算法有ID3、C4.5、CART等 决策树构建过程顾名思义，决策树就是用一棵树来表示我们的整个决策过程。这棵树可以是二叉树（比如 CART 只能是二叉树），也可以是多叉树（比如 ID3、C4.5 可以是多叉树或二叉树）。 根节点包含整个样本集，每个叶节都对应一个决策结果（注意，不同的叶节点可能对应同一个决策结果），每一个内部节点都对应一次决策过程或者说是一次属性测试。从根节点到每个叶节点的路径对应一个判定测试序列。 决策树的构造关键步骤就是分裂属性。分裂属性是指在某个节点按照某一类特征属性的不同划分构建不同的分支，其目标就是让各个分裂子集尽可能的”纯”(让一个分裂子类中待分类的项尽可能的属于同一个类别)。 构建步骤：1）开始，所有记录看做一个节点2）遍历每个节点的每一种分割方式，找到最好的分割点3）将数据分割为两个节点部分N1和N24）对N1和N2分别继续执行2-3步，直到每个节点中的项足够”纯” 特征属性类型：根据特征属性的类型不同，在构建决策树的时候，采用不同的方式，具体如下：1）属性是离散值，而且不要求生成的是二叉决策树，此时一个属性就是一个分支2）属性是离散值，而且要求生成的是二叉决策树，此时使用属性划分的子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支3）属性是连续值，可以确定一个值作为分裂点split_point，按照&gt;split_point和&lt;=split_point生成两个分支 构建停止条件：决策树构建的过程是一个递归的过程，所以必须给定停止条件，常用有两种：1）当每个子节点只有一种类型的时候停止构建2）当前节点中记录数小于某个阈值，同时迭代次数达到给定值时，停止构建，并使用max(p(i))作为节点对应类型。方式一可能会使树的节点过多，导致过拟合(Overfiting)等问题；比较常用的方式是使用方式二作为停止条件 决策树算法是一种贪心算法策略，只考虑在当前数据特征情况下的最好分割方式，不能进行回溯操作。 决策树的纯度子集越纯，子集中待分类的项就越可能属于同一个类别。 决策树的构建是基于样本概率和纯度进行构建操作的，那么进行判断数据集是否”纯”可以通过三个公式进行判断，分别是Gini系数、熵(Entropy)、错误率，这三个公式值越大，表示数据越”不纯”；越小表示越”纯”；实践证明这三种公式效果差不多，一般情况使用熵公式。 根据熵公式计算出各个特征属性的量化纯度值后，使用 信息增益度 来选择出当前数据集的分割特征属性；如果信息增益度的值越大，表示在该特征属性上会损失纯度越大，就越应该在决策树的上层。计算公式为：Gain为A为特征对训练数据集D的信息增益，它为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D/A)之差. ID3、C4.5、CART介绍 这三个是非常著名的决策树算法。简单粗暴来说，ID3 使用 信息增益 作为选择特征的准则；C4.5 使用 信息增益率 作为选择特征的准则；CART 使用 Gini 系数 作为选择特征的准则。 1、ID3ID3内部使用信息熵以及信息增益来进行构建；每次迭代选择信息增益最大的特征属性作为分割属性。 熵表示的是数据中包含的信息量大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。 信息增益 = 划分前熵 - 划分后熵。信息增益越大，则意味着使用属性 a 来进行划分所获得的 “纯度提升” 越大 **。也就是说，用属性 a 来划分训练集，得到的结果中纯度比较高。 ID3 仅仅适用于二分类问题。ID3 仅仅能够处理离散属性。 优点:决策树构建速度快；实现简单； 缺点： 计算依赖于特征数目较多的特征，而属性值最多的属性并不一定最优 ID3算法不是递增算法 ID3算法是单变量决策树，对于特征属性之间的关系不会考虑 抗噪性差 只适合小规模数据集，需要将数据放到内存中 2、C4.5C4.5 基于ID3算法提出；它克服了 ID3 仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。信息增益比 = 信息增益 / 划分前熵 选择信息增益比最大的作为最优特征。 C4.5 在树的构造过程中会进行剪枝操作进行优化，也能够自动完成对连续属性的离散化处理；C4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。 优点： 产生的规则易于理解 准确率较高 实现简单 缺点： 对数据集需要进行多次顺序扫描和排序，所以效率较低 只适合小规模数据集，需要将数据放到内存中 ID3算法和C4.5算法总结1）ID3和C4.5算法均只适合在小规模数据集上使用2）ID3和C4.5算法都是单变量决策树3）当属性值取值比较多的时候，最好考虑C4.5算法，ID3得出的效果会比较差4）决策树分类一般情况只适合小数据量的情况(数据可以放内存) 3、CARTCART(Classification And Regression Tree，分类回归树) 的全称是分类与回归树。从这个名字中就应该知道，CART 既可以用于分类问题，也可以用于回归问题。 CART 与 ID3，C4.5 不同之处在于 CART 生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。 回归树中，使用平方误差最小化准则来选择特征并进行划分。每一个叶子节点给出的预测值，是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。 要确定最优化分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在此种划分情况下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。 分类树种，使用 Gini 指数最小化准则来选择特征并进行划分； Gini 指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的概率。 4、信息增益 vs 信息增益比之所以引入了信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题。 5、Gini 指数 vs 熵既然这两个都可以表示数据的不确定性，不纯度。那么这两个有什么区别那？ Gini 指数的计算不需要对数运算，更加高效； Gini 指数更偏向于连续属性，熵更偏向于离散属性。 剪枝决策树算法很容易过拟合（overfitting），剪枝算法就是用来防止决策树过拟合，提高泛华性能的方法。 剪枝分为预剪枝与后剪枝。 预剪枝是指在决策树的生成过程中，对每个节点在划分前先进行评估，若当前的划分不能带来泛化性能的提升，则停止划分，并将当前节点标记为叶节点。 后剪枝是指先从训练集生成一颗完整的决策树，然后自底向上对非叶节点进行考察，若将该节点对应的子树替换为叶节点，能带来泛化性能的提升，则将该子树替换为叶节点。 那么怎么来判断是否带来泛化性能的提升那？最简单的就是留出法，即预留一部分数据作为验证集来进行性能评估。 决策树优化策略 剪枝优化决策树过度拟合一般情况是由于节点太多导致的，剪枝优化对决策树的正确率影响比较大。 K-Fold Cross Validation(K交叉验证)首先计算出整体的决策树T，叶子点个数为N，记i属于[1,N]。对每个i，使用K交叉验证方法计算决策树，并使决策树的叶子节点数量为i个(剪枝)，计算错误率，得出平均错误率；使用最小错误率的i作为最优决策树的叶子点数量，并对原始决策树T进行裁剪。 Random Forest利用训练数据随机产生多个决策树，形成一个森林。然后使用这个森林对数据进行预测，选取最多结果作为预测结果。 决策树总结决策树算法主要包括三个部分：特征选择、树的生成、树的剪枝。常用算法有 ID3、C4.5、CART。 特征选择。特征选择的目的是选取能够对训练集分类的特征。特征选择的关键是准则：信息增益、信息增益比、Gini 指数； 决策树的生成。通常是利用信息增益最大、信息增益比最大、Gini 指数最小作为特征选择的准则。从根节点开始，递归的生成决策树。相当于是不断选取局部最优特征，或将训练集分割为基本能够正确分类的子集； 决策树的剪枝。决策树的剪枝是为了防止树的过拟合，增强其泛化能力。包括预剪枝和后剪枝。 随机森林如何处理缺失值方法一（na.roughfix）简单粗暴，对于训练集，同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。 方法二（rfImpute）这个方法计算量大，至于比方法一好坏不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似12。 随机森林如何评估特征重要性衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：1) Decrease GINI： 对于回归问题，直接使用argmax(VarVarLeftVarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。 为什么xgboost要用泰勒展开，优势在哪里xgboost使用了一阶和二阶偏导， 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式， 可以在不选定损失函数具体形式的情况下， 仅仅依靠输入数据的值就可以进行叶子分裂优化计算， 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性， 使得它按需选取损失函数， 可以用于分类， 也可以用于回归。 xgboost如何寻找最优特征，是有放回还是无放回xgboost在训练的过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据， 从而记忆了每个特征对在模型训练时的重要性 — 从根到叶子中间节点涉及某特征的次数作为该特征重要性排序.xgboost属于boosting集成学习方法， 样本是不放回的， 因而每轮计算样本不重复. 另一方面， xgboost支持子采样， 也就是每轮计算可以不使用全部样本， 以减少过拟合. 进一步地， xgboost 还有列采样， 每轮计算按百分比随机采样一部分特征， 既提高计算速度又减少过拟合。","link":"/2017/10/21/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/"},{"title":"机器学习开发策略","text":"吴大大结构化机器学习项目总结，完善中… ML策略假设你构建了一个喵咪分类器，训练之后准确率达到90%，但在测试集上还不够好。此时你可以想到的优化方法有哪些呢？总结后大致如下： 收集更多的数据 收集更多的多样化训练集，比如不同姿势的猫咪图片等 用梯度下降法训练更长时间 尝试Adam算法 尝试更大的网路 尝试小一点的网络 尝试dropout随机失活算法 加上L2正则项 改善网络结构，如变更激活函数，变更隐藏层节点数量 优化的方法虽然很多，但如果方向错误，可能白费几个月时间。那通过哪些策略可以减少错误发生的几率呢？怎么判断哪些方法可以尝试，哪些方法可以丢弃呢？ 正交化 Orthogonalization优化前首先需要明白正交化。教科书式定义（可以直接略过 =。=）：正交化是一种系统设计属性，它确保修改指令或算法的组成部分不会对系统的其他组件产生或传播副作用。独立地验证某部分而不对其他部分产生影响，能有效减少测试和开发时间。 正交性很好理解，就像以前的老式黑白电视机，它有很多调节画面的旋钮。假设第一个旋钮控制上下方向，第二个旋钮控制左右方向。当我们需要调节画面时，调节上下方向不会影响左右方向。这样互不影响的设计能大大减少调节画面的时间。 同样，要弄好一个监督学习系统，我们也需要考虑系统的旋钮。通常我们需要保证下面四个方面是正交的。 首先系统在训练集上表现良好 如果拟合不好，那么尝试使用更大的神经网络或者切换更好的优化算法，比如Adam等 其次系统在开发集上表现良好 如果拟合不好，尝试正则化或者更大的训练集 然后系统在测试集上表现良好 如果拟合不好，尝试更大的开发集 最后系统在真实(生产)环境中表现良好 如果表现不好，意味着开发测试集分布设置可能不对，或者损失函数不能有效反映算法在现实世界的表现 我们需要使用正交化的思想去分析系统的瓶颈究竟出自哪一个方面，当系统表现不佳时，哪些旋钮是值得去尝试的。尝试过程中需要训练网络，吴大大在这特意提到，他自己训练神经网络时通常不会提前停止网络训练，因为这会让问题的分析复杂化。比如提前停止训练集的训练，它在影响训练集的拟合的同时会改善开发集的表现，这样问题就不正交化了。 单一数字评估指标 Single number evaluation metric当我们知道模型在哪个集合拟合不好时，我们就有了优化的方向。无论是调整超参数，还是尝试更好的优化算法，为了更好更快的重新评估模型，我们都需要为问题设置一个单一的数字评估指标。 下面是分别训练的两个分类器的 Precision(精准率)、Recall（召回率）以及F1 score。 由上表可以看出，以 Precision 为指标，则分类器 A 的分类效果好；以 Recall 为指标，则分类器 B 的分类效果好。所以仅用以上判定指标，我们有时很难决定出 A 好还是 B 好。 这里以 Precision 和 Recall 为基础，构成一个综合指标 F1 Score ，那么我们利用F1 Score便可以更容易的评判出分类器A的效果更好。 指标介绍： 在二分类问题中，通过预测我们得到下面的真实值 y 和预测值 y^ 的表： Precision（精准率）： $Precision = \\dfrac{True\\ positive}{Number\\ of\\ predicted\\ positive} \\times 100\\%= \\dfrac{True\\ positive}{True\\ positive + False\\ positive}$ 假设在是否为猫的分类问题中，查准率代表：所有模型预测为猫的图片中，确实为猫的概率。 Recall（召回率）：$Recall = \\dfrac{True\\ positive}{Number\\ of\\ actually\\ positive} \\times 100\\%= \\dfrac{True\\ positive}{True\\ positive + False\\ negative}$ 假设在是否为猫的分类问题中，查全率代表：真实为猫的图片中，预测正确的概率。 F1 Score： $vF1-Socre = \\dfrac {2} {\\dfrac{1}{p}+\\dfrac{1}{r}}$ 相当与精准率和召回率的一个特别形式的平均指标。 示例：下面是另外一个问题多种分类器在不同的国家中的分类错误率结果： 模型在各个地区有不同的表现，这里用地区的平均值来对模型效果进行评估，转换为单一数字评估指标，就可以很容易的得出表现最好的模型。 满足和优化指标有时候把我们所有顾及的事情组成单一数字评估指标并不容易。这个时候，可以尝试把指标划分为满足指标和优化指标。 比如现在有三个不同的分类器性能表现如下： 假设我们对模型效果有一定的要求，不仅要求准确率，还要求运行时间在100 ms以内。那么我们可以以 Accuracy 为优化指标，以 Running time 为满足指标。一旦Running time达到要求，我们就可以把全部精力放在优化指标上。以这个思想，我们很快可以从中选出B是满足条件的最好的分类器。 一般的，如果要考虑N个指标，则选择一个指标为优化指标，其他N-1个指标都是满足指标： N_{metric}:\\left\\{ \\begin{array}{l} 1\\qquad \\qquad \\qquad Optimizing\\ metric\\\\ N_{metric}-1\\qquad Satisificing\\ metric \\end{array} \\right. 训练/开发/测试集划分确立了评估指标，就是确定了靶心，团队拿到数据就可以快速迭代不断逼近指标。这个时候正确的数据集的划分就非常重要了，它直接关乎你的团队效率。 我们知道训练集是用来训练模型的，开发集(也要交叉验证集)用来尝试迭代各种想法，用来优化性能，得到一个满意的损失结果后，最后用测试集评估。 现在假设有这样的一个数据集，需要将他们划分为开发集和测试集， 有些人可能会随机选择几个国家的作为开发集，剩下的作为测试集，就如下图所示。 有些人可能觉得没问题，其实问题很大！因为开发集和测试集不服从同一分布，这就好像你在准备托福考试，你尽可能的得到了所有的考试技巧和其他资料，最后你的确得到了不错的成绩。但是后来因为工作需求需要你会说俄语，此时如果你用之前托福的资料来对付俄语考试则显然不对，这也就是为什么有时候开发集准确率高，但是测试集低。 所以数据要随机洗牌，然后放到训练、开发和测试集中。其次，这些随机洗牌的数据的来源也需要注意，即你选择的数据集，要能反应出你未来希望得到的数据，即模型数据要和未来数据相似，这样模型在生产环境中面对新的数据才能有好的表现。 开发集和测试集的大小机器学习发展到现在，数据集的划分与传统稍有不同。具体详情见下图： 开发集有时候需要足够大才能评估不同的想法，至于测试集的量，除非你对最终投产的系统有非常高的精准指标，否则一般情况下，1W的数据量足够。注意测试集的划分不再需要按照传统占据30%，以现在的数据量，30%很可能超过百万。 什么时候该改变开发集/测试集和指标在针对某一问题我们设置开发集和评估指标后，这就像把目标定在某个位置，后面的过程就聚焦在该位置上。但有时候在这个项目的过程中，可能会发现目标的位置设置错了，所以要移动改变我们的目标。 example1 假设有两个猫的图片的分类器： 评估指标：分类错误率 算法A：3%错误率 算法B：5%错误率 这样来看，算法A的表现更好。但是在实际的测试中，算法A可能因为某些原因，将很多色情图片分类成了猫。所以当我们在线上部署的时候，算法A会给爱猫人士推送更多更准确的猫的图片（因为其误差率只有3%），但同时也会给用户推送一些色情图片，这是不能忍受的。所以，虽然算法A的错误率很低，但是它却不是一个好的算法。 这个时候我们就需要改变开发集、测试集或者评估指标。 假设开始我们的评估指标如下：$Error = \\dfrac{1}{m{dev}}\\sum\\limits{i=1}^{m{dev}}I{y^{(i)}{pred}\\neq y^{(i)}}$ 该评估指标对色情图片和非色情图片一视同仁，但是我们希望，分类器不会错误将色情图片标记为猫。 修改的方法，在其中加入权重$w^{(i)}$： $Error = \\dfrac{1}{\\sum w^{(i)}}\\sum\\limits{i=1}^{m{dev}} w^{(i)}I{y^{(i)}_{pred}\\neq y^{(i)}}$其中： w^{(i)}=\\left\\{ \\begin{array}{l} 1\\qquad \\qquad \\qquad 如果x^{(i)}不是色情图片\\\\ 10或100\\qquad \\qquad如果x^{(i)}是色情图片 \\end{array} \\right.这样通过设置权重，当算法将色情图片分类为猫时，误差项会快速变大。总结来说就是：如果评估指标无法正确评估算法的排名，则需要重新定义一个新的评估指标。 example2 同样针对example1中的两个不同的猫图片的分类器A和B。 但实际情况是对，我们一直使用的是网上下载的高质量的图片进行训练；而当部署到手机上时，由于图片的清晰度及拍照水平的原因，当实际测试算法时，会发现算法B的表现其实更好。 如果在训练开发测试的过程中得到的模型效果比较好，但是在实际应用中自己所真正关心的问题效果却不好的时候，就需要改变开发、测试集或者评估指标。 Guideline： 定义正确的评估指标来更好的给分类器的好坏进行排序； 优化评估指标。 比较人类表现水平很多机器学习模型的诞生是为了取代人类的工作，因此其表现也会跟人类表现水平作比较。 上图展示了随着时间的推进，机器学习系统和人的表现水平的变化。一般的，当机器学习超过人的表现水平后，它的进步速度逐渐变得缓慢，最终性能无法超过某个理论上限，这个上限被称为贝叶斯最优误差（Bayes Optimal Error）。 贝叶斯最优误差一般认为是理论上可能达到的最优误差，换句话说，其就是理论最优函数，任何从 x 到精确度 y 映射的函数都不可能超过这个值。例如，对于语音识别，某些音频片段嘈杂到基本不可能知道说的是什么，所以完美的识别率不可能达到 100%。 因为人类对于一些自然感知问题的表现水平十分接近贝叶斯最优误差，所以当机器学习系统的表现超过人类后，就没有太多继续改善的空间了。 也因此，只要建立的机器学习模型的表现还没达到人类的表现水平时，就可以通过各种手段来提升它。例如采用人工标记过的数据进行训练，通过人工误差分析了解为什么人能够正确识别，或者是进行偏差、方差分析。 当模型的表现超过人类后，这些手段起的作用就微乎其微了。 可避免偏差通过与贝叶斯最优误差，或者说，与人类表现水平的比较，可以表明一个机器学习模型表现的好坏程度，由此判断后续操作应该注重于减小偏差还是减小方差。 模型在训练集上的误差与人类表现水平的差值被称作可避免偏差（Avoidable Bias）。可避免偏差低便意味着模型在训练集上的表现很好，而训练集与验证集之间错误率的差值越小，意味着模型在验证集与测试集上的表现和训练集同样好。 如果可避免偏差大于训练集与验证集之间错误率的差值，之后的工作就应该专注于减小偏差；反之，就应该专注于减小方差。 理解人类表现水平我们一般用人类水平误差（Human-level Error）来代表贝叶斯最优误差（或者简称贝叶斯误差）。对于不同领域的例子，不同人群由于其经验水平不一，错误率也不同。一般来说，我们将表现最好的作为人类水平误差。但是实际应用中，不同人选择人类水平误差的基准是不同的，这会带来一定的影响。 例如，如果某模型在训练集上的错误率为 0.7%，验证集的错误率为 0.8%。如果选择的人类水平误差为 0.5%，那么偏差（bias）比方差（variance）更加突出；而如果选择的人类水平误差为 0.7%，则方差更加突出。也就是说，根据人类水平误差的不同选择，我们可能因此选择不同的优化操作。 这种问题只会发生在模型表现很好，接近人类水平误差的时候才会出现。人类水平误差给了我们一种估计贝叶斯误差的方式，而不是像之前一样将训练的错误率直接对着 0% 的方向进行优化。 当机器学习模型的表现超过了人类水平误差时，很难再通过人的直觉去判断模型还能够往什么方向优化以提高性能。 总结想让一个监督学习算法达到使用程度，应该做到以下两点：1）算法对训练集的拟合很好，可以看作可避免偏差很低；2）推广到验证集和测试集效果也很好，即方差不是很大。 根据正交化的思想，我们有一些措施可以独立地优化二者之一。","link":"/2017/12/10/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%AD%96%E7%95%A5%E6%80%BB%E7%BB%93/"},{"title":"回归算法","text":"持续更新中。。。 示例代码 什么是回归算法 回归算法推导：目标函数、对数似然及最小二乘 回归怎么预防过拟合 回归要调的参数有哪些 局部加权回归 Softmax回归 梯度的定义 什么是梯度下降？为什么要用梯度下降 梯度下降法容易收敛到局部最优，为什么应用广泛 梯度下降法找到的一定是下降最快的方向么 BGD、SGD、MBGD介绍与区别 BGD、SGD、MBGD的推导 牛顿法 拟牛顿法 牛顿法和梯度下降法的区别 共轭梯度 什么是正则化？L1正则与L2正则介绍 正则化为什么可以防止过拟合 L1和L2正则先验分别服从什么分布 简单介绍下LR LR与线性回归的区别与联系 LR模型为什么要使用sigmoid函数 逻辑回归相关问题 逻辑回归为什么要对特征进行离散化 逻辑回归并行化怎么做，有几种并行化方式，读过哪些开源的实现 什么是回归算法回归算法是一种比较常用的机器学习算法，用来建立”解释”变量和观测值之间的关系；从机器学习的角度来讲，就是通过学习，构建一个算法模型(函数)，来做属性与标签之间的映射关系。 回归算法中算法(函数)的最终结果是一个 连续 的数据值，输入值(属性值)是一个d维度的属性/数值向量。 回归涉及的算法模型：线性回归(Linear)、岭回归(Ridge)、LASSO回归、Elastic Net弹性网络算法正则化：L1-norm、L2-norm损失函数/目标函数：θ求解方式：最小二乘法(直接计算，目标函数是平方和损失函数)、梯度下降(BGD\\SGD\\MBGD) 广义线性模型对样本要求不必要服从正态分布、只需要服从指数分布簇(二项分布、泊松分布、伯努利分布、指数分布等)即可；广义线性模型的自变量可以是连续的也可以是离散的。 BGD、SGD、MBGD介绍与区别 BGD：批量梯度下降法 Batch Gradient Descent更新每一参数时都使用所有的样本来进行更新。优点：全局最优解；易于并行实现；缺点：当样本数目很多时，训练过程会很慢。 SGD：随机梯度下降法 Stochastic Gradient Descent随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。优点：训练速度快；缺点：准确度下降，并不是全局最优；不易于并行实现。 MBGD：小批量梯度下降法 Mini-batch Gradient Descent。如果即需要保证算法的训练速度，又需要保证最终参数训练的准确率，可以采取折衷方案MBGD。MBGD在每次更新参数时使用b个样本（b一般为10）。 BGD和SGD比较1）SGD速度比BGD快(迭代次数少)2）SGD在某些情况下(全局存在多个相对最优解/J(θ)不是一个二次)有可能跳出某些小的局部最优解，所以不会比BGD坏3）BGD一定能够得到一个局部最优解(在线性回归模型中一定是得到一个全局最优解)，SGD由于随机性的存在可能导致最终结果比BGD的差注意：两者优先选择SGD 损失函数：$J{train}(\\theta)=1/(2m)\\sum{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^{2}$ 回归要调的参数有哪些对于各种算法模型(线性回归)来讲，我们需要获取θ、λ、p的值；θ的求解其实就是算法模型的求解，一般不需要开发人员参与(算法已经实现)，主要需要求解的是λ和p的值，这个过程就叫做调参(超参) 局部加权回归普通线性回归损失函数：局部加权回归损失函数： $W^i$是权重，它根据要预测的点与数据集中的点的距离来为数据集中的点赋权值。当某点离要预测的点越远，其权重越小，否则越大。常用值选择公式为： 该函数称为指数衰减函数，其中k为波长参数，它控制了权值随距离下降的速率注意：使用该方式主要应用到样本之间的相似性考虑，主要内容在SVM中再考虑(核函数) Softmax回归Softmax回归是logistic回归的一般化，适用于K分类的问题，第k类的参数为向量$θ_k$ ，组成的二维矩阵为$θ_k*n$ 。Softmax函数的本质就是将一个K维的任意实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于（0，1）之间。 softmax回归概率函数为： 什么是正则化？L1正则与L2正则介绍为了防止过拟合，在成本函数中加入一个正则项，惩罚模型的复杂度。 Logistic 回归中的正则化对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数： J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}{||w||}^2_2 L1 正则化（LASSO回归 Least Absolute Shrinkage and Selection Operator）：\\frac{\\lambda}{2m}{||w||}_1 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}{|w_j|} L2 正则化（Ridge回归 岭回归）：\\frac{\\lambda}{2m}{||w||}^2_2 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}w^2_j = \\frac{\\lambda}{2m}w^Tw 其中，λ 为正则化因子，是超参数。注意，lambda在 Python 中属于保留字，所以在编程的时候，用lambd代替这里的正则化因子。 L1范数可以使权值稀疏，方便特征提取。 L2范数可以防止过拟合，提升模型的泛化能力。 L2-norm（岭回归）中，由于对于各个维度的参数缩放是在一个圆内缩放的，不可能导致有维度参数变为0的情况，那么也就不会产生稀疏解；实际应用中，数据的维度中是存在噪音和冗余的，L1（LASSO回归）稀疏的解可以找到有用的维度并且减少冗余，因此LASSO模型也具有较高的求解速度 同时使用L1正则和L2正则的线性回归模型就称为Elasitc Net算法(弹性网络算法) 正则化为什么可以防止过拟合正则化是在成本函数中加入一个正则化项，惩罚模型的复杂度。是针对过拟合而提出的。 直观解释正则化因子设置的足够大的情况下，为了使成本函数最小化，相应的参数就会被调小，甚至接近于0值，直观上相当于消除了很多特征值的影响。而且一般情况下参数越小(或者特征值越少)，函数越光滑，模型越简单，奥卡姆剃刀原理。 其他解释在权值 w变小之下，输入样本 X 随机的变化不会对模型造成过大的影响，模型受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。 神经网络防止过拟合还有一种数学解释，即每层权重变小，在 z 较小（接近于 0）的区域里，tanh(z)函数就近似线性，所以每层的函数就近似线性函数，所以防止过拟合。 L1和L2正则先验分别服从什么分布面试中遇到的，L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。 先验就是优化的起跑线， 有先验的好处就是可以在较小的数据集中有良好的泛化性能，当然这是在先验分布是接近真实分布的情况下得到的了，从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。对参数引入高斯正态先验分布相当于L2正则化， 这个大家都熟悉：对参数引入拉普拉斯先验等价于 L1正则化， 如下图：从上面两图可以看出， L2先验趋向零周围， L1先验趋向零本身。 梯度的定义 什么是梯度下降？为什么要用梯度下降推荐一篇文章：吴恩达机器学习线性回归总结 梯度下降法（gradient descent）是解决无约束优化问题的最简单和最古老的方法之一，常用在机器学习中 逼近最小偏差。搜索方向为梯度的负方向，越接近目标值，步长越小，前进越慢。 X1，X2..Xn 描述的是feature里面的分量，比如x1=房间的面积，x2=房间的朝向等等。θ是我们要调的参数，通过调整θ可以控制feature中每个分量的影响力。 损失函数J(θ)是用来判断θ取值是否比较好。换言之，我们把对x(i)的估计值与真实值y(i)差的平方和作为损失函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了。 如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)，另外一种就是梯度下降法。 梯度下降法的算法流程：1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。 为了描述的更清楚，给出下面的图： 这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取值，我们需要的是，能够让J(θ)的值尽量的低，也就是达到深蓝色的部分。θ0，θ1表示θ向量的两个维度。在上面提到梯度下降法的第一步是给θ给一个初值，假设随机给的初值是在图上的十字点。然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如下图所示，算法的结束将是在θ下降到无法继续下降为止。 当然，可能梯度下降的最终点并非是全局最小点，即也可能是一个局部最小点，如下图所示： 上面这张图就是描述的一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。下面我将用一个例子描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J： 下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。 一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。 梯度下降法容易收敛到局部最优，为什么应用广泛深度神经网络“容易收敛到局部最优”，很可能是一种想象，实际情况是，你可能从来没有找到过“局部最优”，更别说全局最优了。 大部分情况你达到的是 鞍点，鞍点详解见我的深度学习笔记 梯度下降法找到的一定是下降最快的方向么梯度下降法并不是下降最快的方向，它只是目标函数在当前点的切平面下降最快的方向。一般认为牛顿方向（考虑海森距离）才是下降最快的方向。 牛顿法和梯度下降法的区别效率对比：（a）从收敛速度上看 ，牛顿法是二阶收敛，梯度下降是一阶收敛，前者牛顿法收敛速度更快。但牛顿法仍然是局部算法，只是在局部上看的更细致，梯度法仅考虑方向，牛顿法不但考虑了方向还兼顾了步子的大小，其对步长的估计使用的是二阶逼近。（b）根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。 牛顿法的优缺点总结：优点：二阶收敛，收敛速度快；缺点：函数必须具有连续的一阶二阶偏导数，海森矩阵必须正定。其次计算非常复杂。 共轭梯度共轭：两个概率分布如果具有相同的形式，我们就说它们是共轭的 共轭梯度法是介于梯度下降法（最速下降法）与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hessian矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有逐步收敛性，稳定性高，而且不需要任何外来参数。 下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图： 注：绿色为梯度下降法，红色代表共轭梯度法 简单介绍下LR1）吴恩达机器学习逻辑回归总结2）Logistic Regression 的前世今生3）机器学习算法与Python实践之七-逻辑回归 把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。 LR与线性回归的区别与联系整理中。。。 LR 工业上一般指Logistic Regression（逻辑回归）而不是Linear Regression（线性回归）。LR在线性回归的实数范围输出值上施加 sigmoid 函数将值收敛到 0~1 范围，其目标函数也因此从差平方和函数变为对数损失函数，以提供最优化所需导数（sigmoid函数是softmax函数的二元特例， 其导数均为函数值的f*(1-f)形式）。 请注意， LR 往往是解决二元 0/1 分类问题的， 只是它和线性回归耦合太紧， 不自觉也冠了个回归的名字。若要求多元分类，就要把sigmoid换成大名鼎鼎的softmax了。 个人感觉逻辑回归和线性回归首先都是广义的线性回归， 其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数， 另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0，1]。逻辑回归就是一种减小预测范围，将预测值限定为[0，1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。 逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。 LR模型为什么要使用sigmoid函数假设W已知，现有函数 P(y|x) = f(wx)，求分布f，使得在正样本里 P(y=1|w,x)尽可能大，在负样本里尽可能小。根据最大熵原则，我们知道所有可能的分布模型集合中，熵最大的模型是最好的模型。即对未知分布最合理的推断就是符合已有前提下最不确定或最随机的推断。又已知LR符合伯努利分布，将伯努利分布转换为指数分布的过程中，可以得到sigmod函数，这就是LR的理论基础。 LR模型使用sigmoid函数背后的数学原理是什么？ 广义线性模型 什么是最大熵 逻辑回归相关问题（3）L1-norm和L2-norm 其实稀疏的根本还是在于L0-norm也就是直接统计参数不为0的个数作为规则项，但实际上却不好执行于是引入了L1-norm；而L1norm本质上是假设参数先验是服从Laplace分布的，而L2-norm是假设参数先验为Gaussian分布，我们在网上看到的通常用图像来解答这个问题的原理就在这。 但是L1-norm的求解比较困难，可以用坐标轴下降法或是最小角回归法求解。 （4）LR和SVM对比 首先，LR和SVM最大的区别在于损失函数的选择，LR的损失函数为Log损失（或者说是逻辑损失都可以）、而SVM的损失函数为hinge loss。 其次，两者都是线性模型。 最后，SVM只考虑支持向量（也就是和分类相关的少数点） （5）LR和随机森林区别 随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。 （6）常用的优化方法 逻辑回归本身是可以用公式求解的，但是因为需要求逆的复杂度太高，所以才引入了梯度下降算法。 一阶方法：梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。 二阶方法：牛顿法、拟牛顿法： 这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。 缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。 拟牛顿法：不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。 逻辑回归为什么要对特征进行离散化在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点： 离散特征的增加和减少都很容易，易于模型的快速迭代； 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。 逻辑回归并行化怎么做，有几种并行化方式，读过哪些开源的实现http://www.csdn.net/article/2014-02-13/2818400-2014-02-13","link":"/2017/10/19/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"},{"title":"机器学习笔记","text":"置顶 理解矩阵 显卡、GPU和CUDA简介 OpenCV、OpenCV、CUDA python Flask WEB微框架 有监督、无监督与半监督学习 判别式模型与生成式模型 机器学习项目开发流程 过拟合？欠拟合？ 防止过拟合的方法 机器学习中，有哪些特征选择的工程方法 如何进行特征选择 你知道有哪些数据处理和特征工程的处理 数据预处理 数据不平衡问题 特征向量的归一化方法有哪些 机器学习中，为何要经常对数据做归一化 哪些机器学习算法不需要做归一化处理 对于树形结构为什么不需要归一化 标准化与归一化的区别 模型评估指标 ROC与AUC 常见损失函数 除了MSE，还有哪些模型效果判断方法？区别是什么 线性分类器与非线性分类器的区别以及优劣 对于维度极低的特征，选择线性还是非线性分类器 常见的分类算法有哪些 特征比数据量还大时，选择什么样的分类器 PCA_吴恩达 异常值检测_吴恩达 推荐系统_吴恩达 协同过滤和基于内容推荐有什么区别 回归算法 示例代码 什么是回归算法 回归算法推导：目标函数、对数似然及最小二乘 回归怎么预防过拟合 回归要调的参数有哪些 局部加权回归 Softmax回归 梯度的定义 什么是梯度下降？为什么要用梯度下降 梯度下降法容易收敛到局部最优，为什么应用广泛 梯度下降法找到的一定是下降最快的方向么 BGD、SGD、MBGD介绍与区别 BGD、SGD、MBGD的推导 牛顿法 拟牛顿法 牛顿法和梯度下降法的区别 共轭梯度 什么是正则化？L1正则与L2正则介绍 正则化为什么可以防止过拟合 L1和L2的区别 L1和L2正则先验分别服从什么分布 简单介绍下LR LR与线性回归的区别与联系 LR模型为什么要使用sigmoid函数 逻辑回归相关问题 逻辑回归为什么要对特征进行离散化 逻辑回归并行化怎么做，有几种并行化方式，读过哪些开源的实现 决策树、随机森林和提升算法 示例代码 信息熵、联合熵、条件熵、相对熵、互信息的定义 什么是最大熵 什么是决策树 决策树构建过程 决策树的纯度 ID3、C4.5、CART介绍 决策树优化策略 集成方法 bootstrap, boosting, bagging 简介 提升算法 Adboost、GBDT、Xgboost 简介 GBDT简介 从回归树到GBDT GBDT与随机森林 GBDT与XGBOOST 支持向量机 示例代码 SVM目标函数推导 大边界的直观理解与数学解释 核函数 常用核函数及核函数的条件 逻辑回归、SVM和神经网络使用场景 吴恩达SVM视频笔记 支持向量机通俗导论（理解SVM的三层境界） 带核的SVM为什么能分类非线性问题 贝叶斯 示例代码 贝叶斯定理相关公式 条件概率和后验概率区别 朴素贝叶斯 高斯朴素贝叶斯 伯努利朴素贝叶斯 多项式朴素贝叶斯 贝叶斯网络 怎么通俗易懂地解释贝叶斯网络和它的应用 用贝叶斯机率说明Dropout的原理 如何用贝叶斯算法实现垃圾邮件检测 当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑 KNN 简述KNN算法过程 KNN中的K如何选取的 kmeans kmeans的复杂度 优化Kmeans KMeans初始类簇中心点的选取。 在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离 常用的聚类划分方式有哪些列举代表算法 说说常见的损失函数 协方差和相关性有什么区别 谈谈判别式模型和生成式模型 LR与线性回归的区别与联系 LR和SVM的联系与区别 SVM、LR、决策树的对比 请比较下EM算法、HMM、CRF RF与GBDT之间的区别与联系 请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么 对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法 说说常见的优化算法及其优缺点 请大致对比下plsa和LDA的区别 请简要说说EM算法 什么最小二乘法 解释对偶的概念 试证明样本空间任意点x到超平面(w，b)的距离为(6.2) 什么是共线性， 跟过拟合有什么关联 什么是ill-condition病态问题 如何准备机器学习工程师的面试 如何判断某个人的机器学习水平 理解矩阵有人说，矩阵的本质就是线性方程式，两者是一一对应关系链接：http://www.ruanyifeng.com/blog/2015/09/matrix-multiplication.html 也有人说，矩阵的本质是运动的描述。简而言之，就是在线性空间中选定基之后，向量刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动。链接：https://pan.baidu.com/s/1BLyrQH5_VAw832jKkCgpBA 密码：ljwq 机器学习项目开发流程1 抽象成数学问题机器学习的训练过程很耗时，胡乱尝试时间成本太高，所以明确问题是进行机器学习的第一步。抽象成数学问题，指的明确我们可以获得什么样的数据，目标问题属于分类、回归还是聚类，如果都不是，如何转换为其中的某类问题。 2 获取数据数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。数据要有代表性，否则必然会过拟合。而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。 3 特征预处理与特征选择良好的数据要能够提取出良好的特征才能真正发挥效力。特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。 筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。 4 训练模型与调优直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。 5 模型诊断如何确定模型调优的方向与思路呢这就需要对模型进行诊断的技术。过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。 误差分析，也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。 6 模型融合一般来说，模型融合后都能使得效果有一定提升。而且效果很好。工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。 7 上线运行这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。 有监督、无监督与半监督学习 有监督学习：对有标记的训练样本进行学习，对训练样本外的数据进行预测。如 LR（Logistic Regression），SVM（Support Vector Machine），RF（RandomForest），GBDT（Gradient Boosting Decision Tree），感知机（Perceptron）、BP神经网络（Back Propagation）等。 无监督学习：对未标记的样本进行训练学习，试图发现样本中的内在结构。如聚类(KMeans)、降维、文本处理、DL。 半监督学习(Semi-Supervised Learning，SSL)：主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类，是有监督学习和无监督学习的结合。半监督学习对于减少标注代价，提高学习机器性能具有重大实际意义。 判别式模型与生成式模型判别式模型(Discriminative Model)：直接对条件概率p(y|x)进行建模，常见判别模型有：线性回归、决策树、支持向量机SVM、k近邻、神经网络、集成学习、条件随机场CRF等； 生成式模型(Generative Model)：对联合分布概率p(x,y)进行建模，常见生成式模型有：隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM及其他混合模型、LDA和限制波兹曼机等； 生成式模型更普适，判别式模型更直接，目标性更强。生成式模型关注数据是如何产生的，寻找的是数据分布模型。判别式模型关注的数据的差异性，寻找的是分类面。由生成式模型可以产生判别是模型，但是由判别式模式没法形成生成式模型 机器学习中，有哪些特征选择的工程方法数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已 计算每一个特征与响应变量的相关性工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性，而**互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了； 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征； 通过L1正则项来选择特征L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验； 训练能够对特征打分的预选模型RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型； 通过特征组合后再来选择特征如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型。 通过深度学习来进行特征选择目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。 如何进行特征选择特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解 常见的特征选择方式： 去除方差较小的特征，比如PCA 正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。 你知道有哪些数据处理和特征工程的处理 数据预处理1、特征向量的缺失值处理:1）缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的noise。2）缺失值较少，其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理: 把NaN直接作为一个特征，假设用0表示； 连续值，用均值填充； 用随机森林等算法预测填充； 2、连续值：离散化。有的模型（如决策树）需要离散值3、对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。如图像操作4、皮尔逊相关系数，去除高度相关的列 在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离闵可夫斯基距离(Minkowski)（1）当p为1的时候是曼哈顿距离(Manhattan)（城市街区距离）（2）当p为2的时候是欧式距离(Euclidean)（又称欧几里得度量，最常见的两点之间或多点之间的距离表示法）（3）当p为无穷大的时候是切比雪夫距离(Chebyshev) 不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。而欧氏距离可用于任何空间的距离计算。因为数据点可以存在于任何空间，所以欧氏距离是更可行的选择。 但欧氏距离也有明显的缺点。它将样本不同属性（即各指标或各变量量纲）之间的差别等同看待，这一点有时不能满足实际要求。 曼哈顿距离和欧式距离一般用途不同，无相互替代性。 过拟合？欠拟合？欠拟合：算法不太符合样本的数据特征过拟合：算法太符合样本的数据特征，对实际生产中的数据特征却无法拟合 过拟合(overfitting)具体现象体现为，随着训练过程的进行，模型复杂度增加，在训练集上的错误率渐渐减小，但是在验证集上的错误率却渐渐增大。 特征过多，特征数量级过大，训练数据过少，都可能导致过度拟合。过拟合会让模型泛化能力变差。 防止过拟合的方法降低过拟合的办法一般如下： 通过特征选择/特征降维，使用更简单的模型：特征过多或者过复杂都会导致过拟合 数据集扩增：从数据源头获取、根据当前数据生成、数据增强等 交叉验证 集成方法 Bootstrap/Bagging 正则化(Regularization) L2正则化：目标函数中增加所有权重w参数的平方之和， 逼迫所有w尽可能趋向零但不为零. 因为过拟合的时候， 拟合函数需要顾忌每一个点， 最终形成的拟合函数波动很大， 在某些很小的区间里， 函数值的变化很剧烈， 也就是某些w非常大. 为此， L2正则化的加入就惩罚了权重变大的趋势. L1正则化：目标函数中增加所有权重w参数的绝对值之和， 逼迫更多w为零(也就是变稀疏. L2因为其导数也趋0， 奔向零的速度不如L1给力了). 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的特征权重反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为0。 随机失活(dropout)在训练的运行的时候，让神经元以超参数p的概率被激活(也就是1-p的概率被设置为0)， 每个w因此随机参与， 使得任意w都不是不可或缺的， 效果类似于数量巨大的模型集成。 逐层归一化(batch normalization)这个方法给每层的输出都做一次归一化(网络上相当于加了一个线性变换层)， 使得下一层的输入接近高斯分布. 这个方法相当于下一层的w训练时避免了其输入以偏概全， 因而泛化效果非常好. 提前终止(early stopping)理论上可能的局部极小值数量随参数的数量呈指数增长， 到达某个精确的最小值是不良泛化的一个来源. 实践表明， 追求细粒度极小值具有较高的泛化误差。这是直观的，因为我们通常会希望我们的误差函数是平滑的， 精确的最小值处所见相应误差曲面具有高度不规则性， 而我们的泛化要求减少精确度去获得平滑最小值， 所以很多训练方法都提出了提前终止策略. 典型的方法是根据交叉叉验证提前终止: 若每次训练前， 将训练数据划分为若干份， 取一份为测试集， 其他为训练集， 每次训练完立即拿此次选中的测试集自测. 因为每份都有一次机会当测试集， 所以此方法称之为交叉验证. 交叉验证的错误率最小时可以认为泛化性能最好， 这时候训练错误率虽然还在继续下降， 但也得终止继续训练了. kmeans的复杂度 时间复杂度：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数空间复杂度：O((m+K)n)，其中，K为簇的数目，m为记录数，n为维数 对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法没有免费的午餐定理： 对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。 也就是说：对于所有问题，无论学习算法A多聪明，学习算法 B多笨拙，它们的期望性能相同。 但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。 请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么集成学习的集成对象是学习器. Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器， 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器， 迭代集成(平滑加权).决策树属于最常用的学习器， 其学习过程是从根建立树， 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂， CART决策树用基尼指数计算最优分裂， xgboost决策树使用二阶泰勒展开系数计算最优分裂.下面所提到的学习器都是决策树:Bagging方法: 学习器间不存在强依赖关系， 学习器可并行训练生成， 集成方式一般为投票; Random Forest属于Bagging的代表， 放回抽样， 每个学习器随机选择部分特征去优化;Boosting方法: 学习器之间存在强依赖关系、必须串行生成， 集成方式为加权和; Adaboost属于Boosting， 采用指数损失函数替代原本分类任务的0/1损失函数; GBDT属于Boosting的优秀代表， 对函数残差近似值进行梯度下降， 用CART回归树做学习器， 集成为回归模型; xgboost属于Boosting的集大成者， 对函数残差近似值进行梯度下降， 迭代时利用了二阶梯度信息， 集成模型可分类也可回归. 由于它可在特征粒度上并行计算， 结构风险和工程实现都做了很多优化， 泛化， 性能和扩展性都比GBDT要好。关于决策树，这里有篇《决策树算法》（链接：http://blog.csdn.net/v_july_v/article/details/7577684）。而随机森林Random Forest是一个包含多个决策树的分类器。至于AdaBoost，则是英文”Adaptive Boosting”（自适应增强）的缩写，关于AdaBoost可以看下这篇文章《Adaboost 算法的原理与推导》。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。 xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的 说说常见的损失函数对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实值Y可能一致也可能不一致（要知道，有时损失或误差是不可避免的），用一个损失函数来度量预测错误的程度。损失函数记为L(Y， f(X))。 常用的损失函数有以下几种（基本引用自《统计学习方法》）： 如此，SVM有第二种理解，即最优化+损失最小，或如@夏粉_百度所说“可从损失函数和优化算法角度看SVM，boosting，LR等算法，可能会有不同收获”。 关于SVM的更多理解请参考：，链接： 协方差和相关性有什么区别相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。 谈谈判别式模型和生成式模型判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。生成方法：由数据学习联合概率密度分布函数 P（X，Y），然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。由生成模型可以得到判别模型，但由判别模型得不到生成模型。常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机 线性分类器与非线性分类器的区别以及优劣线性和非线性是针对，模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2那么就是非线性模型，如果输入是x和X^2则模型是线性的。线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。常见的线性分类器有：LR，贝叶斯分类，单层感知机、线性回归常见的非线性分类器：决策树、RF、GBDT、多层感知机SVM两种都有（看线性核还是高斯核） 请大致对比下plsa和LDA的区别更多请参见：《通俗理解LDA主题模型》（链接：http://blog.csdn.net/v_july_v/article/details/41209515）。 请简要说说EM算法本题解析来源：@tornadomeet，链接：http://www.cnblogs.com/tornadomeet/p/3395593.html 什么最小二乘法对了，最小二乘法跟SVM有什么联系呢请参见《支持向量机通俗导论（理解SVM的三层境界）》(链接：http://blog.csdn.net/v_july_v/article/details/7624837）。 优化KmeansKmeans总结：https://www.processon.com/view/link/5ad81e5be4b046910642bc4b 使用kd树或者ball tree将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。 KMeans初始类簇中心点的选取。k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。 从输入的数据点集合中随机选择一个点作为第一个聚类中心 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x) 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大 重复2和3直到k个聚类中心被选出来 利用这k个初始的聚类中心来运行标准的k-means算法 解释对偶的概念一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。 数据不平衡问题 采样，对小样本加噪声采样（过拟合），对大样本进行下采样（欠拟合） 数据生成，利用已知样本生成新的样本（深度学习的数据增强） 进行特殊的加权，如在Adaboost中或者SVM中 采用对不平衡数据集不敏感的算法 改变评价标准：用AUC/ROC来进行评价 采用Bagging/Boosting/ensemble等方法 在设计模型的时候考虑数据的先验分布 详见：https://www.cnblogs.com/zhaokui/p/5101301.html 图片数据增强（Data augmentation）： 水平翻转 随机裁剪 样本不平衡, 进行 Label shuffle 其他平移变换；旋转/仿射变换；高斯噪声、模糊处理对颜色的数据增强：图像亮度、饱和度、对比度变化 特征比数据量还大时，选择什么样的分类器线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。 常见的分类算法有哪些SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯 说说常见的优化算法及其优缺点温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。简言之1）随机梯度下降优点：可以一定程度上解决局部最优解的问题缺点：收敛速度较慢2）批量梯度下降优点：容易陷入局部最优解缺点：收敛速度较快3）mini_batch梯度下降综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。4）牛顿法牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难。5）拟牛顿法拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。 具体而言从每个batch的数据来区分梯度下降：每次使用全部数据集进行训练优点：得到的是最优解缺点：运行速度慢，内存可能不够随机梯度下降：每次使用一个数据进行训练优点：训练速度快，无内存问题缺点：容易震荡，可能达不到最优解Mini-batch梯度下降优点：训练速度快，无内存问题，震荡较少缺点：可能达不到最优解 从优化方法上来分：随机梯度下降（SGD）缺点选择合适的learning rate比较难对于所有的参数使用同样的learning rate容易收敛到局部最优可能困在saddle pointSGD+Momentum优点：积累动量，加速训练局部极值附近震荡时，由于动量，跳出陷阱梯度方向发生变化时，动量缓解动荡。Nesterov Mementum与Mementum类似，优点：避免前进太快提高灵敏度AdaGrad优点：控制学习率，每一个分量有各自不同的学习率适合稀疏数据缺点依赖一个全局学习率学习率设置太大，其影响过于敏感后期，调整学习率的分母积累的太大，导致学习率很低，提前结束训练。RMSProp优点：解决了后期提前结束的问题。缺点：依然依赖全局学习率AdamAdagrad和RMSProp的合体优点：结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点为不同的参数计算不同的自适应学习率也适用于大多非凸优化 - 适用于大数据集和高维空间牛顿法牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难拟牛顿法拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。 机器学习中，为何要经常对数据做归一化 归一化后加快了梯度下降求最优解的速度 归一化有可能提高精度。 如上图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛； 而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛（消除了量纲的影响）。 至于为什么能提高精度，比如一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。 值域范围大的特征将在cost函数中得到更大的加权（如果较高幅值的特征改变 1%，则该改变相当大，但是对于较小的特征，该改变相当小），数据归一化使所有特征的权重相等。 特征向量的归一化方法有哪些 线性归一化（Min-Max Normalization），表达式如下：也称为离差标准化，是对原始数据的线性变换，使结果值映射到[0 - 1]之间y=(x-MinValue)/(MaxValue-MinValue)这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。 Z-score标准化，减去均值，除以方差：经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为y=(x-means)/ variance 非线性归一化。经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。 对数函数转换，表达式如下： y=log10 (x) 反余切函数转换 ，表达式如下： y=arctan(x)*2/PI 哪些机器学习算法不需要做归一化处理概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，树形结构就属于概率模型，如决策树、rf。 而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。 归一化和标准化主要是为了使计算更方便，加快求解最优解的速度。比如两个变量的量纲不同，可能一个的数值远大于另一个，那么他们同时作为变量的时候，可能会造成数值计算问题，比如说求矩阵的逆可能很不精确，或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能，量纲也需要调整。 对于树形结构为什么不需要归一化数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。 对于线性模型，比如说LR，我有两个特征，一个是(0，1)的，一个是(0，10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。 另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。 标准化与归一化的区别简单来说，归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。 标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。 模型评估指标有时候为了更好更快的评估模型，我们需要设置评估指标。常见的评估指标有：准确率/召回率/精准率/F值1）准确率(Accuracy) = 预测正确的样本数/总样本数2）召回率(Recall) = 预测正确的正例样本数/样本中的全部正例样本数3）精准率(Precision) = 预测正确的正例样本数/预测为正例的样本数4）F值 = PrecisionRecall2 / (Precision+Recall) (即F值为正确率和召回率的调和平均值) ROC与AUC对于分类器，或者说分类算法，评价指标主要有精准率(Precision)，召回率(Recall)，F-score1，以及现在所说的ROC和AUC。ROC、AUC相比准确率、召回率、F-score这样的评价指标，有这样一个很好的特性：当测试集中正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。 ROC曲线的纵轴是“真正例率”（True Positive Rate 简称TPR），横轴是“假正例率” （False Positive Rate 简称FPR）。ROC曲线反映了FPR与TPR之间权衡的情况，通俗地来说，即在TPR随着FPR递增的情况下，谁增长得更快，快多少的问题。TPR增长得越快，曲线越往上屈，AUC就越大，反映了模型的分类性能就越好。当正负样本不平衡时，这种模型评价方式比起一般的精确度评价方式的好处尤其显著。 AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而AUC作为数值可以直观的评价分类器的好坏，值越大越好。1）AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。2）0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。3）AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。4）AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。 至于ROC曲线具体是如何画出来的，这里推荐一篇文章：ROC和AUC介绍以及如何计算AUC 机器学习和统计里面的auc怎么理解？为什么比accuracy更常用？ 常见损失函数 铰链损失（Hinge Loss）：主要用于支持向量机（SVM） 中； $\\ell(y) = \\max(0, 1-t \\cdot y)$ 交叉熵损失 （Cross Entropy Loss，Softmax Loss ）：用于Logistic 回归与Softmax 分类中； 平方和损失（Square Loss）：主要是普通最小二乘法( Ordinary Least Square，OLS)中； 指数损失（Exponential Loss） ：主要用于Adaboost 集成学习算法中，详见 Adaboost与指数损失 其他损失（如0-1损失，绝对值损失） 参考文献： https://blog.csdn.net/u010976453/article/details/78488279 http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/ 除了MSE，还有哪些模型效果判断方法？区别是什么 SSE(The sum of squares due to error)和方差、误差平方和。计算的是拟合数据和原始数据对应点的误差的平方和，越趋近于0表示模型越拟合训练数据。 SSE=\\sum_{t=1}^{N}(observed_t-predicted_t)^2 MSE(Mean squared error)：均方误差。计算的是预测数据和原始数据对应点误差的平方和的均值，越趋近于0表示模型越拟合训练数据。 MSE=\\frac{1}{N}\\sum_{t=1}^{N}(observed_t-predicted_t)^2 RMSE(Root mean squared error)：均方根，也叫回归系统的拟合标准差，是MSE的平方根 RMSE=\\sqrt{\\frac{1}{N}\\sum_{t=1}^{N}(observed_t-predicted_t)^2} MAE(Mean Absolute Error)：平均绝对误差是绝对误差的平均值。平均绝对误差能更好地反映预测值误差的实际情况. MAE={\\frac{1}{N}\\sum_{i=1}^{N}\\lvert(observed_t-predicted_t)\\rvert} RMS(Root Mean Square)：均方根值计算方法是先平方、再平均、然后开方，维基百科定义 SD(standard Deviation)：标准差又常称均方差，是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的两组组数据，标准差未必相同。 SD=\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_i-u)^2} $R^2(R-Squared)$ ：取值范围(负无穷,1]，值越大表示模型越拟合训练数据；最优解是1；当模型预测为随机值的时候，有可能为负；若预测值恒为样本期望，$R^2$ 为0 Adjusted R-Squared: R_{adj}^2=1-\\frac{(n-1)(1-R^2)}{n-p-1}$R^2$ 及其校正函数用处详见第18题：机器学习笔试题精选 TSS(Total Sum of Squares)：总平方和，TSS表示样本之间的差异情况，是伪方差的m倍 RSS(Residual Sum of Squares)：残差平方和，RSS表示预测值和样本值之间的差异情况，是MSE的m倍 试证明样本空间任意点x到超平面(w，b)的距离为(6.2) 什么是共线性， 跟过拟合有什么关联共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。共线性会造成冗余，导致过拟合。解决方法：排除变量的相关性／加入权重正则。 对于维度极低的特征，选择线性还是非线性分类器非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。 什么是ill-condition病态问题训练完的模型，测试样本稍作修改就会得到差别很大的结果，就是病态问题，模型对未知数据的预测能力很差，即泛化误差大。 简述KNN算法过程谓k最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别； KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成反比。 KNN中的K如何选取的关于什么是KNN，可以查看此文：《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》（链接：http://blog.csdn.net/v_july_v/article/details/8203674）。KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说： 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合； 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。 K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。 在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。 常用的聚类划分方式有哪些列举代表算法 基于划分的聚类:K-means，k-medoids，CLARANS。 基于层次的聚类：AGNES（自底向上），DIANA（自上向下）。 基于密度的聚类：DBSACN，OPTICS，BIRCH(CF-Tree)，CURE。 基于网格的方法：STING，WaveCluster。 基于模型的聚类：EM，SOM，COBWEB。 LR和SVM的联系与区别联系：1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。区别：1、LR是参数模型，SVM是非参数模型。2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算。5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。 SVM、LR、决策树的对比 模型复杂度：SVM支持核函数，可处理线性非线性问题; LR模型简单，训练速度快，适合处理线性问题; 决策树容易过拟合，需要进行剪枝. 损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失 数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感 数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核 请比较下EM算法、HMM、CRF这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。（1）EM算法 EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。 注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。（2）HMM算法 隐马尔可夫模型是用于标注问题的生成模型。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。马尔科夫三个基本问题：概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）（3）条件随机场CRF 给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。 之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。（4）HMM和CRF对比 其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。 RF与GBDT之间的区别与联系1）相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。2）不同点：a 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成b 组成随机森林的树可以并行生成，而GBDT是串行生成c 随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和d 随机森林对异常值不敏感，而GBDT对异常值比较敏感e 随机森林是减少模型的方差，而GBDT是减少模型的偏差f 随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化","link":"/2018/01/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"机器学习选择题集锦","text":"回归1、对于线性回归，我们应该有以下哪些假设？ 找到离群点很重要, 因为线性回归对离群点很敏感 线性回归要求所有变量必须符合正态分布 线性回归假设数据没有多重线性相关性A 1 和 2B 2 和 3C 1,2 和 3D 以上都不是 答案：D解析：第1个假设, 离群点要着重考虑, 第一点是对的第2个假设, 正态分布不是必须的. 当然, 如果是正态分布, 训练效果会更好第3个假设, 有少量的多重线性相关性也是可以的, 但是我们要尽量避免 只含有一个解释变量的线性回归模型称为“一元线性回归模型”。在一个方程式中含有一个以上的解释变量的线性回归模型称为“多元线性回归模型”。在多元线性回归模型中，各个解释变量之间不能存在线性相关关系。如果存在，则称该模型具有“多重共线性”。多重线性回归是简单直线回归的推广，研究一个因变量与多个自变量之间的数量依存关系。在建立多元线性回归模型时，在变量的选取上要避免出现多重共线性问题。 2、当我们构造线性模型时, 我们注意变量间的相关性. 在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论: Var1和Var2是非常相关的 因为Var1和Var2是非常相关的, 我们可以去除其中一个 Var3和Var1的1.23相关系数是不可能的A 1 and 3B 1 and 2C 1,2 and 3D 1 答案：C解析：相关性系数范围应该是 [-1,1]一般如果相关系数大于0.7或者小于-0.7, 是高相关的.Var1和Var2相关系数是接近负1, 所以这是多重线性相关, 我们可以考虑去除其中一个.所以1, 2, 3个结论都是对的, 选C. 3、机器学习中L1正则化和L2正则化的区别是？A 使用L1可以得到稀疏的权值B 使用L1可以得到平滑的权值C 使用L2可以得到稀疏的权值 答案：A解析：L1 lasso回归，L2岭回归L1正则化偏向于稀疏，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0.L2主要功能是为了防止过拟合，当所求参数越小时，说明模型越简单，而模型越简单则，越趋向于平滑，从而防止过拟合。 L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。 L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。 可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。 因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。 4、你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？A 第一个 w2 成了 0，接着 w1 也成了 0B 第一个 w1 成了 0，接着 w2 也成了 0C w1 和 w2 同时成了 0D 即使在 C 成为大值之后，w1 和 w2 都不能成 0 答案：C解析：L1正则化的函数如下图，所以w1和w2可以为0。同时w1和w2是对称的，不会导致一个为0另一个不为0的状态。 5、如果在一个高度非线性并且复杂的一些变量中, 一个树模型可能比一般的回归模型效果更好. 这是（）A 对的B 错的 答案：A 6、在Logistic Regression 中,如果同时加入L1和L2范数,不会产生什么效果()A 以做特征选择,并在一定程度上防止过拟合B 能解决维度灾难问题C 能加快计算速度D 可以获得更准确的结果 答案：D解析：在代价函数后面加上正则项，L1即是Losso回归，L2是岭回归。L1范数是指向量中各个元素绝对值之和，用于特征选择。L2范数 是指向量各元素的平方和然后求平方根，用于 防止过拟合，提升模型的泛化能力。 L1范数具有系数解的特性，但是要注意的是，L1没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。所以能加快计算速度和缓解维数灾难. 7、对数几率回归（logistics regression）和一般回归分析有什么区别？A 对数几率回归是设计用来预测事件可能性的B 对数几率回归可以用来度量模型拟合程度C 对数几率回归可以用来估计回归系数D 以上所有 答案：D解析：A: 对数几率回归其实是设计用来解决分类问题的B: 对数几率回归可以用来检验模型对数据的拟合度C: 虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数。就我认为，这只是估计回归系数，不能直接用来做回归模型。 8、回归模型中存在多重共线性, 你如何解决这个问题？1 去除这两个共线性变量2 我们可以先去除一个共线性变量3 计算VIF(方差膨胀因子), 采取相应措施4 为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归A 1B 2C 2和3D 2, 3和4 答案：D解析：解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值&lt;=4说明相关性不是很高, VIF值&gt;=10说明相关性较高.我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。 9、给线性回归模型添加一个不重要的特征可能会造成？A 增加 R-squareB 减少 R-square 答案：A解析：在给特征空间添加了一个特征后，不论特征是重要还是不重要，R-square 通常会增加。R-square定义如下:在给特征空间添加了一个特征后，分子会增加一个残差平方项, 分母会增加一个均值差平方项, 前者一般小于后者, 所以不论特征是重要还是不重要，R-square 通常会增加 10、对于线性回归模型，包括附加变量在内，以下的可能正确的是 : R-Squared 和 Adjusted R-squared都是递增的 R-Squared 是常量的，Adjusted R-squared是递增的 R-Squared 是递减的， Adjusted R-squared 也是递减的 R-Squared 是递减的， Adjusted R-squared是递增的A 1 和 2B 1 和 3C 2 和 4D 以上都不是 答案：D解析：R-squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-squared有R-squared 和 predicted R-squared 所没有的问题。每次你为模型加入预测器，R-squared递增或不变. 11、线性回归的基本假设不包括哪个？A 随机误差项是一个期望值为0的随机变量B 对于解释变量的所有观测值，随机误差项有相同的方差C 随机误差项彼此相关D 解释变量是确定性变量不是随机变量，与随机误差项之间相互独立E 随机误差项服从正态分布 答案：C 8、一监狱人脸识别准入系统用来识别待进入人员的身份，此系统一共包括识别4种不同的人员：狱警，小偷，送餐员，其他。下面哪种学习方法最适合此种应用需求：A 二分类问题B 多分类问题C 层次聚类问题D k-中心点聚类问题E 回归问题F 结构分析问题 答案：B解析：二分类：每个分类器只能把样本分为两类。监狱里的样本分别为狱警、小偷、送餐员、其他。二分类肯 定行不通。瓦普尼克95年提出来基础的支持向量机就是个二分类的分类器，这个分类器学习过 程就是解一个基于正负二分类推导而来的一个最优规划问题（对偶问题），要解决多分类问题 就要用决策树把二分类的分类器级联，VC维的概念就是说的这事的复杂度。层次聚类： 创建一个层次等级以分解给定的数据集。监狱里的对象分别是狱警、小偷、送餐员、或者其 他，他们等级应该是平等的，所以不行。此方法分为自上而下（分解）和自下而上（合并）两种操作方式。K-中心点聚类：挑选实际对象来代表簇，每个簇使用一个代表对象。它是围绕中心点划分的一种规则，所以这里并不合适。回归分析：处理变量之间具有相关性的一种统计方法，这里的狱警、小偷、送餐员、其他之间并没有什 么直接关系。结构分析： 结构分析法是在统计分组的基础上，计算各组成部分所占比重，进而分析某一总体现象的内部结构特征、总体的性质、总体内部结构依时间推移而表现出的变化规律性的统计方法。结构分析法的基本表现形式，就是计算结构指标。这里也行不通。多分类问题： 针对不同的属性训练几个不同的弱分类器，然后将它们集成为一个强分类器。这里狱警、 小偷、送餐员 以及他某某，分别根据他们的特点设定依据，然后进行区分识别。 贝叶斯 NB1、Nave Bayes(朴素贝叶斯)是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是（）A 各类别的先验概率P(C)是相等的B 以0为均值，sqr(2)/2为标准差的正态分布C 特征变量X的各个维度是类别条件独立随机变量D P(X|C)是高斯分布 答案：C解析：朴素贝叶斯的基本假设就是每个变量相互独立。 7、假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中不正确的是？A 模型效果相比无重复特征的情况下精确度会降低B 如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样C 当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题 答案：B解析：朴素贝叶斯的条件就是每个变量相互独立。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。此外，若高度相关的特征在模型中引入两次, 这样增加了这一特征的重要性, 则它的性能因数据包含高度相关的特征而下降。正确做法是评估特征的相关矩阵，并移除那些高度相关的特征。 7、符号集 a 、 b 、 c 、 d ，它们相互独立，相应概率为 1/2 、 1/4 、 1/8/ 、 1/16 ，其中包含信息量最小的符号是（ ）A aB bC cD d 答案：A解析：因为消息出现的概率越小，则消息中所包含的信息量就越大。因此选a,同理d信息量最大。 HMM9、隐马尔可夫模型三个基本问题以及相应的算法说法错误的是（ ）A 评估—前向后向算法B 解码—维特比算法C 学习—Baum-Welch算法D 学习—前向后向算法 答案：D解析：评估问题，可以使用前向算法、后向算法、前向后向算法。 9、解决隐马模型中预测问题的算法是A 前向算法B 后向算法C Baum-Welch算法D 维特比算法 答案：D解析：@刘炫320，本题题目及解析来源：http://blog.csdn.net/column/details/16442.htmlA、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。C：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；D：维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。 6、在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计（）A EM算法B 维特比算法C 前向后向算法D 极大似然估计 答案：D解析：EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法维特比算法： 用动态规划解决HMM的预测问题，不是参数估计前向后向算法：用来算概率极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。 5、在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计()A EM算法B 维特比算法C 前向后向算法D 极大似然估计 答案：D解析：EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法维特比算法： 用动态规划解决HMM的预测问题，不是参数估计前向后向算法：用来算概率极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。 8、请选择下面可以应用隐马尔科夫(HMM)模型的选项A 基因序列数据集B 电影浏览数据集C 股票市场数据集D 所有以上 答案：D解析：只要是和时间序列问题有关的 , 都可以试试HMM 6、下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）A 特征灵活B 速度快C 可容纳较多上下文信息D 全局最优答案： B解析：CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较 CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较 SVM5、假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分？A 设C=1B 设C=0C 设C=无穷大D 以上都不对 答案：C解析：无穷大保证了所有的线性不可分都是可以忍受的.最大间距分类只有当参数 C 是非常大的时候才起效，模型会过拟合。 6、训练完SVM模型后, 不是支持向量的那些样本我们可以丢掉, 也可以继续分类A 正确B 错误 答案：A解析：SVM模型中, 真正影响决策边界的是支持向量 4、关于支持向量机SVM,下列说法错误的是（）A L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力B Hinge 损失函数，作用是最小化经验分类错误C 分类间隔为1/||w||，||w||代表向量的模D 当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习 答案：C解析：A正确。考虑加入正则化项的原因：想象一个完美的数据集，y&gt;1是正类，y&lt;-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。 Hinge 可以用来解 间距最大化 的问题，最有代表性的就是SVM 问题，详见：https://blog.csdn.net/u010976453/article/details/78488279 10、有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是( )A 2x+y=4B x+2y=5C x+2y=3D 2x-y=0 答案：C解析：这道题简化了，对于两个点来说，最大间隔就是垂直平分线，因此求出垂直平分线即可。斜率是两点连线的斜率的负倒数-1/((-1-3)/(0-2)) = -1/2, 可得y=-(1/2)x + c, 过中点((0+2)/2, (-1+3)/2) = (1, 1), 可得c=3/2, 故选C. 10、关于logit 回归和SVM 不正确的是（ ） 和下题矛盾A Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。B Logit回归的输出就是样本属于正类别的几率，可以计算出概率。C SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。D SVM可以通过正则化系数控制模型的复杂度，避免过拟合。 答案：A解析：Logit回归目标函数是最小化后验概率，SVM目标是结构风险最小化，SVM可以有效避免模型过拟合。 9、关于 logit 回归和 SVM 不正确的是（）A Logit回归目标函数是最小化后验概率B Logit回归可以用于预测事件发生概率的大小C SVM目标是结构风险最小化D SVM可以有效避免模型过拟合 答案：A解析：A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。 10、下图是同一个SVM模型, 但是使用了不同的径向基核函数的gamma参数, 依次是g1, g2, g3 , 下面大小比较正确的是Ag1 &gt; g2 &gt; g3Bg1 = g2 = g3Cg1 &lt; g2 &lt; g3Dg1 &gt;= g2 &gt;= g3E. g1 &lt;= g2 &lt;= g3 答案：C解析：所谓径向基函数 (Radial Basis Function 简称 RBF), 就是某种沿径向对称的标量函数。 通常定义为空间中任一点x到某一中心点xc之间欧氏距离的单调函数 , 可记作 k(||x-xc||), 其作用往往是局部的 , 即当x远离xc时函数取值很小。最常用的径向基函数是高斯核函数 ,形式为 k(||x-xc||)=exp{- ||x-xc||^2/(2σ^2) } 其中xc为核函数中心,σ为函数的宽度参数 , 控制了函数的径向作用范围。由radial basis: exp(-gamma|u-v|^2)可知, gamma越小, 模型越简单, 平滑度越好, 分类边界越不容易过拟合, 所以选C。 8、下列不是SVM核函数的是A 多项式核函数B logistic核函数C 径向基核函数D Sigmoid核函数 答案： B解析：SVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数. 核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K(xi, xj) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：(1)线性核函数K ( x , x i ) = x ⋅ x i(2)多项式核K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d(3)径向基核（RBF）K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 )Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。(4)傅里叶核K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 )(5)样条核K ( x , x i ) = B 2 n + 1 ( x − x i )(6)Sigmoid核函数K ( x , x i ) = tanh ( κ ( x , x i ) − δ )采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。 核函数的选择在选取核函数解决实际问题时，通常采用的方法有：一是利用专家的先验知识预先选定核函数；二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多．三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想． 决策树、随机森林、提升算法2、对于信息增益, 决策树分裂节点, 下面说法正确的是（）1 纯度高的节点需要更多的信息去区分2 信息增益可以用”1比特-熵”获得3 如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的A 1B 2C 2和3D 所有以上 答案：C 3、我们想在大数据集上训练决策树, 为了使用较少时间, 我们可以A 增加树的深度B 增加学习率 (learning rate)C 减少树的深度D 减少树的数量 答案：C解析：增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)决策树只有一棵树, 不是随机森林。 4、对于随机森林和GradientBoosting Trees, 下面说法正确的是:1.在随机森林的单个树中, 树和树之间是有依赖的, 而GradientBoosting Trees中的单个树之间是没有依赖的.2.这两个模型都使用随机特征子集, 来生成许多单个的树.3.我们可以并行地生成GradientBoosting Trees单个树, 因为它们之间是没有依赖的, GradientBoosting Trees训练模型的表现总是比随机森林好A 2B 1 and 2C 1 and 3D 2 and 3 答案：A解析：1.随机森林是基于bagging的, 而Gradient Boosting trees是基于boosting的, 所有说反了,在随机森林的单个树中, 树和树之间是没有依赖的, 而GradientBoosting Trees中的单个树之间是有依赖关系.2.这两个模型都使用随机特征子集, 来生成许多单个的树.所以A是正确的 3、bootstrap数据是什么意思？（提示：考“bootstrap”和“boosting”区别）A 有放回地从总共M个特征中抽样m个特征B 无放回地从总共M个特征中抽样m个特征C 有放回地从总共N个样本中抽样n个样本D 无放回地从总共N个样本中抽样n个样本 答案：C解析：boostrap是提鞋自举的意思(武侠小说作者所说的左脚踩右脚腾空而起). 它的过程是对样本(而不是特征)进行有放回的抽样, 抽样次数等同于样本总数. 这个随机抽样过程决定了最终抽样出来的样本, 去除重复之后, 占据原有样本的1/e比例. 5、数据科学家可能会同时使用多个算法（模型）进行预测， 并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是A 单个模型之间有高相关性B 单个模型之间有低相关性C 在集成学习中使用“平均权重”而不是“投票”会比较好D 单个模型都是用的一个算法 答案：B 2、以下说法中错误的是（）A SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性B 在adaboost算法中，所有被分错样本的权重更新比例不相同C boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重D 给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的 答案：C解析：A 软间隔分类器对噪声是有鲁棒性的。B 请参考http://blog.csdn.net/v_july_v/article/details/40718799C boosting是根据分类器正确率确定权重，bagging不是。D 训练集变大会提高模型鲁棒性。 8、下面对集成学习模型中的弱学习者描述错误的是？A 他们经常不会过拟合B 他们通常带有高偏差，所以其并不能解决复杂学习问题C 他们通常会过拟合 答案：C解析：弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。 8、下面关于ID3算法中说法错误的是（）A ID3算法要求特征必须离散化B 信息增益可以用熵，而不是GINI系数来计算C 选取信息增益最大的特征，作为树的根节点D ID3算法是一个二叉树模型 答案：D解析：ID3算法（IterativeDichotomiser3迭代二叉树3代）是一个由RossQuinlan发明的用于决策树的算法。可以归纳为以下几点：使用所有没有使用的属性并计算与之相关的样本熵值选取其中熵值最小的属性生成包含该属性的节点 D3算法对数据的要求：1)所有属性必须为离散量；2)所有的训练例的所有属性必须有一个明确的值；3)相同的因素必须得到相同的结论且训练例必须唯一。 条件熵1、一个二进制源X发出符号集为{-1,1}，经过离散无记忆信道传输，由于信道中噪音的存在，接收端Y收到符号集为{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求条件熵H(Y|X)（ ）A 0.2375B 0.3275C 0.5273D 0.5372 答案：A解析：信息熵公式：H(X)= -∑P(x)log(x)条件熵公式：H(Y|X)= -∑P(y,x)logP(y|x)= -∑P(y|x)P(x)logP(y|x)，将(y=-1,x=-1), (y=0,x=-1), (y=1,x=1), (y=0,x=1)四种情况带入公式求和，得 H(Y|X)≈-(-0.01938-0.03495-0.07028-0.11289)=0.2375。 9、目标变量在训练集上的 8 个实际值 [0,0,0,1,1,1,1,1]，目标变量的熵是多少？A -(5/8 log(5/8) + 3/8 log(3/8))B 5/8 log(5/8) + 3/8 log(3/8)C 3/8 log(5/8) + 5/8 log(3/8)D 5/8 log(3/8) – 3/8 log(5/8) 答案：A 特征选择1、下列哪个不属于常用的文本分类的特征选择算法？A 卡方检验值B 互信息C 信息增益D 主成分分析 答案：D解析：主成分分析是特征转换算法（特征抽取），而不是特征选择常采用特征选择方法。常见的六种特征选择方法：1）DF(Document Frequency) 文档频率DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性2）MI(Mutual Information) 互信息法互信息法用于衡量特征词与文档类别直接的信息量。如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向”低频”的特征词。相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。3）(Information Gain) 信息增益法通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。4）CHI(Chi-square) 卡方检验法利用了统计学中的”假设检验”的基本思想：首先假设特征词与类别直接是不相关的如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。5）WLLR(Weighted Log Likelihood Ration)加权对数似然6）WFO（Weighted Frequency and Odds）加权频率和可能性本题解析来源：http://blog.csdn.net/ztf312/article/details/50890099 1、机器学习中做特征选择时，可能用到的方法有？A 卡方B 信息增益C 平均互信息D 期望交叉熵E 以上都有 答案：E 降维2、为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做？A 将数据转换成零均值B 将数据转换成零中位数C 无法做到 答案：A解析：当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。 2、下列方法中，不可以用于特征降维的方法包括A 主成分分析PCAB 线性判别分析LDAC 深度学习SparseAutoEncoderD 矩阵奇异值分解SVD 答案：C 解析：特征降维方法主要有：PCA，LLE，IsomapSVD和PCA类似，也可以看成一种降维方法LDA:线性判别分析，可用于降维AutoEncoder：AutoEncoder的结构与神经网络的隐含层相同，由输入L1,输出 L2组成，中间则是权重连接。Autoencoder通过L2得到输入的重构L3，最小化L3与L1的差别 进行训练得到权重。在这样的权重参数下，得到的L2可以尽可能的保存L1的信息。Autoencoder的输出L2的维度由输出的神经元个数决定。当输出维度大于L1时，则需要在训练目标函数中加入sparse 惩罚项，避免L2直接复制L1（权重全为1）。所以称为sparseAutoencoder( Andrew Ng提出的)。结论：SparseAutoencoder大多数情况下都是升维的，所以称之为特征降维的方法不准确。 4、下列哪些不特别适合用来对高维数据进行降维A LASSOB 主成分分析法C 聚类分析D 小波分析法E 线性判别法F 拉普拉斯特征映射 答案：C解析：lasso通过参数缩减达到降维的目的；pca就不用说了线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维；小波分析有一些变换的操作降低其他干扰可以看做是降维拉普拉斯请看这个http://f.dataguru.cn/thread-287243-1-1.html 9、我们建立一个5000个特征, 100万数据的机器学习模型. 我们怎么有效地应对这样的大数据训练 :A 我们随机抽取一些样本, 在这些少量样本之上训练B 我们可以试用在线机器学习算法C 我们应用PCA算法降维, 减少特征数D B 和 CE A 和 BF 以上所有 答案：F解析：样本数过多, 或者特征数过多, 而不能单机完成训练, 可以用小批量样本训练, 或者在线累计式训练, 或者主成分PCA降维方式减少特征数量再进行训练. 10、我们想要减少数据集中的特征数, 即降维. 选择以下适合的方案 : 使用前向特征选择方法 使用后向特征排除方法 我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后我们去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表现比原来还要好, 我们可以去除这个特征. 查看相关性表, 去除相关性最高的一些特征A 1 和 2B 2, 3和4C 1, 2和4D All 答案：D解析：1.前向特征选择方法和后向特征排除方法是我们特征选择的常用方法2.如果前向特征选择方法和后向特征排除方法在大数据上不适用, 可以用这里第三种方法.3.用相关性的度量去删除多余特征, 也是一个好方法所有D是正确的 2、对于PCA(主成分分析)转化过的特征 , 朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 :A 正确的B 错误的 答案：B解析：这个说法是错误的, 首先, “不依赖”和”不相关”是两回事, 其次, 转化过的特征, 也可能是相关的 3、对于PCA说法正确的是 : 我们必须在使用PCA前规范化数据 我们应该选择使得模型有最大variance的主成分 我们应该选择使得模型有最小variance的主成分 我们可以使用PCA在低维度上做数据可视化A 1, 2 and 4B 2 and 4C 3 and 4D 1 and 3E 1, 3 and 4 答案：A解析：1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分).2）我们总是应该选择使得模型有最大variance的主成分3）有时在低维度上左图是需要PCA的降维帮助的 4、对于下图, 最好的主成分选择是多少 ?A 7B 30C 35D Can’t Say 答案：B解析：主成分选择使variance越大越好， 在这个前提下， 主成分越少越好。 10、最出名的降维算法是 PCA 和 t-SNE。将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？A X_projected_PCA 在最近邻空间能得到解释B X_projected_tSNE 在最近邻空间能得到解释C 两个都在最近邻空间能得到解释D 两个都不能在最近邻空间得到解释 答案：B解析：t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。 KNN3、使用k=1的knn算法, 下图二类分类问题, “+” 和 “o” 分别代表两个类, 那么, 用仅拿出一个测试样本的交叉验证方法, 交叉验证的错误率是多少：A 0%B 100%C 0%到100D 以上都不是 答案： B解析：knn算法就是, 在样本周围看k个样本, 其中大多数样本的分类是A类, 我们就把这个样本分成A类. 显然, k=1 的knn在上图不是一个好选择, 分类的错误率始终是100%。 9、以下哪个图是KNN算法的训练边界A BB AC DD CE 都不是 答案：B解析：KNN算法肯定不是线性的边界， 所以直的边界就不用考虑了。另外这个算法是看周围最近的k个样本的分类用以确定分类，所以边界一定是坑坑洼洼的。 10、一般，k-NN最近邻方法在（）的情况下效果较好A 样本较多但典型性不好B 样本较少但典型性好C 样本呈团状分布D 样本呈链状分布 答案： B（也有选A的），答案待定解析：K近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。 分类方法1、以下哪些方法不可以直接来对文本分类？A KmeansB 决策树C 支持向量机D KNN 答案：A解析：Kmeans是聚类方法，典型的无监督学习方法。分类是监督学习方法，BCD都是常见的分类方法。 2、以下( )不属于线性分类器最佳准则？A 感知准则函数B 贝叶斯分类C 支持向量机D Fisher准则 答案：B解析：线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。 1、下面有关分类算法的准确率，召回率，F1 值的描述，错误的是？A 准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率B 召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率C 正确率、召回率和 F 值取值都在0和1之间，数值越接近0，查准率或查全率就越高D 为了解决准确率和召回率冲突问题，引入了F1分数 答案：C解析：对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：TP——将正类预测为正类数FN——将正类预测为负类数FP——将负类预测为正类数TN——将负类预测为负类数由此：精准率定义为：P = TP / (TP + FP)召回率定义为：R = TP / (TP + FN)F1值定义为： F1 = 2 P R / (P + R)精准率和召回率和F1取值都在0和1之间，精准率和召回率高，F1值也会高，不存在数值越接近0越高的说法，应该是数值越接近1越高。 3、假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值, 那么现在关于模型说法, 正确的是 :1 模型分类的召回率会降低或不变2 模型分类的召回率会升高3 模型分类准确率会升高或不变4 模型分类准确率会降低A 1B 2C 1和3D 2和4E 以上都不是 答案：A解析：精确率, 准确率和召回率是广泛用于信息检索和统计学分类领域的度量值，用来评价结果的质量。下图可以帮助理解和记忆它们之间的关系, 其中精确率(precision)和准确率(accuracy)都是关于预测效果的描述. 召回率是关于预测样本的描述。精确率表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP), 也就是P = TP / (TP + FP)。 准确率表示的是预测的正负样本有多少是真实的正和负, 预测正确的数量占全部预测数量的比例, 也就是A = (TP + TN) / (TP + FP + TN + FN) = (TP + TN) / 全部样本。 召回率表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN), 也就是R = TP / (TP + FN)。精确率和召回率二者计算方法其实就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。提高分界阈值大于0.5, 则预测为正的样本数要降低, 相当于把图中圆圈变小, 按下图则可计算 召回率的分子变小分母不变, 所以召回率会变小或不变;精确率的分子分母同步变化, 所以精确率的变化不能确定;准确率的分子为圆内绿色加圆外右侧矩形面积所围样本, 两者之和变化不能确定; 分母为矩形所含全部样本不变化, 所以准确率的变化不能确定;综上, 所以选A。 聚类算法答案：D解析：所有都可以用来调试以找到全局最小。 1、模式识别中，不属于马式距离（协方差距离）较之于欧式距离的优点的是（ ）A 平移不变性B 尺度不变性C 考虑了模式的分布 答案：A还有一种距离叫做曼哈顿距离 7、以下属于欧式距离特性的有（）A 旋转不变性B 尺度缩放不变性C 不受量纲影响的特性 答案：A 2、以下不属于影响聚类算法结果的主要因素有（）A 已知类别的样本质量B 分类准则C 特征选取D 模式相似性测度 答案：A 4、在 k-均值算法中，以下哪个选项可用于获得全局最小？A 尝试为不同的质心（centroid）初始化运行算法B 调整迭代的次数C 找到集群的最佳数量D 以上所有 3、影响基本K-均值算法的主要因素有（）A 样本输入顺序B 模式相似性测度C 聚类准则 答案：B 5、如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有（ ）A 已知类别样本质量B 分类准则C 量纲 答案： B 5、以下对k-means聚类算法解释正确的是A 能自动识别类的个数,随即挑选初始点为中心点计算B 能自动识别类的个数,不是随即挑选初始点为中心点计算C 不能自动识别类的个数,随即挑选初始点为中心点计算D 不能自动识别类的个数,不是随即挑选初始点为中心点计算 答案：C解析：（1）适当选择c个类的初始中心；（2）在第k次迭代中，对任意一个样本，求其到c个中心的距离，将该样本归到距离最短的中心所在的类；（3）利用均值等方法更新该类的中心值；（4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。以上是KMeans（C均值）算法的具体步骤，可以看出需要选择类别数量，但初次选择是随机的，最终的聚类中心是不断迭代稳定以后的聚类中心。所以答案选C。 其他4、在统计模式分类问题中，当先验概率未知时，可以使用（）A 最小损失准则B 最小最大损失准则C 最小误判概率准则 答案：B 2、以下几种模型方法属于判别式模型(Discriminative Model)的有( )1)混合高斯模型2)条件随机场模型3)区分度训练4)隐马尔科夫模型A 2,3B 3,4C 1,4D 1,2 答案：A解析：常见的判别式模型有：Logistic regression（logistical 回归）Linear discriminant analysis（线性判别分析）Supportvector machines（支持向量机）Boosting（集成学习）Conditional random fields（条件随机场）Linear regression（线性回归）Neural networks（神经网络） 常见的生成式模型有:Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型）Hidden Markov model（隐马尔可夫）NaiveBayes（朴素贝叶斯）AODE（平均单依赖估计）Latent Dirichlet allocation（LDA主题模型）Restricted Boltzmann Machine（限制波兹曼机）生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果。 10、在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题？A 增加训练集量B 减少神经网络隐藏层节点数C 删除稀疏的特征D SVM算法中使用高斯核/RBF核代替线性核 答案：D解析：一般情况下，越复杂的系统，过拟合的可能性就越高，一般模型相对简单的话泛化能力会更好一点。B.一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， svm高斯核函数比线性核函数模型更复杂，容易过拟合D.径向基(RBF)核函数/高斯核函数的说明,这个核函数可以将原始空间映射到无穷维空间。对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数 之一。 4、“过拟合”只在监督学习中出现，在非监督学习中，没有“过拟合”，这是（）A 对的B 错的 答案： B解析：我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数（adjusted rand score）https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index 8、下面哪个/些超参数的增加可能会造成随机森林数据过拟合？A 树的数量B 树的深度C 学习速率 答案：B解析：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率并不是随机森林的超参数。增加树的数量可能会造成欠拟合。 5、假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。A 如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它B 对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大C log-loss 越低，模型越好D 以上都是 答案：D 6、下面哪个选项中哪一项属于确定性算法？A PCAB K-MeansC 以上都不是 答案：A解析：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。PCA没有需要调试的参数，Kmeans之所以每次的结果不同，因为初始化的点是随机的 7、两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。这句描述是正确还是错误？A 正确B 错误 答案：A解析：Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。如y=x^2，x和y有很强的非线性关系。 5、下列属于无监督学习的是A k-meansB SVMC 最大熵D CRF 答案：A解析：A是聚类，属于无监督学习。BC是分类，属于监督学习。至于D是序列化标注，也是有监督学习。 1、在一个n维的空间中， 最好的检测outlier(离群点)的方法是（）A 作正态分布概率图B 作盒形图C 马氏距离D 作散点图 5、对于k折交叉验证, 以下对k的说法正确的是（）A k越大, 不一定越好, 选择大的k会加大评估时间B 选择更大的k, 就会有更小的bias偏差(因为训练集更加接近总数据集)C 在选择k时, 要最小化数据集之间的方差D 以上所有 答案：D解析：k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差.高方差，表名数据扰动所造成的影响大，没有排除噪音影响，过拟合高偏差，说明学习能力弱，不能很好的分类或者回归，欠拟合 7、模型的高bias是什么意思, 我们如何降低它 ？A 在特征空间中减少特征B 在特征空间中增加特征C 增加数据点D B和CE 以上所有 答案：B解析：bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 ! 2、“点击率问题”是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是A 模型预测准确率已经很高了, 我们不需要做什么了B 模型预测准确率不高, 我们需要做点什么改进模型C 无法下结论D 以上都不对 答案： B解析：99%的预测准确率可能说明, 你预测的没有点进去的人很准确 (因为有99%的人是不会点进去的, 这很好预测). 不能说明你的模型对点进去的人预测准确, 所以, 对于这样的非平衡数据集, 我们要把注意力放在小部分的数据上, 即那些点击进去的人.详细可以参考这篇文章：https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/ 6、（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（）A Accuracy:(TP+TN)/allB F-value:2recallprecision/(recall+precision)C G-mean:sqrt(precision*recall)D AUC:曲线下面积 答案：A解析：题目提到测试集正例和负例数量不均衡，那么假设正例数量很少占10%，负例数量占大部分90%。而且算法能正确识别所有负例，但正例只有一半能正确判别。那么TP=0.05×all,TN=0.9×all，Accuracy=95%。虽然Accuracy很高，precision是100%,但正例recall只有50% 7、以下哪些算法, 可以用神经网络去构造: KNN 线性回归 对数几率回归A 1和 2B 2 和 3C 1, 2 和 3D 以上都不是 答案： B解析： KNN算法不需要训练参数, 而所有神经网络都需要训练参数, 因此神经网络帮不上忙 最简单的神经网络, 感知器, 其实就是线性回归的训练 我们可以用一层的神经网络构造对数几率回归 6、在有监督学习中， 我们如何使用聚类方法？ 我们可以先创建聚类类别， 然后在每个类别上用监督学习分别进行学习 我们可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习 在进行监督学习之前， 我们不能新建聚类类别 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习A 2 和 4B 1 和 2C 3 和 4D 1 和 3 答案：B解析：我们可以为每个聚类构建不同的模型， 提高预测准确率。“类别id”作为一个特征项去训练， 可以有效地总结了数据特征。 7、以下说法正确的是 一个机器学习模型，如果有较高准确率，总是说明这个分类器是好的 如果增加模型复杂度， 那么模型的测试错误率总是会降低 如果增加模型复杂度， 那么模型的训练错误率总是会降低 我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习A 1B 2C 3D 1 and 3 答案：C解析：1的模型中, 如果负样本占比非常大,也会有很高的准确率, 对正样本的分类不一定很好;2,3的模型中, 增加复杂度则对训练集, 而不是测试集, 有过拟合, 所以训练错误率总会降低;4的模型中, “类别id”可以作为一个特征项去训练, 这样会有效地总结了数据特征。 8、对应GradientBoosting tree算法， 以下说法正确的是: 当增加最小样本分裂个数，我们可以抵制过拟合 当增加最小样本分裂个数，会导致过拟合 当我们减少训练单个学习器的样本个数，我们可以降低variance 当我们减少训练单个学习器的样本个数，我们可以降低biasA 2 和 4B 2 和 3C 1 和 3D 1 和 4 答案：C解析：最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节。第二点是靠bias和variance概念的。 10、如果一个训练好的模型在测试集上有100%的准确率， 这是不是意味着在一个新的数据集上，也会有同样好的表现？A 是的，这说明这个模型的范化能力已经足以支持新的数据集合了B 不对，依然后其他因素模型没有考虑到，比如噪音数据 答案：B解析：没有一个模型是可以总是适应新数据的。我们不可能可到100%准确率。 2、变量选择是用来选择最好的判别器子集， 如果要考虑模型效率，我们应该做哪些变量选择的考虑？ 多个变量其实有相同的用处 变量对于模型的解释有多大作用 特征携带的信息 交叉验证A 1 和 4B 1, 2 和 3C 1,3 和 4D 以上所有 答案：C解析：注意， 这题的题眼是考虑模型效率，所以不要考虑选项2. 4、对于下面三个模型的训练情况， 下面说法正确的是: 第一张图的训练错误与其余两张图相比，是最大的 最后一张图的训练效果最好，因为训练错误最小 第二张图比第一和第三张图鲁棒性更强，是三个里面表现最好的模型 第三张图相对前两张图过拟合了 三个图表现一样，因为我们还没有测试数据集A 1 和 3B 1 和 3C 1, 3 和 4D 5 答案：C解析：最后一张过拟合, 训练错误最小, 第一张相反, 训练错误就是最大了. 所以1是对的;仅仅训练错误最小往往说明过拟合, 所以2错, 4对;第二张图平衡了拟合和过拟合, 所以3对; 9、下面哪个/些选项对 K 折交叉验证的描述是正确的？1.增大 K 将导致交叉验证结果时需要更多的时间2.更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心3.如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量A 1 和 2B 2 和 3C 1 和 3D 1、2 和 3 答案：D解析：大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。 1、给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？A D1= C1, D2 &lt; C2, D3 &gt; C3B D1 = C1, D2 &gt; C2, D3 &gt; C3C D1 = C1, D2 &gt; C2, D3 &lt; C3D D1 = C1, D2 &lt; C2, D3 &lt; C3E D1 = C1, D2 = C2, D3 = C3 答案：E解析：特征之间的相关性系数不会因为特征加或减去一个数而改变。 3、假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。注意：所有其他超参数是相同的，所有其他因子不受影响。1.深度为 4 时将有高偏差和低方差2.深度为 4 时将有低偏差和低方差A 只有 1B 只有 2C 1 和 2D 没有一个 答案：A解析：如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。 4、在以下不同的场景中,使用的分析方法不正确的有A 根据商家最近一年的经营及服务数据,用聚类算法判断出天猫商家在各自主营类目下所属的商家层级B 根据商家近几年的成交数据,用聚类算法拟合出用户未来一个月可能的消费金额公式C 用关联规则算法分析出购买了汽车坐垫的买家,是否适合推荐汽车脚垫D 根据用户最近购买的商品信息,用决策树算法识别出淘宝买家可能是男还是女 答案：B解析：预测消费更合适的算法是用回归模型来做。而不是聚类算法。 10、在大规模的语料中，挖掘词的相关性是一个重要的问题。以下哪一个信息不能用于确定两个词的相关性。A 互信息B 最大熵C 卡方检验D 最大似然比 答案：B解析：最大熵代表了整体分布的信息，通常具有最大熵的分布作为该随机变量的分布，不能体现两个词的相关性，但是卡方是检验两类事务发生的相关性。所以选B【正解】 1、基于统计的分词方法为（）A 正向最大匹配法B 逆向最大匹配法C 最少切分D 条件随机场 答案：D解析：第一类是基于语法和规则的分词法。其基本思想就是在分词的同时进行句法、语义分析,利用句法信息和语义信息来进行词性标注,以解决分词歧义现象。因为现有的语法知识、句法规则十分笼统、复杂,基于语法和规则的分词法所能达到的精确度远远还不能令人满意,目前这种分词系统还处在试验阶段。 第二类是机械式分词法（即基于词典）。机械分词的原理是将文档中的字符串与词典中的词条进行逐一匹配,如果词典中找到某个字符串,则匹配成功,可以切分,否则不予切分。基于词典的机械分词法,实现简单,实用性强,但机械分词法的最大的缺点就是词典的完备性不能得到保证。据统计,用一个含有70000个词的词典去切分含有15000个词的语料库,仍然有30%以上的词条没有被分出来,也就是说有4500个词没有在词典中登录。 第三类是基于统计的方法。基于统计的分词法的基本原理是根据字符串在语料库中出现的统计频率来决定其是否构成词。词是字的组合,相邻的字同时出现的次数越多,就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映它们成为词的可信度。 2、在下面的图像中，哪一个是多元共线（multi-collinear）特征？A 图 1 中的特征B 图 2 中的特征C 图 3 中的特征D 图 1、2 中的特征E 图 2、3 中的特征F 图 1、3 中的特征 答案：D解析：在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。 陌生的知识点6、关于 ARMA 、 AR 、 MA 模型的功率谱，下列说法正确的是（ ）A MA模型是同一个全通滤波器产生的B MA模型在极点接近单位圆时，MA谱是一个深谷C AR模型在零点接近单位圆时，AR谱是一个尖峰D RMA谱既有尖峰又有深谷 答案：D 4、下列哪个不属于CRF(conditional random field algorithm条件随机场算法)模型对于HMM和MEMM模型的优势A 特征灵活B 速度快C 可容纳较多上下文信息D 全局最优 答案：B解析：HMM模型是对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。 CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。 9、已知一组数据的协方差矩阵P,下面关于主分量说法错误的是（）A 主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小B 在经主分量分解后,协方差矩阵成为对角矩阵C 主分量分析就是K-L变换D 主分量是通过求协方差矩阵的特征值得到 答案：C解析：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。 8、位势函数法的积累势函数K(x)的作用相当于Bayes判决中的()A 后验概率B 先验概率C 类概率密度D 类概率密度与先验概率的和 答案：A解析：具体的，势函数详解请看——《势函数法》。 7、以下哪个是常见的时间序列算法模型A RSIB MACDC ARMAD KDJ 答案：C解析：自回归滑动平均模型(ARMA)其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。 其他三项都不是一个层次的。A.相对强弱指数 (RSI, Relative Strength Index) 是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 .B.移动平均聚散指标 (MACD, Moving Average Convergence Divergence), 是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 .D. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 ( 常为 9 日 ,9 周等 ) 内出现过的最高价 , 最低价及最后一个计算周期的收盘价及这三者之间的比例关系 , 来计算最后一个计算周期的未成熟随机值 RSV, 然后根据平滑移动平均线的方法来计算 K 值 , D 值与 J 值 , 并绘成曲线图来研判股票走势 . 1、下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测。A AR模型B MA模型C ARMA模型D GARCH模型 答案：D解析：AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。 2、Fisher线性判别函数的求解过程是将M维特征矢量投影在（ ）中进行求解。A M-1维空间B 一维空间C 三维空间D 二维空间 答案：B解析：Fisher线性判别函数是将多维空间中的特征矢量投影到一条直线上，也就是把维数压缩到一维。寻找这条最优直线的准则是Fisher准则：两类样本在一维空间的投影满足类内尽可能密集，类间尽可能分开，也就是投影后两类样本均值之差尽可能大，类内部方差尽可能小。一般而言，对于数据分布近似高斯分布的情况，Fisher线性判别准则能够得到很好的分类效果。 3、类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是（ ）A 势函数法B 基于二次准则的H-K算法C 伪逆法D 感知器算法 答案：D解析：线性分类器的设计就是利用训练样本集建立线性判别函数式，也就是寻找最优的权向量的过程。求解权重的过程就是训练过程，训练方法的共同点是，先给出准则函数，再寻找是准则函数趋于极值的优化方法。ABC方法都可以得到线性不可分情况下分类问题近似解。感知器可以解决线性可分的问题，但当样本线性不可分时，感知器算法不会收敛。","link":"/2017/10/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/"},{"title":"深度学习笔记","text":"置顶 人工智能、机器学习和深度学习的关系 深度学习与传统机器学习的数据划分区别 请简要介绍下tensorflow的计算图 name_scope 和 variable_scope的区别 什麽样的资料集不适合用深度学习 偏差方差及其应对方法 神经网络隐层维度规则 损失函数和成本函数 什么是激活函数，为什么要用非线性激活函数 神经网络里的正则化为什么能防止过拟合 ReLu为什么要好过于tanh和sigmoid Sigmoid、Tanh、ReLu有什么优缺点，有没改进的激活函数 简单说下sigmoid激活函数 神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的 什么是梯度消失和梯度爆炸，分别会引发什么问题 如何确定是否出现梯度爆炸 如何利用初始化缓解梯度消失和爆炸 如何解决梯度消失和梯度膨胀 如何解决RNN梯度爆炸和弥散的问题 神经网络初始化权重为什么不能初始化为0 常见的学习率衰减方法 局部最优问题（鞍点） 神经网络的优化算法 参数和超参数 超参数调试处理 deep learning（rnn、cnn）调参经验 怎么加快训练的速度 机器学习开发策略 卷积神经网络示例 卷积操作详解（填充、步长、高维卷积、卷积公式） 为什么使用卷积 为什么要使用许多小卷积核如3x3,而不是几个大卷积核？ 为什么我们对图像使用卷积而不仅仅只使用FC层？ 卷积网络怎么进行边缘检测 CNN的卷积核是单层的还是多层的 池化层简介 池化层的作用 池化为什么对平移不变性有贡献 SPP空间金字塔池化 全连接层的作用 dropout随机失活 Batch Normalization CNN经典网络对比 CNN经典网络总结 RNN GRU LSTM Tensorflow 官网推荐的一篇伟大文章，特别介绍递归神经网络和LSTM GRU与LSTM的区别 LSTM结构推导，为什么比RNN好 为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数 RNN是怎么从单层网络一步一步构造的 seq2seq序列模型 attention注意力模型 语言识别 触发词检测 什么是感知器 什么是端到端的网络 上采样和下采样 CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？ 为什么很多做人脸的Paper会最后加入一个Local Connected Conv 广义线性模型是怎被应用在深度学习中 Dilated Convolution空洞卷积/扩张卷积/膨胀卷积 神经网络发展历史 文本数据抽取 Word2Vec Word2vec之Skip-Gram模型 迁移学习 目标检测_吴恩达 目标检测_RCNN系列 人脸识别 神经风格转移 生成对抗网络 命名实体识别 人工智能、机器学习和深度学习的关系深度学习是基于传统的神经网络算法发展到多隐层的一种算法体现。 什么是梯度消失和梯度爆炸，分别会引发什么问题我们知道神经网络在训练过程中会利用梯度对网络的权重进行更新迭代。当梯度出现指数级递减或指数递增时，称为梯度消失或者梯度爆炸。 假定激活函数 $g(z) = z$, 令 $b^{[l]} = 0$，对于目标输出有：$\\hat{y} = W^{[L]}W^{[L-1]}…W^{[2]}W^{[1]}X$1）对于 W[l]的值小于 1 的情况，激活函数的值将以指数级递减2）对于 W[l]的值大于 1 的情况，激活函数的值将以指数级递增同理的情况会出现在反向求导。 梯度消失时，权重更新缓慢，训练难度大大增加。梯度消失相对梯度爆炸更常见。梯度爆炸时，权重大幅更新，网络变得不稳定。较好的情况是网络无法利用训练数据学习，最差的情况是权值增大溢出，变成网络无法更新的 NaN 值。 如何利用初始化缓解梯度消失和爆炸根据 z=w1x1+w2x2+…+wnxn+b可知，当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。为了得到较小的 wi，设置Var(wi)=1/n，这里称为 Xavier initialization。 WL = np.random.randn(WL.shape[0], WL.shape[1]) * np.sqrt(1/n) 其中 n 是输入的神经元个数，即WL.shape[1]。这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。 同理，也有 He Initialization。它和 Xavier initialization 唯一的区别是Var(wi)=2/n，适用于 ReLU 作为激活函数时。当激活函数使用 ReLU 时，Var(wi)=2/n；当激活函数使用 tanh 时，Var(wi)=1/n。 如何确定是否出现梯度爆炸信号如下： 训练过程中，每个节点和层的误差梯度值持续超过1.0。 模型不稳定，梯度显著变化，快速变大。 训练过程中，模型权重变成 NaN 值。 模型无法从训练数据中获得更新。 如何解决梯度消失和梯度膨胀梯度消失和梯度爆炸都可以通过激活函数或Batch Normalization来解决。吴恩达：BN有效原因 对于梯度爆炸，这里列举一些最佳实验方法： 重新设计网络模型在深度神经网络中，梯度爆炸可以通过重新设计层数更少的网络来解决。使用更小的批尺寸对网络训练也有好处。在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。 使用 ReLU 激活函数在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。 使用长短期记忆网络在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，如随时间的反向传播本质上将循环网络转换成深度多层感知机神经网络。使用长短期记忆（LSTM）单元和相关的门类型神经元结构可以减少梯度爆炸问题。采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。 使用梯度截断（Gradient Clipping）在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。处理梯度爆炸有一个简单有效的解决方案：如果梯度超过阈值，就截断它们。——《Neural Network Methods in Natural Language Processing》，2017.具体来说，检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。梯度截断可以一定程度上缓解梯度爆炸问题（梯度截断，即在执行梯度下降步骤之前将梯度设置为阈值）。 ——《深度学习》，2016.在 Keras 深度学习库中，你可以在训练之前设置优化器上的 clipnorm 或 clipvalue 参数，来使用梯度截断。默认值为 clipnorm=1.0 、clipvalue=0.5。详见：https://keras.io/optimizers/。 使用权重正则化（Weight Regularization）如果梯度爆炸仍然存在，可以尝试另一种方法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。对循环权重使用 L1 或 L2 惩罚项有助于缓解梯度爆炸。——On the difficulty of training recurrent neural networks，2013.在 Keras 深度学习库中，你可以通过在层上设置 kernel_regularizer 参数和使用 L1 或 L2 正则化项进行权重正则化。 如何解决RNN梯度爆炸和弥散的问题梯度爆炸：当梯度大于一定阈值的的时候，将它截断为一个较小的数。下图可视化了梯度截断的效果。它展示了一个小的rnn（其中W为权值矩阵，b为bias项）的决策面。这个模型是一个一小段时间的rnn单元组成；实心箭头表明每步梯度下降的训练过程。当梯度下降过程中，模型的目标函数取得了较高的误差时，梯度将被送到远离决策面的位置。截断模型产生了一个虚线，它将误差梯度拉回到离原始梯度接近的位置。 梯度弥散：第一种方法是将随机初始化改为一个有关联的矩阵初始化。第二种方法是使用ReLU代替sigmoid函数。ReLU的导数不是0就是1.因此，神经元的梯度将始终为1，而不会当梯度传播了一定时间之后变小。 基本的 RNN 不擅长捕获这种长期依赖关系， LSTM 和 GRU 都可以作为缓解梯度消失问题的方案。 神经网络初始化权重为什么不能初始化为0将所有权重初始化为零将无法破坏网络的对称性。这意味着每一层的每个神经元都会学到相同的东西，这样的神经元网络并不比线性分类器如逻辑回归更强大。 需要注意的是，需要初始化去破坏网络对称性(symmetry)的只有W，b可以全部初始化为0。 常见的学习率衰减方法如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。 而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。 最常用的学习率衰减方法：$\\alpha = \\frac{1}{1 + decay\\_rate * epoch\\_num} * \\alpha_0$其中，decay_rate为衰减率（超参数），epoch_num为将所有的训练样本完整过一遍的次数。 指数衰减：$\\alpha = 0.95^{epoch\\_num} * \\alpha_0$ 其他：$\\alpha = \\frac{k}{\\sqrt{epoch\\_num}} * \\alpha_0$ 离散下降对于较小的模型，也有人会在训练时根据进度手动调小学习率。 局部最优问题（鞍点）鞍点（saddle）是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值。 减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。 结论：（1）在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；（2）鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。 神经网络的优化算法吴恩达优化算法章节总结链接：http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/优化算法 深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。 常见优化算法：1、batch 梯度下降法，同时处理整个训练集2、Mini-Batch 梯度下降法（1）mini-batch 的大小为 1，即是随机梯度下降法（stochastic gradient descent），每个样本都是独立的 mini-batch（2）mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法（3）batch 的不同大小（size）带来的影响 batch 梯度下降法： 对所有 m 个训练样本执行一次梯度下降，每一次迭代时间较长，训练过程慢； 相对噪声低一些，幅度也大一些； 成本函数总是向减小的方向下降。 随机梯度下降法： 对每一个训练样本执行一次梯度下降，训练速度快，但丢失了向量化带来的计算加速； 有很多噪声，减小学习率可以适当； 成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。因此，选择一个1 &lt; size &lt; m的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。 （4）mini-batch 大小的选择 如果训练样本的大小比较小，如 m ⩽ 2000 时，选择 batch 梯度下降法； 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 26、27、…、29； mini-batch 的大小要符合 CPU/GPU 内存。mini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。 3、了解加权平均4、动量梯度下降法，是计算梯度的指数加权平均数，并利用该值来更新参数值5、RMSProp算法，是在对梯度进行指数加权平均的基础上，引入平方和平方根6、Adam优化算法（Adaptive Moment Estimation，自适应矩估计），基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果 参数和超参数参数即是我们在过程中想要模型学习到的信息（模型自己能计算出来的），例如 $W^{[l]}$，$b^{[l]}$。而超参数（hyper parameters）即为控制参数的输出值的一些网络信息（需要人经验判断）。超参数的改变会导致最终得到的参数 $W^{[l]}$，$b^{[l]}$ 的改变。 典型的超参数有：（1）学习速率：α（2）迭代次数：N（3）隐藏层的层数：L（4）每一层的神经元个数：$n^{[1]}$，$n^{[2]}$，…（5）激活函数 g(z) 的选择 当开发新应用时，预先很难准确知道超参数的最优值应该是什么。因此，通常需要尝试很多不同的值。应用深度学习领域是一个很大程度基于经验的过程。 比如常见的梯度下降问题，好的超参数能使你尽快收敛： 超参数调试处理重要程度排序目前已经讲到过的超参数中，重要程度依次是（仅供参考）： 最重要： 学习率 α； 其次重要： β：动量衰减参数，常设置为 0.9； hidden units：各隐藏层神经元个数； mini-batch 的大小； 再次重要： β1，β2，ϵ：Adam 优化算法的超参数，常设为 0.9、0.999、$10^{-8}$； layers：神经网络层数; decay_rate：学习衰减率； 调参技巧系统地组织超参调试过程的技巧： 随机选择点（而非均匀选取），用这些点实验超参数的效果。这样做的原因是我们提前很难知道超参数的重要程度，可以通过选择更多值来进行更多实验； 由粗糙到精细：聚焦效果不错的点组成的小区域，在其中更密集地取值，以此类推； 选择合适的范围 对于学习率 α，用对数标尺而非线性轴更加合理：0.0001、0.001、0.01、0.1 等，然后在这些刻度之间再随机均匀取值； 对于 β，取 0.9 就相当于在 10 个值中计算平均值，而取 0.999 就相当于在 1000 个值中计算平均值。可以考虑给 1-β 取值，这样就和取学习率类似了。 上述操作的原因是当 β 接近 1 时，即使 β 只有微小的改变，所得结果的灵敏度会有较大的变化。例如，β 从 0.9 增加到 0.9005 对结果（1/(1-β)）几乎没有影响，而 β 从 0.999 到 0.9995 对结果的影响巨大（从 1000 个值中计算平均值变为 2000 个值中计算平均值）。 一些建议 深度学习如今已经应用到许多不同的领域。不同的应用出现相互交融的现象，某个应用领域的超参数设定有可能通用于另一领域。不同应用领域的人也应该更多地阅读其他研究领域的 paper，跨领域地寻找灵感； 考虑到数据的变化或者服务器的变更等因素，建议每隔几个月至少一次，重新测试或评估超参数，来获得实时的最佳模型； 根据你所拥有的计算资源来决定你训练模型的方式： Panda（熊猫方式）：在在线广告设置或者在计算机视觉应用领域有大量的数据，但受计算能力所限，同时试验大量模型比较困难。可以采用这种方式：试验一个或一小批模型，初始化，试着让其工作运转，观察它的表现，不断调整参数； Caviar（鱼子酱方式）：拥有足够的计算机去平行试验很多模型，尝试很多不同的超参数，选取效果最好的模型； CNN的卷积核是单层的还是多层的描述网络模型中某层的厚度，通常用名词 通道（channel）数或者 特征图（feature map）数。不过人们更习惯把作为数据输入的前层的厚度称之为通道数（比如RGB三色图层称为输入通道数为3），把作为卷积输出的后层的厚度称之为特征图数。 卷积核(filter)一般是3D多层的，除了面积参数, 比如3x3之外, 还有厚度参数H（2D的视为厚度1). 还有一个属性是卷积核的个数N。 卷积核的厚度H，一般等于前层厚度M(输入通道数或feature map数)。特殊情况M &gt; H。 卷积核的个数N，一般等于后层厚度(后层feature maps数，因为相等所以也用N表示)。 卷积核通常从属于后层，为后层提供了各种查看前层特征的视角，这个视角是自动形成的。 卷积核厚度等于1时为2D卷积，对应平面点相乘然后把结果加起来，相当于点积运算； 卷积核厚度大于1时为3D卷积，每片分别平面点求卷积，然后把每片结果加起来，作为3D卷积结果；1x1卷积属于3D卷积的一个特例，有厚度无面积，直接把每片单个点乘以权重再相加。 下面解释一下特殊情况的 M &gt; H：实际上，除了输入数据的通道数比较少之外，中间层的feature map数很多，这样中间层算卷积会累死计算机。所以很多深度卷积网络把全部通道/特征图划分一下，每个卷积核只看其中一部分。这样整个深度网络架构是横向开始分道扬镳了，到最后才又融合。这样看来，很多网络模型的架构不完全是突发奇想，而是是被参数计算量逼得。特别是现在需要在移动设备上进行AI应用计算(也叫推断), 模型参数规模必须更小, 所以出现很多减少握手规模的卷积形式, 现在主流网络架构大都如此。 卷积神经网络示例 随着神经网络计算深度不断加深，图片的高度和宽度 n[l]H、n[l]W一般逐渐减小，而 n[l]c在增加。 一个典型的卷积神经网络通常包含有三种层：卷积层（Convolution layer）、池化层（Pooling layer）、全连接层（Fully Connected layer）。仅用卷积层也有可能构建出很好的神经网络，但大部分神经网络还是会添加池化层和全连接层，它们更容易设计。 在计算神经网络的层数时，通常只统计具有权重和参数的层，池化层没有需要训练的参数，所以和之前的卷积层共同计为一层。 图中的 FC3 和 FC4 为全连接层，与标准的神经网络结构一致。整个神经网络各层的尺寸与参数如下表所示： 推荐一个直观感受卷积神经网络的网站。 为什么使用卷积相比标准神经网络，对于大量的输入数据，卷积过程有效地减少了 CNN 的参数数量，原因有以下两点： 参数共享（Parameter sharing）：特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。 稀疏连接（Sparsity of connections）：在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。池化过程则在卷积后很好地聚合了特征，通过降维来减少运算量。 由于 CNN 参数数量较小，所需的训练样本就相对较少，因此在一定程度上不容易发生过拟合现象。并且 CNN 比较擅长捕捉区域位置偏移。即进行物体检测时，不太受物体在图片中位置的影响，增加检测的准确性和系统的健壮性。 然后这篇文章， 从感受视野的角度出发，解释了参数共享、稀疏连接、平移不变性等 然后卷积层可以看做全连接的一种简化形式:不全连接+不参数共享。所以全连接层的参数才如此之多。 为什么要使用许多小卷积核如3x3,而不是几个大卷积核？这在VGGNet的原始论文中得到了很好的解释。原因有二：首先，您可以使用几个较小的核而不是几个较大的核来获得相同的感受野并捕获更多的空间上下文，而且较小的内核计算量更少。其次，因为使用更小的核，您将使用更多的滤波器，您将能够使用更多的激活函数，从而使您的CNN学习到更具区分性的映射函数。 https://arxiv.org/pdf/1409.1556.pdf 为什么我们对图像使用卷积而不仅仅只使用FC层？这个答案有两部分。首先，卷积保存、编码和实际使用来自图像的空间信息。如果我们只使用FC层，我们将没有相对的空间信息。其次，卷积神经网络( CNNs )具有部分内建的平移不变性，因为每个卷积核充当其自身的滤波器/特征检测器。 CNNs为什么具有平移不变性？因为CNNs是以滑动窗口的方式进行卷积，卷积过程中参数共享并且稀疏连接，那么无论目标在图像的什么位置，都能扫描到。 池化层简介通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，我们对卷积层进行池化处理。池化常用方法为最大池化和平均池化。 最大池化：将输入拆分成不同的区域，输出的每个元素都是对应区域中元素的最大值，如下图所示： 池化过程类似于卷积过程，上图所示的池化过程中相当于使用了一个大小 f=2的滤波器，且池化步长 s=2。卷积过程中的几个计算大小的公式也都适用于池化过程。如果有多个通道，那么就对每个通道分别执行计算过程（池化不压缩通道数，卷积会压缩）。 对最大池化的一种直观解释是，元素值较大可能意味着池化过程之前的卷积过程提取到了某些特定的特征，池化过程中的最大化操作使得只要在一个区域内提取到某个特征，它都会保留在最大池化的输出中。但是，没有足够的证据证明这种直观解释的正确性，而最大池化被使用的主要原因是它在很多实验中的效果都很好。 平均池化：就是从取某个区域的最大值改为求这个区域的平均值： 池化过程的特点之一是，它有一组超参数，但是并没有参数需要学习。池化过程的超参数包括滤波器的大小 f、步长 s，以及选用最大池化还是平均池化。而填充 p则很少用到。 池化过程的输入维度为： n_H \\times n_W \\times n_c输出维度为： \\biggl\\lfloor \\frac{n_H-f}{s}+1 \\biggr\\rfloor \\times \\biggl\\lfloor \\frac{n_W-f}{s}+1 \\biggr\\rfloor \\times n_c 池化层的作用通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，我们对卷积层进行池化处理。 池化层作用：（1）降维，提高计算速度。相对卷积操作，池化是以指数形式降维，理想情况下，还能保留显著特征。（2）同时减小噪声提高所提取特征的稳健性。（3）可以扩大感知野（4）在图像识别领域，池化还能提供平移、旋转和尺度不变性（5）池化的输出是一个固定大小的矩阵，这对分类问题很重要 池化为什么对平移不变性有贡献池化在丢失少量信息的情况下，会对有效信息进行最大程度的激活。以下解答摘自池化-ufldl： 如果人们选择图像中的连续范围作为池化区域，并且只是池化相同(重复)的隐藏单元产生的特征，那么，这些池化单元就具有平移不变性 (translation invariant)。这就意味着即使图像经历了一个小的平移之后，依然会产生相同的 (池化的) 特征。 在很多任务中 (例如物体检测、声音识别)，我们都更希望得到具有平移不变性的特征，因为即使图像经过了平移，样例(图像)的标记仍然保持不变。 例如，如果你处理一个MNIST数据集的数字，把它向左侧或右侧平移，那么不论最终的位置在哪里，你都会期望你的分类器仍然能够精确地将其分类为相同的数字。 再例如，医学图像分割，可以查看感兴趣的区域，从而忽略不需要的区域的干扰。如看骨折，只需要将骨头所表示的特征图像（一般是一定会度值的一块区域）从背景（如肌肉，另一种灰度值）分割出来，而其它的肌肉等则不显示（为黑色） SPP空间金字塔池化卷积神经网络要求输入的图像尺寸固定，这种需要会降低图像识别的精度。SPPNet（Spatial Pyramid Pooling）的初衷非常明晰，就是希望网络对输入的尺寸更加灵活，分析到卷积网络对尺寸并没有要求，固定尺寸的要求完全来源于全连接层部分，因而借助空间金字塔池化的方法来衔接两者，SPPNet在检测领域的重要贡献是避免了R-CNN的变形、重复计算等问题，在效果不衰减的情况下，大幅提高了识别速度。 论文翻译：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition 全连接层的作用全连接层相对卷积层来说，就是:不稀疏连接+不参数共享。全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的，可占全网络参数的80%。 全连接会把上一层的多维特征图转化成一个固定长度的特征向量，这个特征向量虽然丢失了图像的位置信息，但是它组合了图像中最具特点的图像特征，用于后面的Softmax分类。 换一种说法，全连接层的作用是 将网络学习到的特征映射到样本标记空间（卷积池化层层等作用是将原始数据映射到隐层特征空间）。 在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽。 因为传统的网络输出的都是分类，也就是几个类别的概率，甚至就是一个数，比如类别号。那么全连接层就是高度提纯的特征了，方便交给最后的分类器或者回归。 其实卷积神经网络中全连接层的设计，属于人们在传统特征提取+分类思维下的一种”迁移学习”思想，但后期在很多end-to-end模型中，其最初用于分类的功能被弱化了，而全连接层参数又过多，所以人们一直试图设计出各种不含全连接层又能达到相同效果的网络。 知乎：全连接层的作用是什么？ 什么是端到端的网络深度学习中最令人振奋的最新动态之一就是端到端深度学习的兴起。简而言之，以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它。 传统的机器学习任务经常需要分别训练多个模型（涉及人为介入的特征工程），输入数据后，无法通过一个模型直接得到结果，是非端到端的。现在的深度学习模式是【输入→（1个）模型→（直接出）结果】，模型会自动提取特征。 更多详情见 吴恩达 什么是端到端的深度学习 请简要介绍下tensorflow的计算图Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个节点都是计算图上的一个Tensor, 也就是张量，而节点之间的边描述了计算之间的依赖关系(定义时)和数学操作(运算时)。如下两图表示：a=x*y; b=a+z; c=tf.reduce_sum(b); name_scope 和 variable_scope的区别 name_scope： 为了更好地管理变量的命名空间而提出的。比如在 tensorboard 中，因为引入了 name_scope， Graph 看起来才井然有序。 variable_scope： 跟 tf.get_variable() 配合使用，实现变量共享。 详见：https://blog.csdn.net/jacke121/article/details/77622834 怎么加快训练的速度 加快数据的读取速度, 具体详见 数据读取 可以构建tensorflow计算图，然后Feeding，减少与C++后端的交互次数; 或者从文件读取数据，使用多线程与管道; 或者预加载数据，即在TensorFlow图中定义常量或变量来保存所有数据，仅适用于数据量比较小的情况。 加快收敛的速度，详解神经网络的优化算法 要是服务器足够，同时设置验证多个超参，边训练边验证，用tensorflow对比查看，尽快得到满意结果 deep learning（rnn、cnn）调参经验结合最近的项目经历进行整理，待处理。。使用谷歌最近开源的AutoML工具包 NNI知乎:深度学习调参有哪些技巧？ 一、参数初始化下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。下面的n_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)0.5Xavier初始法论文：http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdfHe初始化论文：https://arxiv.org/abs/1502.01852uniform均匀分布初始化：w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)He初始化，适用于ReLU：scale = np.sqrt(6/n)normal高斯分布初始化：w = np.random.randn(n_in,n_out) stdev # stdev为高斯分布的标准差，均值设为0Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)He初始化，适用于ReLU：stdev = np.sqrt(2/n)svd初始化：对RNN有比较好的效果。参考论文：https://arxiv.org/abs/1312.6120 二、数据预处理方式zero-center ,这个挺常用的.X -= np.mean(X, axis = 0) # zero-centerX /= np.std(X, axis = 0) # normalizePCA whitening,这个用的比较少. 三、训练技巧要做梯度归一化,即算出来的梯度除以minibatch sizeclip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15 dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置.关于RNN如何用dropout,可以参考这篇论文:http://arxiv.org/abs/1409.2329 adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。 除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好. word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果. 四、尽量对数据做shuffleLSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值. Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介 五、EnsembleEnsemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式同样的参数,不同的初始化方式不同的参数,通过cross-validation,选取最好的几组同样的参数,模型训练的不同阶段，即不同迭代次数的模型。不同的模型,进行线性融合. 例如RNN和传统模型. LSTM结构推导，为什么比RNN好推导forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM一定程度上可以防止梯度消失或者爆炸 为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数为什么不是选择统一一种sigmoid或者tanh，而是混合使用呢这样的目的是什么 sigmoid 用在了各种gate上，描述每个组件应该通过多少。它的值在 0 到 1 的范围内，且大多数时间非常接近于 0 或 1，值为零意味着“不要让任何信息通过”，而值为1意味着“让所有信息都通过！”。 tanh 用在了状态和输出上，是对数据的处理，这个用 ReLU 或其他激活函数也可以。 Sigmoid、Tanh、ReLu有什么优缺点，有没改进的激活函数Maxout使用两套w,b参数，输出较大值。本质上Maxout可以看做Relu的泛化版本，因为如果一套w,b全都是0的话，那么就是普通的ReLU。Maxout可以克服Relu的缺点，但是参数数目翻倍。 什么是激活函数，为什么要用非线性激活函数如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数 Activation Function。 不用激活函数或使用线性激活函数，和直接使用 Logistic 回归没有区别，因为无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，就成了最原始的感知器了。 非线性激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。非线性激励函数最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入。 ReLu为什么要好过于tanh和sigmoidsigmoid、tanh和RelU函数图： 第一，ReLU本质上是分段线性模型，前向计算和反向传播的偏导非常简单，无需指数之类操作。 第二，ReLU不容易发生梯度消失问题，Tanh和Logistic激活函数在两端的时候导数容易趋近于零，多级连乘后梯度更加约等于0。 第三，ReLU关闭了右边，从而会使得很多的隐层输出为0，即网络变得稀疏，起到了类似L1的正则化作用，可以在一定程度上缓解过拟合。 但是Relu也有自己的缺点，它缺少对数据的控制力，不像sigmoid可以把任意维度的数据压缩到0到1之间。训练过程中有些数据的维度完全没有得到控制，有的幅度到达了上千，有的依然是一个极小的小数。这样看起来，似乎sigmoid前向更靠谱，relu后向更强。那么怎么解决ReLu的振幅问题呢？可以考虑在初始化上做处理，详见 xavier初始化 最后加一句，现在主流的做法，会多做一步batch normalization，尽可能保证每一层网络的输入具有相同的分布[1]。而最新的paper[2]，他们在加入bypass connection之后，发现改变batch normalization的位置会有更好的效果。 [1] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.[2] He, Kaiming, et al. “Identity Mappings in Deep Residual Networks.” arXiv preprint arXiv:1603.05027 (2016). 知乎链接：请问人工神经网络中的activation function的作用具体是什么？为什么ReLu要好过于tanh和sigmoid function? 神经网络里的正则化为什么能防止过拟合 直观解释正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，直观上相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。 数学解释假设神经元中使用的激活函数为g(z) = tanh(z)（sigmoid 同理）。 在加入正则化项后，当 λ 增大，导致 W[l]减小，Z[l]=W[l]a[l−1]+b[l]便会减小。由上图可知，在 z 较小（接近于 0）的区域里，tanh(z)函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。 其他解释在权值 w[L]变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。 什麽样的资料集不适合用深度学习 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。 广义线性模型是怎被应用在深度学习中A Statistical View of Deep Learning (I): Recursive GLMs深度学习从统计学角度，可以看做递归的广义线性模型。 广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。 深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者（这里主要指我）的困惑。 下图是一个对照表 深度学习与传统机器学习的数据划分区别对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分： 1）训练集（train set）：用训练集对算法或模型进行训练过程；2）验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行交叉验证，选择出最好的模型；3）测试集（test set）：最后利用测试集对模型进行测试，获取模型运行的无偏估计（对学习方法进行评估）。 在小数据量的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分： 1）无验证集的情况：70% / 30%；2）有验证集的情况：60% / 20% / 20%； 而在如今的大数据时代，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。 验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。 测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。吴恩达给出的建议是： 1）100 万数据量：98% / 1% / 1%；2）超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%） 神经网络发展历史1949年Hebb提出了神经心理学学习范式——Hebbian学习理论1952年，IBM的Arthur Samuel写出了西洋棋程序1957年，Rosenblatt的感知器算法是第二个有着神经系统科学背景的机器学习模型. 3年之后，Widrow因发明Delta学习规则而载入ML史册，该规则马上就很好的应用到了感知器的训练中感知器的热度在1969被Minskey一盆冷水泼灭了。他提出了著名的XOR问题，论证了感知器在类似XOR问题的线性不可分数据的无力。 尽管BP的思想在70年代就被Linnainmaa以“自动微分的翻转模式”被提出来，但直到1981年才被Werbos应用到多层感知器(MLP)中，NN新的大繁荣。 1991年的Hochreiter和2001年的Hochreiter的工作，都表明在使用BP算法时，NN单元饱和之后会发生梯度损失。又发生停滞。 时间终于走到了当下，随着计算资源的增长和数据量的增长。一个新的NN领域——深度学习出现了。 简言之，MP模型+sgn—-&gt;单层感知机（只能线性）+sgn— Minsky 低谷 —&gt;多层感知机+BP+sigmoid—- (低谷) —&gt;深度学习+pre-training+ReLU/sigmoid 换一种说法 sigmoid会饱和，造成梯度消失。于是有了ReLU。ReLU负半轴是死区，造成梯度变0。于是有了LeakyReLU，PReLU。强调梯度和权值分布的稳定性，由此有了ELU，以及较新的SELU。太深了，梯度传不下去，于是有了highway。干脆连highway的参数都不要，直接变残差，于是有了ResNet。 强行稳定参数的均值和方差，于是有了BatchNorm。在梯度流中增加噪声，于是有了 Dropout。RNN梯度不稳定，于是加几个通路和门控，于是有了LSTM。LSTM简化一下，有了GRU。GAN的JS散度有问题，会导致梯度消失或无效，于是有了WGAN。WGAN对梯度的clip有问题，于是有了WGAN-GP。 神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的 非线性：即导数不是常数。这个条件是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。 几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。 计算简单：非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。 非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x&gt;0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x&lt;0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。 单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。 输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。 接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x&gt;0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。 参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。 归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。 更多详情：https://www.zhihu.com/question/67366051 CNN经典网络对比 更多详情见：从LeNet到AlexNetDeep Learning回顾#之LeNet、AlexNet、GoogLeNet、VGG、ResNet dropout随机失活dropout（随机失活）是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在计算机视觉（Computer Vision）领域。 对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。 因此，通过传播过程，dropout 将产生和 L2 正则化相同的收缩权重的效果。 对于不同的层，设置的keep_prob也不同。一般来说，神经元较少的层，会设keep_prob为 1.0，而神经元多的层则会设置比较小的keep_prob。 dropout 的一大缺点是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将keep_prob全部设置为 1.0 后运行代码，确保 J(w,b)函数单调递减，再打开 dropout。 简单说下sigmoid激活函数常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。这里先简要介绍下最基础的sigmoid函数（btw，在本博客中SVM那篇文章开头有提过）。 sigmoid的函数表达式如下 其中z是一个线性组合，比如z可以等于：b + w1x1 + w2x2。通过代入很大的正数或很小的负数到g(z)函数中可知，其结果趋近于0或1。 因此，sigmoid函数g(z)的图形表示如下（ 横轴表示定义域z，纵轴表示值域g(z) ）： 也就是说，sigmoid函数的功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，g(z)会趋近于1，而z是非常小的负数时，则g(z)会趋近于0。 压缩至0到1有何用处呢用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。 举个例子，如下图（图引自Stanford机器学习公开课） z = b + w1x1 + w2x2，其中b为偏置项 假定取-30，w1、w2都取为20 如果x1 = 0，x2 = 0，则z = -30，g(z) = 1/( 1 + e^-z )趋近于0。此外，从上图sigmoid函数的图形上也可以看出，当z=-30的时候，g(z)的值趋近于0如果x1 = 0，x2 = 1，或x1 =1,x2 = 0，则z = b + w1x1 + w2x2 = -30 + 20 = -10，同样，g(z)的值趋近于0如果x1 = 1，x2 = 1，则z = b + w1x1 + w2x2 = -30 + 201 + 201 = 10，此时，g(z)趋近于1。 换言之，只有x1和x2都取1的时候，g(z)→1，判定为正样本；而当只要x1或x2有一个取0的时候，g(z)→0，判定为负样本，如此达到分类的目的。 综上，sigmod函数，是逻辑斯蒂回归的压缩函数，它的性质是可以把分隔平面压缩到[0,1]区间一个数（向量），在线性分割平面值为0时候正好对应sigmod值为0.5，大于0对应sigmod值大于0.5、小于0对应sigmod值小于0.5；0.5可以作为分类的阀值；exp的形式最值求解时候比较方便，用相乘形式作为logistic损失函数，使得损失函数是凸函数；不足之处是sigmod函数在y趋于0或1时候有死区，控制不好在bp形式传递loss时候容易造成梯度弥撒。 上采样和下采样缩小图像（或称为下采样 subsampled 和降采样 downsampled）的主要目的有两个1）使得图像符合显示区域的大小2）生成对应图像的缩略图。 放大图像（或称为上采样 upsampling 和图像插值 interpolating）的主要目的是放大原图像，从而可以显示在更高分辨率的显示设备上。 上采样原理：内插值，即在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的元素。 什么是感知器当激活函数的 返回值是两个固定值 的时候，可以称为此时的神经网络为感知器。 因为感知器的返回值只有两种情况，所以感知器只能解决二类线性可分的问题，感知器比较适合应用到模式分类问题中 神经网络隐层维度规则 偏差方差及其应对方法“偏差-方差分解”（bias-variance decomposition）是解释学习算法泛化性能的一种重要工具。 泛化误差可分解为偏差、方差与噪声之和： 偏差：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力； 方差：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响； 噪声：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了学习问题本身的难度。 偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。 在欠拟合（underfitting）的情况下，出现高偏差（high bias）的情况，即不能很好地对数据进行分类。 当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现过拟合（overfitting）的情况，在验证集上出现高方差（high variance）的现象。 当训练出一个模型以后，如果： 训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合； 训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合； 训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差； 训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。 偏差和方差的权衡问题对于模型来说十分重要。 最优误差通常也称为“贝叶斯误差”。 应对方法存在高偏差： 扩大网络规模，如添加隐藏层或隐藏单元数目； 寻找合适的网络架构，使用更大的 NN 结构； 花费更长时间训练。 存在高方差： 获取更多的数据； 使用正则化（regularization）技术，降低模型的复杂度； 寻找更合适的网络结构。甚至使用bagging算法比如随机森林，训练多个弱模型，然后组合在一起，进行投票等 不断尝试，直到找到低偏差、低方差的框架。 在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。 CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？Deep Learning -Yann LeCun, Yoshua Bengio &amp; Geoffrey HintonLearn TensorFlow and deep learning, without a Ph.D.The Unreasonable Effectiveness of Deep Learning -LeCun 16 NIPS Keynote 以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示。 CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图： 上图中，如果每一个点的处理使用相同的Filter，则为全卷积，如果使用不同的Filter，则为Local-Conv。 为什么很多做人脸的Paper会最后加入一个Local Connected Conv如果每一个点的处理使用相同的Filter(即参数共享)，则为全卷积，如果使用不同的Filter，则为Local-Conv(local的意思就是参数不共享)。 DeepFace 先进行了两次全卷积＋一次池化，提取了低层次的边缘／纹理等特征。后接了3个Local-Conv层，这里是用Local-Conv的原因是，人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。当数据集具有全局的局部特征分布时，也就是说局部特征之间有较强的相关性，适合用全卷积。 “不存在全局的局部特征分布”，可以这么理解，人的鼻子和嘴是局部特征，但在全局特征(脸)中并不是广泛分布的，它们是独一无二的。所以说不存在局部连接学习了鼻子的特征，然后把特征应用到脸的其他部位。 链接：http://blog.csdn.net/u014365862/article/details/77795902 PS：为什么经常将全连接层转化为全卷积？全卷积和FCN的参数一样多，之所以用全卷积，主要是为了在卷积层上实现滑动窗口，减少重复卷积的计算，在目标检测中很有用。还有一种说法是为了让卷积网络在一张更大的输入图片上滑动，得到每个区域的输出。https://blog.csdn.net/u010548772/article/details/78582250https://blog.csdn.net/nnnnnnnnnnnny/article/details/70194432 Dilated Convolution空洞卷积/扩张卷积/膨胀卷积http://blog.csdn.net/guvcolie/article/details/77884530?locationNum=10&amp;fps=1https://blog.csdn.net/mao_xiao_feng/article/details/77924003http://www.cnblogs.com/ranjiewen/p/7945249.html Word2vec之Skip-Gram模型Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。 Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。本篇文章仅讲解Skip-Gram模型。 结构篇：https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html 训练篇：https://www.leiphone.com/news/201706/eV8j3Nu8SMqGBnQB.html 实现篇：https://www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html PS：吴恩达序列模型课程自然语言处理章节有详细讲解。 文本数据抽取 词袋法：将文本当作一个无序的数据集合，文本特征可以采用文本中的词条T进行体现，那么文本中出现的所有词条及其出现的次数就可以体现文档的特征 TF-IDF: 词条的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降；也就是说词条在文本中出现的次数越多，表示该词条对该文本的重要性越高，词条在所有文本中出现的次数越少，说明这个词条对文本的重要性越高。TF(词频)指某个词条在文本中出现的次数，一般会将其进行归一化处理(该词条数量/该文档中所有词条数量)；IDF(逆向文件频率)指一个词条重要性的度量，一般计算方式为总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。TF-IDF实际上是：TF * IDF 迁移学习深度学习中，最强大的理念之一就是，有的时候神经网络可以从一个任务中习得知识，并将这些知识应用到另一个独立的任务中。所以例如，也许你已经训练好一个神经网络，能够识别像猫这样的对象，然后使用那些知识，或者部分习得的知识去帮助您更好地阅读x射线扫描图，这就是所谓的迁移学习。 通常做法就是，你可以把识别猫的神经网络最后的输出层及其对应的权重删掉，然后设计一个全新的输出层，并为其重新赋予随机权重，然后让它在放射诊断数据上训练。 那什么时候迁移学习是有意义的？如果你想从任务学习A并迁移一些知识到任务B，那么当任务A和任务B都有同样的输入X时，迁移学习是有意义的（比如识别猫和X射线扫描时输入的都是图像）。并且当任务A的数据比任务B多得多时，迁移学习意义更大。 更多细节详见 吴恩达的 “迁移学习” 章节讲解。 TensorFlow Hub 可以实现迁移学习，GitHub 代码示例地址：图像再训练 生成对抗网络GAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的。另一方是discriminator，其目标是判断输入图片是否属于真实训练样本。 更直白的讲，将generator想象成假币制造商，而discriminator是警察。generator目的是尽可能把假币造的跟真的一样，从而能够骗过discriminator，即生成样本并使它看上去好像来自于真实训练样本一样。 命名实体识别命名实体识别(Named EntitiesRecognition, NER)是自然语言处理(Natural LanguageProcessing, NLP)的一个基础任务，常用在信息抽取、信息检索、机器翻译、问答系统中。 命名实体是命名实体识别的研究主体，一般包括3大类(实体类、时间类和数字类)和7小类(人名、地名、机构名、时间、日期、货币和百分比)命名实体。 评判一个命名实体是否被正确识别包括两个方面： 实体的边界是否正确 实体的类型是否标注正确主要错误类型包括文本正确，类型可能错误；反之，文本边界错误,标记的类型正确。 命名实体识别的主要技术方法分为： 基于规则和词典的方法 基于统计的方法 二者混合的方法等。 基于规则的方法多采用语言学专家手工构造规则模板,选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词(如尾字)、中心词等方法，以模式和字符串相匹配为主要手段，这类系统大多依赖于知识库和词典的建立。 基于统计机器学习的方法主要包括：隐马尔可夫模型(HiddenMarkovMode,HMM)、最大熵(MaxmiumEntropy,ME)、支持向量机(Support VectorMachine,SVM)、条件随机场( ConditionalRandom Fields,CRF)等。 命名实体识别方法汇总 神经网络结构在命名实体识别（NER）中的应用 统计自然语言处理梳理一：分词、命名实体识别、词性标注 电子病历命名实体识别项目：https://github.com/Wasim37/NERuselocal 电子病历实体识别项目文档：https://pan.baidu.com/s/1Ypy2F0EHnD5FmX6L_BqWRA 密码：3s24 jieba分词","link":"/2018/01/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"人脸识别","slug":"人脸识别","link":"/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"},{"name":"边缘检测","slug":"边缘检测","link":"/tags/%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"},{"name":"卷积","slug":"卷积","link":"/tags/%E5%8D%B7%E7%A7%AF/"},{"name":"StyleTransfer","slug":"StyleTransfer","link":"/tags/StyleTransfer/"},{"name":"贝叶斯","slug":"贝叶斯","link":"/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"},{"name":"隐马尔科夫模型","slug":"隐马尔科夫模型","link":"/tags/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"经典网络","slug":"经典网络","link":"/tags/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"Attention Model","slug":"Attention-Model","link":"/tags/Attention-Model/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"word2vec","slug":"word2vec","link":"/tags/word2vec/"},{"name":"YOLO","slug":"YOLO","link":"/tags/YOLO/"},{"name":"SVM","slug":"SVM","link":"/tags/SVM/"},{"name":"聚类","slug":"聚类","link":"/tags/%E8%81%9A%E7%B1%BB/"},{"name":"特征工程","slug":"特征工程","link":"/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"name":"决策树","slug":"决策树","link":"/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"name":"随机森林","slug":"随机森林","link":"/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"},{"name":"提升算法","slug":"提升算法","link":"/tags/%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"吴恩达","slug":"吴恩达","link":"/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"},{"name":"线性回归","slug":"线性回归","link":"/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"name":"逻辑回归","slug":"逻辑回归","link":"/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"深度学习","slug":"深度学习","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"categories":[{"name":"深度学习","slug":"深度学习","link":"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"知识总结","slug":"知识总结","link":"/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"基础概念","slug":"基础概念","link":"/categories/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"}]}