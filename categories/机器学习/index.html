<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>分类: 机器学习 - Wasim&#039;s Blog</title><meta description="ai,机器学习,深度学习,算法,leetcode,java"><meta property="og:type" content="blog"><meta property="og:title" content="Wasim37"><meta property="og:url" content="https://wangxin123.com/"><meta property="og:site_name" content="Wasim37"><meta property="og:description" content="ai,机器学习,深度学习,算法,leetcode,java"><meta property="og:locale" content="zh_CN"><meta property="article:author" content="Wasim37"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://wangxin123.com/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://wangxin123.com/"},"headline":"Wasim37","image":["http://wangxin123.com/img/avatar.png"],"author":{"@type":"Person","name":"Wasim37"},"description":"ai,机器学习,深度学习,算法,leetcode,java"}</script><link rel="alternative" href="/atom.xml" title="Wasim&#039;s Blog" type="application/atom+xml"><link rel="icon" href="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/wu.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="/css/style.css"><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="/js/globalUtils.js"></script></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/logo.svg" alt="Wasim&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/wasim37"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">分类</a></li><li class="is-active"><a href="#" aria-current="page">机器学习</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-12-10T14:22:00.000Z">2017-12-10</time><a class="commentCountImg" href="/2017/12/10/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%AD%96%E7%95%A5%E6%80%BB%E7%BB%93/#comment-container"><span class="display-none-class">26981a17655c35f151e2fc009c756884</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="26981a17655c35f151e2fc009c756884"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">30 分钟 读完 (大约 4427 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/12/10/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%AD%96%E7%95%A5%E6%80%BB%E7%BB%93/">机器学习开发策略</a></h1><div class="content"><p>吴大大结构化机器学习项目总结，完善中…</p>
<h3 id="ML策略"><a href="#ML策略" class="headerlink" title="ML策略"></a>ML策略</h3><p>假设你构建了一个喵咪分类器，训练之后准确率达到90%，但在测试集上还不够好。此时你可以想到的优化方法有哪些呢？总结后大致如下：</p>
<ul>
<li>收集更多的数据</li>
<li>收集更多的多样化训练集，比如不同姿势的猫咪图片等</li>
<li>用梯度下降法训练更长时间</li>
<li>尝试Adam算法</li>
<li>尝试更大的网路</li>
<li>尝试小一点的网络</li>
<li>尝试dropout随机失活算法</li>
<li>加上L2正则项</li>
<li>改善网络结构，如变更激活函数，变更隐藏层节点数量</li>
</ul>
<p>优化的方法虽然很多，但如果方向错误，可能白费几个月时间。<br><strong>那通过哪些策略可以减少错误发生的几率呢？怎么判断哪些方法可以尝试，哪些方法可以丢弃呢？</strong></p></div><div class="level is-mobile is-flex"><div class="level-start"><div class="is-uppercase article-more button is-small size-small"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="link-muted mr-2" rel="tag" href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/">吴恩达</a></div></div><div class="level-start"><div class="level-item"><a class="article-more button is-small size-small link-muted" href="/2017/12/10/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E7%AD%96%E7%95%A5%E6%80%BB%E7%BB%93/#more"><i class="fas fa-book-reader has-text-grey"> </i>阅读更多&gt;&gt;</a></div></div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-11-02T14:22:00.000Z">2017-11-02</time><a class="commentCountImg" href="/2017/11/02/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/#comment-container"><span class="display-none-class">22ba2f198e13fbfd8be60c3bcdf5f1b9</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="22ba2f198e13fbfd8be60c3bcdf5f1b9"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">39 分钟 读完 (大约 5910 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/11/02/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></h1><div class="content"><h2 id="特征工程是什么"><a href="#特征工程是什么" class="headerlink" title="特征工程是什么"></a>特征工程是什么</h2><p><strong>数据和特征决定机器学习上限，而模型和算法只是逼近这个上限</strong>。<br><strong>特征工程目的：最大限度地从原始数据中提取特征以供算法和模型使用</strong>。</p>
<hr>
<h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><p>数据清洗的结果直接关系到模型效果以及最终的结论。在实际的工作中，数据清洗通常占开发过程的 50%-80% 的时间。</p>
<p>在数据预处理过程主要考虑两个方面，如下：</p>
<ul>
<li>选择数据处理工具：关系型数据库戒者Python</li>
<li>查看数据的元数据以及数据特征：一是查看元数据，包括字段解释、数据来源等一切可以描述数据的信息；另外是抽取一部分数据，通过人工查看的方式，对数据本身做一个比较直观的了解，并且初步发现一些问题，为之后的数据处理做准备。</li>
</ul>
<h3 id="缺省值清洗"><a href="#缺省值清洗" class="headerlink" title="缺省值清洗"></a>缺省值清洗</h3><p>缺省值是数据中最常见的一个问题，处理缺省值有很多方式，主要包括以下四个步骤进行缺省值处理：</p>
<ul>
<li>确定缺省值范围</li>
<li>去除不需要的字段</li>
<li>填充缺省值内容</li>
<li>重新获取数据</li>
</ul>
<p>注意：最重要的是 <strong>缺省值内容填充。</strong></p>
<p>在进行确定缺省值范围的时候，对每个字段都计算其缺失比例，然后按照缺失比例和字段重要性分别指定不同的策略。<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523141349.png" alt=""></p>
<p>在进行去除不需要的字段的时候，需要注意的是：删除操作最好不要直接操作不原始数据上，最好的是抽取部分数据进行删除字段后的模型构建，查看模型效果，如果效果不错，那么再到全量数据上进行删除字段操作。总而言之：该过程简单但是必须慎用，不过一般效果不错，删除一些丢失率高以及重要性低的数据可以降低模型的训练复杂度，同时又不会降低模型的效果。</p>
<p><strong>填充缺省值很重要，常用方法如下：</strong></p>
<ul>
<li>以业务知识经验推测填充缺省值</li>
<li>以同一字段指标的计算结果(均值、中位数、众数等)填充缺省值</li>
<li>以不同字段指标的计算结果来推测性的填充缺省值，比如通过身仹证号码计算年龄、通过收货地址来推测家庭住址、通过访问的IP地址来推测家庭/公司/学校的家庭住址等等</li>
</ul>
<p>如果某些指标非常重要，但是缺失率有比较高，而且通过其它字段没法比较精准的计算出指标值的情况下，那么就需要和数据产生方(业务人员、数据收集人员等)沟通协商，是否可以通过其它的渠道获取相关的数据，也就是进行重新获取数据的操作。</p>
<h3 id="格式内容清洗"><a href="#格式内容清洗" class="headerlink" title="格式内容清洗"></a>格式内容清洗</h3><p>一般情况下，数据是由用户/访客产生的，也就有很大的可能性存在格式和内容上不一致的情况，所以在进行模型构建之前需要先进行数据的格式内容清洗操作。格式内容问题主要有以下几类：</p>
<ul>
<li>时间、日期、数值、半全角等显示格式不一致：直接将数据转换为一类格式即可，该问题一般出现在多个数据源整合的情况下。</li>
<li>内容中有不该存在的字符：最典型的就是在头部、中间、尾部的空格等问题，这种</li>
<li>情况下，需要以半自劢校验加半人工方式来找出问题，并去除不需要的字符。内容不该字段应有的内容不符：比如姓名写成了性别、身仹证号写成手机号等问题。</li>
</ul>
<h3 id="逻辑错误清洗"><a href="#逻辑错误清洗" class="headerlink" title="逻辑错误清洗"></a>逻辑错误清洗</h3><p>主要是通过简单的逻辑推理发现数据中的问题数据，防止分析结果走偏，主要包含以下几个步骤：</p>
<ul>
<li>数据去重</li>
<li>去除/替换不合理的值</li>
<li>去除/重构不可靠的字段值(修改矛盾的内容)</li>
</ul>
<h3 id="去除不需要的数据"><a href="#去除不需要的数据" class="headerlink" title="去除不需要的数据"></a>去除不需要的数据</h3><p>一般情况下，我们会尽可能多的收集数据，但是不是所有的字段数据都是可以应用到模型构建过程的，也不是说将所有的字段属性都放到构建模型中，最终模型的效果就一定会好，实际上来讲，字段属性越多，模型的构建就会越慢，所以有时候可以考虑将不要的字段进行删除操作。在进行该过程的时候，要注意备仹原始数据。</p>
<h3 id="关联性验证"><a href="#关联性验证" class="headerlink" title="关联性验证"></a>关联性验证</h3><p>如果数据有多个来源，那么有必要进行关联性验证，该过程常应用到多数据源合并的过程中，通过验证数据之间的关联性来选择比较正确的特征属性，比如：汽车的线下贩买信息和电话客服问卷信息，两者之间可以通过姓名和手机号进行关联操作，匹配两者之间的车辆信息是否是同一辆，如果不是，那么就需要进行数据调整。</p>
<hr>
<h2 id="特征转换"><a href="#特征转换" class="headerlink" title="特征转换"></a>特征转换</h2><p>特征转换主要指将原始数据中的字段数据进行转换操作，从而得到适合进行算法模型构建的输入数据(数值型数据)，在这个过程中主要包括但不限于以下几种数据的处理：</p>
<ul>
<li>文本数据转换为数值型数据</li>
<li>缺省值填充</li>
<li>定性特征属性哑编码</li>
<li>定量特征属性二值化</li>
<li>特征标准化不归一化</li>
</ul>
<h3 id="文本特征属性转换"><a href="#文本特征属性转换" class="headerlink" title="文本特征属性转换"></a>文本特征属性转换</h3><p>机器学习的模型算法均要求输入的数据必须是数值型的，所以对于文本类型的特征属性，需要进行文本数据转换，也就是需要将文本数据转换为数值型数据。常用方式如下：</p>
<ul>
<li>词袋法(BOW/TF)</li>
<li>TF-IDF(Term frequency-inverse document frequency)</li>
<li>HashTF</li>
<li><a href="https://code.google.com/archive/p/word2vec">Word2Vec</a></li>
</ul>
<h4 id="词袋法"><a href="#词袋法" class="headerlink" title="词袋法"></a>词袋法</h4><p>词袋法(Bag of words, BOW)是最早应用于NLP和IR领域的一种文本处理模型，该模型忽略文本的语法和语序，用一组无序的单词(words)来表达一段文字戒者一个文档，词袋法中使用单词在文档中出现的次数(频数)来表示文档。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523142347.png" alt=""></p>
<p>词集法(Set of words, SOW)是词袋法的一种变种，应用的比较多，和词袋法的原理一样，是以文档中的单词来表示文档的一种的模型，区别在于：词袋法使用的是单词的频数，而在词集法中使用的是单词是否出现，如果出现赋值为1，否则为0。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523142450.png" alt=""></p>
<h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>词条的重要性随着它在<strong>文件中出现的次数</strong>成<strong>正比</strong>增加，但同时会随着它在<strong>语料库中出现的频率</strong>成<strong>反比</strong>下降；也就是说词条在文本中出现的次数越多，表示该词条对该文本的重要性越高，词条在所有文本中出现的次数越少，说明这个词条对文本的重要性越高。<strong>TF(词频)指某个词条在文本中出现的次数</strong>，一般会将其进行归一化处理(该词条数量/该文档中所有词条数量)；<strong>IDF(逆向文件频率)指一个词条重要性的度量</strong>，一般计算方式为<strong>总文件数目除以包含该词语之文件的数目</strong>，再将得到的商取对数得到。TF-IDF实际上是：TF * IDF</p>
<p>假设单词用t表示，文档用d表示，语料库用D表示，那么N(t,D)表示包含单词t的文档数量，|D|表示文档数量，|d|表示文档d中的所有单词数量。N(t,d)表示在文档d中单词t出现的次数。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523142659.png" alt=""></p>
<p>TF-IDF除了使用默认的tf和idf公式外，tf和idf公式还可以使用一些扩展之后公式来进行指标的计算，常用的公式有：</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523142756.png" alt=""></p>
<p>有两个文档，单词统计如下，请分别计算各个单词在文档中的TF-IDF值以及这些文档使用单词表示的特征向量。<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523142842.png" alt=""></p>
<h4 id="HashTF-IDF"><a href="#HashTF-IDF" class="headerlink" title="HashTF-IDF"></a>HashTF-IDF</h4><p>不管是前面的词袋法还是TF-IDF，都避免不了计算文档中单词的词频，当文档数量比较少、单词数量比较少的时候，我们的计算量不会太大，但是当这个数量上升到一定程度的时候，程序的计算效率就会降低下去，这个时候可以通过HashTF的形式来解决该问题。</p>
<p>HashTF的计算规则是：在计算过程中，不计算词频，而是计算单词进行hash后的hash值的数量(有的模型中可能存在正则化操作)；</p>
<p>HashTF的特点：运行速度快，但是无法获取高频词，有可能存在单<br>词碰撞问题(hash值一样)</p>
<p>在scikit中，对于文本数据主要提供了三种方式将文本数据转换为数值型的特征向量，同时提供了一种对TF-IDF公式改版的公式。所有的转换方式均位于模块：sklearn.feature_extraction.text</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523143128.png" alt=""></p>
<h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><p>Word2Vec是Google于2013年开源推出的一个用户获取wordvector的工具包，具有简单、高效的特性；Word2Vec通过对文档中所有单词进行分析，获得单词之间的关联程度，从而获取词向量，最终形成一个词向量矩阵。对词向量矩阵的分析：分类、聚类、相似性计算等等；是一种在NLP和大数据机器学习中应用比较多的一种文本转换数值型向量的方式。</p>
<p><a href="http://www.ai-start.com/dl2017/html/lesson5-week2.html#header-n169">吴恩达word2vec视频讲解</a><br><a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Sequence_Models/自然语言处理与词嵌入?id=word2vec">吴恩达word2vec总结</a></p>
<h3 id="无量纲化"><a href="#无量纲化" class="headerlink" title="无量纲化"></a>无量纲化</h3><p>无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。</p>
<h4 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h4><p>标准化：基于特征属性的数据(也就是特征矩阵的列)，获取均值和方差，然后将特征值转换至服从标准正态分布。计算公式如下：</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523143549.png" alt=""></p>
<h4 id="区间缩放法"><a href="#区间缩放法" class="headerlink" title="区间缩放法"></a>区间缩放法</h4><p>区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523143611.png" alt=""></p>
<h4 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h4><p>简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523143629.png" alt=""></p>
<h3 id="对定性特征哑编码"><a href="#对定性特征哑编码" class="headerlink" title="对定性特征哑编码"></a>对定性特征哑编码</h3><p>定性变量是反映”职业”、”教育程度”等现象属性特点的变量，只能反映现象的属性特点，而不能说明具体量的大小和差异。</p>
<p>哑编码(OneHotEncoder)：对于定性的数据(也就是分类的数据)，可以采用N位的状态寄存器来对N个状态进行编码，每个状态都有一个独立的寄存器位，并且在仸意状态下只有一位有效；是一种常用的将特征数字化的方式。比如有一个特征属性:[‘male’,’female’]，那么male使用向量[1,0]表示，female使用[0,1]表示。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523143447.png" alt=""></p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523143505.png" alt=""></p>
<h3 id="对定量特征二值化"><a href="#对定量特征二值化" class="headerlink" title="对定量特征二值化"></a>对定量特征二值化</h3><p>定量变量是反映类似”天气温度”和”月收入”等属性的变量，可以用数值表示其观察结果，这些数值具有明确的数值含义，不仅能分类而且能测量出来具体大小和差异。这些变量就是定量变量也称数值变量，定量变量的观察结果成为定量数据。是说明事物数字特征的一个名称。</p>
<p>二值化(Binarizer)：对于定量的数据根据给定的阈值，将其进行转换，如果大于阈值，那么赋值为1；否则赋值为0</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523143521.png" alt=""></p>
<h3 id="缺省值填充"><a href="#缺省值填充" class="headerlink" title="缺省值填充"></a>缺省值填充</h3><p>对于缺省的数据，在处理之前一定需要进行预处理操作，一般采用中位数、均值戒者众数来进行填充，在scikit中主要通过Imputer类来实现对缺省值的填充</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523143357.png" alt=""><br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523143423.png" alt=""></p>
<h3 id="数据多项式扩充变换"><a href="#数据多项式扩充变换" class="headerlink" title="数据多项式扩充变换"></a>数据多项式扩充变换</h3><p>多项式数据变换主要是指基于输入的特征数据按照既定的多项式规则构建更多的输出特征属性，比如输入特征属性为[a,b]，当设置degree为2的时候，那么输出的多项式特征为[1, a, b, a^2, ab, b^2]</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523155205.png" alt=""></p>
<hr>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>当做完特征转换后，实际上可能会存在很多的特征属性，比如：多项式扩展转换、文本数据转换等等，但是太多的特征属性的存在可能会导致模型构建效率降低，同时模型的效果有可能会变的不好，那么这个时候就需要从这些特征属性中选择出影响最大的特征属性作为最后构建模型的特征属性列表。</p>
<p><strong>在选择模型的过程中，通常从两方面来选择特征</strong>：</p>
<ul>
<li><strong>特征是否发散：</strong>如果一个特征不发散，比如方差解决于0，也就是说这样的特征对于样本的区分没有什么作用。</li>
<li><strong>特征不目标的相关性：</strong>如果不目标相关性比较高，应当优先选择。</li>
</ul>
<p>根据特征选择的形式又可以将特征选择方法分为3种：</p>
<ul>
<li>Filter：过滤法，按照发散性戒者相关性对各个特征进行评分，设定阈值戒者待选择阈值的个数，从而选择特征；常用方法包括方差选择法、相关系数法、卡方检验、互信息法等。</li>
<li>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征戒者排除若干特征；常用方法主要是递归特征消除法。</li>
<li>Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权重系数，根据系数从大到小选择特征；常用方法主要是基于惩罚项的特征选择法。</li>
</ul>
<h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><h4 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h4><p>方差选择法：先计算各个特征属性的方差值，然后根据阈值，获取方差大于阈值的特征。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523155242.png" alt=""></p>
<h4 id="相关系数法"><a href="#相关系数法" class="headerlink" title="相关系数法"></a>相关系数法</h4><p>相关系数法：先计算各个特征属性对于目标值的相关系数以及相关系数的P值，然后获取大于阈值的特征属性。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523155301.png" alt=""></p>
<h4 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h4><p>经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523155347.png" alt=""><br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523155402.png" alt=""></p>
<p>不难发现，<a href="https://link.zhihu.com/?target=http%3A//wiki.mbalib.com/wiki/%25E5%258D%25A1%25E6%2596%25B9%25E6%25A3%2580%25E9%25AA%258C">这个统计量的含义简而言之就是自变量对因变量的相关性</a>。</p>
<h4 id="互信息法"><a href="#互信息法" class="headerlink" title="互信息法"></a>互信息法</h4><p>经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523155621.png" alt=""></p>
<p>为了处理定量数据，最大信息系数法被提出</p>
<h3 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h3><h4 id="递归特征消除法"><a href="#递归特征消除法" class="headerlink" title="递归特征消除法"></a>递归特征消除法</h4><p>递归特征消除法：使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523155740.png" alt=""></p>
<h3 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h3><h4 id="基于惩罚项的特征选择法"><a href="#基于惩罚项的特征选择法" class="headerlink" title="基于惩罚项的特征选择法"></a>基于惩罚项的特征选择法</h4><p>在使用惩罚项的基模型，除了可以筛选出特征外，同时还可以进行降维操作。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523155756.png" alt=""></p>
<h4 id="基于树模型的特征选择法"><a href="#基于树模型的特征选择法" class="headerlink" title="基于树模型的特征选择法"></a>基于树模型的特征选择法</h4><p>树模型中GBDT在构建的过程会对特征属性进行权重的给定，所以GBDT也可以应用在基模型中进行特征选择。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523155908.png" alt=""></p>
<hr>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p>特征选择完成后，可以直接进行模型训练，但是可能由于特征矩阵过大，导致计算量大，为了节省训练时长，可以降低特征矩阵的维度。</p>
<p>常见的降维方法除了基于L1的惩罚模型外，还有主成分析法(PCA)和线性判别分析法(LDA)，这两种方法的<strong>本质都是将原始数据映射到维度更低的样本空间中</strong>；但是采用的方式不同，<strong>PCA是为了让映射后的样本具有更大的发散性，LDA是为了让映射后的样本有最好的分类性能</strong> 。</p>
<h3 id="主成分分析法-PCA"><a href="#主成分分析法-PCA" class="headerlink" title="主成分分析法 PCA"></a>主成分分析法 PCA</h3><p>主成分析(PCA)：将高纬的特征向量合并成低纬的特征属性，是一种无监督的降维方法。<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523160110.png" alt=""><br><a href="http://www.ai-start.com/ml2014/html/week8.html#header-n233">吴恩达PCA算法讲解</a> </p>
<h3 id="线性判别分析法-LDA"><a href="#线性判别分析法-LDA" class="headerlink" title="线性判别分析法 LDA"></a>线性判别分析法 LDA</h3><p>线性判断分析(LDA)：LDA是一种基于分类模型进行特征属性合并的操作，是一种有监督的降维方法。<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/20180523160125.png" alt=""></p>
<h2 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h2><p><a href="http://www.ai-start.com/ml2014/html/week9.html#header-n5">吴恩达异常检测讲解</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-29T14:22:00.000Z">2017-10-29</time><a class="commentCountImg" href="/2017/10/29/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/#comment-container"><span class="display-none-class">532aec83173697fd80b564b22f499c3c</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="532aec83173697fd80b564b22f499c3c"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">6 分钟 读完 (大约 962 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/29/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/">机器学习（八）：隐马尔科夫模型</a></h1><div class="content"><p>持续更新中。。。</p></div><div class="level is-mobile is-flex"><div class="level-start"><div class="is-uppercase article-more button is-small size-small"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/">隐马尔科夫模型</a></div></div><div class="level-start"><div class="level-item"><a class="article-more button is-small size-small link-muted" href="/2017/10/29/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/#more"><i class="fas fa-book-reader has-text-grey"> </i>阅读更多&gt;&gt;</a></div></div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-26T14:22:00.000Z">2017-10-26</time><a class="commentCountImg" href="/2017/10/26/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/#comment-container"><span class="display-none-class">f756fd2293917cf1253a6d78ea7adf31</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="f756fd2293917cf1253a6d78ea7adf31"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">11 分钟 读完 (大约 1714 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/26/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/">贝叶斯算法</a></h1><div class="content"><p>持续更新中。。。</p>
<ul>
<li><a href="https://github.com/Wasim37/machine_learning_code/tree/master/06%20%E8%B4%9D%E5%8F%B6%E6%96%AF/notebook">示例代码</a></li>
<li><a href="#贝叶斯定理相关公式">贝叶斯定理相关公式</a></li>
<li><a href="https://www.zhihu.com/question/22905989">条件概率和后验概率区别</a></li>
<li><a href="#朴素贝叶斯">朴素贝叶斯</a></li>
<li><a href="#高斯朴素贝叶斯">高斯朴素贝叶斯</a></li>
<li><a href="#伯努利朴素贝叶斯">伯努利朴素贝叶斯</a></li>
<li><a href="#多项式朴素贝叶斯">多项式朴素贝叶斯</a></li>
<li><a href="#贝叶斯网络">贝叶斯网络</a></li>
<li><a href="https://www.zhihu.com/question/28006799/answer/38996563">怎么通俗易懂地解释贝叶斯网络和它的应用</a></li>
<li><a href="#用贝叶斯机率说明Dropout的原理">用贝叶斯机率说明Dropout的原理</a></li>
<li><a href="#如何用贝叶斯算法实现垃圾邮件检测">如何用贝叶斯算法实现垃圾邮件检测</a></li>
<li><a href="#当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑">当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑</a></li>
</ul>
<hr>
<h3 id="贝叶斯定理相关公式"><a href="#贝叶斯定理相关公式" class="headerlink" title="贝叶斯定理相关公式"></a><h2 id="贝叶斯定理相关公式">贝叶斯定理相关公式</h2></h3><ul>
<li>先验概率P(A)：在不考虑任何情况下，A事件发生的概率</li>
<li>条件概率P(B|A)：A事件发生的情况下，B事件发生的概率</li>
<li>后验概率P(A|B)：在B事件发生之后，对A事件发生的概率的重新评估。<a href="https://www.zhihu.com/question/22905989">条件概率和后验概率区别</a></li>
<li>全概率：如果B和B’构成样本空间的一个划分，那么事件A的概率为：B和B’的概率分别乘以A对这两个事件的概率之和。</li>
<li><strong>贝叶斯定理：</strong> <script type="math/tex">P(B_i|A)=\frac{P(AB_i)}{P(A)}=\frac{P(B_i)P(A|B_i)}{\sum_j P(B_j)P(A|B_j)}</script></li>
</ul>
<p>贝叶斯不同于SVM、逻辑回归与决策树等判别式模型，它属于生成式模型（LDA、HMM等）。<br><strong>贝叶斯思想可以概括为先验概率+数据=后验概率，后验概率就是我们要求的。</strong></p>
<hr>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a><h2 id="朴素贝叶斯">朴素贝叶斯</h2></h3><p>朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是很简单地<strong>假设样本特征彼此独立</strong>. 这个假设<strong>现实中基本上不存在</strong>， 但特征相关性很小的实际情况还是很多， 所以很多时候这个模型仍然能够工作得很好。</p>
<p><strong>如果样本特征属性关联很大，朴素贝叶斯就没法很好解决这类问题，可以考虑使用贝叶斯网络。</strong></p>
<hr>
<h3 id="高斯朴素贝叶斯"><a href="#高斯朴素贝叶斯" class="headerlink" title="高斯朴素贝叶斯"></a><h2 id="高斯朴素贝叶斯">高斯朴素贝叶斯</h2></h3><p>Gaussian Naive Bayes是指当 <strong>特征属性为连续值时，而且分布服从高斯分布</strong>，<br>那么在计算 P(x|y) 的时候可以直接使用高斯分布的概率公式：</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514104856.png" alt=""></p>
<p>因此只需要计算出各个类别中此特征项划分的各个均值和标准差</p>
<hr>
<h3 id="伯努利朴素贝叶斯"><a href="#伯努利朴素贝叶斯" class="headerlink" title="伯努利朴素贝叶斯"></a><h2 id="伯努利朴素贝叶斯">伯努利朴素贝叶斯</h2></h3><p>Bernoulli Naive Bayes是指当 <strong>特征属性为连续值时，而且分布服从伯努利分布，</strong><br>那么在计算 P(x|y) 的时候可以直接使用伯努利分布的概率公式：</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514104905.png" alt=""></p>
<p>伯努利分布是一种离散分布，只有两种可能的结果。1表示成功，出现的概率为p；<br>0表示失败，出现的概率为q=1-p；其中均值为E(x)=p，方差为Var(X)=p(1-p)</p>
<hr>
<h3 id="多项式朴素贝叶斯"><a href="#多项式朴素贝叶斯" class="headerlink" title="多项式朴素贝叶斯"></a><h2 id="多项式朴素贝叶斯">多项式朴素贝叶斯</h2></h3><p>Multinomial Naive Bayes是指当 <strong>特征属性服从多项分布</strong>，从而对于每个类别y，<br>参数为θy =(θy1, θy2,…,θyn)，其中n为特征属性数目，那么P(xi|y)的概率为θyi</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514104934.png" alt=""></p>
<hr>
<h3 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a><h2 id="贝叶斯网络">贝叶斯网络</h2></h3><p><strong>当多个特征属性之间存在着某种相关关系的时候，使用朴素贝叶斯算法就没法解决这类问题，那么贝叶斯网络就是解决这类应用场景的一个非常好的算法。</strong></p>
<p>把某个研究系统中涉及到的随机变量，根据是否条件独立绘制在一个有向图中，就形成了贝叶斯网络。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514111959.png" alt=""></p>
<p>贝叶斯网络(BN)，又称有向无环图模型，是一种概率图模型，根据概率图的拓扑结构，<strong>考察一组随机变量{X1, X2,…,Xn}及其N组条件概率分布的性质。</strong></p>
<p>一般而言，贝叶斯网络的有向无环图中的节点表示随机变量，可以是可观察到的变量，或隐变量，未知参数等等。<strong>连接两个节点之间的箭头代表两个随机变量之间的因果关系</strong>(也就是这两个随机变量之间非条件独立)，如果两个节点间以一个单箭头连接在一起，表示其中一个节点是“因”，另外一个是“果”，从而两节点之间就会产生一个条件概率值。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514111945.png" alt=""></p>
<p><strong>注意：每个节点在给定其直接前驱的时候，条件独立于其后继。</strong></p>
<p>更多细节见我整理的PDF文件。</p>
<hr>
<h3 id="如何用贝叶斯算法实现垃圾邮件检测"><a href="#如何用贝叶斯算法实现垃圾邮件检测" class="headerlink" title="如何用贝叶斯算法实现垃圾邮件检测"></a><h2 id="如何用贝叶斯算法实现垃圾邮件检测">如何用贝叶斯算法实现垃圾邮件检测</h2></h3><p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6%E8%B4%9D%E5%8F%B6%E6%96%AF.png" alt=""></p>
<p>首先通过相除消去分子，大于1即正样本，小于1即负样本。<br>然后将难以计算的联合分布概率P(y=T|x)和P(y=F|x)进行转换即可。</p>
<p>下面是另外两种解释：</p>
<p>原文：<a href="https://github.com/SunnyMarkLiu/NaiveBayesSpamFilter">https://github.com/SunnyMarkLiu/NaiveBayesSpamFilter</a></p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514110848.png" alt=""></p>
<p>原文：<a href="https://blog.csdn.net/Gane_Cheng/article/details/53219332">https://blog.csdn.net/Gane_Cheng/article/details/53219332</a></p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/20180514110919.png" alt=""></p>
<hr>
<h3 id="当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑"><a href="#当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑" class="headerlink" title="当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑"></a><h2 id="当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑">当你输入错误单词时，搜索引擎会进行拼写检查并提示正确单词，如何用贝叶斯算法实现相关逻辑</h2></h3><p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/55.png" alt=""></p>
<hr>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-25T14:22:00.000Z">2017-10-25</time><a class="commentCountImg" href="/2017/10/25/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/#comment-container"><span class="display-none-class">4bf7bf6c585216595e9cce01d69be18b</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="4bf7bf6c585216595e9cce01d69be18b"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">16 分钟 读完 (大约 2418 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/25/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/">聚类算法</a></h1><div class="content"><p>持续更新中。。。</p></div><div class="level is-mobile is-flex"><div class="level-start"><div class="is-uppercase article-more button is-small size-small"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/%E8%81%9A%E7%B1%BB/">聚类</a></div></div><div class="level-start"><div class="level-item"><a class="article-more button is-small size-small link-muted" href="/2017/10/25/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/#more"><i class="fas fa-book-reader has-text-grey"> </i>阅读更多&gt;&gt;</a></div></div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-22T14:22:00.000Z">2017-10-22</time><a class="commentCountImg" href="/2017/10/22/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/SVM/#comment-container"><span class="display-none-class">764fa7077ee5e1d8fc4c2870ac868244</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="764fa7077ee5e1d8fc4c2870ac868244"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">25 分钟 读完 (大约 3772 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/22/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/SVM/">SVM</a></h1><div class="content"><p>持续更新中。。。</p></div><div class="level is-mobile is-flex"><div class="level-start"><div class="is-uppercase article-more button is-small size-small"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/SVM/">SVM</a></div></div><div class="level-start"><div class="level-item"><a class="article-more button is-small size-small link-muted" href="/2017/10/22/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/SVM/#more"><i class="fas fa-book-reader has-text-grey"> </i>阅读更多&gt;&gt;</a></div></div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-21T14:22:00.000Z">2017-10-21</time><a class="commentCountImg" href="/2017/10/21/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/#comment-container"><span class="display-none-class">641bd497a93aa25611f6280e906e542e</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="641bd497a93aa25611f6280e906e542e"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">36 分钟 读完 (大约 5360 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/21/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/">决策树、随机森林和提升算法</a></h1><div class="content"><p>持续更新中。。。</p></div><div class="level is-mobile is-flex"><div class="level-start"><div class="is-uppercase article-more button is-small size-small"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a><a class="link-muted mr-2" rel="tag" href="/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/">随机森林</a><a class="link-muted mr-2" rel="tag" href="/tags/%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/">提升算法</a></div></div><div class="level-start"><div class="level-item"><a class="article-more button is-small size-small link-muted" href="/2017/10/21/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8C%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/#more"><i class="fas fa-book-reader has-text-grey"> </i>阅读更多&gt;&gt;</a></div></div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-19T14:22:00.000Z">2017-10-19</time><a class="commentCountImg" href="/2017/10/19/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/#comment-container"><span class="display-none-class">7ed7ac9bfcd25b77db45d0d4747bd696</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="7ed7ac9bfcd25b77db45d0d4747bd696"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">1 小时 读完 (大约 7589 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/19/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/">回归算法</a></h1><div class="content"><p>持续更新中。。。</p></div><div class="level is-mobile is-flex"><div class="level-start"><div class="is-uppercase article-more button is-small size-small"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">线性回归</a><a class="link-muted mr-2" rel="tag" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">逻辑回归</a></div></div><div class="level-start"><div class="level-item"><a class="article-more button is-small size-small link-muted" href="/2017/10/19/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/#more"><i class="fas fa-book-reader has-text-grey"> </i>阅读更多&gt;&gt;</a></div></div></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2017-10-18T14:22:00.000Z">2017-10-18</time><a class="commentCountImg" href="/2017/10/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/#comment-container"><span class="display-none-class">813dc5c0e202f19ea6c07dd0b5f01f28</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="813dc5c0e202f19ea6c07dd0b5f01f28"> 99+</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">2 小时 读完 (大约 20969 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2017/10/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/">机器学习选择题集锦</a></h1><div class="content"><a id="more"></a>
<h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><p>1、对于线性回归，我们应该有以下哪些假设？</p>
<ol>
<li>找到离群点很重要, 因为线性回归对离群点很敏感</li>
<li>线性回归要求所有变量必须符合正态分布</li>
<li>线性回归假设数据没有多重线性相关性<br>A 1 和 2<br>B 2 和 3<br>C 1,2 和 3<br>D 以上都不是</li>
</ol>
<p><strong>答案</strong>：D<br><strong>解析</strong>：第1个假设, 离群点要着重考虑, 第一点是对的<br>第2个假设, 正态分布不是必须的. 当然, 如果是正态分布, 训练效果会更好<br>第3个假设, 有少量的多重线性相关性也是可以的, 但是我们要尽量避免</p>
<p>只含有一个解释变量的线性回归模型称为“一元线性回归模型”。在一个方程式中含有一个以上的解释变量的线性回归模型称为“多元线性回归模型”。在多元线性回归模型中，<strong>各个解释变量之间不能存在线性相关关系。如果存在，则称该模型具有“多重共线性”</strong>。多重线性回归是简单直线回归的推广，研究一个因变量与多个自变量之间的数量依存关系。<strong>在建立多元线性回归模型时，在变量的选取上要避免出现多重共线性问题</strong>。</p>
<p>2、当我们构造线性模型时, 我们注意变量间的相关性. 在相关矩阵中搜索相关系数时, 如果我们发现3对变量的相关系数是(Var1 和Var2, Var2和Var3, Var3和Var1)是-0.98, 0.45, 1.23 . 我们可以得出什么结论: </p>
<ol>
<li>Var1和Var2是非常相关的</li>
<li>因为Var1和Var2是非常相关的, 我们可以去除其中一个</li>
<li>Var3和Var1的1.23相关系数是不可能的<br>A 1 and 3<br>B 1 and 2<br>C 1,2 and 3<br>D 1</li>
</ol>
<p><strong>答案</strong>：C<br><strong>解析</strong>：<strong>相关性系数范围应该是 [-1,1]一般如果相关系数大于0.7或者小于-0.7, 是高相关的.</strong><br>Var1和Var2相关系数是接近负1, 所以这是多重线性相关, 我们可以考虑去除其中一个.<br>所以1, 2, 3个结论都是对的, 选C.</p>
<p>3、机器学习中L1正则化和L2正则化的区别是？<br>A 使用L1可以得到稀疏的权值<br>B 使用L1可以得到平滑的权值<br>C 使用L2可以得到稀疏的权值</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：<br>L1 lasso回归，L2岭回归<br><strong>L1正则化偏向于稀疏</strong>，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0.<br><strong>L2主要功能是为了防止过拟合</strong>，当所求参数越小时，说明模型越简单，而<strong>模型越简单则，越趋向于平滑</strong>，从而防止过拟合。</p>
<p>L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。</p>
<p>L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，<strong>最明显的一点就是，L2正则化会让系数的取值变得平均</strong>。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。</p>
<p><strong>可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动</strong>。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。</p>
<p>因此，一句话总结就是：<strong>L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。</strong></p>
<p><font color="red">4、你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？<br>A 第一个 w2 成了 0，接着 w1 也成了 0<br>B 第一个 w1 成了 0，接着 w2 也成了 0<br>C w1 和 w2 同时成了 0<br>D 即使在 C 成为大值之后，w1 和 w2 都不能成 0</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：L1正则化的函数如下图，所以w1和w2可以为0。同时w1和w2是对称的，不会导致一个为0另一个不为0的状态。</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/1.png" alt=""></p>
<p>5、如果在一个高度非线性并且复杂的一些变量中, 一个树模型可能比一般的回归模型效果更好. 这是（）<br>A 对的<br>B 错的</p>
<p><strong>答案</strong>：A</p>
<p>6、在Logistic Regression 中,如果同时加入L1和L2范数,不会产生什么效果()<br>A 以做特征选择,并在一定程度上防止过拟合<br>B 能解决维度灾难问题<br>C 能加快计算速度<br>D 可以获得更准确的结果</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：<br>在代价函数后面加上正则项，L1即是Losso回归，L2是岭回归。L1范数是指向量中各个元素绝对值之和，用于特征选择。L2范数 是指向量各元素的平方和然后求平方根，用于 防止过拟合，提升模型的泛化能力。</p>
<p>L1范数具有系数解的特性，但是要注意的是，L1没有选到的特征不代表不重要，原因是两个高相关性的特征可能只保留一个。如果需要确定哪个特征重要，再通过交叉验证。它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。所以能加快计算速度和缓解维数灾难. </p>
<p>7、对数几率回归（logistics regression）和一般回归分析有什么区别？<br>A 对数几率回归是设计用来预测事件可能性的<br>B 对数几率回归可以用来度量模型拟合程度<br>C 对数几率回归可以用来估计回归系数<br>D 以上所有</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：<br>A: 对数几率回归其实是设计用来解决分类问题的<br>B: 对数几率回归可以用来检验模型对数据的拟合度<br>C: <strong>虽然对数几率回归是用来解决分类问题的，但是模型建立好后，就可以根据独立的特征，估计相关的回归系数</strong>。就我认为，这只是估计回归系数，不能直接用来做回归模型。</p>
<p>8、回归模型中存在多重共线性, 你如何解决这个问题？<br>1 去除这两个共线性变量<br>2 我们可以先去除一个共线性变量<br>3 计算VIF(方差膨胀因子), 采取相应措施<br>4 为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归<br>A 1<br>B 2<br>C 2和3<br>D 2, 3和4</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：解决多重公线性, 可以使用相关矩阵去去除相关性高于75%的变量 (有主观成分). 也可以VIF, 如果VIF值&lt;=4说明相关性不是很高, VIF值&gt;=10说明相关性较高.<br>我们也可以用 岭回归和lasso回归的带有惩罚正则项的方法. 我们也可以在一些变量上加随机噪声, 使得变量之间变得不同, 但是这个方法要小心使用, 可能会影响预测效果。</p>
<p>9、给线性回归模型添加一个不重要的特征可能会造成？<br>A 增加 R-square<br>B 减少 R-square</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：在给特征空间添加了一个特征后，不论特征是重要还是不重要，R-square 通常会增加。<br>R-square定义如下:<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/13.png" alt=""><br><strong>在给特征空间添加了一个特征后，分子会增加一个残差平方项, 分母会增加一个均值差平方项, 前者一般小于后者, 所以不论特征是重要还是不重要，R-square 通常会增加</strong></p>
<p><font color="red">10、对于线性回归模型，包括附加变量在内，以下的可能正确的是 :</p>
<ol>
<li>R-Squared 和 Adjusted R-squared都是递增的</li>
<li>R-Squared 是常量的，Adjusted R-squared是递增的</li>
<li>R-Squared 是递减的， Adjusted R-squared 也是递减的</li>
<li>R-Squared 是递减的， Adjusted R-squared是递增的<br>A 1 和 2<br>B 1 和 3<br>C 2 和 4<br>D 以上都不是</li>
</ol>
<p><strong>答案</strong>：D<br><strong>解析</strong>：R-squared不能决定系数估计和预测偏差，这就是为什么我们要估计残差图。但是，R-squared有R-squared 和 predicted R-squared 所没有的问题。<br>每次你为模型加入预测器，R-squared递增或不变.</p>
<p><font color="red">11、线性回归的基本假设不包括哪个？<br>A 随机误差项是一个期望值为0的随机变量<br>B 对于解释变量的所有观测值，随机误差项有相同的方差<br>C 随机误差项彼此相关<br>D 解释变量是确定性变量不是随机变量，与随机误差项之间相互独立<br>E 随机误差项服从正态分布</p>
<p><strong>答案</strong>：C</p>
<p>8、一监狱人脸识别准入系统用来识别待进入人员的身份，此系统一共包括识别4种不同的人员：狱警，小偷，送餐员，其他。下面哪种学习方法最适合此种应用需求：<br>A 二分类问题<br>B 多分类问题<br>C 层次聚类问题<br>D k-中心点聚类问题<br>E 回归问题<br>F 结构分析问题</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：二分类：每个分类器只能把样本分为两类。监狱里的样本分别为狱警、小偷、送餐员、其他。二分类肯 定行不通。瓦普尼克95年提出来基础的支持向量机就是个二分类的分类器，这个分类器学习过 程就是解一个基于正负二分类推导而来的一个最优规划问题（对偶问题），要解决多分类问题 就要用决策树把二分类的分类器级联，VC维的概念就是说的这事的复杂度。<br>层次聚类： 创建一个层次等级以分解给定的数据集。监狱里的对象分别是狱警、小偷、送餐员、或者其 他，他们等级应该是平等的，所以不行。此方法分为自上而下（分解）和自下而上（合并）两种操作方式。<br>K-中心点聚类：挑选实际对象来代表簇，每个簇使用一个代表对象。它是围绕中心点划分的一种规则，所以这里并不合适。<br>回归分析：处理变量之间具有相关性的一种统计方法，这里的狱警、小偷、送餐员、其他之间并没有什 么直接关系。<br>结构分析： 结构分析法是在统计分组的基础上，计算各组成部分所占比重，进而分析某一总体现象的内部结构特征、总体的性质、总体内部结构依时间推移而表现出的变化规律性的统计方法。结构分析法的基本表现形式，就是计算结构指标。这里也行不通。<br>多分类问题： 针对不同的属性训练几个不同的弱分类器，然后将它们集成为一个强分类器。这里狱警、 小偷、送餐员 以及他某某，分别根据他们的特点设定依据，然后进行区分识别。</p>
<h3 id="贝叶斯-NB"><a href="#贝叶斯-NB" class="headerlink" title="贝叶斯 NB"></a>贝叶斯 NB</h3><p>1、Nave Bayes(朴素贝叶斯)是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是（）<br>A 各类别的先验概率P(C)是相等的<br>B 以0为均值，sqr(2)/2为标准差的正态分布<br>C 特征变量X的各个维度是类别条件独立随机变量<br>D P(X|C)是高斯分布</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：朴素贝叶斯的基本假设就是每个变量相互独立。</p>
<p><font color="red">7、假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中不正确的是？<br>A 模型效果相比无重复特征的情况下精确度会降低<br>B 如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样<br>C 当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：朴素贝叶斯的条件就是每个变量相互独立。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。<br>此外，若高度相关的特征在模型中引入两次, 这样增加了这一特征的重要性, 则它的性能因数据包含高度相关的特征而下降。正确做法是评估特征的相关矩阵，并移除那些高度相关的特征。</p>
<p><font color="red">7、符号集 a 、 b 、 c 、 d ，它们相互独立，相应概率为 1/2 、 1/4 、 1/8/ 、 1/16 ，其中包含信息量最小的符号是（ ）<br>A a<br>B b<br>C c<br>D d</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：<strong>因为消息出现的概率越小，则消息中所包含的信息量就越大</strong>。因此选a,同理d信息量最大。</p>
<hr>
<h3 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h3><p>9、隐马尔可夫模型三个基本问题以及相应的算法说法错误的是（ ）<br>A 评估—前向后向算法<br>B 解码—维特比算法<br>C 学习—Baum-Welch算法<br>D 学习—前向后向算法</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：<br>评估问题，可以使用前向算法、后向算法、前向后向算法。</p>
<p>9、解决隐马模型中预测问题的算法是<br>A 前向算法<br>B 后向算法<br>C Baum-Welch算法<br>D 维特比算法</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：<br>@刘炫320，本题题目及解析来源：<a href="http://blog.csdn.net/column/details/16442.html">http://blog.csdn.net/column/details/16442.html</a><br>A、B：前向、后向算法解决的是一个评估问题，即给定一个模型，求某特定观测序列的概率，用于评估该序列最匹配的模型。<br>C：Baum-Welch算法解决的是一个模型训练问题，即参数估计，是一种无监督的训练方法，主要通过EM迭代实现；<br>D：维特比算法解决的是给定 一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。如通过海藻变化（输出序列）来观测天气（状态序列），是预测问题，通信中的解码问题。</p>
<p>6、在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计（）<br>A EM算法<br>B 维特比算法<br>C 前向后向算法<br>D 极大似然估计</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法<br>维特比算法： 用动态规划解决HMM的预测问题，不是参数估计<br>前向后向算法：用来算概率<br>极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数<br>注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。</p>
<p>5、在HMM中,如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计()<br>A EM算法<br>B 维特比算法<br>C 前向后向算法<br>D 极大似然估计</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：<br>EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法<br>维特比算法： 用动态规划解决HMM的预测问题，不是参数估计<br>前向后向算法：用来算概率<br>极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数<br>注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。</p>
<p>8、请选择下面可以应用隐马尔科夫(HMM)模型的选项<br>A 基因序列数据集<br>B 电影浏览数据集<br>C 股票市场数据集<br>D 所有以上</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：只要是和时间序列问题有关的 , 都可以试试HMM</p>
<p>6、下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）<br>A  特征灵活<br>B 速度快<br>C 可容纳较多上下文信息<br>D 全局最优<br><strong>答案</strong>： B<br><strong>解析</strong>：CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢<br>CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样） ————与HMM比较<br>同时，由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。 ­­————与MEMM比较</p>
<p>CRF是在给定需要标记的观察序列的条件下，使用维特比算法，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。————与ME比较</p>
<hr>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p>5、假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分？<br>A 设C=1<br>B 设C=0<br>C 设C=无穷大<br>D 以上都不对</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：无穷大保证了所有的线性不可分都是可以忍受的.<br>最大间距分类只有当参数 C 是非常大的时候才起效，模型会过拟合。</p>
<p>6、训练完SVM模型后, 不是支持向量的那些样本我们可以丢掉, 也可以继续分类<br>A 正确<br>B 错误</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：SVM模型中, 真正影响决策边界的是支持向量</p>
<p>4、关于支持向量机SVM,下列说法错误的是（）<br>A L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力<br>B Hinge 损失函数，作用是最小化经验分类错误<br>C 分类间隔为1/||w||，||w||代表向量的模<br>D 当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：A正确。考虑加入正则化项的原因：想象一个完美的数据集，y&gt;1是正类，y&lt;-1是负类，决策面y=0，加入一个y=-30的正类噪声样本，那么决策面将会变“歪”很多，分类间隔变小，泛化能力减小。加入正则项之后，对噪声样本的容错能力增强，前面提到的例子里面，决策面就会没那么“歪”了，使得分类间隔变大，提高了泛化能力。</p>
<p>Hinge 可以用来解 间距最大化 的问题，最有代表性的就是SVM 问题，详见：<a href="https://blog.csdn.net/u010976453/article/details/78488279">https://blog.csdn.net/u010976453/article/details/78488279</a></p>
<p>10、有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是( )<br>A 2x+y=4<br>B x+2y=5<br>C x+2y=3<br>D 2x-y=0</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：这道题简化了，对于两个点来说，最大间隔就是垂直平分线，因此求出垂直平分线即可。斜率是两点连线的斜率的负倒数-1/((-1-3)/(0-2)) = -1/2, 可得y=-(1/2)x + c, 过中点((0+2)/2, (-1+3)/2) = (1, 1), 可得c=3/2, 故选C.</p>
<p><font color="red">10、关于logit 回归和SVM 不正确的是（ ） 和下题矛盾<br>A Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。<br>B Logit回归的输出就是样本属于正类别的几率，可以计算出概率。<br>C SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。<br>D SVM可以通过正则化系数控制模型的复杂度，避免过拟合。</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：<strong>Logit回归目标函数是最小化后验概率，SVM目标是结构风险最小化，SVM可以有效避免模型过拟合。</strong></p>
<p>9、关于 logit 回归和 SVM 不正确的是（）<br>A Logit回归目标函数是最小化后验概率<br>B Logit回归可以用于预测事件发生概率的大小<br>C SVM目标是结构风险最小化<br>D SVM可以有效避免模型过拟合</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。而最小化后验概率是朴素贝叶斯算法要做的。A错误<br>B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确<br>C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化。<br>D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。</p>
<p><font color="red">10、下图是同一个SVM模型, 但是使用了不同的径向基核函数的gamma参数, 依次是g1, g2, g3 , 下面大小比较正确的是<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/4.png" alt=""><br>A<br>g1 &gt; g2 &gt; g3<br>B<br>g1 = g2 = g3<br>C<br>g1 &lt; g2 &lt; g3<br>D<br>g1 &gt;= g2 &gt;= g3E. g1 &lt;= g2 &lt;= g3</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：<br>所谓径向基函数 (Radial Basis Function 简称 RBF), 就是某种沿径向对称的标量函数。 通常定义为空间中任一点x到某一中心点xc之间欧氏距离的单调函数 , 可记作 k(||x-xc||), 其作用往往是局部的 , 即当x远离xc时函数取值很小。最常用的径向基函数是高斯核函数 ,形式为 k(||x-xc||)=exp{- ||x-xc||^2/(2<em>σ^2) } 其中xc为核函数中心,σ为函数的宽度参数 , 控制了函数的径向作用范围。由radial basis: exp(-gamma</em>|u-v|^2)可知, gamma越小, 模型越简单, 平滑度越好, 分类边界越不容易过拟合, 所以选C。</p>
<p><font color="red">8、下列不是SVM核函数的是<br>A 多项式核函数<br>B logistic核函数<br>C 径向基核函数<br>D Sigmoid核函数</p>
<p><strong>答案</strong>： B<br><strong>解析</strong>：<br><strong>SVM核函数包括线性核函数、多项式核函数、径向基核函数、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数.</strong></p>
<p>核函数的定义并不困难，根据泛函的有关理论，只要一种函数 K(xi, xj) 满足Mercer条件，它就对应某一变换空间的内积．对于判断哪些函数是核函数到目前为止也取得了重要的突破，得到Mercer定理和以下常用的核函数类型：<br>(1)线性核函数<br>K ( x , x i ) = x ⋅ x i<br>(2)多项式核<br>K ( x , x i ) = ( ( x ⋅ x i ) + 1 ) d<br>(3)径向基核（RBF）<br>K ( x , x i ) = exp ( − ∥ x − x i ∥ 2 σ 2 )<br>Gauss径向基函数则是局部性强的核函数，其外推能力随着参数 σ 的增大而减弱。多项式形式的核函数具有良好的全局性质。局部性较差。<br>(4)傅里叶核<br>K ( x , x i ) = 1 − q 2 2 ( 1 − 2 q cos ( x − x i ) + q 2 )<br>(5)样条核<br>K ( x , x i ) = B 2 n + 1 ( x − x i )<br>(6)Sigmoid核函数<br>K ( x , x i ) = tanh ( κ ( x , x i ) − δ )<br>采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络，应用SVM方法，隐含层节点数目(它确定神经网络的结构)、隐含层节点对输入节点的权值都是在设计(训练)的过程中自动确定的。而且支持向量机的理论基础决定了它最终求得的是全局最优值而不是局部最小值，也保证了它对于未知样本的良好泛化能力而不会出现过学习现象。</p>
<p>核函数的选择<br>在选取核函数解决实际问题时，通常采用的方法有：<br>一是利用专家的先验知识预先选定核函数；<br>二是采用Cross-Validation方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数．如针对傅立叶核、RBF核，结合信号处理问题中的函数回归问题，通过仿真实验，对比分析了在相同数据条件下，采用傅立叶核的SVM要比采用RBF核的SVM误差小很多．<br>三是采用由Smits等人提出的混合核函数方法，该方法较之前两者是目前选取核函数的主流方法，也是关于如何构造核函数的又一开创性的工作．将不同的核函数结合起来后会有更好的特性，这是混合核函数方法的基本思想．</p>
<hr>
<h3 id="决策树、随机森林、提升算法"><a href="#决策树、随机森林、提升算法" class="headerlink" title="决策树、随机森林、提升算法"></a>决策树、随机森林、提升算法</h3><p><font color="red">2、对于信息增益, 决策树分裂节点, 下面说法正确的是（）<br>1 纯度高的节点需要更多的信息去区分<br>2 信息增益可以用”1比特-熵”获得<br>3 如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的<br>A 1<br>B 2<br>C 2和3<br>D 所有以上</p>
<p><strong>答案</strong>：C</p>
<p>3、我们想在大数据集上训练决策树, 为了使用较少时间, 我们可以<br>A 增加树的深度<br>B 增加学习率 (learning rate)<br>C 减少树的深度<br>D 减少树的数量</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：<br>增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.<br><strong>决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)</strong><br>决策树只有一棵树, 不是随机森林。</p>
<p>4、对于随机森林和GradientBoosting Trees, 下面说法正确的是:<br>1.在随机森林的单个树中, 树和树之间是有依赖的, 而GradientBoosting Trees中的单个树之间是没有依赖的.<br>2.这两个模型都使用随机特征子集, 来生成许多单个的树.<br>3.我们可以并行地生成GradientBoosting Trees单个树, 因为它们之间是没有依赖的, GradientBoosting Trees训练模型的表现总是比随机森林好<br>A 2<br>B 1 and 2<br>C 1 and 3<br>D 2 and 3</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：1.随机森林是基于bagging的, 而Gradient Boosting trees是基于boosting的, 所有说反了,在随机森林的单个树中, 树和树之间是没有依赖的, 而GradientBoosting Trees中的单个树之间是有依赖关系.<br>2.这两个模型都使用随机特征子集, 来生成许多单个的树.<br>所以A是正确的</p>
<p>3、bootstrap数据是什么意思？（提示：考“bootstrap”和“boosting”区别）<br>A 有放回地从总共M个特征中抽样m个特征<br>B 无放回地从总共M个特征中抽样m个特征<br>C 有放回地从总共N个样本中抽样n个样本<br>D 无放回地从总共N个样本中抽样n个样本</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：<br>boostrap是提鞋自举的意思(武侠小说作者所说的左脚踩右脚腾空而起). 它的过程是对样本(而不是特征)进行有放回的抽样, 抽样次数等同于样本总数. 这个随机抽样过程决定了最终抽样出来的样本, 去除重复之后, 占据原有样本的1/e比例. </p>
<p>5、数据科学家可能会同时使用多个算法（模型）进行预测， 并且最后把这些算法的结果集成起来进行最后的预测（集成学习），以下对集成学习说法正确的是<br>A 单个模型之间有高相关性<br>B 单个模型之间有低相关性<br>C 在集成学习中使用“平均权重”而不是“投票”会比较好<br>D 单个模型都是用的一个算法</p>
<p><strong>答案</strong>：B</p>
<p>2、以下说法中错误的是（）<br>A SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性<br>B 在adaboost算法中，所有被分错样本的权重更新比例不相同<br>C boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重<br>D 给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：<br>A 软间隔分类器对噪声是有鲁棒性的。<br>B 请参考<a href="http://blog.csdn.net/v_july_v/article/details/40718799">http://blog.csdn.net/v_july_v/article/details/40718799</a><br>C boosting是根据分类器正确率确定权重，bagging不是。<br>D 训练集变大会提高模型鲁棒性。</p>
<p>8、下面对集成学习模型中的弱学习者描述错误的是？<br>A 他们经常不会过拟合<br>B 他们通常带有高偏差，所以其并不能解决复杂学习问题<br>C 他们通常会过拟合</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：弱学习者是问题的特定部分。所以他们通常不会过拟合，这也就意味着弱学习者通常拥有低方差和高偏差。</p>
<p><font color="red">8、下面关于ID3算法中说法错误的是（）<br>A ID3算法要求特征必须离散化<br>B 信息增益可以用熵，而不是GINI系数来计算<br>C 选取信息增益最大的特征，作为树的根节点<br>D ID3算法是一个二叉树模型</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：<br>ID3算法（IterativeDichotomiser3迭代二叉树3代）是一个由RossQuinlan发明的用于决策树的算法。可以归纳为以下几点：<br>使用所有没有使用的属性并计算与之相关的样本熵值<br>选取其中熵值最小的属性<br>生成包含该属性的节点</p>
<p>D3算法对数据的要求：<br>1)所有属性必须为离散量；<br>2)所有的训练例的所有属性必须有一个明确的值；<br>3)相同的因素必须得到相同的结论且训练例必须唯一。</p>
<hr>
<h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p>1、一个二进制源X发出符号集为{-1,1}，经过离散无记忆信道传输，由于信道中噪音的存在，接收端Y收到符号集为{-1,1,0}。已知P(x=-1)=1/4，P(x=1)=3/4，P(y=-1|x=-1)=4/5，P(y=0|x=-1)=1/5，P(y=1|x=1)=3/4，P(y=0|x=1)=1/4，求条件熵H(Y|X)（ ）<br>A 0.2375<br>B 0.3275<br>C 0.5273<br>D 0.5372</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：<img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/14.png" alt=""><br>信息熵公式：H(X)= -∑P(x)log(x)<br>条件熵公式：H(Y|X)= -∑P(y,x)logP(y|x)= -∑P(y|x)P(x)logP(y|x)，<br>将(y=-1,x=-1), (y=0,x=-1), (y=1,x=1), (y=0,x=1)四种情况带入公式求和，<br>得 H(Y|X)≈-(-0.01938-0.03495-0.07028-0.11289)=0.2375。</p>
<p>9、目标变量在训练集上的 8 个实际值 [0,0,0,1,1,1,1,1]，目标变量的熵是多少？<br>A -(5/8 log(5/8) + 3/8 log(3/8))<br>B 5/8 log(5/8) + 3/8 log(3/8)<br>C 3/8 log(5/8) + 5/8 log(3/8)<br>D 5/8 log(3/8) – 3/8 log(5/8)</p>
<p><strong>答案</strong>：A</p>
<hr>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p><font color="red">1、下列哪个不属于常用的文本分类的特征选择算法？<br>A 卡方检验值<br>B 互信息<br>C 信息增益<br>D 主成分分析</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：主成分分析是特征转换算法（特征抽取），而不是特征选择<br>常采用特征选择方法。常见的六种特征选择方法：<br>1）DF(Document Frequency) 文档频率<br>DF:统计特征词出现的文档数量，用来衡量某个特征词的重要性<br>2）MI(Mutual Information) 互信息法<br>互信息法用于衡量特征词与文档类别直接的信息量。<br>如果某个特征词的频率很低，那么互信息得分就会很大，因此互信息法倾向”低频”的特征词。<br>相对的词频很高的词，得分就会变低，如果这词携带了很高的信息量，互信息法就会变得低效。<br>3）(Information Gain) 信息增益法<br>通过某个特征词的缺失与存在的两种情况下，语料中前后信息的增加，衡量某个特征词的重要性。<br>4）CHI(Chi-square) 卡方检验法<br>利用了统计学中的”假设检验”的基本思想：首先假设特征词与类别直接是不相关的<br>如果利用CHI分布计算出的检验值偏离阈值越大，那么更有信心否定原假设，接受原假设的备则假设：特征词与类别有着很高的关联度。<br>5）WLLR(Weighted Log Likelihood Ration)加权对数似然<br>6）WFO（Weighted Frequency and Odds）加权频率和可能性<br>本题解析来源：<a href="http://blog.csdn.net/ztf312/article/details/50890099">http://blog.csdn.net/ztf312/article/details/50890099</a></p>
<p>1、机器学习中做特征选择时，可能用到的方法有？<br>A 卡方<br>B 信息增益<br>C 平均互信息<br>D 期望交叉熵<br>E 以上都有</p>
<p><strong>答案</strong>：E</p>
<hr>
<h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><p>2、为了得到和 SVD 一样的投射（projection），你需要在 PCA 中怎样做？<br>A 将数据转换成零均值<br>B 将数据转换成零中位数<br>C 无法做到</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：当数据有一个 0 均值向量时，PCA 有与 SVD 一样的投射，否则在使用 SVD 之前，你必须将数据均值归 0。</p>
<p>2、下列方法中，不可以用于特征降维的方法包括<br>A 主成分分析PCA<br>B 线性判别分析LDA<br>C 深度学习SparseAutoEncoder<br>D 矩阵奇异值分解SVD</p>
<p><strong>答案</strong>：C</p>
<p><strong>解析</strong>：特征降维方法主要有：<br>PCA，LLE，Isomap<br>SVD和PCA类似，也可以看成一种降维方法<br>LDA:线性判别分析，可用于降维<br>AutoEncoder：AutoEncoder的结构与神经网络的隐含层相同，由输入L1,输出  L2组成，中间则是权重连接。Autoencoder通过L2得到输入的重构L3，最小化L3与L1的差别  进行训练得到权重。在这样的权重参数下，得到的L2可以尽可能的保存L1的信息。<br>Autoencoder的输出L2的维度由输出的神经元个数决定。当输出维度大于L1时，则需要在训练目标函数中加入sparse  惩罚项，避免L2直接复制L1（权重全为1）。所以称为sparseAutoencoder( Andrew Ng提出的)。<br>结论：SparseAutoencoder大多数情况下都是升维的，所以称之为特征降维的方法不准确。<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/2.png" alt=""></p>
<p>4、下列哪些不特别适合用来对高维数据进行降维<br>A LASSO<br>B 主成分分析法<br>C 聚类分析<br>D 小波分析法<br>E 线性判别法<br>F 拉普拉斯特征映射</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：<br>lasso通过参数缩减达到降维的目的；<br>pca就不用说了<br>线性鉴别法即LDA通过找到一个空间使得类内距离最小类间距离最大所以可以看做是降维；<br>小波分析有一些变换的操作降低其他干扰可以看做是降维<br>拉普拉斯请看这个<a href="http://f.dataguru.cn/thread-287243-1-1.html">http://f.dataguru.cn/thread-287243-1-1.html</a></p>
<p>9、我们建立一个5000个特征, 100万数据的机器学习模型. 我们怎么有效地应对这样的大数据训练 :<br>A 我们随机抽取一些样本, 在这些少量样本之上训练<br>B 我们可以试用在线机器学习算法<br>C 我们应用PCA算法降维, 减少特征数<br>D B 和 C<br>E A 和 B<br>F 以上所有</p>
<p><strong>答案</strong>：F<br><strong>解析</strong>：样本数过多, 或者特征数过多, 而不能单机完成训练, 可以用小批量样本训练, 或者在线累计式训练, 或者主成分PCA降维方式减少特征数量再进行训练. </p>
<p>10、我们想要减少数据集中的特征数, 即降维. 选择以下适合的方案 :</p>
<ol>
<li>使用前向特征选择方法</li>
<li>使用后向特征排除方法</li>
<li>我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后我们去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表现比原来还要好, 我们可以去除这个特征.</li>
<li>查看相关性表, 去除相关性最高的一些特征<br>A 1 和 2<br>B 2, 3和4<br>C 1, 2和4<br>D All</li>
</ol>
<p><strong>答案</strong>：D<br><strong>解析</strong>：<br>1.前向特征选择方法和后向特征排除方法是我们特征选择的常用方法<br>2.如果前向特征选择方法和后向特征排除方法在大数据上不适用, 可以用这里第三种方法.<br>3.用相关性的度量去删除多余特征, 也是一个好方法<br>所有D是正确的</p>
<p>2、对于PCA(主成分分析)转化过的特征 ,  朴素贝叶斯的”不依赖假设”总是成立, 因为所有主要成分是正交的, 这个说法是 :<br>A 正确的<br>B 错误的</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：这个说法是错误的, 首先, “不依赖”和”不相关”是两回事, 其次, 转化过的特征, 也可能是相关的</p>
<p>3、对于PCA说法正确的是 :</p>
<ol>
<li>我们必须在使用PCA前规范化数据</li>
<li>我们应该选择使得模型有最大variance的主成分</li>
<li>我们应该选择使得模型有最小variance的主成分</li>
<li>我们可以使用PCA在低维度上做数据可视化<br>A 1, 2 and 4<br>B 2 and 4<br>C 3 and 4<br>D 1 and 3<br>E 1, 3 and 4</li>
</ol>
<p><strong>答案</strong>：A<br><strong>解析</strong>：<br>1）PCA对数据尺度很敏感, 打个比方, 如果单位是从km变为cm, 这样的数据尺度对PCA最后的结果可能很有影响(从不怎么重要的成分变为很重要的成分).<br>2）我们总是应该选择使得模型有最大variance的主成分<br>3）有时在低维度上左图是需要PCA的降维帮助的</p>
<p>4、对于下图, 最好的主成分选择是多少 ?<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/9.png" alt=""><br>A 7<br>B 30<br>C 35<br>D Can’t Say</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：主成分选择使variance越大越好， 在这个前提下， 主成分越少越好。</p>
<p>10、最出名的降维算法是 PCA 和 t-SNE。将这两个算法分别应用到数据「X」上，并得到数据集「X_projected_PCA」，「X_projected_tSNE」。下面哪一项对「X_projected_PCA」和「X_projected_tSNE」的描述是正确的？<br>A X_projected_PCA 在最近邻空间能得到解释<br>B X_projected_tSNE 在最近邻空间能得到解释<br>C 两个都在最近邻空间能得到解释<br>D 两个都不能在最近邻空间得到解释</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：t-SNE 算法考虑最近邻点而减少数据维度。所以在使用 t-SNE 之后，所降的维可以在最近邻空间得到解释。但 PCA 不能。</p>
<hr>
<h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><p>3、使用k=1的knn算法, 下图二类分类问题, “+” 和 “o” 分别代表两个类, 那么, 用仅拿出一个测试样本的交叉验证方法, 交叉验证的错误率是多少：<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/20180514205546.png" alt=""><br>A 0%<br>B 100%<br>C 0%到100<br>D 以上都不是</p>
<p><strong>答案</strong>： B<br><strong>解析</strong>：knn算法就是, 在样本周围看k个样本, 其中大多数样本的分类是A类, 我们就把这个样本分成A类. 显然, k=1 的knn在上图不是一个好选择, 分类的错误率始终是100%。</p>
<p>9、以下哪个图是KNN算法的训练边界<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/10.png" alt=""><br>A B<br>B A<br>C D<br>D C<br>E 都不是</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：<strong>KNN算法肯定不是线性的边界， 所以直的边界就不用考虑了。另外这个算法是看周围最近的k个样本的分类用以确定分类，所以边界一定是坑坑洼洼的。</strong></p>
<p><font color="red">10、一般，k-NN最近邻方法在（）的情况下效果较好<br>A 样本较多但典型性不好<br>B 样本较少但典型性好<br>C 样本呈团状分布<br>D 样本呈链状分布</p>
<p><strong>答案</strong>： B（也有选A的），答案待定<br><strong>解析</strong>：<br>K近邻算法主要依靠的是周围的点，因此如果样本过多，那肯定是区分不出来的。因此应当选择B<br>样本呈团状颇有迷惑性，这里应该指的是整个样本都是呈团状分布，这样kNN就发挥不出其求近邻的优势了，整体样本应该具有典型性好，样本较少，比较适宜。</p>
<hr>
<h3 id="分类方法"><a href="#分类方法" class="headerlink" title="分类方法"></a>分类方法</h3><p>1、以下哪些方法不可以直接来对文本分类？<br>A Kmeans<br>B 决策树<br>C 支持向量机<br>D KNN</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：Kmeans是聚类方法，典型的无监督学习方法。分类是监督学习方法，BCD都是常见的分类方法。</p>
<p><font color="red">2、以下( )不属于线性分类器最佳准则？<br>A 感知准则函数<br>B 贝叶斯分类<br>C 支持向量机<br>D Fisher准则</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。<br>感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。<br>支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）<br>Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。<br>根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。</p>
<p>1、下面有关分类算法的准确率，召回率，F1 值的描述，错误的是？<br>A 准确率是检索出相关文档数与检索出的文档总数的比率，衡量的是检索系统的查准率<br>B 召回率是指检索出的相关文档数和文档库中所有的相关文档数的比率，衡量的是检索系统的查全率<br>C 正确率、召回率和 F 值取值都在0和1之间，数值越接近0，查准率或查全率就越高<br>D 为了解决准确率和召回率冲突问题，引入了F1分数</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：对于二类分类问题常用的评价指标是精准度（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：<br>TP——将正类预测为正类数<br>FN——将正类预测为负类数<br>FP——将负类预测为正类数<br>TN——将负类预测为负类数<br>由此：<br>精准率定义为：P = TP / (TP + FP)<br>召回率定义为：R = TP / (TP + FN)<br>F1值定义为： F1 = 2 P R / (P + R)<br>精准率和召回率和F1取值都在0和1之间，精准率和召回率高，F1值也会高，不存在数值越接近0越高的说法，应该是数值越接近1越高。</p>
<p><font color="red">3、假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值,  那么现在关于模型说法, 正确的是 :<br>1 模型分类的召回率会降低或不变<br>2 模型分类的召回率会升高<br>3 模型分类准确率会升高或不变<br>4 模型分类准确率会降低<br>A 1<br>B 2<br>C 1和3<br>D 2和4<br>E 以上都不是</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：<br>精确率, 准确率和召回率是广泛用于信息检索和统计学分类领域的度量值，用来评价结果的质量。下图可以帮助理解和记忆它们之间的关系, 其中精确率(precision)和准确率(accuracy)都是关于预测效果的描述. 召回率是关于预测样本的描述。<br>精确率表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP), 也就是P = TP / (TP + FP)。</p>
<p>准确率表示的是预测的正负样本有多少是真实的正和负, 预测正确的数量占全部预测数量的比例, 也就是A = (TP + TN) / (TP + FP + TN + FN) = (TP + TN) / 全部样本。</p>
<p>召回率表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN), 也就是R = TP / (TP + FN)。<br>精确率和召回率二者计算方法其实就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。<br>提高分界阈值大于0.5, 则预测为正的样本数要降低, 相当于把图中圆圈变小, 按下图则可计算<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/7.png" alt=""><br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/8.png" alt=""></p>
<p>召回率的分子变小分母不变, 所以召回率会变小或不变;<br>精确率的分子分母同步变化, 所以精确率的变化不能确定;<br>准确率的分子为圆内绿色加圆外右侧矩形面积所围样本, 两者之和变化不能确定; 分母为矩形所含全部样本不变化, 所以准确率的变化不能确定;<br>综上, 所以选A。</p>
<hr>
<h3 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h3><p><strong>答案</strong>：D<br><strong>解析</strong>：所有都可以用来调试以找到全局最小。</p>
<p><font color="red">1、模式识别中，不属于马式距离（协方差距离）较之于欧式距离的优点的是（ ）<br>A 平移不变性<br>B 尺度不变性<br>C 考虑了模式的分布</p>
<p><strong>答案</strong>：A<br>还有一种距离叫做曼哈顿距离</p>
<p>7、以下属于欧式距离特性的有（）<br>A 旋转不变性<br>B 尺度缩放不变性<br>C 不受量纲影响的特性</p>
<p><strong>答案</strong>：A</p>
<p><font color="red">2、以下不属于影响聚类算法结果的主要因素有（）<br>A 已知类别的样本质量<br>B 分类准则<br>C 特征选取<br>D 模式相似性测度</p>
<p><strong>答案</strong>：A</p>
<p>4、在 k-均值算法中，以下哪个选项可用于获得全局最小？<br>A 尝试为不同的质心（centroid）初始化运行算法<br>B 调整迭代的次数<br>C 找到集群的最佳数量<br>D 以上所有</p>
<p><font color="red">3、影响基本K-均值算法的主要因素有（）<br>A 样本输入顺序<br>B 模式相似性测度<br>C 聚类准则</p>
<p><strong>答案</strong>：B</p>
<p><font color="red">5、如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有（ ）<br>A 已知类别样本质量<br>B 分类准则<br>C 量纲</p>
<p><strong>答案</strong>： B</p>
<p>5、以下对k-means聚类算法解释正确的是<br>A 能自动识别类的个数,随即挑选初始点为中心点计算<br>B 能自动识别类的个数,不是随即挑选初始点为中心点计算<br>C 不能自动识别类的个数,随即挑选初始点为中心点计算<br>D 不能自动识别类的个数,不是随即挑选初始点为中心点计算</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：<br>（1）适当选择c个类的初始中心；<br>（2）在第k次迭代中，对任意一个样本，求其到c个中心的距离，将该样本归到距离最短的中心所在的类；<br>（3）利用均值等方法更新该类的中心值；<br>（4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。<br>以上是KMeans（C均值）算法的具体步骤，可以看出需要选择类别数量，但初次选择是随机的，最终的聚类中心是不断迭代稳定以后的聚类中心。所以答案选C。</p>
<hr>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p><font color="red">4、在统计模式分类问题中，当先验概率未知时，可以使用（）<br>A 最小损失准则<br>B 最小最大损失准则<br>C 最小误判概率准则</p>
<p><strong>答案</strong>：B</p>
<p><font color="red">2、以下几种模型方法属于判别式模型(Discriminative Model)的有( )<br>1)混合高斯模型<br>2)条件随机场模型<br>3)区分度训练<br>4)隐马尔科夫模型<br>A 2,3<br>B 3,4<br>C 1,4<br>D 1,2</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：<br>常见的判别式模型有：<br>Logistic regression（logistical 回归）<br>Linear discriminant analysis（线性判别分析）<br>Supportvector machines（支持向量机）<br>Boosting（集成学习）<br><strong>Conditional random fields（条件随机场）</strong><br>Linear regression（线性回归）<br>Neural networks（神经网络）</p>
<p>常见的生成式模型有:<br><strong>Gaussian mixture model and othertypes of mixture model（高斯混合及其他类型混合模型）</strong><br>Hidden Markov model（隐马尔可夫）<br>NaiveBayes（朴素贝叶斯）<br>AODE（平均单依赖估计）<br>Latent Dirichlet allocation（LDA主题模型）<br><strong>Restricted Boltzmann Machine（限制波兹曼机）</strong><br>生成式模型是根据概率乘出结果，而判别式模型是给出输入，计算出结果。</p>
<p>10、在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题？<br>A 增加训练集量<br>B 减少神经网络隐藏层节点数<br>C 删除稀疏的特征<br>D SVM算法中使用高斯核/RBF核代替线性核</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：<br>一般情况下，越复杂的系统，过拟合的可能性就越高，一般模型相对简单的话泛化能力会更好一点。<br>B.一般认为，增加隐层数可以降低网络误差（也有文献认为不一定能有效降低），提高精度，但也使网络复杂化，从而增加了网络的训练时间和出现“过拟合”的倾向， svm高斯核函数比线性核函数模型更复杂，容易过拟合<br>D.径向基(RBF)核函数/高斯核函数的说明,这个核函数可以将原始空间映射到无穷维空间。对于参数 ，如果选的很大，高次特征上的权重实际上衰减得非常快，实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调整参数 ，高斯核实际上具有相当高的灵活性，也是 使用最广泛的核函数 之一。</p>
<p>4、“过拟合”只在监督学习中出现，在非监督学习中，没有“过拟合”，这是（）<br>A 对的<br>B 错的</p>
<p><strong>答案</strong>： B<br><strong>解析</strong>：<br>我们可以评估无监督学习方法通过无监督学习的指标，如：我们可以评估聚类模型通过调整兰德系数（adjusted rand score）<br><a href="https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index">https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index</a></p>
<p>8、下面哪个/些超参数的增加可能会造成随机森林数据过拟合？<br>A 树的数量<br>B 树的深度<br>C 学习速率</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：通常情况下，我们增加树的深度有可能会造成模型过拟合。学习速率并不是随机森林的超参数。增加树的数量可能会造成欠拟合。</p>
<p><font color="red">5、假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。<br>A 如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它<br>B 对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大<br>C log-loss 越低，模型越好<br>D 以上都是</p>
<p><strong>答案</strong>：D</p>
<p>6、下面哪个选项中哪一项属于确定性算法？<br>A PCA<br>B K-Means<br>C 以上都不是</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：确定性算法表明在不同运行中，算法输出并不会改变。如果我们再一次运行算法，PCA 会得出相同的结果，而 k-means 不会。<br>PCA没有需要调试的参数，Kmeans之所以每次的结果不同，因为初始化的点是随机的</p>
<p>7、两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。这句描述是正确还是错误？<br>A 正确<br>B 错误</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：<strong>Pearson相关系数只能衡量线性相关性，但无法衡量非线性关系。</strong>如y=x^2，x和y有很强的非线性关系。</p>
<p>5、下列属于无监督学习的是<br>A k-means<br>B SVM<br>C 最大熵<br>D CRF</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：A是聚类，属于无监督学习。BC是分类，属于监督学习。至于D是序列化标注，也是有监督学习。</p>
<p>1、在一个n维的空间中， 最好的检测outlier(离群点)的方法是（）<br>A 作正态分布概率图<br>B 作盒形图<br>C 马氏距离<br>D 作散点图<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/5.png" alt=""></p>
<p>5、对于k折交叉验证, 以下对k的说法正确的是（）<br>A k越大, 不一定越好, 选择大的k会加大评估时间<br>B 选择更大的k, 就会有更小的bias偏差(因为训练集更加接近总数据集)<br>C 在选择k时, 要最小化数据集之间的方差<br>D 以上所有</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：k越大, bias越小, 训练时间越长. 在训练时, 也要考虑数据集间方差差别不大的原则. 比如, 对于二类分类问题, 使用2-折交叉验证, 如果测试集里的数据都是A类的, 而训练集中数据都是B类的, 显然, 测试效果会很差.<br>高方差，表名数据扰动所造成的影响大，没有排除噪音影响，过拟合<br>高偏差，说明学习能力弱，不能很好的分类或者回归，欠拟合</p>
<p>7、模型的高bias是什么意思, 我们如何降低它 ？<br>A 在特征空间中减少特征<br>B 在特征空间中增加特征<br>C 增加数据点<br>D B和C<br>E 以上所有</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：bias太高说明模型太简单了, 数据维数不够, 无法准确预测数据, 所以, 升维吧 !</p>
<p>2、“点击率问题”是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是<br>A 模型预测准确率已经很高了, 我们不需要做什么了<br>B 模型预测准确率不高, 我们需要做点什么改进模型<br>C 无法下结论<br>D 以上都不对</p>
<p><strong>答案</strong>： B<br><strong>解析</strong>：<br>99%的预测准确率可能说明, 你预测的没有点进去的人很准确 (因为有99%的人是不会点进去的, 这很好预测). 不能说明你的模型对点进去的人预测准确, 所以, 对于这样的非平衡数据集, 我们要把注意力放在小部分的数据上, 即那些点击进去的人.<br>详细可以参考这篇文章：<a href="https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/">https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/</a></p>
<p>6、（假设precision=TP/(TP+FP),recall=TP/(TP+FN)。）在二分类问题中，当测试集的正例和负例数量不均衡时，以下评价方案哪个是相对不合理的（）<br>A Accuracy:(TP+TN)/all<br>B F-value:2<em>recall</em>precision/(recall+precision)<br>C G-mean:sqrt(precision*recall)<br>D AUC:曲线下面积</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：题目提到测试集正例和负例数量不均衡，那么假设正例数量很少占10%，负例数量占大部分90%。<br>而且算法能正确识别所有负例，但正例只有一半能正确判别。<br>那么TP=0.05×all,TN=0.9×all，Accuracy=95%。<br>虽然Accuracy很高，precision是100%,但正例recall只有50%</p>
<p>7、以下哪些算法, 可以用神经网络去构造: </p>
<ol>
<li>KNN</li>
<li>线性回归</li>
<li>对数几率回归<br>A 1和 2<br>B 2 和 3<br>C 1, 2 和 3<br>D 以上都不是</li>
</ol>
<p><strong>答案</strong>： B<br><strong>解析</strong>：</p>
<ol>
<li>KNN算法不需要训练参数, 而所有神经网络都需要训练参数, 因此神经网络帮不上忙</li>
<li>最简单的神经网络, 感知器, 其实就是线性回归的训练</li>
<li>我们可以用一层的神经网络构造对数几率回归</li>
</ol>
<p>6、在有监督学习中， 我们如何使用聚类方法？ </p>
<ol>
<li>我们可以先创建聚类类别， 然后在每个类别上用监督学习分别进行学习</li>
<li>我们可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习</li>
<li>在进行监督学习之前， 我们不能新建聚类类别</li>
<li>我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习<br>A 2 和 4<br>B 1 和 2<br>C 3 和 4<br>D 1 和 3</li>
</ol>
<p><strong>答案</strong>：B<br><strong>解析</strong>：我们可以为每个聚类构建不同的模型， 提高预测准确率。<br>“类别id”作为一个特征项去训练， 可以有效地总结了数据特征。</p>
<p>7、以下说法正确的是 </p>
<ol>
<li>一个机器学习模型，如果有较高准确率，总是说明这个分类器是好的</li>
<li>如果增加模型复杂度， 那么模型的测试错误率总是会降低</li>
<li>如果增加模型复杂度， 那么模型的训练错误率总是会降低</li>
<li>我们不可以使用聚类“类别id”作为一个新的特征项， 然后再用监督学习分别进行学习<br>A 1<br>B 2<br>C 3<br>D 1 and 3</li>
</ol>
<p><strong>答案</strong>：C<br><strong>解析</strong>：<br>1的模型中, 如果负样本占比非常大,也会有很高的准确率, 对正样本的分类不一定很好;<br>2,3的模型中, 增加复杂度则对训练集, 而不是测试集, 有过拟合, 所以训练错误率总会降低;<br>4的模型中, “类别id”可以作为一个特征项去训练, 这样会有效地总结了数据特征。</p>
<p><font color="red">8、对应GradientBoosting tree算法， 以下说法正确的是:</p>
<ol>
<li>当增加最小样本分裂个数，我们可以抵制过拟合</li>
<li>当增加最小样本分裂个数，会导致过拟合</li>
<li>当我们减少训练单个学习器的样本个数，我们可以降低variance</li>
<li>当我们减少训练单个学习器的样本个数，我们可以降低bias<br>A 2 和 4<br>B 2 和 3<br>C 1 和 3<br>D 1 和 4</li>
</ol>
<p><strong>答案</strong>：C<br><strong>解析</strong>：最小样本分裂个数是用来控制“过拟合”参数。太高的值会导致“欠拟合”，这个参数应该用交叉验证来调节。<br>第二点是靠bias和variance概念的。</p>
<p>10、如果一个训练好的模型在测试集上有100%的准确率， 这是不是意味着在一个新的数据集上，也会有同样好的表现？<br>A 是的，这说明这个模型的范化能力已经足以支持新的数据集合了<br>B 不对，依然后其他因素模型没有考虑到，比如噪音数据</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：没有一个模型是可以总是适应新数据的。我们不可能可到100%准确率。</p>
<p>2、变量选择是用来选择最好的判别器子集， 如果要考虑模型效率，我们应该做哪些变量选择的考虑？</p>
<ol>
<li>多个变量其实有相同的用处</li>
<li>变量对于模型的解释有多大作用</li>
<li>特征携带的信息</li>
<li>交叉验证<br>A 1 和 4<br>B 1, 2 和 3<br>C 1,3 和 4<br>D 以上所有</li>
</ol>
<p><strong>答案</strong>：C<br><strong>解析</strong>：注意， 这题的题眼是考虑模型效率，所以不要考虑选项2.</p>
<p>4、对于下面三个模型的训练情况， 下面说法正确的是:<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/11.png" alt=""></p>
<ol>
<li>第一张图的训练错误与其余两张图相比，是最大的</li>
<li>最后一张图的训练效果最好，因为训练错误最小</li>
<li>第二张图比第一和第三张图鲁棒性更强，是三个里面表现最好的模型</li>
<li>第三张图相对前两张图过拟合了</li>
<li>三个图表现一样，因为我们还没有测试数据集<br>A 1 和 3<br>B 1 和 3<br>C 1, 3 和 4<br>D 5</li>
</ol>
<p><strong>答案</strong>：C<br><strong>解析</strong>：最后一张过拟合, 训练错误最小, 第一张相反, 训练错误就是最大了. 所以1是对的;<br>仅仅训练错误最小往往说明过拟合, 所以2错, 4对;<br>第二张图平衡了拟合和过拟合, 所以3对;</p>
<p>9、下面哪个/些选项对 K 折交叉验证的描述是正确的？<br>1.增大 K 将导致交叉验证结果时需要更多的时间<br>2.更大的 K 值相比于小 K 值将对交叉验证结构有更高的信心<br>3.如果 K=N，那么其称为留一交叉验证，其中 N 为验证集中的样本数量<br>A 1 和 2<br>B 2 和 3<br>C 1 和 3<br>D 1、2 和 3</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：大 K 值意味着对过高估计真实预期误差（训练的折数将更接近于整个验证集样本数）拥有更小的偏差和更多的运行时间（并随着越来越接近极限情况：留一交叉验证）。我们同样在选择 K 值时需要考虑 K 折准确度和方差间的均衡。</p>
<p>1、给定三个变量 X，Y，Z。(X, Y)、(Y, Z) 和 (X, Z) 的 Pearson 相关性系数分别为 C1、C2 和 C3。现在 X 的所有值加 2（即 X+2），Y 的全部值减 2（即 Y-2），Z 保持不变。那么运算之后的 (X, Y)、(Y, Z) 和 (X, Z) 相关性系数分别为 D1、D2 和 D3。现在试问 D1、D2、D3 和 C1、C2、C3 之间的关系是什么？<br>A D1= C1, D2 &lt; C2, D3 &gt; C3<br>B D1 = C1, D2 &gt; C2, D3 &gt; C3<br>C D1 = C1, D2 &gt; C2, D3 &lt; C3<br>D D1 = C1, D2 &lt; C2, D3 &lt; C3<br>E D1 = C1, D2 = C2, D3 = C3</p>
<p><strong>答案</strong>：E<br><strong>解析</strong>：<strong>特征之间的相关性系数不会因为特征加或减去一个数而改变。</strong></p>
<p>3、假设我们有一个数据集，在一个深度为 6 的决策树的帮助下，它可以使用 100% 的精确度被训练。现在考虑一下两点，并基于这两点选择正确的选项。<br>注意：所有其他超参数是相同的，所有其他因子不受影响。<br>1.深度为 4 时将有高偏差和低方差<br>2.深度为 4 时将有低偏差和低方差<br>A 只有 1<br>B 只有 2<br>C 1 和 2<br>D 没有一个</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：如果在这样的数据中你拟合深度为 4 的决策树，这意味着其更有可能与数据欠拟合。因此，在欠拟合的情况下，你将获得高偏差和低方差。</p>
<p>4、在以下不同的场景中,使用的分析方法不正确的有<br>A 根据商家最近一年的经营及服务数据,用聚类算法判断出天猫商家在各自主营类目下所属的商家层级<br>B 根据商家近几年的成交数据,用聚类算法拟合出用户未来一个月可能的消费金额公式<br>C 用关联规则算法分析出购买了汽车坐垫的买家,是否适合推荐汽车脚垫<br>D 根据用户最近购买的商品信息,用决策树算法识别出淘宝买家可能是男还是女</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：预测消费更合适的算法是用回归模型来做。而不是聚类算法。</p>
<p><font color="red">10、在大规模的语料中，挖掘词的相关性是一个重要的问题。以下哪一个信息不能用于确定两个词的相关性。<br>A 互信息<br>B 最大熵<br>C 卡方检验<br>D 最大似然比</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：最大熵代表了整体分布的信息，通常具有最大熵的分布作为该随机变量的分布，不能体现两个词的相关性，但是卡方是检验两类事务发生的相关性。所以选B【正解】</p>
<p><font color="red">1、基于统计的分词方法为（）<br>A 正向最大匹配法<br>B 逆向最大匹配法<br>C 最少切分<br>D 条件随机场</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：第一类是基于语法和规则的分词法。其基本思想就是在分词的同时进行句法、语义分析,利用句法信息和语义信息来进行词性标注,以解决分词歧义现象。因为现有的语法知识、句法规则十分笼统、复杂,基于语法和规则的分词法所能达到的精确度远远还不能令人满意,目前这种分词系统还处在试验阶段。</p>
<p>第二类是机械式分词法（即基于词典）。机械分词的原理是将文档中的字符串与词典中的词条进行逐一匹配,如果词典中找到某个字符串,则匹配成功,可以切分,否则不予切分。基于词典的机械分词法,实现简单,实用性强,但机械分词法的最大的缺点就是词典的完备性不能得到保证。据统计,用一个含有70000个词的词典去切分含有15000个词的语料库,仍然有30%以上的词条没有被分出来,也就是说有4500个词没有在词典中登录。</p>
<p>第三类是基于统计的方法。基于统计的分词法的基本原理是根据字符串在语料库中出现的统计频率来决定其是否构成词。词是字的组合,相邻的字同时出现的次数越多,就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映它们成为词的可信度。</p>
<p>2、在下面的图像中，哪一个是多元共线（multi-collinear）特征？<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/12.png" alt=""><br>A 图 1 中的特征<br>B 图 2 中的特征<br>C 图 3 中的特征<br>D 图 1、2 中的特征<br>E 图 2、3 中的特征<br>F 图 1、3 中的特征</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：在图 1 中，特征之间有高度正相关，图 2 中特征有高度负相关。所以这两个图的特征是多元共线特征。</p>
<hr>
<h3 id="陌生的知识点"><a href="#陌生的知识点" class="headerlink" title="陌生的知识点"></a>陌生的知识点</h3><p>6、关于 ARMA 、 AR 、 MA 模型的功率谱，下列说法正确的是（ ）<br>A MA模型是同一个全通滤波器产生的<br>B MA模型在极点接近单位圆时，MA谱是一个深谷<br>C AR模型在零点接近单位圆时，AR谱是一个尖峰<br>D RMA谱既有尖峰又有深谷</p>
<p><strong>答案</strong>：D</p>
<p>4、下列哪个不属于CRF(conditional random field algorithm条件随机场算法)模型对于HMM和MEMM模型的优势<br>A 特征灵活<br>B 速度快<br>C 可容纳较多上下文信息<br>D 全局最优</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：HMM模型是对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。</p>
<p>CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。</p>
<p>9、已知一组数据的协方差矩阵P,下面关于主分量说法错误的是（）<br>A 主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小<br>B 在经主分量分解后,协方差矩阵成为对角矩阵<br>C 主分量分析就是K-L变换<br>D 主分量是通过求协方差矩阵的特征值得到</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。</p>
<p>8、位势函数法的积累势函数K(x)的作用相当于Bayes判决中的()<br>A 后验概率<br>B 先验概率<br>C 类概率密度<br>D 类概率密度与先验概率的和</p>
<p><strong>答案</strong>：A<br><strong>解析</strong>：<br>具体的，势函数详解请看——《势函数法》。</p>
<p>7、以下哪个是常见的时间序列算法模型<br>A RSI<br>B MACD<br>C ARMA<br>D KDJ</p>
<p><strong>答案</strong>：C<br><strong>解析</strong>：自回归滑动平均模型(ARMA)<br>其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。</p>
<p>其他三项都不是一个层次的。<br>A.相对强弱指数 (RSI, Relative Strength Index) 是通过比较一段时期内的平均收盘涨数和平均收盘跌数来分析市场买沽盘的意向和实力 , 从而作出未来市场的走势 .<br>B.移动平均聚散指标 (MACD, Moving Average Convergence Divergence), 是根据均线的构造原理 , 对股票价格的收盘价进行平滑处理 , 求出算术平均值以后再进行计算 , 是一种趋向类指标 .<br>D. 随机指标 (KDJ) 一般是根据统计学的原理 , 通过一个特定的周期 ( 常为 9 日 ,9 周等 ) 内出现过的最高价 , 最低价及最后一个计算周期的收盘价及这三者之间的比例关系 , 来计算最后一个计算周期的未成熟随机值 RSV, 然后根据平滑移动平均线的方法来计算 K 值 , D 值与 J 值 , 并绘成曲线图来研判股票走势 .</p>
<p>1、下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测。<br>A AR模型<br>B MA模型<br>C ARMA模型<br>D GARCH模型</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：<br>AR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点），所以其本质类似于插值。<br>MA模型(moving average model)滑动平均模型，其中使用趋势移动平均法建立直线趋势的预测模型。<br>ARMA模型(auto regressive moving average model)自回归滑动平均模型，模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐。<br>GARCH模型称为广义ARCH模型，是ARCH模型的拓展，由Bollerslev(1986)发展起来的。它是ARCH模型的推广。GARCH(p,0)模型，相当于ARCH(p)模型。GARCH模型是一个专门针对金融数据所量体订做的回归模型，除去和普通回归模型相同的之处，GARCH对误差的方差进行了进一步的建模。特别适用于波动性的分析和预测，这样的分析对投资者的决策能起到非常重要的指导性作用，其意义很多时候超过了对数值本身的分析和预测。</p>
<p>2、Fisher线性判别函数的求解过程是将M维特征矢量投影在（ ）中进行求解。<br>A M-1维空间<br>B 一维空间<br>C 三维空间<br>D 二维空间</p>
<p><strong>答案</strong>：B<br><strong>解析</strong>：Fisher线性判别函数是将多维空间中的特征矢量投影到一条直线上，也就是把维数压缩到一维。寻找这条最优直线的准则是Fisher准则：两类样本在一维空间的投影满足类内尽可能密集，类间尽可能分开，也就是投影后两类样本均值之差尽可能大，类内部方差尽可能小。一般而言，对于数据分布近似高斯分布的情况，Fisher线性判别准则能够得到很好的分类效果。</p>
<p>3、类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是（ ）<br>A 势函数法<br>B 基于二次准则的H-K算法<br>C 伪逆法<br>D 感知器算法</p>
<p><strong>答案</strong>：D<br><strong>解析</strong>：线性分类器的设计就是利用训练样本集建立线性判别函数式，也就是寻找最优的权向量的过程。求解权重的过程就是训练过程，训练方法的共同点是，先给出准则函数，再寻找是准则函数趋于极值的优化方法。ABC方法都可以得到线性不可分情况下分类问题近似解。感知器可以解决线性可分的问题，但当样本线性不可分时，感知器算法不会收敛。</p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="is-rounded" src="https://cdn.jsdelivr.net/gh/removeif/removeif.github.io@latest/img/avatar.jpg" alt="Wasim37"></figure><p class="title is-size-4 is-block line-height-inherit">Wasim37</p><p class="is-size-6 is-block">Developer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>GuangZhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">21</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">24</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/wasim37" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/wasim37"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Weibo" href="/"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Email" href="/"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="Next" href="/"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-white is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><hr></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"><span class="level-start"><span class="level-item">基础概念</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">深度学习</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"><span class="level-start"><span class="level-item">知识总结</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">最新评论</h3><span class="body_hot_comment">加载中，最新评论有1分钟延迟...</span></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2018-07-10T14:22:00.000Z">2018-07-10</time></p><p class="title is-6"><a class="link-muted" href="/2018/07/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%90%B4%E6%81%A9%E8%BE%BE%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E8%AF%BE%E5%A0%82%E7%AC%94%E8%AE%B0/">吴恩达目标检测课堂笔记</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2018-03-10T14:22:00.000Z">2018-03-10</time></p><p class="title is-6"><a class="link-muted" href="/2018/03/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">序列模型与注意力机制</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2018-03-02T14:22:00.000Z">2018-03-02</time></p><p class="title is-6"><a class="link-muted" href="/2018/03/02/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5/">自然语言处理与词嵌入</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2018-02-27T14:22:00.000Z">2018-02-27</time></p><p class="title is-6"><a class="link-muted" href="/2018/02/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BE%AA%E7%8E%AF%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">循环训练模型</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2018-01-27T14:22:00.000Z">2018-01-27</time></p><p class="title is-6"><a class="link-muted" href="/2018/01/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E7%A7%BB/">神经风格转移</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/03/"><span class="level-start"><span class="level-item">三月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/02/"><span class="level-start"><span class="level-item">二月 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/01/"><span class="level-start"><span class="level-item">一月 2018</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2017/12/"><span class="level-start"><span class="level-item">十二月 2017</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8F%90%E5%8D%87%E7%AE%97%E6%B3%95/"><span class="tag">提升算法</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SVM/"><span class="tag">SVM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/StyleTransfer/"><span class="tag">StyleTransfer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/YOLO/"><span class="tag">YOLO</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/word2vec/"><span class="tag">word2vec</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"><span class="tag">人脸识别</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"><span class="tag">决策树</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8D%B7%E7%A7%AF/"><span class="tag">卷积</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"><span class="tag">吴恩达</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Attention-Model/"><span class="tag">Attention Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"><span class="tag">特征工程</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><span class="tag">线性回归</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/"><span class="tag">经典网络</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%81%9A%E7%B1%BB/"><span class="tag">聚类</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"><span class="tag">贝叶斯</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/logo.svg" alt="Wasim&#039;s Blog" height="28"></a><p class="size-small"><span>&copy; 2020 wasim37</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/wasim37"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'http://wangxin123.com',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back-to-top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script src="/js/gallery.js" defer></script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            MathJax.Hub.Config({
                'HTML-CSS': {
                    matchFontHeight: false
                },
                SVG: {
                    matchFontHeight: false
                },
                CommonHTML: {
                    matchFontHeight: false
                },
                tex2jax: {
                    inlineMath: [
                        ['$','$'],
                        ['\\(','\\)']
                    ]
                }
            });
        });</script><!--!--><script src="/js/main.js" defer></script><script src="/js/comment-issue-data.js" defer></script><link rel="stylesheet" href="/css/insight.css"><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>(function (window) {
            var INSIGHT_CONFIG = {
                TRANSLATION: {
                    POSTS: '文章',
                    PAGES: '页面',
                    CATEGORIES: '分类',
                    TAGS: '标签',
                    UNTITLED: '(无标题)',
                },
                CONTENT_URL: '/content.json',
            };
            window.INSIGHT_CONFIG = INSIGHT_CONFIG;
        })(window);</script><script src="/js/insight.js" defer></script></body></html>