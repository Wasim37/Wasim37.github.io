<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>序列模型与注意力机制 - 星空str</title><meta description="ai,机器学习,深度学习,算法,leetcode,java"><meta property="og:type" content="blog"><meta property="og:title" content="Wasim37"><meta property="og:url" content="https://wangxin123.com/"><meta property="og:site_name" content="Wasim37"><meta property="og:description" content="ai,机器学习,深度学习,算法,leetcode,java"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"><meta property="article:published_time" content="2020-05-20T14:22:00.000Z"><meta property="article:modified_time" content="2020-05-29T08:40:37.894Z"><meta property="article:author" content="Wasim37"><meta property="article:tag" content="RNN"><meta property="article:tag" content="seq2seq"><meta property="article:tag" content="Attention Model"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://wangxin123.com/"},"headline":"Wasim37","image":["https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/blog_avatar_lufei.png"],"datePublished":"2020-05-20T14:22:00.000Z","dateModified":"2020-05-29T08:40:37.894Z","author":{"@type":"Person","name":"Wasim37"},"description":"ai,机器学习,深度学习,算法,leetcode,java"}</script><link rel="alternative" href="/atom.xml" title="星空str" type="application/atom+xml"><link rel="icon" href="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/wu.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?060269356a8ca9046e6a13dcba6f9559";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><link rel="stylesheet" href="/css/style.css"><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="/js/globalUtils.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body class="is-2-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/logo.svg" alt="星空str" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/wasim37"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-05-20T14:22:00.000Z">2020-05-20</time><a class="commentCountImg" href="/2020/05/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/#comment-container"><span class="display-none-class">dfdbae100df440537460ac904eb23fdd</span><img class="not-gallery-item" src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/chat.svg"> <span class="commentCount" id="dfdbae100df440537460ac904eb23fdd"> 0</span>    </a><span class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></span><span class="level-item">23 分钟 读完 (大约 3382 个字)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">序列模型与注意力机制</h1><div class="content"><h3 id="Seq2Seq-模型"><a href="#Seq2Seq-模型" class="headerlink" title="Seq2Seq 模型"></a>Seq2Seq 模型</h3><p>Seq2Seq模型的核心思想是，通过深度神经网络将一个作为输入的序列映射为一个作为输出的序列，这一过程由编码输入与解码输出两个环节构成。在经典的实现中，编码器和解码器各由一个循环神经网络构成，既可以选择传统循环神经网络结构，也可以使用长短期记忆模型、门控循环单元等。在Seq2Seq模型中，两个循环神经网络是共同训练的。</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E7%BB%93%E6%9E%84%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt=""></p>
<a id="more"></a>
<p>对应于机器翻译过程，如上图所示。输入的序列是一个源语言的句子，有三个单词A、B、C，编码器依次读入A、B、C和结尾符<EOS>。 在解码的第一步，解码器读入编码器的最终状态，生成第一个目标语言的词W；第二步读入第一步的输出W，生成第二个词X；如此循环，直至输出结尾符<EOS>。输出的序列W、X、Y、Z就是翻译后目标语言的句子。</p>
<p>如下图所示，在图像描述文本生成任务中，输入是图像经过视觉网络的特征，输出的序列是图像的描述短句。在文本摘要任务中，输入的序列是长句子或段落，输出的序列是摘要短句。进行语音识别时，输入的序列是音频信号，输出的序列是识别出的文本。不同场景中，编码器和解码器有不同的设计，但对应Seq2Seq的底层结构却如出一辙。</p>
<p><img src='https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/Image-captioning.png' width="80%" ></p>
<p>Seq2Seq模型最核心的部分是其解码部分，<strong>大量的改进也是在解码环节衍生的</strong>。Seq2Seq模型最基础的解码方法是贪心法，即选取一种度量标准后，每次都在当前状态下选择最佳的一个结果，直到结束。贪心法的计算代价低，适合作为基准结果与其他方法相比较。很显然，贪心法获得的是一个局部最优解，由于实际问题的复杂性，该方法往往并不能取得最好的效果。</p>
<blockquote>
<p>提出 Seq2Seq 模型的相关论文：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1409.3215.pdf">Sutskever et al., 2014. Sequence to sequence learning with neural networks</a></li>
<li><a href="https://arxiv.org/abs/1406.1078">Cho et al., 2014. Learning phrase representaions using RNN encoder-decoder for statistical machine translation</a></li>
</ul>
</blockquote>
<hr>
<h3 id="集束搜索"><a href="#集束搜索" class="headerlink" title="集束搜索"></a>集束搜索</h3><p>集束搜索是常见的改进算法，它是一种启发式算法。该方法会保存beam size（后面简写为b）个当前的较佳选择，然后解码时每一步根据保存的选择进行下一步扩展和排序，接着选择前b个进行保存，循环迭代，直到结束时选择最佳的一个作为解码的结果。下图是b为2时的集束搜索示例。</p>
<p><img src='https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E9%9B%86%E6%9D%9F%E6%90%9C%E7%B4%A2%E7%A4%BA%E4%BE%8B.png'  ></p>
<p>由图可见，当前已经有解码得到的第一个词的两个候选：I和My。然后，将I和My输入到解码器，得到一系列候选的序列，诸如I  decided、My  decision、I thought等。最后，从后续序列中选择最优的两个，作为前两个词的两个候选序列。很显然，<strong>如果b取1，那么会退化为前述的贪心法</strong>。随着b的增大，其搜索的空间增大，最终效果会有所提升，但需要的计算量也相应增大。</p>
<p>在实际的应用（如机器翻译、文本摘要）中，<strong>b往往会选择一个适中的范围，以8～12为佳</strong> 。解码时使用堆叠的RNN、增加Dropout机制、与编码器之间建立残差连接等，均是常见的改进措施。在实际研究工作中，可以依据不同使用场景，有针对地进行选择和实践。</p>
<p>另外，解码环节中一个重要的改进是注意力机制。注意力机制的引入，使得在解码时每一步可以有针对性地关注与当前有关的编码结果，从而减小编码器输出表示的学习难度，也更容易学到长期的依赖关系。</p>
<blockquote>
<p>文章相关：</p>
<ul>
<li><a href="http://www.ai-start.com/dl2017/html/lesson5-week3.html#header-n50">吴恩达课程，集束搜索详解</a></li>
</ul>
</blockquote>
<hr>
<h3 id="Bleu-得分"><a href="#Bleu-得分" class="headerlink" title="Bleu 得分"></a>Bleu 得分</h3><p><strong>Bleu（Bilingual Evaluation Understudy）</strong> 得分用于评估机器翻译的质量，其思想是<strong>机器翻译的结果越接近于人工翻译，则评分越高。</strong></p>
<p>最原始的 Bleu 将机器翻译结果中每个单词在人工翻译中出现的次数作为分子，机器翻译结果总词数作为分母得到。但是容易出现错误，例如，机器翻译结果单纯为某个在人工翻译结果中出现的单词的重复，则按照上述方法得到的 Bleu 为 1，显然有误。改进的方法是将每个单词在人工翻译结果中出现的次数作为分子，在机器翻译结果中出现的次数作为分母。</p>
<p><img src='https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/Bleu-score-on-unigram.png' width="35%" ></p>
<p>上述方法是以单个词为单位进行统计，以单个词为单位的集合称为unigram（一元组）。而以成对的词为单位的集合称为bigram（二元组）。对每个二元组，可以统计其在机器翻译结果（$count$）和人工翻译结果（$count_{clip}$）出现的次数，计算 Bleu 得分。</p>
<p>以此类推，以 n 个单词为单位的集合称为n-gram（多元组），对应的 Blue（即翻译精确度）得分计算公式为：</p>
<script type="math/tex; mode=display">p_n = \frac{\sum_{\text{n-gram} \in \hat y}count_{clip}(\text{n-gram})}{\sum_{\text{n-gram} \in \hat y}count(\text{n-gram})}</script><p>对 N 个 pn 进行几何加权平均得到：</p>
<script type="math/tex; mode=display">p_{ave} = exp(\frac{1}{N}\sum^N_{i=1}log^{p_n})</script><p>有一个问题是，当机器翻译结果短于人工翻译结果时，比较容易能得到更大的精确度分值，因为输出的大部分词可能都出现在人工翻译结果中。改进的方法是设置一个<strong>最佳匹配长度（Best Match Length）</strong>，如果机器翻译的结果短于该最佳匹配长度，则需要接受 <strong>简短惩罚（Brevity Penalty，BP）</strong>：</p>
<script type="math/tex; mode=display">
BP = 
\begin{cases} 
1, &MT\_length \ge BM\_length \\ 
exp(1 - \frac{MT\_length}{BM\_length}), &MT\_length \lt BM\_length 
\end{cases}</script><p>因此，最后得到的 Bleu 得分为：</p>
<script type="math/tex; mode=display">Blue = BP \times exp(\frac{1}{N}\sum^N_{i=1}log^{p_n})</script><p>Bleu 得分的贡献是提出了一个表现不错的<strong>单一实数评估指标</strong>，因此加快了整个机器翻译领域以及其他文本生成领域的进程。</p>
<p>相关论文：<a href="http://www.aclweb.org/anthology/P02-1040.pdf">neni et. al., 2002. A method for automatic evaluation of machine translation</a></p>
<hr>
<h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><blockquote>
<p>Seq2Seq模型引入注意力机制是为了解决什么问题？ 为什么选用了双向的循环神经网络模型？</p>
</blockquote>
<p>在实际任务（例如机器翻译）中，使用Seq2Seq模型，通常会先使用一个循环神经网络作为编码器，将输入序列（源语言句子的词向量序列）编码成为一个向量表示；然后再使用一个循环神经网络模型作为解码器，从编码器得到的向量表示里解码得到输出序列（目标语言句子的词序列）。在Seq2Seq模型中，当前隐状态以及上一个输出词决定了当前输出词，即</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/20200529160819.png" alt=""><br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/20200529160829.png" alt=""></p>
<p>其中f和g是非线性变换，通常是多层神经网络；yi 是输出序列中的一个词，si 是对应的隐状态。</p>
<p>在seq2seq的实际使用中，会发现<strong>随着输入序列的增长，模型的性能发生了显著下降</strong>。这是因为编码时输入序列的全部信息压缩到了一个向量表示中。随着序列增长，句子越前面的词的信息丢失就越严重。<strong>试想翻译一个有100个词的句子，需要将整个句子全部词的语义信息编码在一个向量中。而在解码时，目标语言的第一个词大概率是和源语言的第一个词相对应的，这就意味着第一步的解码就需要考虑100步之前的信息。建模时的一个小技巧是将源语言句子逆序输入，或者重复输入两遍来训练模型，以得到一定的性能提升。</strong>使用长短期记忆模型能够在一定程度上缓解这个问题，但在实践中对于过长的序列仍然难以有很好的表现。</p>
<p><strong>同时，Seq2Seq模型的输出序列中，常常会损失部分输入序列的信息，这是因为在解码时，当前词及对应的源语言词的上下文信息和位置信息在编解码过程中丢失了。Seq2Seq模型中引入注意力机制就是为了解决上述的问题。</strong></p>
<p>在注意力机制中，仍然可以用普通的循环神经网络对输入序列进行编码，得到隐状态h1 ,h2 …hT 。但是在解码时，每一个输出词都依赖于前一个隐状态以及输入序列每一个对应的隐状态，</p>
<p><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/20200529160840.png" alt=""><br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/20200529160848.png" alt=""></p>
<p>其中语境向量c i 是输入序列全部隐状态h 1 ,h 2 …h T 的一个加权和<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/20200529160857.png" alt=""></p>
<p>其中注意力权重参数α ij 并不是一个固定权重，而是由另一个神经网络计算得到<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/20200529160914.png" alt=""><br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/20200529160924.png" alt=""></p>
<p>神经网络a将上一个输出序列隐状态si−1 和输入序列隐状态hj 作为输入，计算出一个xj ，yi 对齐的值eij ，再归一化得到权重αij 。我们可以对此给出一个直观的理解：在生成一个输出词时，会考虑每一个输入词和当前输出词的对齐关系，对齐越好的词，会有越大的权重，对生成当前输出词的影响也就越大。</p>
<p>整个注意力机制解码详情如下：<br><img src='https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/attention-decoder%20for%20each%20time%20step.png
' width="70%" ></p>
<p>下图展示了翻译时注意力机制的权重分布，在互为翻译的词对上会有最大的权重。<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/20200529160943.png" alt=""></p>
<p><strong>那为什么注意力机制的编码网络，通常选用双向循环神经网络呢？</strong></p>
<p>在机器翻译这样一个典型的Seq2Seq模型里，生成一个输出词yj ，会用到第i个输入词对应的隐状态hi 以及对应的注意力权重αij 。如果只使用一个方向的循环神经网络来计算隐状态，那么hi 只包含了x0 到xi 的信息，相当于在αij 这里丢失了xi 后面的词的信息。而使用双向循环神经网络进行建模，第i个输入词对应的隐状态包含了和 ，前者编码x0 到xi 的信息，后者编码xi 及之后所有词的信息，防止了前后文信息的丢失，如下图所示。<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/20200529161129.png" alt=""></p>
<p>注意力机制是一种思想，可以有多种不同的实现方式，在Seq2Seq模型以外的场景也有不少应用。下图展示了在图像描述文本生成任务中的结果，可以看到在生成对应词时，图片上对应物体的部分有较大的注意力权重.<br><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/20200529161207.png" alt=""></p>
<blockquote>
<p>相关文章：</p>
<ul>
<li><a href="http://www.ai-start.com/dl2017/html/lesson5-week3.html#header-n168">吴恩达课程，注意力机制详解</a></li>
<li><a href="https://blog.csdn.net/thriving_fcl/article/details/74853556">带Attention机制的Seq2Seq框架梳理</a></li>
</ul>
<p>相关论文：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1409.0473.pdf">Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate</a></li>
<li><a href="https://arxiv.org/pdf/1502.03044.pdf">Xu et. al., 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a>：将注意力模型应用到图像标注中</li>
</ul>
</blockquote>
<hr>
<h3 id="吴恩达课程代码"><a href="#吴恩达课程代码" class="headerlink" title="吴恩达课程代码"></a>吴恩达课程代码</h3><p><a href="https://github.com/Wasim37/deeplearning-assignment/tree/master/5%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week3%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6">吴恩达-序列模型和注意力机制-代码示例</a></p>
</div><div class="article-tags size-small is-uppercase mb-4"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted mr-2" rel="tag" href="/tags/RNN/">RNN</a><a class="link-muted mr-2" rel="tag" href="/tags/seq2seq/">seq2seq</a><a class="link-muted mr-2" rel="tag" href="/tags/Attention-Model/">Attention Model</a></div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/ali-zfb.png" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/weixin-pay.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/05/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94/"><span class="level-item">知识追踪模型调研</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.css"><script src="/js/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: 'dfdbae100df440537460ac904eb23fdd',
            repo: 'blog_comment',
            owner: 'Wasim37',
            clientID: '55fcbea044b44e047f89',
            clientSecret: '47addd42a9469d80bd508118ac73033915e3142c',
            admin: ["wasim37"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: 'last',
            
            
            enableHotKey: true,
            isLocked: false
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget toc-scroll" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex" id="toc-item-Seq2Seq-模型" href="#Seq2Seq-模型"><span>Seq2Seq 模型</span></a></li><li><a class="is-flex" id="toc-item-集束搜索" href="#集束搜索"><span>集束搜索</span></a></li><li><a class="is-flex" id="toc-item-Bleu-得分" href="#Bleu-得分"><span>Bleu 得分</span></a></li><li><a class="is-flex" id="toc-item-注意力机制" href="#注意力机制"><span>注意力机制</span></a></li><li><a class="is-flex" id="toc-item-吴恩达课程代码" href="#吴恩达课程代码"><span>吴恩达课程代码</span></a></li></ul></div></div><script type="text/javascript" async>
        $(document).ready(function () { //参考自 https://github.com/ppoffice/hexo-theme-icarus/pull/616/files
            var observerTopMargin;
            var scrollObserver;
            var headerElems = $(".headerlink");
            var activeTocItem;
        
            function initIntersectionObserver(docHeight) {
                observerTopMargin = docHeight;
                scrollObserver = new IntersectionObserver(scrollCallBack,
                    {
                        root: null,  // viewpoint
                        rootMargin: docHeight + "px 0px -80% 0px"  // cover top 30% of viewport to the top of document
                    })
            }
        
            function scrollCallBack(entries, observer) {
                if ($(window).scrollTop() > observerTopMargin * 0.7) { 
                    // User somehow scroll to 70% of observerTopMargin (which is inited as 200% document height)
                    // Observer top margin need to extend to cover all the space to the top of the document
                    initIntersectionObserver(observerTopMargin * 2)
                    observer.disconnect();
                    return;
                }
                let toActive;
                if (entries[0].intersectionRatio == 1) {  // enter viewed area
                    let entry = entries.reduce((u, v) => (u.target.toc_id > v.target.toc_id ? u : v));  // get the lowest item
                    toActive = $("#toc-item-" + $(entry.target).attr("href").substr(1));
                } else {
                    let entry = entries.reduce((u, v) => (u.target.toc_id < v.target.toc_id ? u : v));  // get the highest item
                    let idx = Math.max(entry.target.toc_id - 1, 0);
                    toActive = $("#toc-item-" + $(headerElems[idx]).attr("href").substr(1));
                }
                if (activeTocItem) activeTocItem.removeClass("is-current");
                activeTocItem = toActive
                activeTocItem.addClass("is-current");
            }
        
            initIntersectionObserver($(document).height() * 2);
            headerElems.each(function (index, obj) {
                obj.toc_id = index;
                scrollObserver.observe(obj);
            })
        });</script></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2020-05-20T14:22:00.000Z">2020-05-20</time></p><p class="title is-6"><a class="link-muted" href="/2020/05/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">序列模型与注意力机制</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2019-05-21T14:22:00.000Z">2019-05-21</time></p><p class="title is-6"><a class="link-muted" href="/2019/05/21/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E7%9F%A5%E8%AF%86%E8%BF%BD%E8%B8%AA%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94/">知识追踪模型调研</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2019-05-01T14:22:00.000Z">2019-05-01</time></p><p class="title is-6"><a class="link-muted" href="/2019/05/01/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5/%E8%AF%95%E9%A2%98%E7%9F%A5%E8%AF%86%E7%82%B9%E6%99%BA%E8%83%BD%E6%A0%87%E6%B3%A8/">试题知识点智能标注</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5/">项目实践</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2019-04-10T16:00:00.000Z">2019-04-11</time></p><p class="title is-6"><a class="link-muted" href="/2019/04/11/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E4%BB%8ENB%E5%88%B0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">从NB到语言模型</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2019-04-05T16:00:00.000Z">2019-04-06</time></p><p class="title is-6"><a class="link-muted" href="/2019/04/06/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6/">贝叶斯与垃圾邮件</a></p><p class="is-uppercase"><i class="fas fa-folder-open has-text-grey"> </i><a class="link-muted" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"><span class="level-start"><span class="level-item">知识总结</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><span class="level-start"><span class="level-item">神经网络</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B/"><span class="level-start"><span class="level-item">经典模型</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"><span class="level-start"><span class="level-item">自然语言处理</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"><span class="level-start"><span class="level-item">计算机视觉</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><span class="level-start"><span class="level-item">论文阅读</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5/"><span class="level-start"><span class="level-item">项目实践</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/05/"><span class="level-start"><span class="level-item">五月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/05/"><span class="level-start"><span class="level-item">五月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/04/"><span class="level-start"><span class="level-item">四月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/09/"><span class="level-start"><span class="level-item">九月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2018/07/"><span class="level-start"><span class="level-item">七月 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/logo.svg" alt="星空str" height="28"></a><p class="size-small"><span>&copy; 2020 wasim37</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/wasim37"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'http://wangxin123.com',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back-to-top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script src="/js/gallery.js" defer></script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            MathJax.Hub.Config({
                'HTML-CSS': {
                    matchFontHeight: false
                },
                SVG: {
                    matchFontHeight: false
                },
                CommonHTML: {
                    matchFontHeight: false
                },
                tex2jax: {
                    inlineMath: [
                        ['$','$'],
                        ['\\(','\\)']
                    ]
                }
            });
        });</script><!--!--><script src="/js/main.js" defer></script><script src="/js/comment-issue-data.js" defer></script><link rel="stylesheet" href="/css/insight.css"><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="想要查找什么..."><span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"></div></div></div></div><script>(function (window) {
            var INSIGHT_CONFIG = {
                TRANSLATION: {
                    POSTS: '文章',
                    PAGES: '页面',
                    CATEGORIES: '分类',
                    TAGS: '标签',
                    UNTITLED: '(无标题)',
                },
                CONTENT_URL: '/content.json',
            };
            window.INSIGHT_CONFIG = INSIGHT_CONFIG;
        })(window);</script><script src="/js/insight.js" defer></script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>