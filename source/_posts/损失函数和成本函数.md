---
title: 损失函数和成本函数
tags:
  - 人脸识别
categories:
  - 深度学习
date: 2018-01-05 22:22:00
toc: true
mathjax: true

---

**首先区分一个概念，损失函数针对的是单个样本，代价函数或者成本函数针对的是全体样本。**

---

### Softmax 回归

二分类问题，神经网络输出层通常只有一个神经元，表示预测输出 $\hat y$是正类的概率 P(y = 1|x)，$\hat y$ > 0.5 则判断为正类，反之判断为负类。

对于**多分类**问题，用 C 表示种类个数，则神经网络输出层，也就是第 L 层的单元数量 $n^{[L]} = C$。每个神经元的输出依次对应属于该类的概率，即 $P(y = c|x), c = 0, 1, .., C-1$。有一种 Logistic 回归的一般形式，叫做**Softmax 回归**，可以处理多分类问题。

<!-- more -->

对于 Softmax 回归模型的输出层，即第 L 层，有：

$$Z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]}$$

for i in range(L)，有：

$$a^{[L]}_i = \frac{e^{Z^{[L]}_i}}{\sum^C_{i=1}e^{Z^{[L]}_i}}$$

为输出层每个神经元的输出，对应属于该类的概率，满足：

$$\sum^C_{i=1}a^{[L]}_i = 1$$

一个直观的计算例子如下：

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/understanding-softmax.png)

定义 **损失函数** 为：

$$L(\hat y, y) = -\sum^C_{j=1}y_jlog\hat y_j$$

当 i 为样本真实类别，则有：

$$y_j = 0, j \ne i$$

因此，损失函数可以简化为：

$$L(\hat y, y) = -y_ilog\hat y_i = log \hat y_i$$

所有 m 个样本的 **成本函数** 为：

$$J = \frac{1}{m}\sum^m_{i=1}L(\hat y, y)$$

--- 

### 逻辑回归

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/2.png)
http://blog.csdn.net/oldbai001/article/details/49872433
http://blog.csdn.net/programmer_wei/article/details/52072939

---

### SVM

---

### 神经网络