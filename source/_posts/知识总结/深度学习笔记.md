---
title: 深度学习笔记
tags:
  - 深度学习
categories:
  - 知识总结
date: 2018-07-12
toc: true
thumbnail: 
recommend: 0
mathjax: true

---

### 神经网络初始化权重为什么不能初始化为0

将所有权重初始化为零将无法破坏网络的对称性。这意味着每一层的每个神经元都会学到相同的东西，这样的神经元网络并不比线性分类器如逻辑回归更强大。

需要注意的是，需要初始化去破坏网络对称性(symmetry)的只有W，b可以全部初始化为0。


<!-- more -->

---

### 常见的学习率衰减方法
如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。

而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。

**最常用的学习率衰减方法**：
$\alpha = \frac{1}{1 + decay\\_rate \* epoch\\_num} \* \alpha_0$
其中，decay_rate为衰减率（超参数），epoch_num为将所有的训练样本完整过一遍的次数。

**指数衰减**：
$\alpha = 0.95^{epoch\\_num} \* \alpha_0$

**其他**：
$\alpha = \frac{k}{\sqrt{epoch\\_num}} \* \alpha_0$

**离散下降**
对于较小的模型，也有人会在训练时根据进度手动调小学习率。

---

### 局部最优问题（鞍点）

![鞍点](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/saddle.png)
**鞍点（saddle）是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值**。

**减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点**。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。

结论：
（1）在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，**困在极差的局部最优中是不大可能的**；
（2）**鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。**

---

### 神经网络的优化算法

吴恩达优化算法章节总结链接：http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/优化算法

深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。

常见优化算法：
1、batch 梯度下降法，同时处理整个训练集
2、Mini-Batch 梯度下降法
（1）mini-batch 的大小为 1，即是随机梯度下降法（stochastic gradient descent），每个样本都是独立的 mini-batch
（2）mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法
（3）batch 的不同大小（size）带来的影响
- batch 梯度下降法：
 - 对所有 m 个训练样本执行一次梯度下降，每一次迭代时间较长，训练过程慢；
 - 相对噪声低一些，幅度也大一些；
 - 成本函数总是向减小的方向下降。
- 随机梯度下降法：
 - 对每一个训练样本执行一次梯度下降，训练速度快，但丢失了向量化带来的计算加速；
 - 有很多噪声，减小学习率可以适当；
 - 成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。
因此，选择一个1 < size < m的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。

（4）mini-batch 大小的选择
- 如果训练样本的大小比较小，如 m ⩽ 2000 时，选择 batch 梯度下降法；
- 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 26、27、...、29；
- mini-batch 的大小要符合 CPU/GPU 内存。
mini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。

3、了解加权平均
4、动量梯度下降法，是计算梯度的指数加权平均数，并利用该值来更新参数值
5、RMSProp算法，是在对梯度进行指数加权平均的基础上，引入平方和平方根
6、Adam优化算法（Adaptive Moment Estimation，自适应矩估计），基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果

---


### 卷积神经网络示例

![CNN-Example](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/CNN-Example.jpg)

随着神经网络计算深度不断加深，图片的高度和宽度 n[l]H、n[l]W一般逐渐减小，而 n[l]c在增加。

一个典型的卷积神经网络通常包含有三种层：**卷积层**（Convolution layer）、**池化层**（Pooling layer）、**全连接层**（Fully Connected layer）。仅用卷积层也有可能构建出很好的神经网络，但大部分神经网络还是会添加池化层和全连接层，它们更容易设计。

在计算神经网络的层数时，通常**只统计具有权重和参数的层**，**池化层没有需要训练的参数，所以和之前的卷积层共同计为一层**。

图中的 FC3 和 FC4 为全连接层，与标准的神经网络结构一致。整个神经网络各层的尺寸与参数如下表所示：

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/summary.png)

推荐[一个直观感受卷积神经网络的网站](http://scs.ryerson.ca/~aharley/vis/conv/)。

---


### 什么是端到端的网络
深度学习中最令人振奋的最新动态之一就是端到端深度学习的兴起。简而言之，以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它。

传统的机器学习任务经常需要分别训练多个模型（涉及人为介入的特征工程），输入数据后，无法通过一个模型直接得到结果，是非端到端的。现在的深度学习模式是【输入→（1个）模型→（直接出）结果】，模型会自动提取特征。

更多详情见 吴恩达 [什么是端到端的深度学习](http://www.ai-start.com/dl2017/html/lesson3-week2.html#header-n341)

---

### 请简要介绍下tensorflow的计算图

Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个节点都是计算图上的一个Tensor, 也就是张量，而节点之间的边描述了计算之间的依赖关系(定义时)和数学操作(运算时)。如下两图表示：
a=x*y; b=a+z; c=tf.reduce_sum(b);  
 ![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/8.gif)


---

### name_scope 和 variable_scope的区别

- **name_scope：** 为了更好地管理变量的命名空间而提出的。比如在 tensorboard 中，因为引入了 name_scope， Graph 看起来才井然有序。
- **variable_scope：** 跟 tf.get_variable() 配合使用，实现变量共享。

详见：https://blog.csdn.net/jacke121/article/details/77622834

---

### 怎么加快训练的速度
- 加快数据的读取速度, 具体详见 [数据读取](http://www.tensorfly.cn/tfdoc/how_tos/reading_data.html)
 - 可以构建tensorflow计算图，然后Feeding，减少与C++后端的交互次数; 
 - 或者从文件读取数据，使用多线程与管道; 
 - 或者预加载数据，即在TensorFlow图中定义常量或变量来保存所有数据，仅适用于数据量比较小的情况。
- 加快收敛的速度，详解神经网络的优化算法
- 要是服务器足够，同时设置验证多个超参，边训练边验证，用tensorflow对比查看，尽快得到满意结果

---

### LSTM结构推导，为什么比RNN好

推导forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的cell informaton是通过input gate控制之后**叠加**的，**RNN是叠乘**，因此LSTM一定程度上可以防止梯度消失或者爆炸

---



### 什麽样的资料集不适合用深度学习

- **数据集太小**，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。

- 数据集**没有局部相关特性**，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

---

### 广义线性模型是怎被应用在深度学习中

A Statistical View of Deep Learning (I): Recursive GLMs
深度学习从统计学角度，可以看做递归的广义线性模型。

广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。

深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者（这里主要指我）的困惑。

下图是一个对照表
![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/18.png)

---

### 深度学习与传统机器学习的数据划分区别
对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：

1）训练集（train set）：用训练集对算法或模型进行训练过程；
2）验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）**进行交叉验证，选择出最好的模型**；
3）测试集（test set）：最后利用测试集对模型进行测试，获取模型运行的无偏估计（对学习方法进行评估）。

**在小数据量的时代**，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：

1）无验证集的情况：70% / 30%；
2）有验证集的情况：60% / 20% / 20%；

而在如今的**大数据时代**，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以**验证集和测试集所占的比重会趋向于变得更小**。

**验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。**

测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。吴恩达给出的建议是：

1）100 万数据量：98% / 1% / 1%；
2）超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）

---




### 上采样和下采样

**缩小图像（或称为下采样 subsampled 和降采样 downsampled）**的主要目的有两个
1）使得图像符合显示区域的大小
2）生成对应图像的缩略图。

**放大图像（或称为上采样 upsampling 和图像插值 interpolating）**的主要目的是**放大原图像**，从而可以显示在更高分辨率的显示设备上。

上采样原理：内插值，即在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的元素。

---

### 什么是感知器
当激活函数的 **返回值是两个固定值** 的时候，可以称为此时的神经网络为感知器。

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/20180204144443.png)
因为感知器的返回值只有两种情况，所以感知器只能解决二类线性可分的问题，感知器比较适合应用到模式分类问题中

---

### 神经网络隐层维度规则

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%B4%E5%BA%A6%E8%A7%84%E5%88%99.png)

---

### 偏差方差及其应对方法
“偏差-方差分解”（bias-variance decomposition）是解释学习算法泛化性能的一种重要工具。

**泛化误差可分解为偏差、方差与噪声之和**：
- **偏差**：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了**学习算法本身的拟合能力**；
- **方差**：度量了同样大小的训练集的变动所导致的学习性能的变化，即**刻画了数据扰动所造成的影响**；
- **噪声**：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了学习问题本身的难度。

偏差-方差分解说明，**泛化性**能是由学习**算法的能力、数据的充分性**以及**学习任务本身的难度**所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。

在**欠拟合**（underfitting）的情况下，出现**高偏差**（high bias）的情况，即不能很好地对数据进行分类。

当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现**过拟合**（overfitting）的情况，在验证集上出现**高方差**（high variance）的现象。

当训练出一个模型以后，如果：
- 训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合；
- 训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合；
- 训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差；
- 训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。
- 偏差和方差的权衡问题对于模型来说十分重要。

最优误差通常也称为“贝叶斯误差”。

#### **应对方法**
存在高偏差：

- 扩大网络规模，如添加隐藏层或隐藏单元数目；
- 寻找合适的网络架构，使用更大的 NN 结构；
- 花费更长时间训练。

存在高方差：

- 获取更多的数据；
- 使用正则化（regularization）技术，降低模型的复杂度；
- 寻找更合适的网络结构。甚至使用bagging算法比如随机森林，训练多个弱模型，然后组合在一起，进行投票等
- 不断尝试，直到找到低偏差、低方差的框架。

在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。

---

### CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？

[Deep Learning -Yann LeCun, Yoshua Bengio & Geoffrey Hinton](http://link.zhihu.com/?target=https%3A//www.researchgate.net/publication/277411157_Deep_Learning)
[Learn TensorFlow and deep learning, without a Ph.D.](http://link.zhihu.com/?target=https%3A//docs.google.com/presentation/d/1TVixw6ItiZ8igjp6U17tcgoFrLSaHWQmMOwjlgQY9co/pub%3Fslide%3Did.p)
[The Unreasonable Effectiveness of Deep Learning -LeCun 16 NIPS Keynote](http://link.zhihu.com/?target=http%3A//vdisk.weibo.com/s/AoN5oNl5t04h)

以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示。
 ![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/9.png)

CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。
局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图：
 ![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/10.png)

上图中，如果每一个点的处理使用相同的Filter，则为全卷积，如果使用不同的Filter，则为Local-Conv。

---

### 为什么很多做人脸的Paper会最后加入一个Local Connected Conv

如果每一个点的处理使用相同的Filter(即参数共享)，则为全卷积，如果使用不同的Filter，则为Local-Conv(local的意思就是参数不共享)。

DeepFace 先进行了两次全卷积＋一次池化，提取了低层次的边缘／纹理等特征。后接了3个Local-Conv层，这里是用Local-Conv的原因是，**人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。**当数据集具有全局的局部特征分布时，也就是说局部特征之间有较强的相关性，适合用全卷积。

"不存在全局的局部特征分布"，可以这么理解，人的鼻子和嘴是局部特征，但在全局特征(脸)中并不是广泛分布的，它们是独一无二的。所以说不存在局部连接学习了鼻子的特征，然后把特征应用到脸的其他部位。

链接：http://blog.csdn.net/u014365862/article/details/77795902

PS：为什么经常将全连接层转化为全卷积？
全卷积和FCN的参数一样多，之所以用全卷积，主要是为了在卷积层上实现滑动窗口，减少重复卷积的计算，在目标检测中很有用。
还有一种说法是为了让卷积网络在一张更大的输入图片上滑动，得到每个区域的输出。
https://blog.csdn.net/u010548772/article/details/78582250
https://blog.csdn.net/nnnnnnnnnnnny/article/details/70194432

---

### Dilated Convolution空洞卷积/扩张卷积/膨胀卷积
http://blog.csdn.net/guvcolie/article/details/77884530?locationNum=10&fps=1
https://blog.csdn.net/mao_xiao_feng/article/details/77924003
http://www.cnblogs.com/ranjiewen/p/7945249.html


---

### Word2vec之Skip-Gram模型
Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。Embedding其实就是一个映射，将单词从原先所属的空间映射到新的多维空间中，也就是把原先词所在空间嵌入到一个新的空间中去。

Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。本篇文章仅讲解Skip-Gram模型。

- 结构篇：https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html
- 训练篇：https://www.leiphone.com/news/201706/eV8j3Nu8SMqGBnQB.html
- 实现篇：https://www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html

PS：吴恩达序列模型课程自然语言处理章节有详细讲解。

---

### 文本数据抽取

- **词袋法**：将文本当作一个无序的数据集合，文本特征可以采用文本中的词条T进行体现，那么文本中出现的所有词条及其出现的次数就可以体现文档的特征

- **TF-IDF**: 词条的重要性随着它在**文件中出现的次数**成**正比**增加，但同时会随着它在**语料库中出现的频率**成**反比**下降；也就是说词条在文本中出现的次数越多，表示该词条对该文本的重要性越高，词条在所有文本中出现的次数越少，说明这个词条对文本的重要性越高。**TF(词频)指某个词条在文本中出现的次数**，一般会将其进行归一化处理(该词条数量/该文档中所有词条数量)；**IDF(逆向文件频率)指一个词条重要性的度量**，一般计算方式为**总文件数目除以包含该词语之文件的数目**，再将得到的商取对数得到。TF-IDF实际上是：TF * IDF
![TF-IDF](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/tf-idf.png)

---

### 迁移学习

深度学习中，最强大的理念之一就是，有的时候神经网络可以从一个任务中习得知识，并将这些知识应用到另一个独立的任务中。所以例如，也许你已经训练好一个神经网络，能够识别像猫这样的对象，然后使用那些知识，或者部分习得的知识去帮助您更好地阅读x射线扫描图，这就是所谓的迁移学习。

**通常做法就是**，你可以把识别猫的神经网络最后的输出层及其对应的权重删掉，然后设计一个全新的输出层，并为其重新赋予随机权重，然后让它在放射诊断数据上训练。

**那什么时候迁移学习是有意义的？**如果你想从任务学习A并迁移一些知识到任务B，那么当任务A和任务B都有同样的输入X时，迁移学习是有意义的（比如识别猫和X射线扫描时输入的都是图像）。并且当任务A的数据比任务B多得多时，迁移学习意义更大。

- 更多细节详见 吴恩达的 ["迁移学习"](http://www.ai-start.com/dl2017/html/lesson3-week2.html#header-n242) 章节讲解。
- TensorFlow Hub 可以实现迁移学习，GitHub 代码示例地址：[图像再训练](https://github.com/Wasim37/models/tree/master/tutorials/image/image_retrain)

---

### 生成对抗网络

GAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的。另一方是discriminator，其目标是判断输入图片是否属于真实训练样本。

更直白的讲，将generator想象成假币制造商，而discriminator是警察。generator目的是尽可能把假币造的跟真的一样，从而能够骗过discriminator，即生成样本并使它看上去好像来自于真实训练样本一样。

---


- [损失函数和成本函数](http://wangxin123.com/2018/01/05/损失函数和成本函数/)
- [机器学习开发策略](http://wangxin123.com/2017/12/10/机器学习开发策略总结/)
- [卷积操作详解（填充、步长、高维卷积、卷积公式）](http://wangxin123.com/2018/01/10/卷积操作详解（填充、步长、高维卷积、卷积公式）/)
- [卷积网络怎么进行边缘检测](http://wangxin123.com/2017/12/18/卷积网络的边缘检测/)
- [Batch Normalization](http://wangxin123.com/2017/12/19/Batch%20Normalization%20批标准化/)
- [CNN经典网络总结](http://wangxin123.com/2018/01/15/CNN经典网络总结/)


- [RNN](http://wangxin123.com/2018/02/27/循环训练模型/)
- [GRU](http://wangxin123.com/2018/02/27/循环训练模型/#GRU（门控循环单元）)
- [LSTM](http://wangxin123.com/2018/02/27/循环训练模型/#LSTM（长短期记忆）)
- Tensorflow 官网推荐的一篇[伟大文章](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)，特别介绍递归神经网络和LSTM
- [RNN是怎么从单层网络一步一步构造的](https://zhuanlan.zhihu.com/p/28054589)


- [seq2seq序列模型](http://wangxin123.com/2018/03/10/序列模型与注意力机制/#Seq2Seq-模型)
- [attention注意力模型](http://wangxin123.com/2018/03/10/序列模型与注意力机制/#注意力模型)
- [语言识别](http://wangxin123.com/2018/03/10/序列模型与注意力机制/#语音识别)
- [触发词检测](http://wangxin123.com/2018/03/10/序列模型与注意力机制/#触发词检测)


- [迁移学习](#迁移学习)
- [目标检测_吴恩达](http://wangxin123.com/2018/02/10/目标检测/)
- [目标检测_RCNN系列](https://blog.csdn.net/linolzhang/article/details/54344350)
- [人脸识别](http://wangxin123.com/2018/01/21/人脸识别/)
- [神经风格转移](http://wangxin123.com/2018/01/27/神经风格转移/)
- [生成对抗网络](#生成对抗网络)
- [命名实体识别](#命名实体识别)

