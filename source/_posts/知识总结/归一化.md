---
title: 归一化
tags:
categories:
  - 知识总结
date: 2019-11-2 22:22:00
toc: true
mathjax: true

---

<!-- more -->


# 机器学习中，为何要经常对数据做归一化

- 归一化后加快了梯度下降求最优解的速度
- 归一化有可能提高精度。

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/20180419093726.png)

如上图所示，蓝色的圈圈图代表的是两个特征的等高线。其中左图两个特征X1和X2的区间相差非常大，X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。**当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；**

**而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛（消除了量纲的影响）。**

至于**为什么能提高精度**，比如一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。

值域范围大的特征将在cost函数中得到更大的加权（如果较高幅值的特征改变 1%，则该改变相当大，但是对于较小的特征，该改变相当小），数据归一化使所有特征的权重相等。

---

# 特征向量的归一化方法有哪些
- **线性归一化（Min-Max Normalization）**，表达式如下：
也称为离差标准化，是对原始数据的线性变换，**使结果值映射到[0 - 1]之间**
y=(x-MinValue)/(MaxValue-MinValue)
这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。**实际使用中可以用经验常量值来替代max和min。**

- **Z-score标准化**，减去均值，除以方差：
经过处理的数据符合标准正态分布，**即均值为0，标准差为1**，其转化函数为
y=(x-means)/ variance

- **非线性归一化**。经常用在**数据分化比较大的场景，有些数值很大，有些很小**。通过一些数学函数，将原始值进行映射。**该方法包括 log、指数，正切等**。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。
 - 对数函数转换，表达式如下： y=log10 (x)
 - 反余切函数转换 ，表达式如下： y=arctan(x)*2/PI

---

# 哪些机器学习算法不需要做归一化处理

**概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，树形结构就属于概率模型，如决策树、rf。**

而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。

**归一化和标准化主要是为了使计算更方便，加快求解最优解的速度**。比如两个变量的**量纲不同**，可能一个的数值远大于另一个，那么他们同时作为变量的时候，可能会**造成数值计算问题**，比如说求矩阵的逆可能很不精确，或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能，量纲也需要调整。

---

# 对于树形结构为什么不需要归一化

**数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。**

对于线性模型，比如说LR，我有两个特征，一个是(0，1)的，一个是(0，10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。

另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。

---

# 标准化与归一化的区别

简单来说，归一化是依照特征矩阵的**行处理**数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“**单位向量**”。

标准化是依照特征矩阵的**列处理**数据，其通过求z-score的方法，将样本的特征值转换到**同一量纲**下。

---