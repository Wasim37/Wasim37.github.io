---
title: 损失函数和成本函数
tags:
categories:
  - 知识总结
date: 2019-11-2 22:22:00
toc: true
mathjax: true

---

**损失函数针对的是单个样本，代价函数或者成本函数针对的是全体样本。**

---

### 逻辑回归

Logistic 回归是一个用于二分分类的算法。

Logistic 回归中使用的参数如下：

- 输入的特征向量：$x \in R^{n_x}$，其中 ${n_x}$ 是特征数量；
- 用于训练的标签：$y \in 0,1$
- 权重：$w \in R^{n_x}$
- 偏置： $b \in R$
- 输出：$\hat{y} = \sigma(w^Tx+b)$
- **Sigmoid 函数**：
$$s = \sigma(w^Tx+b) = \sigma(z) = \frac{1}{1+e^{-z}}$$

<!-- more -->

为将 wTx+b 约束在 [0, 1] 间，引入 Sigmoid 函数。从下图可看出，Sigmoid 函数的值域为 [0, 1]。

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0/20180424114511_sigmoid.png)

Logistic 回归可以看作是一个非常小的神经网络。下图是一个典型例子：

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0/20180424114556_LogReg_kiank.png)

损失函数（loss function）用于衡量预测结果与真实值之间的误差。

最简单的损失函数定义方式为均方平方差损失：
$$L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^2$$
平方差很好理解，预测值与真实值直接相减，为了避免得到负数取绝对值或者平方，再做平均就是均方平方差。

**但 Logistic 回归中我们并不倾向于使用这样的损失函数**，因为之后讨论的优化问题会变成非凸的，最后会得到很多个局部最优解，梯度下降法可能找不到全局最优值。

**损失函数一般使用：**
$$L(\hat{y},y) = -(y\log\hat{y})-(1-y)\log(1-\hat{y})$$

**损失函数**是在**单个训练样本**中定义的，它衡量了在单个训练样本上的表现。而**代价函数**（cost function，或者称作成本函数）衡量的是在**全体训练样本**上的表现，即衡量参数 w 和 b 的效果。

$$J(w,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})$$

**逻辑回归参数迭代推导**：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/2.png)
http://blog.csdn.net/oldbai001/article/details/49872433
http://blog.csdn.net/programmer_wei/article/details/52072939

---

### Softmax 回归

二分类问题，神经网络输出层通常只有一个神经元，表示预测输出 $\hat y$是正类的概率 P(y = 1|x)，$\hat y$ > 0.5 则判断为正类，反之判断为负类。

对于**多分类**问题，用 C 表示种类个数，则神经网络输出层，也就是第 L 层的单元数量 $n^{[L]} = C$。每个神经元的输出依次对应属于该类的概率，即 $P(y = c|x), c = 0, 1, .., C-1$。有一种 Logistic 回归的一般形式，叫做**Softmax 回归**，可以处理多分类问题。

对于 Softmax 回归模型的输出层，即第 L 层，有：

$$Z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]}$$

for i in range(L)，有：

$$a^{[L]}_i = \frac{e^{Z^{[L]}_i}}{\sum^C_{i=1}e^{Z^{[L]}_i}}$$

为输出层每个神经元的输出，对应属于该类的概率，满足：

$$\sum^C_{i=1}a^{[L]}_i = 1$$

一个直观的计算例子如下：

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/understanding-softmax.png)

定义 **损失函数** 为：

$$L(\hat y, y) = -\sum^C_{j=1}y_jlog\hat y_j$$

当 i 为样本真实类别，则有：

$$y_j = 0, j \ne i$$

因此，损失函数可以简化为：

$$L(\hat y, y) = -y_ilog\hat y_i = log \hat y_i$$

所有 m 个样本的 **成本函数** 为：

$$J = \frac{1}{m}\sum^m_{i=1}L(\hat y, y)$$

--- 

### SVM

---

### 神经网络

---



# 常见损失函数

1. 铰链损失（[Hinge Loss](https://en.wikipedia.org/wiki/Hinge_loss)）：主要用于支持向量机（SVM） 中； $\ell(y) = \max(0, 1-t \cdot y)$
2. 交叉熵损失 （Cross Entropy Loss，Softmax Loss ）：用于Logistic 回归与Softmax 分类中； 
3. 平方和损失（Square Loss）：主要是普通最小二乘法( Ordinary Least Square，OLS)中； 
4. 指数损失（Exponential Loss） ：主要用于Adaboost 集成学习算法中，详见 [Adaboost与指数损失](http://breezedeus.github.io/2015/07/12/breezedeus-adaboost-exponential-loss.html) 
5. 其他损失（如0-1损失，绝对值损失）

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/85389915.jpg)

参考文献：
- https://blog.csdn.net/u010976453/article/details/78488279
- http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/

---


# 说说常见的损失函数


对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实值Y可能一致也可能不一致（要知道，有时损失或误差是不可避免的），用一个损失函数来度量预测错误的程度。损失函数记为L(Y， f(X))。

常用的损失函数有以下几种（基本引用自《统计学习方法》）：
![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/44.png)
![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/45.png)

如此，SVM有第二种理解，即最优化+损失最小，或如@夏粉_百度所说“可从损失函数和优化算法角度看SVM，boosting，LR等算法，可能会有不同收获”。

关于SVM的更多理解请参考：，链接：

---