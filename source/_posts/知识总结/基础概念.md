---
title: 基础概念
tags:
  - 机器学习
categories:
  - 知识总结
date: 2017-10-02
toc: true
thumbnail: 
recommend: 0
mathjax: true

---


# 理解矩阵
有人说，矩阵的本质就是线性方程式，两者是一一对应关系
链接：http://www.ruanyifeng.com/blog/2015/09/matrix-multiplication.html

也有人说，矩阵的本质是运动的描述。简而言之，就是在线性空间中选定基之后，向量刻画对象，矩阵刻画对象的运动，用矩阵与向量的乘法施加运动。
链接：https://pan.baidu.com/s/1BLyrQH5_VAw832jKkCgpBA 密码：ljwq


<!-- more -->

---

# CPU、GPU和TPU区别
CPU和GPU都是由三个部分组成：计算单元(ALU)、控制单元和存储单元。
GPU是CPU的变种，CPU至少30%都是用在了控制单元，各个单元占比还算均衡，而GPU就夸张了，80%以上都用在了计算单元。
正是由于这种区别，导致CPU精于控制和复杂运算，而GPU精于简单且重复的运算。

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/20200529170708.png)

GPU，就像你有个工作需要计算几亿次一百以内加减乘除一样，最好的办法就是雇上几十个小学生一起算，一人算一部分，反正这些计算也没什么技术含量，纯粹体力活而已；而CPU就像老教授，积分微分都会算，就是工资高，一个老教授资顶二十个小学生。GPU就是用很多简单的计算单元去完成大量的计算任务，纯粹的人海战术。

CPU和GPU另外一个最大区别：CPU是顺序执行运算，而GPU是可以大量并发的执行运算，通俗的说就是CPU做事情是一件一件来做，而GPU是很多件事情同时做。

[CPU/GPU/TPU/NPU傻傻分不清楚](https://zhuanlan.zhihu.com/p/101550272)

---

# 什么是深度前馈网络？前馈怎么理解
深度前馈网络（Deep Feedforward Networks）是一种典型的深度学习模型。其目标为拟合某个函数f，即定义映射y=f (x;θ)将输入x转化为某种预测的输出y，并同时学习网络参数θ的值，使模型得到最优的函数近似。

**由于从输入到输出的过程中不存在与模型自身的反馈连接，此类模型被称为“前馈”**。前馈简单理解，就是不瞻前顾后。

深度前馈网络是一类网络模型的统称，我们常见的多层感知机、自编码器、限制玻尔兹曼机，以及卷积神经网络等，都是其中的成员。

[前馈到反馈：解析循环神经网络（RNN）及其tricks](https://www.jianshu.com/p/8f75b32cc278)

---

# 什么是共线性， 跟过拟合有什么关联
共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。
共线性会造成冗余，导致过拟合。
解决方法：排除变量的相关性／加入权重正则。

---

# 极大似然估计的简单解释
极大似然估计可以拆成三个词，分别是“极大”、“似然”、“估计”，分别的意思如下：
极大：最大的概率
似然：看起来是这个样子的
估计：就是这个样子的
连起来就是，最大的概率看起来是这个样子的那就是这个样子的。

通俗理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！

---

# 什么是解释对偶

一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。

---


# 什么是边缘分布？边缘概率分布和边缘分布是同一个意思吗？
边缘分布是边缘概率分布的缩写，两者是同一个名词。
- 某一组概率的加和，叫边缘概率。边缘概率的分布情况，就叫边缘分布。和“边缘”两个字本身没太大关系，因为是求和，在表格中往往将5261这种值放在margin（表头）的位置。
- 如果我们把每一4102个1653变量的概率分布称为一个概率分布，那么边缘分布就是若干个变量的概率加和所表现出的分布。
- 对于一个任意大小(n*n)的概率矩阵X，每一个元素表示一个概率，对于其中任一行或任一列求和，得到的概率就是边缘概率。
- 就是指的某一些概率的加和值的分布，其实就对应一个等式，让它等于版某种概率加和运算。
- 这个值曾经用于表示某一个概率矩阵中某一行或某一列的概率加和，而这个加和在table中往往放在margin（表头）的位权置，所以叫marginal distribution，翻译过来变成了边缘概率。

---

# 协方差和相关性有什么区别

相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。
![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/46.png)
为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。
![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/47.png)

---

# 如何对数值型特征进行分桶
分桶是离散化的常用方法，将连续型特征离线化为一系列 0/1 的离散特征；
当数值特征跨越不同的数量级的时候，模型可能会只对大的特征值敏感，这种情况可以考虑分桶操作。
分桶操作可以看作是对数值变量的离散化，之后通过二值化进行 one-hot 编码。
详见：https://blog.csdn.net/lc013/article/details/104454135


---

# jieba分词
[jieba分词](https://github.com/fxsjy/jieba)

---

