---
title: 机器学习笔记
tags:
  - 机器学习
categories:
  - 知识总结
date: 2019-11-2 22:22:00
toc: true
thumbnail: 
recommend: 0
mathjax: true

---

- [PCA_吴恩达](http://www.ai-start.com/ml2014/html/week8.html#header-n186)
- [异常值检测_吴恩达](http://www.ai-start.com/ml2014/html/week9.html#header-n5)
- [推荐系统_吴恩达](http://www.ai-start.com/ml2014/html/week9.html#header-n279)
- [协同过滤和基于内容推荐有什么区别](https://www.zhihu.com/question/19971859)


- [如何准备机器学习工程师的面试](https://www.zhihu.com/question/23259302)
- [如何判断某个人的机器学习水平](https://www.zhihu.com/question/62482926)

---


<!-- more -->

# 机器学习项目开发流程

**1 抽象成数学问题**
机器学习的训练过程很耗时，胡乱尝试时间成本太高，所以明确问题是进行机器学习的第一步。
抽象成数学问题，指的明确我们可以获得什么样的数据，目标问题属于分类、回归还是聚类，如果都不是，如何转换为其中的某类问题。

**2 获取数据**
数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
数据要有代表性，否则必然会过拟合。而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。
对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。

**3 特征预处理与特征选择**
良好的数据要能够提取出良好的特征才能真正发挥效力。
特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。

筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。

**4 训练模型与调优**
直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。

**5 模型诊断**
如何确定模型调优的方向与思路呢这就需要对模型进行诊断的技术。
过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。

误差分析，也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……
诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。

**6 模型融合**
一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。

**7 上线运行**
这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。

---

# 有监督、无监督与半监督学习
- **有监督学习**：
对有标记的训练样本进行学习，对训练样本外的数据进行预测。
如 LR（Logistic Regression），SVM（Support Vector Machine），RF（RandomForest），GBDT（Gradient Boosting Decision Tree），感知机（Perceptron）、BP神经网络（Back Propagation）等。

- **无监督学习**：
对未标记的样本进行训练学习，试图发现样本中的内在结构。
如聚类(KMeans)、降维、文本处理、DL。

- **半监督学习(Semi-Supervised Learning，SSL)**：
主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类，是有监督学习和无监督学习的结合。
半监督学习对于减少标注代价，提高学习机器性能具有重大实际意义。

---

# 判别式模型与生成式模型
**判别式模型(Discriminative Model)**：直接对**条件概率p(y|x)**进行建模，常见判别模型有：
线性回归、决策树、支持向量机SVM、k近邻、神经网络、集成学习、条件随机场CRF等；

**生成式模型(Generative Model)**：对**联合分布概率p(x,y)**进行建模，常见生成式模型有：
隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM及其他混合模型、LDA和限制波兹曼机等；

生成式模型更普适，判别式模型更直接，目标性更强。
生成式模型关注数据是如何产生的，寻找的是数据分布模型。
判别式模型关注的数据的差异性，寻找的是分类面。
由生成式模型可以产生判别是模型，但是由判别式模式没法形成生成式模型


---




# 对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法
没有免费的午餐定理：

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/40.png)

- 对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。
- 也就是说：对于所有问题，无论学习算法A多聪明，学习算法 B多笨拙，它们的期望性能相同。
- 但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，**在优化算法时，针对具体问题进行分析，是算法优化的核心所在。**

---


# 请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么


集成学习的集成对象是学习器. Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器， 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器， 迭代集成(平滑加权).
决策树属于最常用的学习器， 其学习过程是从根建立树， 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂， CART决策树用基尼指数计算最优分裂， xgboost决策树使用二阶泰勒展开系数计算最优分裂.
下面所提到的学习器都是决策树:
Bagging方法: 
    学习器间不存在强依赖关系， 学习器可并行训练生成， 集成方式一般为投票;
    Random Forest属于Bagging的代表， 放回抽样， 每个学习器随机选择部分特征去优化;
Boosting方法: 
   学习器之间存在强依赖关系、必须串行生成， 集成方式为加权和;
    Adaboost属于Boosting， 采用指数损失函数替代原本分类任务的0/1损失函数;
    GBDT属于Boosting的优秀代表， 对函数残差近似值进行梯度下降， 用CART回归树做学习器， 集成为回归模型;
    xgboost属于Boosting的集大成者， 对函数残差近似值进行梯度下降， 迭代时利用了二阶梯度信息， 集成模型可分类也可回归. 由于它可在特征粒度上并行计算， 结构风险和工程实现都做了很多优化， 泛化， 性能和扩展性都比GBDT要好。
关于决策树，这里有篇《决策树算法》（链接：http://blog.csdn.net/v_july_v/article/details/7577684）。而随机森林Random Forest是一个包含多个决策树的分类器。至于AdaBoost，则是英文"Adaptive Boosting"（自适应增强）的缩写，关于AdaBoost可以看下这篇文章《Adaboost 算法的原理与推导》。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。

xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：
1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数
2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性
3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的


---



# 谈谈判别式模型和生成式模型


判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。
生成方法：由数据学习联合概率密度分布函数 P（X，Y），然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。
由生成模型可以得到判别模型，但由判别模型得不到生成模型。
常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场
常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机

---

# 线性分类器与非线性分类器的区别以及优劣


线性和非线性是针对，模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2那么就是非线性模型，如果输入是x和X^2则模型是线性的。
线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。
非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。
常见的线性分类器有：LR，贝叶斯分类，单层感知机、线性回归
常见的非线性分类器：决策树、RF、GBDT、多层感知机
SVM两种都有（看线性核还是高斯核）

---

# 请大致对比下plsa和LDA的区别
![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/56.png)
![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/57.png)
更多请参见：《通俗理解LDA主题模型》（链接：http://blog.csdn.net/v_july_v/article/details/41209515）。

---

# 请简要说说EM算法
![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/58.png)
![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/59.png)
本题解析来源：@tornadomeet，链接：http://www.cnblogs.com/tornadomeet/p/3395593.html

---


# 特征比数据量还大时，选择什么样的分类器

线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。

---

# 常见的分类算法有哪些


SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯

---

# 说说常见的优化算法及其优缺点


温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。
简言之
1）随机梯度下降
优点：可以一定程度上解决局部最优解的问题
缺点：收敛速度较慢
2）批量梯度下降
优点：容易陷入局部最优解
缺点：收敛速度较快
3）mini_batch梯度下降
综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。
4）牛顿法
牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算  Hessian矩阵比较困难。
5）拟牛顿法
拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。

具体而言
从每个batch的数据来区分
梯度下降：每次使用全部数据集进行训练
优点：得到的是最优解
缺点：运行速度慢，内存可能不够
随机梯度下降：每次使用一个数据进行训练
优点：训练速度快，无内存问题
缺点：容易震荡，可能达不到最优解
Mini-batch梯度下降
优点：训练速度快，无内存问题，震荡较少
缺点：可能达不到最优解

从优化方法上来分：
随机梯度下降（SGD）
缺点
选择合适的learning	rate比较难	
对于所有的参数使用同样的learning rate	
容易收敛到局部最优
可能困在saddle point
SGD+Momentum
优点：
积累动量，加速训练
局部极值附近震荡时，由于动量，跳出陷阱
梯度方向发生变化时，动量缓解动荡。
Nesterov Mementum
与Mementum类似，优点：
避免前进太快
提高灵敏度
AdaGrad
优点：
控制学习率，每一个分量有各自不同的学习率
适合稀疏数据
缺点
依赖一个全局学习率
学习率设置太大，其影响过于敏感
后期，调整学习率的分母积累的太大，导致学习率很低，提前结束训练。
RMSProp
优点：
解决了后期提前结束的问题。
缺点：
依然依赖全局学习率
Adam
Adagrad和RMSProp的合体
优点：
结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
为不同的参数计算不同的自适应学习率
也适用于大多非凸优化 -	适用于大数据集和高维空间
牛顿法
牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难
拟牛顿法
拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。

---



# 试证明样本空间任意点x到超平面(w，b)的距离为(6.2)
![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/69.png)

---

# >什么是共线性， 跟过拟合有什么关联


共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。
共线性会造成冗余，导致过拟合。
解决方法：排除变量的相关性／加入权重正则。

---

# 对于维度极低的特征，选择线性还是非线性分类器


非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。
1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。

---

# 什么是ill-condition病态问题


训练完的模型，测试样本稍作修改就会得到差别很大的结果，就是病态问题，模型对未知数据的预测能力很差，即泛化误差大。

---


# 常用的聚类划分方式有哪些列举代表算法


1. 基于划分的聚类:K-means，k-medoids，CLARANS。
2. 基于层次的聚类：AGNES（自底向上），DIANA（自上向下）。
3. 基于密度的聚类：DBSACN，OPTICS，BIRCH(CF-Tree)，CURE。
4. 基于网格的方法：STING，WaveCluster。
5. 基于模型的聚类：EM，SOM，COBWEB。

---


# LR和SVM的联系与区别


联系： 
1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 
2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。 
区别： 
1、LR是参数模型，SVM是非参数模型。 
2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。 
3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。 
4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算。 
5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

---

# SVM、LR、决策树的对比

- 模型复杂度：SVM支持核函数，可处理线性非线性问题; LR模型简单，训练速度快，适合处理线性问题; 决策树容易过拟合，需要进行剪枝.
- 损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失
- 数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感
- 数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核

---

# 请比较下EM算法、HMM、CRF

这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。 
（1）EM算法 
　　EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。 
　　注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。 
（2）HMM算法 
　　隐马尔可夫模型是用于标注问题的生成模型。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。 
马尔科夫三个基本问题：
概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法
学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。
预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）
（3）条件随机场CRF 
　　给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。 
　　之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。
（4）HMM和CRF对比 
　　其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。

---

# RF与GBDT之间的区别与联系

1）相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。
2）不同点：
a 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成
b 组成随机森林的树可以并行生成，而GBDT是串行生成
c 随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和
d 随机森林对异常值不敏感，而GBDT对异常值比较敏感
e 随机森林是减少模型的方差，而GBDT是减少模型的偏差
f 随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化




