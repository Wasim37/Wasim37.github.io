---
title: 过拟合与欠拟合
tags:
  - 过拟合
  - 欠拟合
categories:
  - 知识总结
date: 2020-03-02 22:22:00
toc: true
thumbnail: 
recommend: 0
mathjax: true

---


# 过拟合？欠拟合？

**欠拟合**：算法不太符合样本的数据特征
**过拟合**：算法太符合样本的数据特征，对实际生产中的数据特征却无法拟合

过拟合(overfitting)具体现象体现为，随着训练过程的进行，模型复杂度增加，在训练集上的错误率渐渐减小，但是在验证集上的错误率却渐渐增大。

特征过多，特征数量级过大，训练数据过少，都可能导致过度拟合。过拟合会让模型泛化能力变差。

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/4.png)

---

# 防止过拟合的方法

降低过拟合的办法一般如下：

- 通过特征选择/特征降维，使用更简单的模型：特征过多或者过复杂都会导致过拟合

- 数据集扩增：从数据源头获取、根据当前数据生成、数据增强等

- 交叉验证

- 集成方法 Bootstrap/Bagging

- 正则化(Regularization)
 - L2正则化：目标函数中增加所有权重w参数的平方之和， 逼迫所有w尽可能趋向零但不为零. 因为过拟合的时候， 拟合函数需要顾忌每一个点， 最终形成的拟合函数波动很大， 在某些很小的区间里， 函数值的变化很剧烈， 也就是某些w非常大. 为此， L2正则化的加入就惩罚了权重变大的趋势.
 - L1正则化：目标函数中增加所有权重w参数的绝对值之和， 逼迫更多w为零(也就是变稀疏. L2因为其导数也趋0， 奔向零的速度不如L1给力了). 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的特征权重反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为0。

- 随机失活(dropout)
在训练的运行的时候，让神经元以超参数p的概率被激活(也就是1-p的概率被设置为0)， 每个w因此随机参与， 使得任意w都不是不可或缺的， 效果类似于数量巨大的模型集成。

- 逐层归一化(batch normalization)
这个方法给每层的输出都做一次归一化(网络上相当于加了一个线性变换层)， 使得下一层的输入接近高斯分布. 这个方法相当于下一层的w训练时避免了其输入以偏概全， 因而泛化效果非常好. 

- 提前终止(early stopping)
理论上可能的局部极小值数量随参数的数量呈指数增长， 到达某个精确的最小值是不良泛化的一个来源. 实践表明， 追求细粒度极小值具有较高的泛化误差。这是直观的，因为我们通常会希望我们的误差函数是平滑的， 精确的最小值处所见相应误差曲面具有高度不规则性， 而我们的泛化要求减少精确度去获得平滑最小值， 所以很多训练方法都提出了提前终止策略. 典型的方法是根据交叉叉验证提前终止: 若每次训练前， 将训练数据划分为若干份， 取一份为测试集， 其他为训练集， 每次训练完立即拿此次选中的测试集自测. 因为每份都有一次机会当测试集， 所以此方法称之为交叉验证. 交叉验证的错误率最小时可以认为泛化性能最好， 这时候训练错误率虽然还在继续下降， 但也得终止继续训练了.  


---


### 神经网络里的正则化为什么能防止过拟合

- 直观解释
正则化因子设置的足够大的情况下，为了使成本函数最小化，**权重矩阵 W 就会被设置为接近于 0 的值，直观上相当于消除了很多神经元的影响**，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。

- 数学解释
假设神经元中使用的激活函数为g(z) = tanh(z)（sigmoid 同理）。

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/63.png)

在加入正则化项后，当 λ 增大，导致 W[l]减小，Z[l]=W[l]a[l−1]+b[l]便会减小。**由上图可知，在 z 较小（接近于 0）的区域里，tanh(z)函数近似线性**，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。

- 其他解释
在权值 w[L]变小之下，**输入样本 X 随机的变化不会对神经网络模造成过大的影响**，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。

---