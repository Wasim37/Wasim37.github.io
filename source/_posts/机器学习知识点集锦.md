---
title: 机器学习知识点集锦
tags:
  - 机器学习
categories:
  - 知识总结
date: 2019-1-1 22:22:00
toc: false
mathjax: true

---

置顶
<!-- more -->

- [机器学习定义](#机器学习定义)
- [有监督、无监督与半监督学习](#有监督、无监督与半监督学习)
- [判别式模型与生成式模型](#判别式模型与生成式模型)
- [机器学习项目开发流程](#机器学习项目开发流程)


- [各算法损失函数汇总【待整理】](#各算法损失函数汇总)
- [逻辑回归推导](#逻辑回归公式推导)
- SVM推导
- 神经网络推导


- [简单说说特征工程](#简单说说特征工程)
- [机器学习中，有哪些特征选择的工程方法](#机器学习中，有哪些特征选择的工程方法)
- [如何进行特征选择](#如何进行特征选择)
- [你知道有哪些数据处理和特征工程的处理](#你知道有哪些数据处理和特征工程的处理)
- [数据预处理](#数据预处理)
- [数据不平衡问题](#数据不平衡问题)


- [特征向量的归一化方法有哪些](#特征向量的归一化方法有哪些)
- [机器学习中，为何要经常对数据做归一化](#机器学习中，为何要经常对数据做归一化)
- [哪些机器学习算法不需要做归一化处理](#哪些机器学习算法不需要做归一化处理)
- [对于树形结构为什么不需要归一化](#对于树形结构为什么不需要归一化)
- [数据归一化（或者标准化，注意归一化和标准化不同）的原因](#数据归一化（或者标准化，注意归一化和标准化不同）的原因)
- [标准化与归一化的区别](#标准化与归一化的区别)


- [简单介绍下LR](#简单介绍下LR)
- [LR与线性回归的区别与联系](#LR与线性回归的区别与联系)
- [LR模型为什么要使用sigmoid函数](#LR模型为什么要使用sigmoid函数)
- [逻辑回归相关问题](#逻辑回归相关问题)
- [逻辑回归为什么要对特征进行离散化](#逻辑回归为什么要对特征进行离散化)
- [逻辑回归并行化怎么做，有几种并行化方式，读过哪些开源的实现](#逻辑回归并行化怎么做，有几种并行化方式，读过哪些开源的实现)


- [过拟合？欠拟合？](#过拟合？欠拟合？)
- [防止过拟合的方法](#防止过拟合的方法)
- [什么是正则化](#什么是正则化)
- [L1和L2的区别](#L1和L2的区别)
- [L1和L2正则先验分别服从什么分布](#L1和L2正则先验分别服从什么分布)


- [梯度是什么](#梯度是什么)
- [说说梯度下降法](#说说梯度下降法)
- 为什么要用梯度下降法？怎么引出这个概念的
- [梯度下降法容易收敛到局部最优，为什么应用广泛](#梯度下降法容易收敛到局部最优，为什么应用广泛)
- [梯度下降法找到的一定是下降最快的方向么](#梯度下降法找到的一定是下降最快的方向么)
- [请说说随机梯度下降法的问题和挑战](#请说说随机梯度下降法的问题和挑战)
- [什麽造成梯度消失问题?](#什麽造成梯度消失问题?)
- [什么是拟牛顿法（Quasi-Newton Methods）](#什么是拟牛顿法（Quasi-Newton Methods）)
- [牛顿法和梯度下降法有什么不同](#牛顿法和梯度下降法有什么不同)
- [说说共轭梯度法](#说说共轭梯度法)


- [请简要介绍下SVM的原理和推导](#请简要介绍下SVM的原理和推导)
- [支持向量机通俗导论（理解SVM的三层境界）](#支持向量机通俗导论（理解SVM的三层境界）)
- [说说你知道的核函数](#说说你知道的核函数)
- [请说说常用核函数及核函数的条件](#请说说常用核函数及核函数的条件)
- [带核的SVM为什么能分类非线性问题](#带核的SVM为什么能分类非线性问题)


- [简述KNN最近邻分类算法的过程](#简述KNN最近邻分类算法的过程)
- [KNN中的K如何选取的](#KNN中的K如何选取的)


- [kmeans的复杂度](#kmeans的复杂度)
- [优化Kmeans](#优化Kmeans)
- [KMeans初始类簇中心点的选取。](#KMeans初始类簇中心点的选取。)
- [在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别](#在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别)
- [常用的聚类划分方式有哪些列举代表算法。](#常用的聚类划分方式有哪些列举代表算法。)


- [说说常见的损失函数](#说说常见的损失函数)
- [协方差和相关性有什么区别](#协方差和相关性有什么区别)
- [谈谈判别式模型和生成式模型](#谈谈判别式模型和生成式模型)


- [线性分类器与非线性分类器的区别以及优劣](#线性分类器与非线性分类器的区别以及优劣)
- [衡量分类器的好坏](衡量分类器的好坏)
- [对于维度极低的特征，选择线性还是非线性分类器](#对于维度极低的特征，选择线性还是非线性分类器)
- [常见的分类算法有哪些](#常见的分类算法有哪些)
- [特征比数据量还大时，选择什么样的分类器](#特征比数据量还大时，选择什么样的分类器)



- [简单说说贝叶斯定理](#简单说说贝叶斯定理)
- [为什么朴素贝叶斯如此“朴素”](#为什么朴素贝叶斯如此“朴素”)
- [用贝叶斯机率说明Dropout的原理](#用贝叶斯机率说明Dropout的原理)
- [经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“Julw”时，系统会猜测你的意图：是不是要搜索“July”，如下图所示：这叫做拼写检查。根据谷歌一员工写的文章显示，Google的拼写检查基于贝叶斯方法。请说说的你的理解，具体Google是怎么利用贝叶斯方法，实现"拼写检查"的功能。](#经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“Julw”时，系统会猜测你的意图：是不是要搜索“July”，如下图所示：这叫做拼写检查。根据谷歌一员工写的文章显示，Google的拼写检查基于贝叶斯方法。请说说的你的理解，具体Google是怎么利用贝叶斯方法，实现"拼写检查"的功能。)


- [LR与线性回归的区别与联系](#LR与线性回归的区别与联系)
- [LR和SVM的联系与区别](#LR和SVM的联系与区别)
- [SVM、LR、决策树的对比](#SVM、LR、决策树的对比)
- [请比较下EM算法、HMM、CRF](#请比较下EM算法、HMM、CRF)
- [RF与GBDT之间的区别与联系](#RF与GBDT之间的区别与联系)
- [请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么](#请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么)


- [对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法](#对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法)
- [说说常见的优化算法及其优缺点](#说说常见的优化算法及其优缺点)
- [请大致对比下plsa和LDA的区别](#请大致对比下plsa和LDA的区别)
- [请简要说说EM算法](#请简要说说EM算法)
- [什么最小二乘法](#什么最小二乘法)
- [解释对偶的概念](#解释对偶的概念)
- [机器学习和统计里面的auc的物理意义是啥](#机器学习和统计里面的auc的物理意义是啥)
- [观察增益gain， alpha和gamma越大，增益越小](#观察增益gain， alpha和gamma越大，增益越小)
- [试证明样本空间任意点x到超平面(w，b)的距离为(6.2)](#试证明样本空间任意点x到超平面w，b的距离为6.2)
- [什么是共线性， 跟过拟合有什么关联](#什么是共线性， 跟过拟合有什么关联)
- [什么是ill-condition病态问题](#什么是ill-condition病态问题)


- [如何准备机器学习工程师的面试](https://www.zhihu.com/question/23259302)
- [如何判断某个人的机器学习水平](https://www.zhihu.com/question/62482926)

---

### <h2 id="机器学习定义">机器学习定义</h2>
机器学习是一门从数据中研究算法的学科。就是根据已有数据，进行算法选择，并基于算法和数据建模，对未来预测。

美国卡内基梅隆大学机器学习研究领域著名教授 Tom Mitchell 对机器学习的经典定义：对于某给定的任务T，在合理的性能度量方案P的前提下，某计算机程序可以自主学习任务T的经验E; 随着提供合适、优质、大量的经验E，该程序对于任务T的性能逐步提高

其中重要的机器学习对象：任务Task T，一个或多个、经验Experience E、度量性能Performance P。
即：随着任务的不断执行，经验的累积会带来计算机性能的提升。

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/76.png)

---

### <h2 id="机器学习项目开发流程">机器学习项目开发流程</h2>

**1 抽象成数学问题**
机器学习的训练过程很耗时，胡乱尝试时间成本太高，所以明确问题是进行机器学习的第一步。
抽象成数学问题，指的明确我们可以获得什么样的数据，目标问题属于分类、回归还是聚类，如果都不是，如何转换为其中的某类问题。

**2 获取数据**
数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
数据要有代表性，否则必然会过拟合。而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。
对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。

**3 特征预处理与特征选择**
良好的数据要能够提取出良好的特征才能真正发挥效力。
特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。

筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。

**4 训练模型与调优**
直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。

**5 模型诊断**
如何确定模型调优的方向与思路呢这就需要对模型进行诊断的技术。
过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。

误差分析，也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……
诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。

**6 模型融合**
一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。

**7 上线运行**
这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。

---

### <h2 id="有监督、无监督与半监督学习">有监督、无监督与半监督学习</h2>
- **有监督学习**：
对有标记的训练样本进行学习，对训练样本外的数据进行预测。
如 LR（Logistic Regression），SVM（Support Vector Machine），RF（RandomForest），GBDT（Gradient Boosting Decision Tree），感知机（Perceptron）、BP神经网络（Back Propagation）等。

- **无监督学习**：
对未标记的样本进行训练学习，试图发现样本中的内在结构。
如聚类(KMeans)、降维、文本处理、DL。

- **半监督学习(Semi-Supervised Learning，SSL)**：
主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类，是有监督学习和无监督学习的结合。
半监督学习对于减少标注代价，提高学习机器性能具有重大实际意义。

---

### <h2 id="判别式模型与生成式模型">判别式模型与生成式模型</h2>
**判别式模型(Discriminative Model)**：直接对**条件概率p(y|x)**进行建模，常见判别模型有：
线性回归、决策树、支持向量机SVM、k近邻、神经网络等；

**生成式模型(Generative Model)**：对**联合分布概率p(x,y)**进行建模，常见生成式模型有：
隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等；

生成式模型更普适，判别式模型更直接，目标性更强。
生成式模型关注数据是如何产生的，寻找的是数据分布模型。
判别式模型关注的数据的差异性，寻找的是分类面。
由生成式模型可以产生判别是模型，但是由判别式模式没法形成生成式模型

---

### <h2 id="各算法损失函数汇总">各算法损失函数汇总</h2>
首先区分一个概念，损失函数针对的是单个样本，代价函数或者成本函数针对的是全体样本。
待总结。

---

### <h2 id="简单说说特征工程">简单说说特征工程</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/66.png)

---

### <h2 id="机器学习中，有哪些特征选择的工程方法">机器学习中，有哪些特征选择的工程方法</h2>
数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已
- 计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了；
- 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征；
- 通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验；
- 训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；
- 通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型。
- 通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。

---

### <h2 id="如何进行特征选择">如何进行特征选择</h2>


特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解

常见的特征选择方式：
- 去除方差较小的特征
- 正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。
- 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。
- 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。

---

### <h2 id="你知道有哪些数据处理和特征工程的处理">你知道有哪些数据处理和特征工程的处理</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/67.png)

---

### <h2 id="数据预处理">数据预处理</h2>
1、特征向量的缺失值处理:
1）缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的noise。
2）缺失值较少，其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理:
- 把NaN直接作为一个特征，假设用0表示；
- 连续值，用均值填充；
- 用随机森林等算法预测填充；

2、连续值：离散化。有的模型（如决策树）需要离散值
3、对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。如图像操作
4、皮尔逊相关系数，去除高度相关的列

---

### <h2 id="请简要介绍下SVM的原理和推导">请简要介绍下SVM的原理和推导</h2>
SVM，全称是support vector machine，中文名叫支持向量机。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。

此外，这里有个视频也是关于SVM的推导：《纯白板手推SVM》，链接：http://www.julyedu.com/video/play/18/429

---

### <h2 id="支持向量机通俗导论（理解SVM的三层境界）">支持向量机通俗导论（理解SVM的三层境界）</h2>
在线阅读链接：http://blog.csdn.net/v_july_v/article/details/7624837
网盘下载地址：https://pan.baidu.com/s/1htfvbzI 密码：qian
建议下载网盘里的pdf阅读，文档附带完整书签。

---

### <h2 id="说说你知道的核函数">说说你知道的核函数</h2>

通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/25.png)
多项式核，显然刚才我们举的例子是这里多项式核的一个特例（R = 1，d = 2）。虽然比较麻烦，而且没有必要，不过这个核所对应的映射实际上是可以写出来的，该空间的维度是![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/26.png)，其中是原始空间的维度。
高斯核![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/27.png)，这个核就是最开始提到过的会将原始空间映射为无穷维空间的那个家伙。不过，如果选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。下图所示的例子便是把低维线性不可分的数据通过高斯核函数映射到了高维空间：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/28.png)
线性核，这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了(意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的)。

---

### <h2 id="带核的SVM为什么能分类非线性问题">带核的SVM为什么能分类非线性问题</h2> 


核函数的本质是两个函数的內积，通过核函数将其隐射到高维空间，在高维空间非线性问题转化为线性问题， SVM得到超平面是高维空间的线性分类平面， 如图:
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/70.png)
 其分类结果也视为低维空间的非线性分类结果， 因而带核的SVM就能分类非线性问题。

---

### <h2 id="哪些机器学习算法不需要做归一化处理">哪些机器学习算法不需要做归一化处理</h2>

概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。

我理解归一化和标准化主要是为了使计算更方便。比如两个变量的量纲不同，可能一个的数值远大于另一个，那么他们同时作为变量的时候，可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确，或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能，量纲也需要调整，所以我估计lr 和 knn 保准话一下应该有好处。至于其他的算法 我也觉得如果变量量纲差距很大的话

---

### <h2 id="对于树形结构为什么不需要归一化">对于树形结构为什么不需要归一化</h2>

数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。

对于线性模型，比如说LR，我有两个特征，一个是(0，1)的，一个是(0，10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。

另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。

---

### <h2 id="在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别">在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别</h2>


欧氏距离，最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 x = (x1，...，xn) 和 y = (y1，...，yn) 之间的距离为：

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/1.png)

欧氏距离虽然很有用，但也有明显的缺点。它将样品的不同属性（即各指标或各变量量纲）之间的差别等同看待，这一点有时不能满足实际要求。例如，在教育研究中，经常遇到对人的分析和判别，个体的不同属性对于区分个体有着不同的重要性。因此，欧氏距离适用于向量各分量的度量标准统一的情况。
曼哈顿距离，我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标（x1， y1）的点P1与坐标（x2， y2）的点P2的曼哈顿距离为：，要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。当坐标轴变动时，点间的距离就会不同。
通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。
曼哈顿距离和欧式距离一般用途不同，无相互替代性。另，关于各种距离的比较参看《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》（链接：http://blog.csdn.net/v_july_v/article/details/8203674）。

---

### <h2 id="数据归一化（或者标准化，注意归一化和标准化不同）的原因">数据归一化（或者标准化，注意归一化和标准化不同）的原因</h2>



要强调：能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。

有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。
有些模型伸缩有与原来等价，如：LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最坏进行数据归一化。

本题解析来源：@我愛大泡泡，链接：http://blog.csdn.net/woaidapaopao/article/details/77806273
补充：其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。

---


### <h2 id="逻辑回归为什么要对特征进行离散化">逻辑回归为什么要对特征进行离散化</h2>


在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：
0. 离散特征的增加和减少都很容易，易于模型的快速迭代；
1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。
本题解析来源：@严林，链接：https://www.zhihu.com/question/31989952

---

### <h2 id="LR模型为什么要使用sigmoid函数">LR模型为什么要使用sigmoid函数</h2>
假设W已知，现有函数 P(y|x) = f(wx)，求分布f，使得在正样本里 P(y=1|w,x)尽可能大，在负样本里尽可能小。根据最大熵原则，我们知道所有可能的分布模型集合中，熵最大的模型是最好的模型。即对未知分布最合理的推断就是符合已有前提下最不确定或最随机的推断。又已知LR符合伯努利分布，将伯努利分布转换为指数分布的过程中，可以得到sigmod函数，这就是LR的理论基础。

- [LR模型使用sigmoid函数背后的数学原理是什么？](https://www.zhihu.com/question/35322351)
- [广义线性模型](https://zhuanlan.zhihu.com/p/24967776)
- [什么是最大熵](#什么是最大熵)

---

### <h2 id="逻辑回归公式推导">逻辑回归公式推导</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/2.png)
http://blog.csdn.net/oldbai001/article/details/49872433
http://blog.csdn.net/programmer_wei/article/details/52072939

---

### <h2 id="逻辑回归并行化怎么做，有几种并行化方式，读过哪些开源的实现">逻辑回归并行化怎么做，有几种并行化方式，读过哪些开源的实现</h2>
http://www.csdn.net/article/2014-02-13/2818400-2014-02-13

---

### <h2 id="逻辑回归相关问题">逻辑回归相关问题</h2>


（3）L1-norm和L2-norm 
　　其实稀疏的根本还是在于L0-norm也就是直接统计参数不为0的个数作为规则项，但实际上却不好执行于是引入了L1-norm；而L1norm本质上是假设参数先验是服从Laplace分布的，而L2-norm是假设参数先验为Gaussian分布，我们在网上看到的通常用图像来解答这个问题的原理就在这。 
　　但是L1-norm的求解比较困难，可以用坐标轴下降法或是最小角回归法求解。

（4）LR和SVM对比 
　　首先，LR和SVM最大的区别在于损失函数的选择，LR的损失函数为Log损失（或者说是逻辑损失都可以）、而SVM的损失函数为hinge loss。 
 
　　其次，两者都是线性模型。 
　　最后，SVM只考虑支持向量（也就是和分类相关的少数点） 

（5）LR和随机森林区别 
　　随机森林等树算法都是非线性的，而LR是线性的。LR更侧重全局优化，而树模型主要是局部的优化。 

（6）常用的优化方法 
　　逻辑回归本身是可以用公式求解的，但是因为需要求逆的复杂度太高，所以才引入了梯度下降算法。 
　　一阶方法：梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。 

　　二阶方法：牛顿法、拟牛顿法： 
　　这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。
    缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。 

拟牛顿法：不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。

---

### <h2 id="简单介绍下LR">简单介绍下LR</h2>
1）[吴恩达机器学习逻辑回归总结](http://www.ai-start.com/ml2014/week3.html)
2）[Logistic Regression 的前世今生](http://blog.csdn.net/cyh_24/article/details/50359055)
3）[机器学习算法与Python实践之七-逻辑回归](http://blog.csdn.net/zouxy09/article/details/20319673)


![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/52.png)
把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。

---

### <h2 id="过拟合？欠拟合？">过拟合？欠拟合？</h2>

**欠拟合**：算法不太符合样本的数据特征
**过拟合**：算法太符合样本的数据特征，对实际生产中的数据特征却无法拟合

过拟合(overfitting)具体现象体现为，随着训练过程的进行，模型复杂度增加，在训练集上的错误率渐渐减小，但是在验证集上的错误率却渐渐增大。

特征过多，特征数量级过大，训练数据过少，都可能导致过度拟合。过拟合会让模型泛化能力变差。

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/4.png)

---

### <h2 id="防止过拟合的方法">防止过拟合的方法</h2>

降低过拟合的办法一般如下：
- 正则化(Regularization)
 - L2正则化：目标函数中增加所有权重w参数的平方之和， 逼迫所有w尽可能趋向零但不为零. 因为过拟合的时候， 拟合函数需要顾忌每一个点， 最终形成的拟合函数波动很大， 在某些很小的区间里， 函数值的变化很剧烈， 也就是某些w非常大. 为此， L2正则化的加入就惩罚了权重变大的趋势.
 - L1正则化：目标函数中增加所有权重w参数的绝对值之和， 逼迫更多w为零(也就是变稀疏. L2因为其导数也趋0， 奔向零的速度不如L1给力了). 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的特征权重反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为0。
- 随机失活(dropout)
在训练的运行的时候，让神经元以超参数p的概率被激活(也就是1-p的概率被设置为0)， 每个w因此随机参与， 使得任意w都不是不可或缺的， 效果类似于数量巨大的模型集成。
- 逐层归一化(batch normalization)
这个方法给每层的输出都做一次归一化(网络上相当于加了一个线性变换层)， 使得下一层的输入接近高斯分布. 这个方法相当于下一层的w训练时避免了其输入以偏概全， 因而泛化效果非常好. 
- 提前终止(early stopping)
理论上可能的局部极小值数量随参数的数量呈指数增长， 到达某个精确的最小值是不良泛化的一个来源. 实践表明， 追求细粒度极小值具有较高的泛化误差。这是直观的，因为我们通常会希望我们的误差函数是平滑的， 精确的最小值处所见相应误差曲面具有高度不规则性， 而我们的泛化要求减少精确度去获得平滑最小值， 所以很多训练方法都提出了提前终止策略. 典型的方法是根据交叉叉验证提前终止: 若每次训练前， 将训练数据划分为若干份， 取一份为测试集， 其他为训练集， 每次训练完立即拿此次选中的测试集自测. 因为每份都有一次机会当测试集， 所以此方法称之为交叉验证. 交叉验证的错误率最小时可以认为泛化性能最好， 这时候训练错误率虽然还在继续下降， 但也得终止继续训练了.  
- 数据集扩增：原有数据增加、原有数据加随机噪声、重采样
- 交叉验证
- 特征选择/特征降维

---


### <h2 id="LR和SVM的联系与区别">LR和SVM的联系与区别</h2>


联系： 
1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 
2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。 
区别： 
1、LR是参数模型，SVM是非参数模型。 
2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。 
3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。 
4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算。 
5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。
本题解析来源：@朝阳在望http://blog.csdn.net/timcompp/article/details/62237986

---

### <h2 id="梯度是什么">梯度是什么</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/73.png)

---

### <h2 id="说说梯度下降法">说说梯度下降法</h2>

下面是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/5.png)
我们用X1，X2..Xn 去描述feature里面的分量，比如x1=房间的面积，x2=房间的朝向等等，我们可以做出一个估计函数：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/6.png)
θ在这儿称为参数，在这儿的意思是调整feature中每个分量的影响力，就是到底是房屋的面积更重要还是房屋的地段更重要。为了如果我们令X0 = 1，就可以用向量的方式来表示了：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/7.png)

我们程序也需要一个机制去评估我们θ是否比较好，所以说需要对我们做出的h函数进行评估，一般这个进行评估的函数称为损失函数（loss function），描述h函数不好的程度，在下面，我们称这个函数为J函数
在这儿我们可以做出下面的一个损失函数：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/8.png)

换言之，我们把对x(i)的估计值与真实值y(i)差的平方和作为损失函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了。

如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)，是一种完全是数学描述的方法，另外一种就是梯度下降法。

梯度下降法的算法流程如下：
1）首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。
2）改变θ的值，使得J(θ)按梯度下降的方向进行减少。

为了描述的更清楚，给出下面的图：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/9.png)

这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取值，我们需要的是，能够让J(θ)的值尽量的低，也就是达到深蓝色的部分。θ0，θ1表示θ向量的两个维度。
在上面提到梯度下降法的第一步是给θ给一个初值，假设随机给的初值是在图上的十字点。
然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如下图所示，算法的结束将是在θ下降到无法继续下降为止。
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/10.png)

当然，可能梯度下降的最终点并非是全局最小点，即也可能是一个局部最小点，如下图所示：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/11.png)

上面这张图就是描述的一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。
下面我将用一个例子描述一下梯度减少的过程，对于我们的函数J(θ)求偏导J：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/12.png)

下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/13.png)

一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。

用更简单的数学语言进行描述步骤2）是这样的：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/14.png)

本题解析来源：@LeftNotEasy，链接：http://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html

---

### <h2 id="梯度下降法容易收敛到局部最优，为什么应用广泛">梯度下降法容易收敛到局部最优，为什么应用广泛</h2>

深度神经网络“容易收敛到局部最优”，很可能是一种想象，实际情况是，我们可能从来没有找到过“局部最优”，更别说全局最优了。
很多人都有一种看法，就是“局部最优是神经网络优化的主要难点”。这来源于一维优化问题的直观想象。在单变量的情形下，优化问题最直观的困难就是有很多局部极值，如
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/74.png)

　　人们直观的想象，高维的时候这样的局部极值会更多，指数级的增加，于是优化到全局最优就更难了。然而单变量到多变量一个重要差异是，单变量的时候，Hessian矩阵只有一个特征值，于是无论这个特征值的符号正负，一个临界点都是局部极值。但是在多变量的时候，Hessian有多个不同的特征值，这时候各个特征值就可能会有更复杂的分布，如有正有负的不定型和有多个退化特征值（零特征值）的半定型
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/75.png)

　　在后两种情况下，是很难找到局部极值的，更别说全局最优了。
　　现在看来，神经网络的训练的困难主要是鞍点的问题。在实际中，我们很可能也从来没有真的遇到过局部极值。Bengio组这篇文章Eigenvalues of the Hessian in Deep Learning（https://arxiv.org/abs/1611.07476）里面的实验研究给出以下的结论：
• Training stops at a point that has a small gradient. The norm of the gradient is not zero, therefore it does not, technically speaking, converge to a critical point.
• There are still negative eigenvalues even when they are small in magnitude.
　　另一方面，一个好消息是，即使有局部极值，具有较差的loss的局部极值的吸引域也是很小的Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes。（https://arxiv.org/abs/1706.10239）
For the landscape of loss function for deep networks, the volume of basin of attraction of good minima dominates over that of poor minima, which guarantees optimization methods with random initialization to converge to good minima.
　　所以，很可能我们实际上是在“什么也没找到”的情况下就停止了训练，然后拿到测试集上试试，“咦，效果还不错”。
　　补充说明，这些都是实验研究结果。理论方面，各种假设下，深度神经网络的Landscape 的鞍点数目指数增加，而具有较差loss的局部极值非常少。

本题解析来源：@李振华，链接：https://www.zhihu.com/question/68109802/answer/262143638

---

### <h2 id="牛顿法和梯度下降法有什么不同">牛顿法和梯度下降法有什么不同</h2>


1）牛顿法（Newton's method）
牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。
具体步骤：
首先，选择一个接近函数 f (x)零点的 x0，计算相应的 f (x0) 和切线斜率f  ' (x0)（这里f ' 表示函数 f  的导数）。然后我们计算穿过点(x0，  f  (x0)) 并且斜率为f '(x0)的直线和 x 轴的交点的x坐标，也就是求如下方程的解：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/15.png)

我们将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f  (x) = 0的解。因此我们现在可以利用x1开始下一轮迭代。迭代公式可化简为如下所示：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/16.png)

已经证明，如果f  ' 是连续的，并且待求的零点x是孤立的，那么在零点x周围存在一个区域，只要初始值x0位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果f  ' (x)不为0， 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。
由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法的搜索路径（二维情况）如下图所示：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/17.png)

关于牛顿法和梯度下降法的效率对比：
a）从收敛速度上看 ，牛顿法是二阶收敛，梯度下降是一阶收敛，前者牛顿法收敛速度更快。但牛顿法仍然是局部算法，只是在局部上看的更细致，梯度法仅考虑方向，牛顿法不但考虑了方向还兼顾了步子的大小，其对步长的估计使用的是二阶逼近。
b）根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/18.png)

注：红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。
牛顿法的优缺点总结：
	优点：二阶收敛，收敛速度快；
	缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
本题解析来源：@wtq1993，链接：http://blog.csdn.net/wtq1993/article/details/51607040

---

### <h2 id="什么是拟牛顿法（Quasi-Newton Methods）">什么是拟牛顿法（Quasi-Newton Methods）</h2>


拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

具体步骤：
拟牛顿法的基本思想如下。首先构造目标函数在当前迭代xk的二次模型：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/33.png)

这里Bk是一个对称正定矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/29.png)

其中我们要求步长ak 满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hessian矩阵Bk 代替真实的Hessian矩阵。所以拟牛顿法最关键的地方就是每一步迭代中矩阵Bk 的更新。现在假设得到一个新的迭代xk+1，并得到一个新的二次模型：   
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/30.png)

我们尽可能地利用上一步的信息来选取Bk。具体地，我们要求
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/31.png)

从而得到
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/32.png)

这个公式被称为割线方程。常用的拟牛顿法有DFP算法和BFGS算法。
本题解析来源：@wtq1993，链接：http://blog.csdn.net/wtq1993/article/details/51607040

---

### <h2 id="kmeans的复杂度">kmeans的复杂度</h2>

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/34.png)

时间复杂度：O(tKmn)，其中，t为迭代次数，K为簇的数目，m为记录数，n为维数
空间复杂度：O((m+K)n)，其中，K为簇的数目，m为记录数，n为维数

---

### <h2 id="请说说随机梯度下降法的问题和挑战">请说说随机梯度下降法的问题和挑战</h2>

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/35.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/36.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/37.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/38.png)

那到底如何优化随机梯度法呢详情请点击：论文公开课第一期：详解梯度下降等各类优化算法（含视频和PPT下载）（链接：https://ask.julyedu.com/question/7913）

---

### <h2 id="说说共轭梯度法">说说共轭梯度法</h2>

共轭：两个概率分布如果具有相同的形式，我们就说它们是共轭的

共轭梯度法是介于梯度下降法（最速下降法）与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hessian矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有逐步收敛性，稳定性高，而且不需要任何外来参数。

下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/39.png)

注：绿色为梯度下降法，红色代表共轭梯度法
本题解析来源： @wtq1993，链接：http://blog.csdn.net/wtq1993/article/details/51607040

---

### <h2 id="对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法">对所有优化问题来说， 有没有可能找到比現在已知算法更好的算法</h2>


![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/40.png)
没有免费的午餐定理：
对于训练样本（黑点），不同的算法A/B在不同的测试样本（白点）中有不同的表现，这表示：对于一个学习算法A，若它在某些问题上比学习算法 B更好，则必然存在一些问题，在那里B比A好。
也就是说：对于所有问题，无论学习算法A多聪明，学习算法 B多笨拙，它们的期望性能相同。
但是：没有免费午餐定力假设所有问题出现几率相同，实际应用中，不同的场景，会有不同的问题分布，所以，在优化算法时，针对具体问题进行分析，是算法优化的核心所在。
本题解析来源：@抽象猴，链接：https://www.zhihu.com/question/41233373/answer/145404190

---

### <h2 id="LR与线性回归的区别与联系">LR与线性回归的区别与联系</h2>

引用自：@AntZ
LR 工业上一般指Logistic Regression（逻辑回归）而不是Linear Regression（线性回归）。LR在线性回归的实数范围输出值上施加 sigmoid 函数将值收敛到 0~1 范围，其目标函数也因此从差平方和函数变为对数损失函数，以提供最优化所需导数（sigmoid函数是softmax函数的二元特例， 其导数均为函数值的f*(1-f)形式）。

请注意， LR 往往是解决二元 0/1 分类问题的， 只是它和线性回归耦合太紧， 不自觉也冠了个回归的名字。若要求多元分类，就要把sigmoid换成大名鼎鼎的softmax了。

引用自：@nishizhen
个人感觉逻辑回归和线性回归首先都是广义的线性回归，
其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，
另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0，1]。逻辑回归就是一种减小预测范围，将预测值限定为[0，1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。

引用自：@乖乖癞皮狗
逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。

---

### <h2 id="请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么">请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么</h2>


集成学习的集成对象是学习器. Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器， 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器， 迭代集成(平滑加权).
决策树属于最常用的学习器， 其学习过程是从根建立树， 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂， CART决策树用基尼指数计算最优分裂， xgboost决策树使用二阶泰勒展开系数计算最优分裂.
下面所提到的学习器都是决策树:
Bagging方法: 
    学习器间不存在强依赖关系， 学习器可并行训练生成， 集成方式一般为投票;
    Random Forest属于Bagging的代表， 放回抽样， 每个学习器随机选择部分特征去优化;
Boosting方法: 
   学习器之间存在强依赖关系、必须串行生成， 集成方式为加权和;
    Adaboost属于Boosting， 采用指数损失函数替代原本分类任务的0/1损失函数;
    GBDT属于Boosting的优秀代表， 对函数残差近似值进行梯度下降， 用CART回归树做学习器， 集成为回归模型;
    xgboost属于Boosting的集大成者， 对函数残差近似值进行梯度下降， 迭代时利用了二阶梯度信息， 集成模型可分类也可回归. 由于它可在特征粒度上并行计算， 结构风险和工程实现都做了很多优化， 泛化， 性能和扩展性都比GBDT要好。
关于决策树，这里有篇《决策树算法》（链接：http://blog.csdn.net/v_july_v/article/details/7577684）。而随机森林Random Forest是一个包含多个决策树的分类器。至于AdaBoost，则是英文"Adaptive Boosting"（自适应增强）的缩写，关于AdaBoost可以看下这篇文章《Adaboost 算法的原理与推导》。GBDT（Gradient Boosting Decision Tree），即梯度上升决策树算法，相当于融合决策树和梯度上升boosting算法。
引用自：@AntZ
xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：
1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数
2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性
3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的
引用自：@Xijun LI

---

### <h2 id="什么是正则化">什么是正则化</h2>
正则化是在成本函数中加入一个正则化项，惩罚模型的复杂度。是针对过拟合而提出的。

**Logistic 回归中的正则化**
对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数：
$$J(w,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}{||w||}^2_2$$

- L2 正则化：
$$\frac{\lambda}{2m}{||w||}^2_2 = \frac{\lambda}{2m}\sum_{j=1}^{n_x}w^2_j = \frac{\lambda}{2m}w^Tw$$
- L1 正则化：
$$\frac{\lambda}{2m}{||w||}_1 = \frac{\lambda}{2m}\sum_{j=1}^{n_x}{|w_j|}$$

其中，λ 为正则化因子，是超参数。
由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。
注意，lambda在 Python 中属于保留字，所以在编程的时候，用lambd代替这里的正则化因子。

---

### <h2 id="正则化为什么可以防止过拟合">正则化为什么可以防止过拟合</h2>
正则化是在成本函数中加入一个正则化项，惩罚模型的复杂度。是针对过拟合而提出的。

- 直观解释
正则化因子设置的足够大的情况下，为了使成本函数最小化，相应的参数就会被调小，甚至接近于0值，直观上相当于消除了很多特征值的影响。而且一般情况下参数越小(或者特征值越少)，可以理解为函数越光滑，那么对于的模型也越简单。有效、简单，奥卡姆剃刀原理。

- 其他解释
在权值 w变小之下，**输入样本 X 随机的变化不会对模型造成过大的影响**，模型受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。

- 神经网络防止过拟合还有一种数学解释，即每层权重变小，在 z 较小（接近于 0）的区域里，tanh(z)函数就近似线性，所以每层的函数就近似线性函数，所以防止过拟合。

---

### <h2 id="说说常见的损失函数">说说常见的损失函数</h2>


对于给定的输入X，由f(X)给出相应的输出Y，这个输出的预测值f(X)与真实值Y可能一致也可能不一致（要知道，有时损失或误差是不可避免的），用一个损失函数来度量预测错误的程度。损失函数记为L(Y， f(X))。

常用的损失函数有以下几种（基本引用自《统计学习方法》）：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/44.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/45.png)

如此，SVM有第二种理解，即最优化+损失最小，或如@夏粉_百度所说“可从损失函数和优化算法角度看SVM，boosting，LR等算法，可能会有不同收获”。

关于SVM的更多理解请参考：，链接：

---

### <h2 id="协方差和相关性有什么区别">协方差和相关性有什么区别</h2>


相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/46.png)
为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/47.png)

---


### <h2 id="谈谈判别式模型和生成式模型">谈谈判别式模型和生成式模型</h2>


判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。
生成方法：由数据学习联合概率密度分布函数 P（X，Y），然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。
由生成模型可以得到判别模型，但由判别模型得不到生成模型。
常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场
常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机

---

### <h2 id="线性分类器与非线性分类器的区别以及优劣">线性分类器与非线性分类器的区别以及优劣</h2>


线性和非线性是针对，模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2那么就是非线性模型，如果输入是x和X^2则模型是线性的。
线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。
非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。
常见的线性分类器有：LR，贝叶斯分类，单层感知机、线性回归
常见的非线性分类器：决策树、RF、GBDT、多层感知机
SVM两种都有（看线性核还是高斯核）
引用自@伟祺

---

### <h2 id="L1和L2的区别">L1和L2的区别</h2>


L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 
比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|.
简单总结一下就是： 
L1范数: 为x向量各个元素绝对值之和。 
L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或者Frobenius范数 
Lp范数: 为x向量各个元素绝对值p次方和的1/p次方.
在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。 
L1范数可以使权值稀疏，方便特征提取。 
L2范数可以防止过拟合，提升模型的泛化能力。

L1和L2的差别，为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢看导数一个是1一个是w便知， 在靠进零附近， L1以匀速下降到零， 而L2则完全停下来了. 这说明L1是将不重要的特征(或者说， 重要性不在一个数量级上)尽快剔除， L2则是把特征贡献尽量压缩最小但不至于为零. 两者一起作用， 就是把重要性在一个数量级(重要性最高的)的那些特征一起平等共事(简言之， 不养闲人也不要超人)。
引用自：@AntZ

---

### <h2 id="L1和L2正则先验分别服从什么分布">L1和L2正则先验分别服从什么分布</h2>


面试中遇到的，L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。
引用自：@齐同学
先验就是优化的起跑线， 有先验的好处就是可以在较小的数据集中有良好的泛化性能，当然这是在先验分布是接近真实分布的情况下得到的了，从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。
对参数引入高斯正态先验分布相当于L2正则化， 这个大家都熟悉：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/48.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/49.png)
对参数引入拉普拉斯先验等价于 L1正则化， 如下图：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/50.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/51.png)
从上面两图可以看出， L2先验趋向零周围， L1先验趋向零本身。
引用自：@AntZ

---

### <h2 id="经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“Julw”时，系统会猜测你的意图：是不是要搜索“July”，如下图所示：这叫做拼写检查。根据谷歌一员工写的文章显示，Google的拼写检查基于贝叶斯方法。请说说的你的理解，具体Google是怎么利用贝叶斯方法，实现"拼写检查"的功能。">经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，比如当你在Google中输入“Julw”时，系统会猜测你的意图：是不是要搜索“July”，如下图所示：这叫做拼写检查。根据谷歌一员工写的文章显示，Google的拼写检查基于贝叶斯方法。请说说的你的理解，具体Google是怎么利用贝叶斯方法，实现"拼写检查"的功能。</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/55.png)


---

### <h2 id="为什么朴素贝叶斯如此“朴素”">为什么朴素贝叶斯如此“朴素”</h2>


因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。

朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是"很简单很天真"地假设样本特征彼此独立. 这个假设现实中基本上不存在， 但特征相关性很小的实际情况还是很多的， 所以这个模型仍然能够工作得很好。
引用自：@AntZ

---

### <h2 id="请大致对比下plsa和LDA的区别">请大致对比下plsa和LDA的区别</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/56.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/57.png)
更多请参见：《通俗理解LDA主题模型》（链接：http://blog.csdn.net/v_july_v/article/details/41209515）。

---

### <h2 id="请简要说说EM算法">请简要说说EM算法</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/58.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/59.png)
本题解析来源：@tornadomeet，链接：http://www.cnblogs.com/tornadomeet/p/3395593.html

---

### <h2 id="KNN中的K如何选取的">KNN中的K如何选取的</h2>


关于什么是KNN，可以查看此文：《从K近邻算法、距离度量谈到KD树、SIFT+BBF算法》（链接：http://blog.csdn.net/v_july_v/article/details/8203674）。KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说：

如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；
如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。
K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。
    
在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。

---

### <h2 id="机器学习中，为何要经常对数据做归一化">机器学习中，为何要经常对数据做归一化</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/60.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/61.png)
本题解析来源：@zhanlijun，链接：http://www.cnblogs.com/LBSer/p/4440590.html

---

### <h2 id="什么最小二乘法">什么最小二乘法</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/62.png)
对了，最小二乘法跟SVM有什么联系呢请参见《支持向量机通俗导论（理解SVM的三层境界）》(链接：http://blog.csdn.net/v_july_v/article/details/7624837）。


---

### <h2 id="梯度下降法找到的一定是下降最快的方向么">梯度下降法找到的一定是下降最快的方向么</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/63.png)

---

### <h2 id="简单说说贝叶斯定理">简单说说贝叶斯定理</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/64.png)
所以，贝叶斯公式可以直接根据条件概率的定义直接推出。即因为P(A，B) = P(A)P(B|A) = P(B)P(A|B)，所以P(A|B) = P(A)P(B|A)  / P(B)。更多请参见此文：《从贝叶斯方法谈到贝叶斯网络》（链接：http://blog.csdn.net/v_july_v/article/details/40984699）。

---

### <h2 id="标准化与归一化的区别">标准化与归一化的区别</h2>

简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：

---

### <h2 id="优化Kmeans">优化Kmeans</h2>


使用kd树或者ball tree
将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。

---

### <h2 id="KMeans初始类簇中心点的选取。">KMeans初始类簇中心点的选取。</h2>


k-means++算法选择初始seeds的基本思想就是：初始的聚类中心之间的相互距离要尽可能的远。
1. 从输入的数据点集合中随机选择一个点作为第一个聚类中心
2. 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)
3. 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大
4. 重复2和3直到k个聚类中心被选出来
5. 利用这k个初始的聚类中心来运行标准的k-means算法

---

### <h2 id="解释对偶的概念">解释对偶的概念</h2>


一个优化问题可以从两个角度进行考察，一个是primal 问题，一个是dual 问题，就是对偶问题，一般情况下对偶问题给出主问题最优值的下界，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下界，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。

---

### <h2 id="衡量分类器的好坏">衡量分类器的好坏</h2>


　这里首先要知道TP、FN（真的判成假的）、FP（假的判成真）、TN四种（可以画一个表格）。 
几种常用的指标：

精度precision = TP/(TP+FP) = TP/~P （~p为预测为真的数量）
召回率 recall = TP/(TP+FN) = TP/ P
F1值： 2/F1 = 1/recall + 1/precision
ROC曲线：ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR， true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N
更详细请点击：https://siyaozhang.github.io/2017/04/04/%E5%87%86%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8
解析来源：@我愛大泡泡，链接：http://blog.csdn.net/woaidapaopao/article/details/77806273

---

### <h2 id="机器学习和统计里面的auc的物理意义是啥">机器学习和统计里面的auc的物理意义是啥</h2>


解析链接：https://www.zhihu.com/question/39840928

---

### <h2 id="观察增益gain， alpha和gamma越大，增益越小">观察增益gain， alpha和gamma越大，增益越小</h2>


xgboost寻找分割点的标准是最大化gain. 考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中计算Gain按最大值找出最佳的分割点。它的计算公式分为四项， 可以由正则化项参数调整(lamda为叶子权重平方和的系数， gama为叶子数量):



第一项是假设分割的左孩子的权重分数， 第二项为右孩子， 第三项为不分割总体分数， 最后一项为引入一个节点的复杂度损失
由公式可知， gama越大gain越小， lamda越大， gain可能小也可能大.
原问题是alpha而不是lambda， 这里paper上没有提到， xgboost实现上有这个参数. 上面是我从paper上理解的答案，下面是搜索到的:
https://zhidao.baidu.com/question/2121727290086699747.html?fr=iks&word=xgboost+lamda&ie=gbk
lambda[默认1]权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。11、alpha[默认1]权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。
gamma[默认0]在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。
引用自@AntZ

---

### <h2 id="什麽造成梯度消失问题?">什麽造成梯度消失问题?</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/65.png)

---

### <h2 id="数据不平衡问题">数据不平衡问题</h2>

解决方法如下：
- 采样，对小样本加噪声采样，对大样本进行下采样
- 数据生成，利用已知样本生成新的样本
- 进行特殊的加权，如在Adaboost中或者SVM中
- 采用对不平衡数据集不敏感的算法
- 改变评价标准：用AUC/ROC来进行评价
- 采用Bagging/Boosting/ensemble等方法
- 在设计模型的时候考虑数据的先验分布

---

### <h2 id="特征比数据量还大时，选择什么样的分类器">特征比数据量还大时，选择什么样的分类器</h2>


线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分。
来源：
http://blog.sina.com.cn/s/blog_178bcad000102x70r.html 

---

### <h2 id="常见的分类算法有哪些">常见的分类算法有哪些</h2>


SVM、神经网络、随机森林、逻辑回归、KNN、贝叶斯

---

### <h2 id="说说常见的优化算法及其优缺点">说说常见的优化算法及其优缺点</h2>


温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。
简言之
1）随机梯度下降
优点：可以一定程度上解决局部最优解的问题
缺点：收敛速度较慢
2）批量梯度下降
优点：容易陷入局部最优解
缺点：收敛速度较快
3）mini_batch梯度下降
综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。
4）牛顿法
牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算  Hessian矩阵比较困难。
5）拟牛顿法
拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。

具体而言
从每个batch的数据来区分
梯度下降：每次使用全部数据集进行训练
优点：得到的是最优解
缺点：运行速度慢，内存可能不够
随机梯度下降：每次使用一个数据进行训练
优点：训练速度快，无内存问题
缺点：容易震荡，可能达不到最优解
Mini-batch梯度下降
优点：训练速度快，无内存问题，震荡较少
缺点：可能达不到最优解

从优化方法上来分：
随机梯度下降（SGD）
缺点
选择合适的learning	rate比较难	
对于所有的参数使用同样的learning rate	
容易收敛到局部最优
可能困在saddle point
SGD+Momentum
优点：
积累动量，加速训练
局部极值附近震荡时，由于动量，跳出陷阱
梯度方向发生变化时，动量缓解动荡。
Nesterov Mementum
与Mementum类似，优点：
避免前进太快
提高灵敏度
AdaGrad
优点：
控制学习率，每一个分量有各自不同的学习率
适合稀疏数据
缺点
依赖一个全局学习率
学习率设置太大，其影响过于敏感
后期，调整学习率的分母积累的太大，导致学习率很低，提前结束训练。
RMSProp
优点：
解决了后期提前结束的问题。
缺点：
依然依赖全局学习率
Adam
Adagrad和RMSProp的合体
优点：
结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
为不同的参数计算不同的自适应学习率
也适用于大多非凸优化 -	适用于大数据集和高维空间
牛顿法
牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难
拟牛顿法
拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。

---

### <h2 id="特征向量的归一化方法有哪些">特征向量的归一化方法有哪些</h2>
- 线性函数转换，表达式如下：
y=(x-MinValue)/(MaxValue-MinValue)
- 对数函数转换，表达式如下：
y=log10 (x)
- 反余切函数转换 ，表达式如下：
y=arctan(x)*2/PI
- 减去均值，除以方差：
y=(x-means)/ variance

---

### <h2 id="RF与GBDT之间的区别与联系">RF与GBDT之间的区别与联系</h2>


1）相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定。
2）不同点：
a 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成
b 组成随机森林的树可以并行生成，而GBDT是串行生成
c 随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和
d 随机森林对异常值不敏感，而GBDT对异常值比较敏感
e 随机森林是减少模型的方差，而GBDT是减少模型的偏差
f 随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化

---

### <h2 id="试证明样本空间任意点x到超平面w，b的距离为6.2">试证明样本空间任意点x到超平面(w，b)的距离为(6.2)</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/69.png)

---

### <h2 id="请比较下EM算法、HMM、CRF">请比较下EM算法、HMM、CRF</h2>


这三个放在一起不是很恰当，但是有互相有关联，所以就放在这里一起说了。注意重点关注算法的思想。 
（1）EM算法 
　　EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。 
　　注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说EM算法不能保证找到全局最优值。对于EM的导出方法也应该掌握。 
（2）HMM算法 
　　隐马尔可夫模型是用于标注问题的生成模型。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。 
马尔科夫三个基本问题：
概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法
学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。
预测问题：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）
（3）条件随机场CRF 
　　给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。 
　　之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。
（4）HMM和CRF对比 
　　其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。

---

### <h2 id="请说说常用核函数及核函数的条件">请说说常用核函数及核函数的条件</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/71.png)

---

### <h2 id="什么是共线性， 跟过拟合有什么关联">什么是共线性， 跟过拟合有什么关联</h2>


共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。
共线性会造成冗余，导致过拟合。
解决方法：排除变量的相关性／加入权重正则。

本题解析来源：@抽象猴，链接：https://www.zhihu.com/question/41233373/answer/145404190

---

### <h2 id="用贝叶斯机率说明Dropout的原理">用贝叶斯机率说明Dropout的原理</h2>


回想一下使用Bagging学习，我们定义 k 个不同的模型，从训练集有替换采样 构造 k 个不同的数据集，然后在训练集上训练模型 i。

Dropout的目标是在指数 级数量的神经网络上近似这个过程。Dropout训练与Bagging训练不太一样。在Bagging的情况下，所有模型是独立 的。

在Dropout的情况下，模型是共享参数的，其中每个模型继承的父神经网络参 数的不同子集。参数共享使得在有限可用的内存下代表指数数量的模型变得可能。 在Bagging的情况下，每一个模型在其相应训练集上训练到收敛。
在Dropout的情况下，通常大部分模型都没有显式地被训练，通常该模型很大，以致到宇宙毁灭都不 能采样所有可能的子网络。取而代之的是，可能的子网络的一小部分训练单个步骤，参数共享导致剩余的子网络能有好的参数设定。

http://bi.dataguru.cn/article-10459-1.html

---

### <h2 id="对于维度极低的特征，选择线性还是非线性分类器">对于维度极低的特征，选择线性还是非线性分类器</h2>


非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。
1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。

---

### <h2 id="SVM、LR、决策树的对比">SVM、LR、决策树的对比</h2>

- 模型复杂度：SVM支持核函数，可处理线性非线性问题; LR模型简单，训练速度快，适合处理线性问题; 决策树容易过拟合，需要进行剪枝.
- 损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失
- 数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感
- 数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核

---

### <h2 id="什么是ill-condition病态问题">什么是ill-condition病态问题</h2>


训练完的模型，测试样本稍作修改就会得到差别很大的结果，就是病态问题，模型对未知数据的预测能力很差，即泛化误差大。

---

### <h2 id="简述KNN最近邻分类算法的过程">简述KNN最近邻分类算法的过程</h2>


1. 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；
2. 对上面所有的距离值进行排序；
3. 选前k个最小距离的样本；
4. 根据这k个样本的标签进行投票，得到最后的分类类别；

---

### <h2 id="常用的聚类划分方式有哪些列举代表算法。">常用的聚类划分方式有哪些列举代表算法。</h2>


1. 基于划分的聚类:K-means，k-medoids，CLARANS。
2. 基于层次的聚类：AGNES（自底向上），DIANA（自上向下）。
3. 基于密度的聚类：DBSACN，OPTICS，BIRCH(CF-Tree)，CURE。
4. 基于网格的方法：STING，WaveCluster。
5. 基于模型的聚类：EM，SOM，COBWEB。








