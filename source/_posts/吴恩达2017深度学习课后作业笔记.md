---
title: 吴恩达2017深度学习课后作业笔记
tags:
  - 深度学习
  - 吴恩达
categories:
  - 深度学习
date: 2017-12-18 22:22:00
toc: false
mathjax: true

---

### 链接地址
- [吴恩达深度学习视频教程](http://mooc.study.163.com/smartSpec/detail/1001319001.htm)
- [教程对应的课后作业](https://github.com/Wasim37/deeplearning-assignment)
- 再推荐一个优秀的 [博客](http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/)，关于吴恩达深度学习视频总结的。

---

### 课后作业目录

1_神经网络与深度学习
............Week1 深度学习概论
........................[选择题](http://7xvfir.com1.z0.glb.clouddn.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A/%E7%AC%AC%E4%B8%80%E8%AF%BE%E7%AC%AC%E4%B8%80%E5%91%A8%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A.png)
............Week2 神经网络基础<!--more-->
........................assignment2_1
........................assignment2_2
............Week3 浅层神经网络
........................assignment3
............Week4 深层神经网络
........................assignment4_1
........................assignment4_2

2_改善深层神经网络：超参数调试、正则化以及优化
............Week1 深层学习的实用
........................[Initialization](#Initialization)
........................Regularization
........................Gradient_Checking
............Week2 优化算法
........................Optimization+methods
............Week3 超参数调试&正则化&框架
........................Tensorflow+Tutorial

3_结构化机器学习项目
............[选择题1](http://7xvfir.com1.z0.glb.clouddn.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A/%E7%AC%AC%E4%B8%89%E8%AF%BE_%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A_1.png)
............[选择题2](http://7xvfir.com1.z0.glb.clouddn.com/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A/%E7%AC%AC%E4%B8%89%E8%AF%BE_%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A_2.png)

4_卷积神经网络
............Week1 卷积神经网络
........................Convolution model-Application-v1
........................Convolution model-Step by Step-v1
........................Convolution model-Step by Step-v2
............Week2 深层卷积神经网络实例探究
............Week3 目标检测
............Week4 特殊的应用

---

### <h2 id="Initialization">Initialization_神经网络权重初始化</h2>

现在有一个三层的神经网络，需要用不同的方式去初始化它的权重W和b，然后查看它的分类效果。

**精心选择的初始化可以：**
1）加快梯度下降的收敛
2）增加梯度下降收敛到较低的训练（和泛化）错误的几率

**这里尝试了三种初始化方法**
1）Zeros initialization，参数全部初始化为0。
2）Random initialization，参数随机初始化，并初始化为较大的随机值。
3）He initialization，权重的初始化参考了He等人在2015年发表的论文。

- **Zeros initialization：**
分类效果非常差，因为将所有权重初始化为零将无法破坏网络的对称性。这意味着每一层的每个神经元都会学到相同的东西，这样的神经元网络并不比线性分类器如逻辑回归更强大。

需要注意的是，需要初始化去破坏网络对称性(symmetry)的只有W，b可以全部初始化为0。

- **Random initialization：**
随机初始化权重打破了网络的对称性，每个神经元可以根据不同的输入学习不同的功能。
示例中显示，过大的随机数初始化效果不佳，会减慢优化速度。
用小的随机值进行初始化效果会更好，但重要的问题是这些随机值应该小到多少？可以尝试 He initialization。

- **He initialization：**
He等人在2015年的论文中提到，如果你的图层是用ReLU激活的。初始化可以用$\sqrt{\frac{2}{\text{dimension of the previous layer}}}$代替随机初始化中的`np.random.randn(..,..)`*10。

**三种初始化对比结果**
![](20180202173608.png)

**三种初始化的结论：**
1）不同的初始化导致不同的结果
2）随机初始化用于破坏对称性，并确保不同的隐藏单元可以学习不同的东西
3）不要初始化太大的值
4）"He initialization"适用于ReLU激活的网络


**相关代码：**
```
parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))
parameters['b' + str(l)] = 0

parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 10
parameters['b' + str(l)] = np.zeros((layers_dims[l],1))

parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2/layers_dims[l-1])
parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
```




