---
title: 深度学习知识点集锦
tags:
  - 深度学习
categories:
  - 知识总结
date: 2019-1-1 22:22:00
toc: false
mathjax: true

---

置顶
<!-- more -->

- [简述神经网络的发展历史](#简述神经网络的发展历史)
- [请简述神经网络的发展史](#请简述神经网络的发展史)
- [请简要介绍下tensorflow的计算图](#请简要介绍下tensorflow的计算图)
- [深度学习常用方法](#深度学习常用方法)
- [什麽样的资料集不适合用深度学习](#什麽样的资料集不适合用深度学习)
- [广义线性模型是怎被应用在深度学习中](#广义线性模型是怎被应用在深度学习中)
- [深度学习的数据划分与传统机器学习有什么不同](#深度学习的数据划分与传统机器学习有什么不同)
- 深度学习的开发技巧
- 神经网络前向传播和后向传播公式推导


- [什么是激活函数，为什么要用非线性激活函数](#什么是激活函数，为什么要用非线性激活函数)
- [神经网络里的正则化为什么能防止过拟合](#神经网络里的正则化为什么能防止过拟合)
- [人工神经网络中的ReLu为什么要好过于tanh和sigmoid](#人工神经网络中的ReLu为什么要好过于tanh和sigmoid)
- [Sigmoid、Tanh、ReLu有什么优缺点，有没改进的激活函数](#Sigmoid、Tanh、ReLu有什么优缺点，有没改进的激活函数)
- [简单说下sigmoid激活函数](#简单说下sigmoid激活函数)
- [神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的](#神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的)


- [什么是梯度消失和梯度爆炸，分别会引发什么问题](#什么是梯度消失和梯度爆炸，分别会引发什么问题)
- [如何确定是否出现梯度爆炸](#如何确定是否出现梯度爆炸)
- [如何解决梯度消失和梯度膨胀](#如何解决梯度消失和梯度膨胀)
- [如何解决RNN梯度爆炸和弥散的问题](#如何解决RNN梯度爆炸和弥散的问题)


- [什么是卷积](#什么是卷积)
- [卷积网络怎么进行边缘检测](http://wangxin123.com/2017/12/18/卷积网络的边缘检测/)
- [CNN的卷积核是单层的还是多层的](#CNN的卷积核是单层的还是多层的)
- [什么是CNN的池化pool层](#什么是CNN的池化pool层)
- [CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来为什么AlphaGo里也用了CNN这几个不相关的问题的相似性在哪里CNN通过什么手段抓住了这个共性](#CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来为什么AlphaGo里也用了CNN这几个不相关的问题的相似性在哪里CNN通过什么手段抓住了这个共性)
- [简单说说CNN常用的几个模型](#简单说说CNN常用的几个模型)
- [dropout随机失活](#dropout随机失活)


- [什么是RNN](#什么是RNN)
- [RNN是怎么从单层网络一步一步构造的](#RNN是怎么从单层网络一步一步构造的)
- [如何理解LSTM网络](#如何理解LSTM网络)
- [LSTM神经网络输入输出究竟是怎样的](#LSTM神经网络输入输出究竟是怎样的)
- [LSTM结构推导，为什么比RNN好](#LSTM结构推导，为什么比RNN好)
- [为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数](#为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数)


- [学梵高作画的原理是什么](#学梵高作画的原理是什么)
- [为什么很多做人脸的Paper会最后加入一个Local Connected Conv](#为什么很多做人脸的Paper会最后加入一个Local Connected Conv)
- [你有哪些deep learning（rnn、cnn）调参的经验](#你有哪些deep learning（rnn、cnn）调参的经验)
- [简述下什么是生成对抗网络](#简述下什么是生成对抗网络)


- 物体检测思路
- 人脸检测思路
- 神经风格转移StyleTransfer
- 关键点定位思路
- Word2Vec

---

### <h2 id="什么是梯度消失和梯度爆炸，分别会引发什么问题">什么是梯度消失和梯度爆炸，分别会引发什么问题</h2>
我们知道神经网络在训练过程中会利用梯度对网络的权重进行更新迭代。
当梯度出现指数级递减或指数递增时，称为梯度消失或者梯度爆炸。

假定激活函数 $g(z) = z$, 令 $b^{[l]} = 0$，对于目标输出有：
$\hat{y} = W^{[L]}W^{[L-1]}...W^{[2]}W^{[1]}X$
1）对于 W[l]的值小于 1 的情况，激活函数的值将以指数级递减
2）对于 W[l]的值大于 1 的情况，激活函数的值将以指数级递增
同理的情况会出现在反向求导。

梯度消失时，权重更新缓慢，训练难度大大增加。梯度消失相对梯度爆炸更常见。
梯度爆炸时，权重大幅更新，网络变得不稳定。较好的情况是网络无法利用训练数据学习，最差的情况是权值增大溢出，变成网络无法更新的 NaN 值。

---

### <h2 id="如何确定是否出现梯度爆炸">如何确定是否出现梯度爆炸</h2>

信号如下：
- 训练过程中，每个节点和层的误差梯度值持续超过1.0。
- 模型不稳定，梯度显著变化，快速变大。
- 训练过程中，模型权重变成 NaN 值。
- 模型无法从训练数据中获得更新。

---

### <h2 id="如何解决梯度消失和梯度膨胀">如何解决梯度消失和梯度膨胀</h2>

梯度消失和梯度爆炸都可以通过激活函数或Batch Normalization来解决。[吴恩达：BN有效原因](http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Improving_Deep_Neural_Networks/超参数调试、Batch正则化和程序框架?id=bn-%e6%9c%89%e6%95%88%e7%9a%84%e5%8e%9f%e5%9b%a0)


对于梯度爆炸，这里列举一些最佳实验方法：

1. 重新设计网络模型
在深度神经网络中，梯度爆炸可以通过重新设计层数更少的网络来解决。
使用更小的批尺寸对网络训练也有好处。
在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。

2. 使用 ReLU 激活函数
在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。
使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。

3. 使用长短期记忆网络
在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，如随时间的反向传播本质上将循环网络转换成深度多层感知机神经网络。
使用长短期记忆（LSTM）单元和相关的门类型神经元结构可以减少梯度爆炸问题。
采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。

4. 使用梯度截断（Gradient Clipping）
在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。
处理梯度爆炸有一个简单有效的解决方案：如果梯度超过阈值，就截断它们。
 ——《Neural Network Methods in Natural Language Processing》，2017.
具体来说，检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。
梯度截断可以一定程度上缓解梯度爆炸问题（梯度截断，即在执行梯度下降步骤之前将梯度设置为阈值）。
     ——《深度学习》，2016.
在 Keras 深度学习库中，你可以在训练之前设置优化器上的 clipnorm 或 clipvalue 参数，来使用梯度截断。
默认值为 clipnorm=1.0 、clipvalue=0.5。详见：https://keras.io/optimizers/。

5. 使用权重正则化（Weight Regularization）
如果梯度爆炸仍然存在，可以尝试另一种方法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。
对循环权重使用 L1 或 L2 惩罚项有助于缓解梯度爆炸。
——On the difficulty of training recurrent neural networks，2013.
在 Keras 深度学习库中，你可以通过在层上设置 kernel_regularizer 参数和使用 L1 或 L2 正则化项进行权重正则化。

---

### <h2 id="如何解决RNN梯度爆炸和弥散的问题">如何解决RNN梯度爆炸和弥散的问题</h2>

为了解决梯度爆炸问题，Thomas Mikolov首先提出了一个简单的启发性的解决方案，就是当梯度大于一定阈值的的时候，将它截断为一个较小的数。具体如算法1所述：

算法：当梯度爆炸时截断梯度（伪代码）
![](16.png)

下图可视化了梯度截断的效果。它展示了一个小的rnn（其中W为权值矩阵，b为bias项）的决策面。这个模型是一个一小段时间的rnn单元组成；实心箭头表明每步梯度下降的训练过程。当梯度下降过程中，模型的目标函数取得了较高的误差时，梯度将被送到远离决策面的位置。截断模型产生了一个虚线，它将误差梯度拉回到离原始梯度接近的位置。
![](17.png)

梯度爆炸，梯度截断可视化 
为了解决梯度弥散的问题，我们介绍了两种方法。第一种方法是将随机初始化改为一个有关联的矩阵初始化。第二种方法是使用ReLU（Rectified Linear Units）代替sigmoid函数。ReLU的导数不是0就是1.因此，神经元的梯度将始终为1，而不会当梯度传播了一定时间之后变小。
（链接：http://blog.csdn.net/han_xiaoyang/article/details/51932536）

---

### <h2 id="CNN的卷积核是单层的还是多层的">CNN的卷积核是单层的还是多层的</h2>

一般而言，深度卷积网络是一层又一层的。层的本质是特征图, 存贮输入数据或其中间表示值。一组卷积核则是联系前后两层的网络参数表达体, 训练的目标就是每个卷积核的权重参数组。

描述网络模型中某层的厚度，通常用名词通道channel数或者特征图feature map数。不过人们更习惯把作为数据输入的前层的厚度称之为通道数（比如RGB三色图层称为输入通道数为3），把作为卷积输出的后层的厚度称之为特征图数。

卷积核(filter)一般是3D多层的，除了面积参数, 比如3x3之外, 还有厚度参数H（2D的视为厚度1). 还有一个属性是卷积核的个数N。

卷积核的厚度H, 一般等于前层厚度M(输入通道数或feature map数). 特殊情况M > H。
卷积核的个数N, 一般等于后层厚度(后层feature maps数，因为相等所以也用N表示)。
卷积核通常从属于后层，为后层提供了各种查看前层特征的视角，这个视角是自动形成的。
卷积核厚度等于1时为2D卷积，对应平面点相乘然后把结果加起来，相当于点积运算；
卷积核厚度大于1时为3D卷积，每片分别平面点求卷积，然后把每片结果加起来，作为3D卷积结果；1x1卷积属于3D卷积的一个特例，有厚度无面积, 直接把每片单个点乘以权重再相加。

归纳之，卷积的意思就是把一个区域，不管是一维线段，二维方阵，还是三维长方块，全部按照卷积核的维度形状，对应逐点相乘再求和，浓缩成一个标量值也就是降到零维度，作为下一层的一个feature map的一个点的值！
可以比喻一群渔夫坐一个渔船撒网打鱼，鱼塘是多层水域，每层鱼儿不同。
船每次移位一个stride到一个地方，每个渔夫撒一网，得到收获，然后换一个距离stride再撒，如此重复直到遍历鱼塘。
A渔夫盯着鱼的品种，遍历鱼塘后该渔夫描绘了鱼塘的鱼品种分布；
B渔夫盯着鱼的重量，遍历鱼塘后该渔夫描绘了鱼塘的鱼重量分布；
还有N-2个渔夫，各自兴趣各干各的；
最后得到N个特征图，描述了鱼塘的一切！

2D卷积表示渔夫的网就是带一圈浮标的渔网，只打上面一层水体的鱼；
3D卷积表示渔夫的网是多层嵌套的渔网，上中下层水体的鱼儿都跑不掉；
1x1卷积可以视为每次移位stride，甩钩钓鱼代替了撒网；

下面解释一下特殊情况的 M > H：
实际上，除了输入数据的通道数比较少之外，中间层的feature map数很多，这样中间层算卷积会累死计算机（鱼塘太深，每层鱼都打，需要的鱼网太重了）。所以很多深度卷积网络把全部通道/特征图划分一下，每个卷积核只看其中一部分（渔夫A的渔网只打捞深水段，渔夫B的渔网只打捞浅水段）。这样整个深度网络架构是横向开始分道扬镳了，到最后才又融合。这样看来，很多网络模型的架构不完全是突发奇想，而是是被参数计算量逼得。特别是现在需要在移动设备上进行AI应用计算(也叫推断), 模型参数规模必须更小, 所以出现很多减少握手规模的卷积形式, 现在主流网络架构大都如此。

---

### <h2 id="什么是卷积">什么是卷积</h2>

对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。
非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。
![](1.png)

OK，举个具体的例子。比如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据。
![](2.png)

分解下上图
![](3.png)

对应位置上是数字先相乘后相加
![](4.png)

中间滤波器filter与数据窗口做内积，其具体计算过程则是：4*0 + 0*0 + 0*0 + 0*0 + 0*1 + 0*1 + 0*0 + 0*1 + -4*2 = -8

---

### <h2 id="什么是CNN的池化pool层">什么是CNN的池化pool层</h2>

池化，简言之，即取区域平均或最大，如下图所示（图引自cs231n）
![](5.png)

上图所展示的是取区域最大，即上图左边部分中 左上角2x2的矩阵中6最大，右上角2x2的矩阵中8最大，左下角2x2的矩阵中3最大，右下角2x2的矩阵中4最大，所以得到上图右边部分的结果：6 8 3 4。

---

### <h2 id="简述下什么是生成对抗网络">简述下什么是生成对抗网络</h2>

GAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的。另一方是discriminator，其目标是判断输入图片是否属于真实训练样本。

更直白的讲，将generator想象成假币制造商，而discriminator是警察。generator目的是尽可能把假币造的跟真的一样，从而能够骗过discriminator，即生成样本并使它看上去好像来自于真实训练样本一样。
![](6.png)

如下图中的左右两个场景：
![](7.png)

更多请参见此课程：《生成对抗网络班》（链接：https://www.julyedu.com/course/getDetail/83）

---

### <h2 id="学梵高作画的原理是什么">学梵高作画的原理是什么</h2>

这里有篇如何做梵高风格画的实验教程《教你从头到尾利用DL学梵高作画：GTX 1070 cuda 8.0 tensorflow gpu版》（链接：http://blog.csdn.net/v_july_v/article/details/52658965），至于其原理请看这个视频：NeuralStyle艺术化图片（学梵高作画背后的原理）（链接：http://www.julyedu.com/video/play/42/523）。

---

### <h2 id="请简要介绍下tensorflow的计算图">请简要介绍下tensorflow的计算图</h2>

Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个节点都是计算图上的一个Tensor, 也就是张量，而节点之间的边描述了计算之间的依赖关系(定义时)和数学操作(运算时)。如下两图表示：
a=x*y; b=a+z; c=tf.reduce_sum(b);  
 ![](8.gif)

---

### <h2 id="你有哪些deep learning（rnn、cnn）调参的经验">你有哪些deep learning（rnn、cnn）调参的经验</h2>

一、参数初始化
下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。
下面的n_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)*0.5
Xavier初始法论文：http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf
He初始化论文：https://arxiv.org/abs/1502.01852
uniform均匀分布初始化：w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])
Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)
He初始化，适用于ReLU：scale = np.sqrt(6/n)
normal高斯分布初始化：w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0
Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)
He初始化，适用于ReLU：stdev = np.sqrt(2/n)
svd初始化：对RNN有比较好的效果。参考论文：https://arxiv.org/abs/1312.6120

二、数据预处理方式
zero-center ,这个挺常用的.X -= np.mean(X, axis = 0) # zero-centerX /= np.std(X, axis = 0) # normalize
PCA whitening,这个用的比较少.

三、训练技巧
要做梯度归一化,即算出来的梯度除以minibatch size
clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15

dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入->RNN与RNN->输出的位置.关于RNN如何用dropout,可以参考这篇论文:http://arxiv.org/abs/1409.2329

adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。

除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。2. 输入0均值，sigmoid函数的输出不是0均值的。
rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.

word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.

四、尽量对数据做shuffle
LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.

Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift
如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: http://arxiv.org/abs/1505.00387
来自@张馨宇的技巧：一轮加正则，一轮不加正则，反复进行。

五、Ensemble
Ensemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式
同样的参数,不同的初始化方式
不同的参数,通过cross-validation,选取最好的几组
同样的参数,模型训练的不同阶段，即不同迭代次数的模型。
不同的模型,进行线性融合. 例如RNN和传统模型.
更多深度学习技巧，请参见专栏：炼丹实验室 - 知乎专栏（链接：https://zhuanlan.zhihu.com/easyml）

本题解析来源@萧瑟，链接：https://www.zhihu.com/question/41631631/answer/94816420

---

### <h2 id="CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来为什么AlphaGo里也用了CNN这几个不相关的问题的相似性在哪里CNN通过什么手段抓住了这个共性">CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来为什么AlphaGo里也用了CNN这几个不相关的问题的相似性在哪里CNN通过什么手段抓住了这个共性</h2>

Deep Learning -Yann LeCun, Yoshua Bengio & Geoffrey Hinton
Learn TensorFlow and deep learning, without a Ph.D.
The Unreasonable Effectiveness of Deep Learning -LeCun 16 NIPS Keynote

以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示。
 ![](9.png)

CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。
局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图：
 ![](10.png)

上图中，如果每一个点的处理使用相同的Filter，则为全卷积，如果使用不同的Filter，则为Local-Conv。

本题解析来源：@许韩，链接：https://zhuanlan.zhihu.com/p/25005808
另，关于CNN，这里有篇文章《 CNN笔记：通俗理解卷积神经网络》。（链接：http://blog.csdn.net/v_july_v/article/details/51812459）

---

### <h2 id="LSTM结构推导，为什么比RNN好">LSTM结构推导，为什么比RNN好</h2>

推导forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸

---

### <h2 id="Sigmoid、Tanh、ReLu有什么优缺点，有没改进的激活函数">Sigmoid、Tanh、ReLu有什么优缺点，有没改进的激活函数</h2>

Maxout使用两套w,b参数，输出较大值。本质上Maxout可以看做Relu的泛化版本，因为如果一套w,b全都是0的话，那么就是普通的ReLU。Maxout可以克服Relu的缺点，但是参数数目翻倍。 

 ![](12.png)

解析来源：@我愛大泡泡，链接：http://blog.csdn.net/woaidapaopao/article/details/77806273

---

### <h2 id="什么是激活函数，为什么要用非线性激活函数">什么是激活函数，为什么要用非线性激活函数</h2>

如下图，在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数 Activation Function。
![](61.png)

**不用激活函数或使用线性激活函数，和直接使用 Logistic 回归没有区别**，因为无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，就成了最原始的感知器了。

**非线性激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数**，这样神经网络就可以应用到众多的非线性模型中。非线性激励函数最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入。

---

### <h2 id="人工神经网络中的ReLu为什么要好过于tanh和sigmoid">人工神经网络中的ReLu为什么要好过于tanh和sigmoid</h2>

先看sigmoid、tanh和RelU的函数图：
![](14.png)

第一，采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法和指数运算，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。

第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），这种现象称为饱和，从而无法完成深层网络的训练。而ReLU就不会有饱和倾向，不会有特别小的梯度出现。

第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生（以及一些人的生物解释balabala）。当然现在也有一些对relu的改进，比如prelu，random relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进，具体的大家可以找相关的paper看。

多加一句，现在主流的做法，会多做一步batch normalization，尽可能保证每一层网络的输入具有相同的分布[1]。而最新的paper[2]，他们在加入bypass connection之后，发现改变batch normalization的位置会有更好的效果。大家有兴趣可以看下。
[1] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.
[2] He, Kaiming, et al. "Identity Mappings in Deep Residual Networks." arXiv preprint arXiv:1603.05027 (2016). 

本题解析来源：@Begin Again，链接：https://www.zhihu.com/question/29021768

---

### <h2 id="神经网络里的正则化为什么能防止过拟合">神经网络里的正则化为什么能防止过拟合</h2>

- 直观解释
正则化因子设置的足够大的情况下，为了使成本函数最小化，**权重矩阵 W 就会被设置为接近于 0 的值，直观上相当于消除了很多神经元的影响**，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。

- 数学解释
假设神经元中使用的激活函数为g(z) = tanh(z)（sigmoid 同理）。

![](63.png)

在加入正则化项后，当 λ 增大，导致 W[l]减小，Z[l]=W[l]a[l−1]+b[l]便会减小。**由上图可知，在 z 较小（接近于 0）的区域里，tanh(z)函数近似线性**，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。

- 其他解释
在权值 w[L]变小之下，**输入样本 X 随机的变化不会对神经网络模造成过大的影响**，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。

---

### <h2 id="为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数">为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数</h2>

为什么不是选择统一一种sigmoid或者tanh，而是混合使用呢这样的目的是什么
![](15.png)
 本题解析来源：https://www.zhihu.com/question/46197687

sigmoid 用在了各种gate上，产生0~1之间的值，这个一般只有sigmoid最直接了。
tanh 用在了状态和输出上，是对数据的处理，这个用其他激活函数或许也可以。
引用自@beanfrog：二者目的不一样
另可参见A Critical Review of Recurrent Neural Networks for Sequence Learning的section4.1，说了那两个tanh都可以替换成别的。
引用自@hhhh

---

### <h2 id="什麽样的资料集不适合用深度学习">什麽样的资料集不适合用深度学习</h2>

1、数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。

2、数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

本题解析来源：@抽象猴，链接：https://www.zhihu.com/question/41233373

---

### <h2 id="广义线性模型是怎被应用在深度学习中">广义线性模型是怎被应用在深度学习中</h2>

A Statistical View of Deep Learning (I): Recursive GLMs
深度学习从统计学角度，可以看做递归的广义线性模型。

广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。

深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者（这里主要指我）的困惑。

下图是一个对照表
![](18.png)

本题解析来源：@许韩，链接：https://www.zhihu.com/question/41233373/answer/145404190

---

### <h2 id="深度学习的数据划分与传统机器学习有什么不同">深度学习的数据划分与传统机器学习有什么不同</h2>
对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：

1）训练集（train set）：用训练集对算法或模型进行训练过程；
2）验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）**进行交叉验证，选择出最好的模型**；
3）测试集（test set）：最后利用测试集对模型进行测试，获取模型运行的无偏估计（对学习方法进行评估）。

**在小数据量的时代**，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：

1）无验证集的情况：70% / 30%；
2）有验证集的情况：60% / 20% / 20%；

而在如今的**大数据时代**，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以**验证集和测试集所占的比重会趋向于变得更小**。

**验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。**

测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。吴恩达给出的建议是：

1）100 万数据量：98% / 1% / 1%；
2）超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）

---

### <h2 id="简述神经网络的发展历史">简述神经网络的发展历史</h2>

1949年Hebb提出了神经心理学学习范式——Hebbian学习理论
1952年，IBM的Arthur Samuel写出了西洋棋程序
1957年，Rosenblatt的感知器算法是第二个有着神经系统科学背景的机器学习模型.

3年之后，Widrow因发明Delta学习规则而载入ML史册，该规则马上就很好的应用到了感知器的训练中
感知器的热度在1969被Minskey一盆冷水泼灭了。他提出了著名的XOR问题，论证了感知器在类似XOR问题的线性不可分数据的无力。

尽管BP的思想在70年代就被Linnainmaa以“自动微分的翻转模式”被提出来，但直到1981年才被Werbos应用到多层感知器(MLP)中，NN新的大繁荣。

1991年的Hochreiter和2001年的Hochreiter的工作，都表明在使用BP算法时，NN单元饱和之后会发生梯度损失。又发生停滞。

时间终于走到了当下，随着计算资源的增长和数据量的增长。一个新的NN领域——深度学习出现了。

简言之，MP模型+sgn—->单层感知机（只能线性）+sgn— Minsky 低谷 —>多层感知机+BP+sigmoid—- (低谷) —>深度学习+pre-training+ReLU/sigmoid

---

### <h2 id="深度学习常用方法">深度学习常用方法</h2>

全连接DNN（相邻层相互连接、层内无连接）： 
AutoEncoder(尽可能还原输入)、Sparse Coding（在AE上加入L1规范）、RBM（解决概率问题）—–>特征探测器——>栈式叠加 贪心训练 
RBM—->DBN 
解决全连接DNN的全连接问题—–>CNN 
解决全连接DNN的无法对时间序列上变化进行建模的问题—–>RNN—解决时间轴上的梯度消失问题——->LSTM
解析来源：@SmallisBig，链接：http://blog.csdn.net/u010496169/article/details/73550487

@张雨石：现在在应用领域应用的做多的是DNN，CNN和RNN。
DNN是传统的全连接网络，可以用于广告点击率预估，推荐等。其使用embedding的方式将很多离散的特征编码到神经网络中，可以很大的提升结果。

CNN主要用于计算机视觉(Computer Vision)领域，CNN的出现主要解决了DNN在图像领域中参数过多的问题。同时，CNN特有的卷积、池化、batch normalization、Inception、ResNet、DeepNet等一系列的发展也使得在分类、物体检测、人脸识别、图像分割等众多领域有了长足的进步。同时，CNN不仅在图像上应用很多，在自然语言处理上也颇有进展，现在已经有基于CNN的语言模型能够达到比LSTM更好的效果。在最新的AlphaZero中，CNN中的ResNet也是两种基本算法之一。

GAN是一种应用在生成模型的训练方法，现在有很多在CV方面的应用，例如图像翻译，图像超清化、图像修复等等。

RNN主要用于自然语言处理(Natural Language Processing)领域，用于处理序列到序列的问题。普通RNN会遇到梯度爆炸和梯度消失的问题。所以现在在NLP领域，一般会使用LSTM模型。在最近的机器翻译领域，Attention作为一种新的手段，也被引入进来。

除了DNN、RNN和CNN外， 自动编码器(AutoEncoder)、稀疏编码(Sparse Coding)、深度信念网络(DBM)、限制玻尔兹曼机(RBM)也都有相应的研究。

---

### <h2 id="请简述神经网络的发展史">请简述神经网络的发展史</h2>

sigmoid会饱和，造成梯度消失。于是有了ReLU。
ReLU负半轴是死区，造成梯度变0。于是有了LeakyReLU，PReLU。
强调梯度和权值分布的稳定性，由此有了ELU，以及较新的SELU。
太深了，梯度传不下去，于是有了highway。
干脆连highway的参数都不要，直接变残差，于是有了ResNet。

强行稳定参数的均值和方差，于是有了BatchNorm。
在梯度流中增加噪声，于是有了 Dropout。
RNN梯度不稳定，于是加几个通路和门控，于是有了LSTM。
LSTM简化一下，有了GRU。
GAN的JS散度有问题，会导致梯度消失或无效，于是有了WGAN。
WGAN对梯度的clip有问题，于是有了WGAN-GP。

本题解析来源：
@SIY.Z，https://zhuanlan.zhihu.com/p/29435406

---

### <h2 id="神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的">神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的</h2>

1. 非线性：即导数不是常数。这个条件是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。

2. 几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。

3. 计算简单：非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。

4. 非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x>0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x<0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。

5. 单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。

6. 输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。

7. 接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x>0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。

8. 参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。

9. 归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。

更多详情：https://www.zhihu.com/question/67366051

---

### <h2 id="简单说说CNN常用的几个模型">简单说说CNN常用的几个模型</h2>

![](62.png)
![](21.png)

更多详情见：
[从LeNet到AlexNet](http://blog.csdn.net/cyh_24/article/details/51440344)
[Deep Learning回顾#之LeNet、AlexNet、GoogLeNet、VGG、ResNet](https://zhuanlan.zhihu.com/p/22094600)

---

### <h2 id="dropout随机失活">dropout随机失活</h2>
dropout（随机失活）是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在计算机视觉（Computer Vision）领域。
![](64.png)


对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，**所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重**。

**因此，通过传播过程，dropout 将产生和 L2 正则化相同的收缩权重的效果。**

对于不同的层，设置的keep_prob也不同。**一般来说，神经元较少的层，会设keep_prob为 1.0，而神经元多的层则会设置比较小的keep_prob。**

**dropout 的一大缺点是成本函数无法被明确定义。**因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将keep_prob全部设置为 1.0 后运行代码，确保 J(w,b)函数单调递减，再打开 dropout。

---

### <h2 id="为什么很多做人脸的Paper会最后加入一个Local Connected Conv">为什么很多做人脸的Paper会最后加入一个Local Connected Conv</h2>

以FaceBook DeepFace 为例：
![](22.png)

DeepFace 先进行了两次全卷积＋一次池化，提取了低层次的边缘／纹理等特征。后接了3个Local-Conv层，这里是用Local-Conv的原因是，人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。

本题解析来源：@许韩，链接：https://zhuanlan.zhihu.com/p/25005808

---

### <h2 id="什么是RNN">什么是RNN</h2>

RNNs的目的使用来处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。

RNNs之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。

理论上，RNNs能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关，下图便是一个典型的RNNs： 
![](24.png)

RNNs包含输入单元(Input units)，输入集标记为{x0,x1,...,xt,xt+1,...}，而输出单元(Output units)的输出集则被标记为{y0,y1,...,yt,yt+1.,..}。RNNs还包含隐藏单元(Hidden units)，我们将其输出集标记为{s0,s1,...,st,st+1,...}，这些隐藏单元完成了最为主要的工作。你会发现，在图中：有一条单向流动的信息流是从输入单元到达隐藏单元的，与此同时另一条单向流动的信息流从隐藏单元到达输出单元。在某些情况下，RNNs会打破后者的限制，引导信息从输出单元返回隐藏单元，这些被称为“Back Projections”，并且隐藏层的输入还包括上一隐藏层的状态，即隐藏层内的节点可以自连也可以互连。 
![](25.png)

上图将循环神经网络进行展开成一个全神经网络。例如，对一个包含5个单词的语句，那么展开的网络便是一个五层的神经网络，每一层代表一个单词。对于该网络的计算过程如下：

1. xt表示第t,t=1,2,3...步(step)的输入。比如，x1为第二个词的one-hot向量(根据上图，x0为第一个词)； 
2. st为隐藏层的第t步的状态，它是网络的记忆单元。 st根据当前输入层的输出与上一步隐藏层的状态进行计算。st=f(Uxt+Wst−1)，其中f一般是非线性的激活函数，如tanh或ReLU，在计算s0时，即第一个单词的隐藏层状态，需要用到s−1，但是其并不存在，在实现中一般置为0向量；
3. ot是第t步的输出，如下个单词的向量表示，ot=softmax(Vst). 

本题解析来源：@一只鸟的天空，链接：http://blog.csdn.net/heyongluoyao8/article/details/48636251

---

### <h2 id="RNN是怎么从单层网络一步一步构造的">RNN是怎么从单层网络一步一步构造的</h2>

一、从单层网络谈起
在学习RNN之前，首先要了解一下最基本的单层网络，它的结构如图：
![](26.png)

输入是x，经过变换Wx+b和激活函数f得到输出y。相信大家对这个已经非常熟悉了。

二、经典的RNN结构（N vs N）
在实际应用中，我们还会遇到很多序列形的数据：
![](27.png)

如：
1.自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。
2.语音处理。此时，x1、x2、x3……是每帧的声音信号。
3.时间序列问题。例如每天的股票价格等等。

序列形的数据就不太好用原始的神经网络处理了。为了建模序列问题，RNN引入了隐状态h（hidden state）的概念，h可以对序列形的数据提取特征，接着再转换为输出。先从h1的计算开始看：
![](28.png)

图示中记号的含义是：
a）圆圈或方块表示的是向量。
b）一个箭头就表示对该向量做一次变换。如上图中h0和x1分别有一个箭头连接，就表示对h0和x1各做了一次变换。
在很多论文中也会出现类似的记号，初学的时候很容易搞乱，但只要把握住以上两点，就可以比较轻松地理解图示背后的含义。

h2的计算和h1类似。要注意的是，在计算时，每一步使用的参数U、W、b都是一样的，也就是说每个步骤的参数都是共享的，这是RNN的重要特点，一定要牢记。
![](29.png)

依次计算剩下来的（使用相同的参数U、W、b）：
![](30.png)

我们这里为了方便起见，只画出序列长度为4的情况，实际上，这个计算过程可以无限地持续下去。

我们目前的RNN还没有输出，得到输出值的方法就是直接通过h进行计算：
![](31.png)

正如之前所说，一个箭头就表示对对应的向量做一次类似于f(Wx+b)的变换，这里的这个箭头就表示对h1进行一次变换，得到输出y1。

剩下的输出类似进行（使用和y1同样的参数V和c）：
![](32.png)

OK！大功告成！这就是最经典的RNN结构，我们像搭积木一样把它搭好了。它的输入是x1, x2, .....xn，输出为y1, y2, ...yn，也就是说，输入和输出序列必须要是等长的。

由于这个限制的存在，经典RNN的适用范围比较小，但也有一些问题适合用经典的RNN结构建模，如：
1.计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。
2.输入为字符，输出为下一个字符的概率。这就是著名的Char RNN（详细介绍请参考：The Unreasonable Effectiveness of Recurrent Neural Networks，Char RNN可以用来生成文章、诗歌，甚至是代码。此篇博客里有自动生成歌词的实验教程《基于torch学汪峰写歌词、聊天机器人、图像着色/生成、看图说话、字幕生成》）。

三、N VS 1
有的时候，我们要处理的问题输入是一个序列，输出是一个单独的值而不是序列，应该怎样建模呢实际上，我们只在最后一个h上进行输出变换就可以了：
![](33.png)

这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。

四、1 VS N
输入不是序列而输出为序列的情况怎么处理我们可以只在序列开始进行输入计算：
![](34.png)

还有一种结构是把输入信息X作为每个阶段的输入：
![](35.png)

下图省略了一些X的圆圈，是一个等价表示：
![](36.png)

这种1 VS N的结构可以处理的问题有：
1.从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子
2.从类别生成语音或音乐等

五、N vs M
下面我们来介绍RNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。
原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。
为此，Encoder-Decoder结构先将输入数据编码成一个上下文向量c：
![](37.png)

得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。

拿到c之后，就用另一个RNN网络对其进行解码，这部分RNN网络被称为Decoder。具体做法就是将c当做之前的初始状态h0输入到Decoder中：
![](38.png)

还有一种做法是将c当做每一步的输入：
![](39.png)

由于这种Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如：
机器翻译。Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的
文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列。
阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。
语音识别。输入是语音信号序列，输出是文字序列。

本题解析来源：@何之源，链接：https://zhuanlan.zhihu.com/p/28054589

---

### <h2 id="简单说下sigmoid激活函数">简单说下sigmoid激活函数</h2>

常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。这里先简要介绍下最基础的sigmoid函数（btw，在本博客中SVM那篇文章开头有提过）。

sigmoid的函数表达式如下
![](40.png)

其中z是一个线性组合，比如z可以等于：b + w1*x1 + w2*x2。通过代入很大的正数或很小的负数到g(z)函数中可知，其结果趋近于0或1。

因此，sigmoid函数g(z)的图形表示如下（ 横轴表示定义域z，纵轴表示值域g(z) ）：
![](41.png)

也就是说，sigmoid函数的功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，g(z)会趋近于1，而z是非常小的负数时，则g(z)会趋近于0。

压缩至0到1有何用处呢用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。

举个例子，如下图（图引自Stanford机器学习公开课）
![](42.png)

z = b + w1*x1 + w2*x2，其中b为偏置项 假定取-30，w1、w2都取为20
![](43.png)

如果x1 = 0，x2 = 0，则z = -30，g(z) = 1/( 1 + e^-z )趋近于0。此外，从上图sigmoid函数的图形上也可以看出，当z=-30的时候，g(z)的值趋近于0
如果x1 = 0，x2 = 1，或x1 =1,x2 = 0，则z = b + w1*x1 + w2*x2 = -30 + 20 = -10，同样，g(z)的值趋近于0
如果x1 = 1，x2 = 1，则z = b + w1*x1 + w2*x2 = -30 + 20*1 + 20*1 = 10，此时，g(z)趋近于1。

换言之，只有x1和x2都取1的时候，g(z)→1，判定为正样本；而当只要x1或x2有一个取0的时候，g(z)→0，判定为负样本，如此达到分类的目的。

综上，sigmod函数，是逻辑斯蒂回归的压缩函数，它的性质是可以把分隔平面压缩到[0,1]区间一个数（向量），在线性分割平面值为0时候正好对应sigmod值为0.5，大于0对应sigmod值大于0.5、小于0对应sigmod值小于0.5；0.5可以作为分类的阀值；exp的形式最值求解时候比较方便，用相乘形式作为logistic损失函数，使得损失函数是凸函数；不足之处是sigmod函数在y趋于0或1时候有死区，控制不好在bp形式传递loss时候容易造成梯度弥撒。

---

### <h2 id="如何理解LSTM网络">如何理解LSTM网络</h2>

本题解析来源：@Not_GOD翻译Christopher Olah 博文的《理解LSTM网络》，链接：http://www.jianshu.com/p/9dc9f41f0b29/

一、Recurrent Neural Networks
人类并不是每时每刻都从一片空白的大脑开始他们的思考。在你阅读这篇文章时候，你都是基于自己已经拥有的对先前所见词的理解来推断当前词的真实含义。我们不会将所有的东西都全部丢弃，然后用空白的大脑进行思考。我们的思想拥有持久性。

传统的神经网络并不能做到这点，看起来也像是一种巨大的弊端。例如，假设你希望对电影中的每个时间点的时间类型进行分类。传统的神经网络应该很难来处理这个问题——使用电影中先前的事件推断后续的事件。

RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。
![RNN 包含循环](44.png)

在上面的示例图中，神经网络的模块，A，正在读取某个输入 x_i，并输出一个值 h_i。循环可以使得信息可以从当前步传递到下一步。

这些循环使得 RNN 看起来非常神秘。然而，如果你仔细想想，这样也不比一个正常的神经网络难于理解。RNN 可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。所以，如果我们将这个循环展开：
![展开的 RNN](45.png)

链式的特征揭示了 RNN 本质上是与序列和列表相关的。他们是对于这类数据的最自然的神经网络架构。

并且 RNN 也已经被人们应用了！在过去几年中，应用 RNN 在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功，并且这个列表还在增长。我建议大家参考 Andrej Karpathy 的博客文章——The Unreasonable Effectiveness of Recurrent Neural Networks 来看看更丰富有趣的 RNN 的成功应用。

而这些成功应用的关键之处就是 LSTM 的使用，这是一种特别的 RNN，比标准的 RNN 在很多的任务上都表现得更好。几乎所有的令人振奋的关于 RNN 的结果都是通过 LSTM 达到的。这篇博文也会就 LSTM 进行展开。

二、长期依赖（Long-Term Dependencies）问题
RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么答案是，还有很多依赖因素。

有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。
![不太长的相关信息和位置间隔](46.png)

但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France... I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。

不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。
![相当长的相关信息和位置间隔](47.png)

在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。

然而，幸运的是，LSTM 并没有这个问题！

三、LSTM 网络
Long Short Term 网络—— 一般就叫做 LSTM ——是一种 RNN 特殊的类型，可以学习长期依赖信息。如@寒小阳所说：LSTM和基线RNN并没有特别大的结构不同，但是它们用了不同的函数来计算隐状态。LSTM的“记忆”我们叫做细胞/cells，你可以直接把它们想做黑盒，这个黑盒的输入为前状态ht−1和当前输入xt。这些“细胞”会决定哪些之前的信息和状态需要保留/记住，而哪些要被抹去。实际的应用中发现，这种方式可以有效地保存很长时间之前的关联信息。

LSTM 由Hochreiter & Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。

LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！

所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。
![](48.png)
标准 RNN 中的重复模块包含单一的层

LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。
![](49.png)
LSTM 中的重复模块包含四个交互的层

不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。
![](50.png)
LSTM 中的图标

在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。

四、LSTM 的核心思想
LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。

细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。
![](51.png)

LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。
![](52.png)

Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！
LSTM 拥有三个门，来保护和控制细胞状态。

五、逐步理解 LSTM
在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取 h_{t-1} 和 x_t，输出一个在 0 到 1 之间的数值给每个在细胞状态 C_{t-1} 中的数字。1 表示“完全保留”，0 表示“完全舍弃”。

让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。
![](53.png)
决定丢弃信息

下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，\tilde{C}_t，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。

在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。
![](54.png)
确定更新的信息

现在是更新旧细胞状态的时间了，C_{t-1} 更新为 C_t。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。

我们把旧状态与 f_t 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 i_t * \tilde{C}_t。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。

在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。
![](55.png)
更新细胞状态

最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。

在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。
![](56.png)
输出信息

六、LSTM 的变体
我们到目前为止都还在介绍正常的 LSTM。但是不是所有的 LSTM 都长成一个样子的。实际上，几乎所有包含 LSTM 的论文都采用了微小的变体。差异非常小，但是也值得拿出来讲一下。

其中一个流形的 LSTM 变体，就是由 Gers & Schmidhuber (2000) 提出的，增加了 “peephole connection”。是说，我们让 门层 也会接受细胞状态的输入。
![](57.png)
peephole 连接

上面的图例中，我们增加了 peephole 到每个门上，但是许多论文会加入部分的 peephole 而非所有都加。

另一个变体是通过使用 coupled 忘记和输入门。不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。我们仅仅会当我们将要输入在当前位置时忘记。我们仅仅输入新的值到那些我们已经忘记旧的信息的那些状态 。
![](58.png)
coupled 忘记门和输入门

另一个改动较大的变体是 Gated Recurrent Unit (GRU)，这是由 Cho, et al. (2014) 提出。它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。
![](59.png)
GRU

这里只是部分流行的 LSTM 变体。当然还有很多其他的，如Yao, et al. (2015) 提出的 Depth Gated RNN。还有用一些完全不同的观点来解决长期依赖的问题，如Koutnik, et al. (2014) 提出的 Clockwork RNN。

要问哪个变体是最好的其中的差异性真的重要吗Greff, et al. (2015) 给出了流行变体的比较，结论是他们基本上是一样的。Jozefowicz, et al. (2015) 则在超过 1 万种 RNN 架构上进行了测试，发现一些架构在某些任务上也取得了比 LSTM 更好的结果。
![](60.png)
Jozefowicz等人论文截图

七、结论
刚开始，我提到通过 RNN 得到重要的结果。本质上所有这些都可以使用 LSTM 完成。对于大多数任务确实展示了更好的性能！

由于 LSTM 一般是通过一系列的方程表示的，使得 LSTM 有一点令人费解。然而本文中一步一步地解释让这种困惑消除了不少。

LSTM 是我们在 RNN 中获得的重要成功。很自然地，我们也会考虑：哪里会有更加重大的突破呢在研究人员间普遍的观点是：“Yes! 下一步已经有了——那就是注意力！” 这个想法是让 RNN 的每一步都从更加大的信息集中挑选信息。例如，如果你使用 RNN 来产生一个图片的描述，可能会选择图片的一个部分，根据这部分信息来产生输出的词。实际上，Xu, et al.(2015)已经这么做了——如果你希望深入探索注意力可能这就是一个有趣的起点！还有一些使用注意力的相当振奋人心的研究成果，看起来有更多的东西亟待探索……

注意力也不是 RNN 研究领域中唯一的发展方向。例如，Kalchbrenner, et al. (2015) 提出的 Grid LSTM 看起来也是很有前途。使用生成模型的 RNN，诸如Gregor, et al. (2015) Chung, et al. (2015) 和 Bayer & Osendorfer (2015) 提出的模型同样很有趣。在过去几年中，RNN 的研究已经相当的燃，而研究成果当然也会更加丰富！

---

### <h2 id="LSTM神经网络输入输出究竟是怎样的">LSTM神经网络输入输出究竟是怎样的</h2>

第一要明确的是神经网络所处理的单位全部都是：向量
下面就解释为什么你会看到训练数据会是矩阵和张量
常规feedforward 输入和输出：矩阵
        输入矩阵形状：(n_samples, dim_input)
        输出矩阵形状：(n_samples, dim_output)
注：真正测试/训练的时候，网络的输入和输出就是向量而已。加入n_samples这个维度是为了可以实现一次训练多个样本，求出平均梯度来更新权重，这个叫做Mini-batch gradient descent。 如果n_samples等于1，那么这种更新方式叫做Stochastic Gradient Descent (SGD)。
Feedforward 的输入输出的本质都是单个向量。

常规Recurrent (RNN/LSTM/GRU) 输入和输出：张量
        输入张量形状：(time_steps, n_samples,  dim_input)
        输出张量形状：(time_steps, n_samples,  dim_output)
注：同样是保留了Mini-batch gradient descent的训练方式，但不同之处在于多了time step这个维度。 
Recurrent 的任意时刻的输入的本质还是单个向量，只不过是将不同时刻的向量按顺序输入网络。所以你可能更愿意理解为一串向量 a sequence of vectors，或者是矩阵。

python代码表示预测的话：
import numpy as np

#当前所累积的hidden_state,若是最初的vector，则hidden_state全为0
hidden_state=np.zeros((n_samples, dim_input))

#print(inputs.shape)： （time_steps, n_samples,  dim_input)
outputs = np.zeros((time_steps, n_samples, dim_output))

for i in range(time_steps):
    #输出当前时刻的output，同时更新当前已累积的hidden_state
    outputs[i],hidden_state = RNN.predict(inputs[i],hidden_state)
#print(outputs.shape)： (time_steps, n_samples, dim_output)

但需要注意的是，Recurrent nets的输出也可以是矩阵，而非三维张量，取决于你如何设计。
1.若想用一串序列去预测另一串序列，那么输入输出都是张量 (例如语音识别 或机器翻译 一个中文句子翻译成英文句子（一个单词算作一个向量），机器翻译还是个特例，因为两个序列的长短可能不同，要用到seq2seq；
2.若想用一串序列去预测一个值，那么输入是张量，输出是矩阵 （例如，情感分析就是用一串单词组成的句子去预测说话人的心情）

Feedforward 能做的是向量对向量的one-to-one mapping，
Recurrent 将其扩展到了序列对序列 sequence-to-sequence mapping.
但单个向量也可以视为长度为1的序列。所以有下图几种类型：
![](23.png)

除了最左侧的one to one是feedforward 能做的，右侧都是Recurrent所扩展的

若还想知道更多
1.可以将Recurrent的横向操作视为累积已发生的事情，并且LSTM的memory cell机制会选择记忆或者忘记所累积的信息来预测某个时刻的输出。
2.以概率的视角理解的话：就是不断的conditioning on已发生的事情，以此不断缩小sample space
3.RNN的思想是: current output不仅仅取决于current input，还取决于previous state；可以理解成current output是由current input和previous hidden state两个输入计算而出的。并且每次计算后都会有信息残留于previous hidden state中供下一次计算。

本题解析来源：@YJango，链接：https://www.zhihu.com/question/41949741

---













