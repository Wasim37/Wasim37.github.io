---
title: 机器学习（一）：回归算法
tags:
  - 线性回归
  - 逻辑回归
categories:
  - 机器学习
date: 2017-10-19 22:22:00
toc: false
mathjax: true

---

持续更新中。。。

<!-- more -->

- [什么是回归算法](#什么是回归算法)
- 回归算法推导：目标函数、对数似然及最小二乘
- 回归怎么预防过拟合
- L1和L2介绍与区别
- 什么是梯度下降，为什么要用梯度下降
- [BGD、SGD、MBGD介绍与区别](#BGD、SGD、MBGD介绍与区别)
- BGD、SGD、MBGD的推导
- [回归要调的参数有哪些](#回归要调的参数有哪些)
- [局部加权回归](#局部加权回归)
- [Softmax回归](#Softmax回归)
- 案例代码下载地址


- [什么是正则化](#什么是正则化)
- [正则化为什么可以防止过拟合](#正则化为什么可以防止过拟合)
- [L1和L2的区别](#L1和L2的区别)
- [L1和L2正则先验分别服从什么分布](#L1和L2正则先验分别服从什么分布)


- [模型评估指标](#模型评估指标)
- [ROC与AUC](#ROC与AUC)
- [常见的目标损失函数](#常见的目标损失函数)
- [除了MSE，还有哪些模型效果判断方法？区别是什么](#除了MSE，还有哪些模型效果判断方法？区别是什么)


---

### <h2 id="什么是回归算法">什么是回归算法</h2>

回归算法是一种比较常用的机器学习算法，**用来建立"解释"变量和观测值之间的关系；**
从机器学习的角度来讲，就是通过学习，构建一个算法模型(函数)，来做属性与标签之间的映射关系。

回归算法中算法(函数)的最终结果是一个 **连续** 的数据值，输入值(属性值)是一个d维度的属性/数值向量。

**回归涉及的算法模型：**线性回归(Linear)、岭回归(Ridge)、LASSO回归、Elastic Net弹性网络算法
**正则化：**L1-norm、L2-norm
**损失函数/目标函数：**![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/20180130175609.png)![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/20180130173938.png)
**θ求解方式：**最小二乘法(直接计算，目标函数是平方和损失函数)、梯度下降(BGD\SGD\MBGD)

广义线性模型对样本要求不必要服从正态分布、只需要服从指数分布簇(二项分布、泊松分布、伯努利分布、指数分布等)即可；广义线性模型的自变量可以是连续的也可以是离散的。

---

### <h2 id="BGD、SGD、MBGD介绍与区别">BGD、SGD、MBGD介绍与区别</h2>

- BGD：批量梯度下降法 Batch Gradient Descent
更新每一参数时都使用所有的样本来进行更新。
**优点：**全局最优解；易于并行实现；
**缺点：**当样本数目很多时，训练过程会很慢。

![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/20180130174843.png)

- SGD：随机梯度下降法 Stochastic Gradient Descent
随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。
**优点：**训练速度快；
**缺点：**准确度下降，并不是全局最优；不易于并行实现。
![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/20180130174857.png)

- MBGD：小批量梯度下降法 Mini-batch Gradient Descent。
如果即需要保证算法的训练速度，又需要保证最终参数训练的准确率，可以采取折衷方案MBGD。MBGD在每次更新参数时使用b个样本（b一般为10）。

- BGD和SGD比较
1）SGD速度比BGD快(迭代次数少)
2）SGD在某些情况下(全局存在多个相对最优解/J(θ)不是一个二次)有可能跳出某些小的局部最优解，所以不会比BGD坏
3）BGD一定能够得到一个局部最优解(在线性回归模型中一定是得到一个全局最优解)，SGD由于随机性的存在可能导致最终结果比BGD的差
**注意：两者优先选择SGD**

损失函数：$J_{train}(\theta)=1/(2m)\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}$

---

### <h2 id="回归要调的参数有哪些">回归要调的参数有哪些</h2>

对于各种算法模型(线性回归)来讲，我们需要获取θ、λ、p的值；
θ的求解其实就是算法模型的求解，一般不需要开发人员参与(算法已经实现)，
主要需要求解的是λ和p的值，这个过程就叫做调参(超参)

---

### <h2 id="局部加权回归">局部加权回归</h2>
普通线性回归损失函数：![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/线性回归损失函数.png)
局部加权回归损失函数：![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/加权线性损失函数.png)

$W^i$是权重，它根据要预测的点与数据集中的点的距离来为数据集中的点赋权值。
当某点离要预测的点越远，其权重越小，否则越大。常用值选择公式为：![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/加权回归W公式.png)

该函数称为指数衰减函数，其中k为波长参数，它控制了权值随距离下降的速率
**注意：使用该方式主要应用到样本之间的相似性考虑，主要内容在SVM中再考虑(核函数)**

---

### <h2 id="常见的目标损失函数">常见的目标损失函数</h2>
![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/20180130111945.png)

---
### <h2 id="除了MSE，还有哪些模型效果判断方法？区别是什么">除了MSE，还有哪些模型效果判断方法？区别是什么</h2>

- **SSE(The sum of squares due to error)**
和方差、误差平方和。计算的是拟合数据和原始数据对应点的误差的平方和，越趋近于0表示模型越拟合训练数据。
$$SSE=\sum_{t=1}^{N}(observed_t-predicted_t)^2 $$

- **MSE(Mean squared error)：**
均方差、均方误差、方差。计算的是预测数据和原始数据对应点误差的平方和的均值，越趋近于0表示模型越拟合训练数据。
$$MSE=\frac{1}{N}\sum_{t=1}^{N}(observed_t-predicted_t)^2 $$

- **RMSE(Root mean squared error)：**
均方根，也叫回归系统的拟合标准差，是MSE的平方根
$$RMSE=\sqrt{\frac{1}{N}\sum_{t=1}^{N}(observed_t-predicted_t)^2}$$

- **MAE(Mean Absolute Error)：**
平均绝对误差是绝对误差的平均值。平均绝对误差能更好地反映预测值误差的实际情况.
$$MAE={\frac{1}{N}\sum_{i=1}^{N}\lvert(observed_t-predicted_t)\rvert}$$

- **SD(standard Deviation)：**
标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的两组组数据，标准差未必相同。
$$SD=\sqrt{\frac{1}{N}\sum_{i=1}^{N}(x_i-u)^2}$$

- $R^2$ ：取值范围(负无穷,1]，值越大表示模型越拟合训练数据；最优解是1；
当模型预测为随机值的时候，有可能为负；若预测值恒为样本期望，$R^2$ 为0
- **TSS(Total Sum of Squares)**：总平方和，TSS表示样本之间的差异情况，是伪方差的m倍

- **RSS(Residual Sum of Squares)**：残差平方和，RSS表示预测值和样本值之间的差异情况，是MSE的m倍

---

### <h2 id="Softmax回归">Softmax回归</h2>
Softmax回归是logistic回归的一般化，适用于K分类的问题，第k类的参数为向量$θ_k$ ，组成的二维矩阵为$θ_k*n$ 。
Softmax函数的本质就是将一个K维的任意实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于（0，1）之间。

softmax回归概率函数为：![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/20180130181113.png)

---


### <h2 id="什么是正则化">什么是正则化</h2>
正则化是在成本函数中加入一个正则化项，惩罚模型的复杂度。是针对过拟合而提出的。

**Logistic 回归中的正则化**
对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数：
$$J(w,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}{||w||}^2_2$$

- L2 正则化：
$$\frac{\lambda}{2m}{||w||}^2_2 = \frac{\lambda}{2m}\sum_{j=1}^{n_x}w^2_j = \frac{\lambda}{2m}w^Tw$$
- L1 正则化：
$$\frac{\lambda}{2m}{||w||}_1 = \frac{\lambda}{2m}\sum_{j=1}^{n_x}{|w_j|}$$

其中，λ 为正则化因子，是超参数。
由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。
注意，lambda在 Python 中属于保留字，所以在编程的时候，用lambd代替这里的正则化因子。

---

### <h2 id="正则化为什么可以防止过拟合">正则化为什么可以防止过拟合</h2>
正则化是在成本函数中加入一个正则化项，惩罚模型的复杂度。是针对过拟合而提出的。

- 直观解释
正则化因子设置的足够大的情况下，为了使成本函数最小化，相应的参数就会被调小，甚至接近于0值，直观上相当于消除了很多特征值的影响。而且一般情况下参数越小(或者特征值越少)，可以理解为函数越光滑，那么对于的模型也越简单。有效、简单，奥卡姆剃刀原理。

- 其他解释
在权值 w变小之下，**输入样本 X 随机的变化不会对模型造成过大的影响**，模型受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。

- 神经网络防止过拟合还有一种数学解释，即每层权重变小，在 z 较小（接近于 0）的区域里，tanh(z)函数就近似线性，所以每层的函数就近似线性函数，所以防止过拟合。

---

### <h2 id="L1和L2的区别">L1和L2的区别</h2>


L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 
比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|.
简单总结一下就是： 
L1范数: 为x向量各个元素绝对值之和。 
L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或者Frobenius范数 
Lp范数: 为x向量各个元素绝对值p次方和的1/p次方.
在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。 
L1范数可以使权值稀疏，方便特征提取。 
L2范数可以防止过拟合，提升模型的泛化能力。

L1和L2的差别，为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢看导数一个是1一个是w便知， 在靠进零附近， L1以匀速下降到零， 而L2则完全停下来了. 这说明L1是将不重要的特征(或者说， 重要性不在一个数量级上)尽快剔除， L2则是把特征贡献尽量压缩最小但不至于为零. 两者一起作用， 就是把重要性在一个数量级(重要性最高的)的那些特征一起平等共事(简言之， 不养闲人也不要超人)。
引用自：@AntZ


---

### <h2 id="L1和L2正则先验分别服从什么分布">L1和L2正则先验分别服从什么分布</h2>


面试中遇到的，L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。
引用自：@齐同学
先验就是优化的起跑线， 有先验的好处就是可以在较小的数据集中有良好的泛化性能，当然这是在先验分布是接近真实分布的情况下得到的了，从信息论的角度看，向系统加入了正确先验这个信息，肯定会提高系统的性能。
对参数引入高斯正态先验分布相当于L2正则化， 这个大家都熟悉：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/48.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/49.png)
对参数引入拉普拉斯先验等价于 L1正则化， 如下图：
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/50.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/51.png)
从上面两图可以看出， L2先验趋向零周围， L1先验趋向零本身。
引用自：@AntZ

---

### <h2 id="模型评估指标">模型评估指标</h2>
有时候为了更好更快的评估模型，我们需要设置评估指标。常见的评估指标有：准确率/召回率/精准率/F值
1）**准确率(Accuracy)** = 提取出的正确样本数/总样本数
2）**召回率(Recall)** = 正确的正例样本数/样本中的正例样本数
3）**精准率(Precision)** = 正确的正例样本数/预测为正例的样本数
4）**F值** = Precision*Recall*2 / (Precision+Recall) (即F值为正确率和召回率的调和平均值)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/20180130184513.png)
![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/20180130191216.png)

---

### <h2 id="ROC与AUC">ROC与AUC</h2>
**对于分类器，或者说分类算法，评价指标主要有精准率(Precision)，召回率(Recall)，F-score1，以及现在所说的ROC和AUC。**
**ROC、AUC相比准确率、召回率、F-score这样的评价指标，有这样一个很好的特性**：<font color="red">**当测试集中正负样本的分布变化的时候，ROC曲线能够保持不变**</font>。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。

ROC曲线的**纵轴是“真正例率”**（True Positive Rate 简称TPR），**横轴是“假正例率”** （False Positive Rate 简称FPR）。
ROC曲线反映了FPR与TPR之间权衡的情况，通俗地来说，即在TPR随着FPR递增的情况下，谁增长得更快，快多少的问题。<font color="red">**TPR增长得越快，曲线越往上屈，AUC就越大，反映了模型的分类性能就越好**</font>。当正负样本不平衡时，这种模型评价方式比起一般的精确度评价方式的好处尤其显著。

![](http://7xvfir.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92/20180130192629.png)

**AUC（Area Under Curve）被定义为ROC曲线下的面积**，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而AUC作为数值可以直观的评价分类器的好坏，值越大越好。
1）AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
2）0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
3）AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
4）AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

至于ROC曲线具体是如何画出来的，这里推荐一篇文章：[ROC和AUC介绍以及如何计算AUC](http://alexkong.net/2013/06/introduction-to-auc-and-roc/)
