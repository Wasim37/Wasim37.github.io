---
title: 深度学习选择题集锦
tags:
  - 深度学习
categories:
  - 深度学习
date: 2017-12-12 22:22:00
toc: true
mathjax: true

---

### 1-10

1、梯度下降算法的正确步骤是什么？
1.用随机值初始化权重和偏差
2.把输入传入网络，得到输出值
3.计算预测值和真实值之间的误差
4.对每一个产生误差的神经元，调整相应的（权重）值以减小误差
5.重复迭代，直至得到网络权重的最佳值
A 1, 2, 3, 4, 5
B 5, 4, 3, 2, 1
C 3, 2, 1, 5, 4
D 4, 3, 1, 5, 2

**答案**：A

<!-- more -->

2、已知：
1) 大脑是有很多个叫做神经元的东西构成，神经网络是对大脑的简单的数学表达。
2) 每一个神经元都有输入、处理函数和输出。
3) 神经元组合起来形成了网络，可以拟合任何函数。
4) 为了得到最佳的神经网络，我们用梯度下降方法不断更新模型
给定上述关于神经网络的描述，什么情况下神经网络模型被称为深度学习模型？
A 加入更多层，使神经网络的深度增加
B 有维度更高的数据
C 当这是一个图形识别的问题时
D 以上都不正确

**答案**：A，您的选择是：D
**解析**：更多层意味着网络更深。没有严格的定义多少层的模型才叫深度模型，目前如果有超过2层的隐层，那么也可以叫做深度模型。

3、使用CNN时，是否需要对输入进行旋转、平移、缩放等预处理？
A 需要
B 不需要

**答案**：A
**解析**：把数据传入神经网络之前需要做一系列数据预处理（也就是旋转、平移、缩放）工作，神经网络本身不能完成这些变换。

4、下面哪项操作能实现跟神经网络中Dropout的类似效果？
A Boosting
B Bagging
C Stacking
D Mapping

**答案**：B
**解析**：Bagging和Dropout都是为了防止模型过拟合。但是bagging是针对于data，dropout是针对于特征。
Bagging是每次训练时从原训练集中随机抽取样本，然后对该样本进行训练。多次反复后，对最后的结果取均值。
Dropout是在构建神经网络的时候，随机的丢掉一些节点和边，这相当于是对特征进行了随机选择。

5、下列哪一项在神经网络中引入了非线性？
A 随机梯度下降
B 修正线性单元（ReLU）
C 卷积函数
D 以上都不正确

**答案**：B。

6、在训练神经网络时，损失函数(loss)在最初的几个epochs时没有下降，可能的原因是？
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/1.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
A 学习率(learning rate)太低
B 正则参数太高
C 陷入局部最小值
D 以上都有可能

**答案**：D
**解析**：一个epoch是指把所有训练数据完整的训练一遍。
假设训练集有1000个样本，batchsize=10，那么训练完整个样本集需要：100次iteration，1次epoch。

7、下列哪项关于模型能力（model capacity）的描述是正确的？（指神经网络模型能拟合复杂函数的能力）
A 隐藏层层数增加，模型能力增加
B Dropout的比例增加，模型能力增加
C 学习率增加，模型能力增加
D 都不正确

**答案**：A

8、如果增加多层感知机（Multilayer Perceptron）的隐藏层层数，分类误差便会减小。这种陈述正确还是错误？
A 正确
B 错误

**答案**：B
**解析**：层数增加可能导致过拟合。

9、构建一个神经网络，将前一层的输出和它自身作为输入。

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/2.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
指的是下面哪一种架构？
A 循环神经网络
B 卷积神经网络
C 限制玻尔兹曼机(Restricted Boltzmann Machine,简称RBM)
D 都不是

**答案**：A
**解析**：![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/19.png)
http://blog.csdn.net/roger__wong/article/details/43374343
https://deeplearning4j.org/cn/restrictedboltzmannmachine

10、在感知机中（Perceptron）的任务顺序是什么？
1 随机初始化感知机的权重
2 去到数据集的下一批（batch）
3 如果预测值和输出不一致，则调整权重
4 对一个输入样本，计算输出值
A 1, 2, 3, 4
B 4, 3, 2, 1
C 3, 1, 2, 4
D 1, 4, 3, 2
**答案**：D

---

### 11-20

1、假设你需要调整超参数来最小化代价函数（cost function），会使用下列哪项技术？
A 穷举搜索
B 随机搜索
C Bayesian优化
D 都可以

**答案**：D

2、在下面哪种情况下，一阶梯度下降不一定正确工作（可能会卡住）？
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/3.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)

**答案**：C
**解析**：鞍点（saddle point）的数学含义是：目标函数在此点上的梯度（一阶导数）值为 0， 但从该点出发的一个方向是函数的极大值点，而在另一个方向是函数的极小值点。

3、下图显示了训练过的3层卷积神经网络准确度，与参数数量(特征核的数量)的关系。
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/6.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
从图中趋势可见，如果增加神经网络的宽度，精确度会增加到一个特定阈值后，便开始降低。造成这一现象的可能原因是什么？
A 即使增加卷积核的数量，只有少部分的核会被用作预测
B 当卷积核数量增加时，神经网络的预测能力（Power）会降低
C 当卷积核数量增加时，可能学到数据中的噪声，导致过拟合
D 以上都不正确

**答案**：C

<font color="red">
4、假设我们有一个如下图所示的隐藏层。隐藏层在这个网络中起到了一定的降纬作用。假如现在我们用另一种维度下降的方法，比如说主成分分析法(PCA)来替代这个隐藏层。
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/7.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
那么，这两者的输出效果是一样的吗？
A 是
B 否

**答案**：B
**解析**：隐藏层提取的是有预测能力的特征，而 PCA 提取的是数据分布方差比较大的方向。

5、下列哪个函数不可以做激活函数？
A y = tanh(x)
B y = sin(x)
C y = max(x,0)
D y = 2x

**答案**：D
**解析**：线性函数不能作为激活函数。

6、下列哪个神经网络结构会发生权重共享？
A 卷积神经网络
B 循环神经网络
C 全连接神经网络
D 选项A和B

**答案**：D

7、批规范化(Batch Normalization)的好处都有啥？
A 让每一层的输入的范围都大致固定
B 它将权重的归一化平均值和标准差
C 它是一种非常有效的反向传播(BP)方法
D 这些均不是

**答案**：A

8、在一个神经网络中，下面哪种方法可以用来处理过拟合？
A Dropout
B 分批归一化(Batch Normalization)
C 正则化(regularization)
D 都可以

**答案**：D
**解析**：都可以。对于选项C，分批归一化处理过拟合的原理，是因为同一个数据在不同批中被归一化后的值会有差别，相当于做了data augmentatio。

9、如果我们用了一个过大的学习速率会发生什么？
A 神经网络会收敛
B 不好说
C 都不对
D 神经网络不会收敛

**答案**：D

<font color="red">
10、下图所示的网络用于训练识别字符H和T，如下所示

![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/8.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
可能是A或B，取决于神经网络的权重设置

**答案**：D
**解析**：不知道神经网络的权重和偏差是什么，则无法判定它将会给出什么样的输出。

---
### 21-30

1、神经网络模型（Neural Network）因受人类大脑的启发而得名
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/11.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
神经网络由许多神经元（Neuron）组成，每个神经元接受一个输入，对输入进行处理后给出一个输出，如下图所示。请问下列关于神经元的描述中，哪一项是正确的？
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/12.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
A 每个神经元可以有一个输入和一个输出
B 每个神经元可以有多个输入和一个输出
C 每个神经元可以有一个输入和多个输出
D 每个神经元可以有多个输入和多个输出
E 上述都正确

**答案**：E

2、在一个神经网络中，知道每一个神经元的权重和偏差是最重要的一步。如果知道了神经元准确的权重和偏差，便可以近似任何函数，但怎么获知每个神经的权重和偏移呢？
A 搜索每个可能的权重和偏差组合，直到得到最佳值
B 赋予一个初始值，然后检查跟最佳值的差值，不断迭代调整权重
C 随机赋值，听天由命
D 以上都不正确的

**答案**： B

<font color="red">
3、基于二次准则函数的H-K算法较之于感知器算法的优点是？
A 计算量小
B 可以判别问题是否线性可分
C 其解完全适用于非线性可分的情况

**答案**：B
**解析**：HK算法思想很朴实,就是在最小均方误差准则下求得权矢量.
他相对于感知器算法的优点在于,他适用于线性可分和非线性可分得情况,对于线性可分的情况,给出最优权矢量,对于非线性可分得情况,能够判别出来,以退出迭代过程。

4、输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为
A 95 &nbsp;&nbsp;&nbsp; B 96 &nbsp;&nbsp;&nbsp;C 97 &nbsp;&nbsp;&nbsp;D 98

**答案**：C
**解析：公式(n+2p-f)/s+1，padding指的是向外扩展的边缘大小，stride指的是步长，filter指的是卷积核大小。
经过第一次卷积后的大小为: (200+2x1-5)/2+1 为 99.5，取99
经过第一次池化后的大小为：(99-3)÷1+1 为 97
经过第二次卷积后的大小为：(97-3+2x1)/1+1为 97**

<font color="red"/>
5、深度学习是当前很热门的机器学习算法，在深度学习中，涉及到大量的矩阵相乘，现在需要计算三个稠密矩阵A,B,C的乘积ABC,假设三个矩阵的尺寸分别为mxn，nxp，pxq，且m < n < p < q，以下计算顺序效率最高的是（）
A (AB)C
B AC(B) 
C A(BC)
D 所以效率都相同

**答案**：A

**解析**：首先，根据简单的矩阵知识，因为 AxB， A 的列数必须和 B 的行数相等。因此可以排除 B 选项。
再看A和C，一个mxn的矩阵A乘以nxq的矩阵B。我们会用矩阵A的第一行，乘以矩阵B的第一列并相加。
这一运算需要耗费n次乘法以及n-1次加法，矩阵B有q列，矩阵A有m行，所以AxB的复杂度为m(2n-1)q。
根据上面的分析，选项A的复杂度为m(2n-1)p + m(2p-1)q，选项C为m(2n-1)q+n(2p-1)q，显然A效率更高。

6、当在卷积神经网络中加入池化层(pooling layer)时，变换的不变性会被保留，是吗？
A 不知道
B 看情况
C 是
D 否

**答案**：C
**解析：池化算法比如取最大值/取平均值等, 都是输入数据旋转后结果不变, 所以多层叠加后也有这种不变性**

7、当数据过大以至于无法在RAM中同时处理时，哪种梯度下降方法更加有效？
A 随机梯度下降法(Stochastic Gradient Descent)
B 不知道
C 整批梯度下降法(Full Batch Gradient Descent)
D 都不是

**答案**：A
**解析**：梯度下降法分随机梯度下降(每次用一个样本)、小批量梯度下降法(每次用一小批样本算出总损失, 因而反向传播的梯度折中)、全批量梯度下降(一次性使用全部样本)。这三个方法, 对于全体样本的损失函数曲面来说, 梯度指向一个比一个准确. 但是在工程应用中,受到内存/磁盘IO的吞吐性能制约, 若要最小化梯度下降的实际运算时间, 需要在梯度方向准确性和数据传输性能之间取得最好的平衡. 所以, 对于数据过大以至于无法在RAM中同时处理时, RAM每次只能装一个样本, 那么只能选随机梯度下降法。

8、在选择神经网络的深度时，下面哪些参数需要考虑？
1 神经网络的类型(如MLP多层神经网络,CNN卷积神经网络)
2 输入数据
3 计算能力(硬件和软件能力决定)
4 学习速率
5 映射的输出函数
A 1,2,4,5
B 2,3,4,5
C 都需要考虑
D 1,3,4,5

**答案**：C
**解析**：所有上述因素对于选择神经网络模型的深度都是重要的。特征抽取所需分层越多, 输入数据维度越高, 映射的输出函数非线性越复杂, 所需深度就越深. 另外为了达到最佳效果, 增加深度所带来的参数量增加, 也需要考虑硬件计算能力和学习速率以设计合理的训练时间。

9、考虑某个具体问题时，你可能只有少量数据来解决这个问题。不过幸运的是你有一个类似问题已经预先训练好的神经网络。可以用下面哪种方法来利用这个预先训练好的网络？
A 把除了最后一层外所有的层都冻住，重新训练最后一层
B 对新数据重新训练整个模型
C 只对最后几层进行调参(fine tune)
D 对每一层模型进行评估，选择其中的少数来用

**答案**：C
**解析**：如果有个预先训练好的神经网络, 就相当于网络各参数有个很靠谱的先验代替随机初始化. **若新的少量数据来自于先前训练数据**(或者先前训练数据量很好地描述了数据分布, 而新数据采样自完全相同的分布), 则冻结前面所有层而重新训练最后一层即可;** 但一般情况下, 新数据分布跟先前训练集分布有所偏差, 所以先验网络不足以完全拟合新数据时, 可以冻结大部分前层网络, 只对最后几层进行训练调参(这也称之为fine tune)。**

10、下图是一个利用sigmoid函数作为激活函数的含四个隐藏层的神经网络训练的梯度下降图。这个神经网络遇到了梯度消失的问题。下面哪个叙述是正确的？
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/13.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
A 第一隐藏层对应D，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应A
B 第一隐藏层对应A，第二隐藏层对应C，第三隐藏层对应B，第四隐藏层对应D
C 第一隐藏层对应A，第二隐藏层对应B，第三隐藏层对应C，第四隐藏层对应D
D 第一隐藏层对应B，第二隐藏层对应D，第三隐藏层对应C，第四隐藏层对应A

**答案**：A
**解析**：由于反向传播算法进入起始层，学习能力降低，这就是梯度消失。换言之，梯度消失是梯度在前向传播中逐渐减为0, 按照图标题所说, 四条曲线是4个隐藏层的学习曲线, 那么第一层梯度最高(损失函数曲线下降明显), 最后一层梯度几乎为零(损失函数曲线变成平直线). 所以D是第一层, A是最后一层。

--- 
### 31-40

1、增加卷积核的大小对于改进卷积神经网络的效果是必要的吗？
A 没听说过
B 是
C 否
D 不知道

**答案**：C
**解析**：增加核函数的大小不一定会提高性能。这个问题在很大程度上取决于数据集。

2、假设我们已经在ImageNet数据集(物体识别)上训练好了一个卷积神经网络。然后给这张卷积神经网络输入一张全白的图片。对于这个输入的输出结果为任何种类的物体的可能性都是一样的，对吗？
A 对的
B 不知道
C 看情况
D 不对

**答案**：D

**解析**：已经训练好的卷积神经网络, 各个神经元已经精雕细作完工, 对于全白图片的输入, **其层层激活后输出给最后的全连接层的值几乎不可能恒等, 再经softmax转换之后也不会相等**, 所以"输出结果为任何种类的等可能性一样"也就是softmax的每项均相等, 这个概率是极低的。

3、对于一个分类任务，如果开始时神经网络的权重不是随机赋值的，二是都设成0，下面哪个叙述是正确的？
A 其他选项都不对
B 没啥问题，神经网络会正常开始训练
C 神经网络可以训练，但是所有的神经元最后都会变成识别同样的东西
D 神经网络不会开始训练，因为没有梯度改变

**答案**：C
**解析**：令所有权重都初始化为0这个一个听起来还蛮合理的想法也许是一个我们假设中最好的一个假设了, 但结果是错误的，因为如果神经网络计算出来的输出值都一个样，那么反向传播算法计算出来的梯度值一样，并且参数更新值也一样(w=w−α∗dw)。**更一般地说，如果权重初始化为同一个值，网络即是对称的, 最终所有的神经元最后都会变成识别同样的东西。**

4、下图显示，当开始训练时，误差一直很高，这是因为神经网络在往全局最小值前进之前一直被卡在局部最小值里。为了避免这种情况，我们可以采取下面哪种策略？
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/14.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
A 改变学习速率，比如一开始的几个训练周期不断更改学习速率
B 一开始将学习速率减小10倍，然后用动量项(momentum)
C 增加参数数目，这样神经网络就不会卡在局部最优处
D 其他都不对

**答案**：A
**解析**：选项A可以将陷于局部最小值的神经网络提取出来。

5、对于一个图像识别问题(在一张照片里找出一只猫)，下面哪种神经网络可以更好地解决这个问题？
A 循环神经网络
B 感知机
C 多层感知机
D 卷积神经网络

**答案**：D
**解析**：卷积神经网络将更好地适用于图像相关问题，因为考虑到图像附近位置变化的固有性质。


6、假设在训练中我们突然遇到了一个问题，在几次循环之后，误差瞬间降低
你认为数据有问题，于是你画出了数据并且发现也许是数据的偏度过大造成了这个问题。 
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/15.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
你打算怎么做来处理这个问题？
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/16.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
A 对数据作归一化
B 对数据取对数变化
C 都不对
D 对数据作主成分分析(PCA)和归一化

<font color="red">
**答案**：D
**解析**：首先您将相关的数据去掉，然后将其置零。具体来说，误差瞬间降低, 一般原因是多个数据样本有强相关性且突然被拟合命中, 或者含有较大方差数据样本突然被拟合命中. 所以对数据作主成分分析(PCA)和归一化能够改善这个问题。

7、下面那个决策边界是神经网络生成的？
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/17.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
A A
B D
C C
D B
E 以上都对

**答案**：E
**解析：神经网络可以以逼近方式拟合任意函数**。

8、在下图中，我们可以观察到误差出现了许多小的"涨落"。 这种情况我们应该担心吗？
![](http://7xvfir.com1.z0.glb.clouddn.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%80%89%E6%8B%A9%E9%A2%98%E9%9B%86%E9%94%A6/18.png?imageView2/0/q/75|watermark/1/image/aHR0cDovLzd4dmZpci5jb20xLnowLmdsYi5jbG91ZGRuLmNvbS8lRTYlQjAlQjQlRTUlOEQlQjAvJUU1JThEJTlBJUU1JUFFJUEyJUU2JUIwJUI0JUU1JThEJUIwLnBuZw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim)
A 需要，这也许意味着神经网络的学习速率存在问题
B 不需要，只要在训练集和交叉验证集上有累积的下降就可以了
C 不知道
D 不好说

**答案**：B
**解析**：选项B是正确的，为了减少这些“起伏”，可以尝试增加批尺寸(batch size)。具体来说，在曲线整体趋势为下降时, 为了减少这些“起伏”，可以尝试增加批尺寸(batch size)以缩小batch综合梯度方向摆动范围. 当整体曲线趋势为平缓时出现可观的“起伏”, 可以尝试降低学习率以进一步收敛. “起伏”不可观时应该提前终止训练以免过拟合

9、对于神经网络的说法, 下面正确的是: 
1）增加神经网络层数, 可能会增加测试数据集的分类错误率
2）减少神经网络层数, 总是能减小测试数据集的分类错误率
3）增加神经网络层数, 总是能减小训练数据集的分类错误率
A 1
B 1 和 3
C 1 和 2
D 2
**答案**：A
**解析**：深度神经网络的成功, 已经证明, 增加神经网络层数, 可以增加模型范化能力, 即训练数据集和测试数据集都表现得更好. 但更多的层数, 也不一定能保证有更好的表现（https://arxiv.org/pdf/1512.03385v1.pdf）. 所以, 不能绝对地说层数多的好坏, 只能选A
