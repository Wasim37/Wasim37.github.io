---
title: 卷积操作详解（填充、步长、高维卷积、卷积公式）
tags:
  - 卷积
categories:
  - 神经网络
date: 2018-1-10 22:22:00
toc: true
mathjax: true

---

**对图像**（不同的数据窗口数据）**和滤波矩阵**（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）**做内积**（逐个元素相乘再求和）**的操作就是所谓的『卷积』操作**，也是卷积神经网络的名字来源。

![卷积](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/4.png)

<!-- more -->

---

### 填充

假设输入图片的大小为 **n×n**，而滤波器的大小为 **f×f**，则卷积后的输出图片大小为 **(n−f+1)×(n−f+1)**。

**这样就有两个问题：**

- 每次卷积运算后，输出图片的**尺寸缩小**；
- 原始图片的角落、边缘区像素点在输出中采用较少，输出图片**丢失边缘位置的很多信息**。

为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行**填充（Padding）**，以增加矩阵的大小。通常将 0 作为填充值。

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/Padding.jpg)

设每个方向扩展像素点数量为 p，则填充后原始图片的大小为 **(n+2p)×(n+2p)$，滤波器大小保持 $f×f$不变，则输出图片大小为 $(n+2p−f+1)×(n+2p−f+1)$。

因此，在进行卷积运算时，我们有两种选择：

- **Valid 卷积**：不填充，直接卷积。结果大小为 $(n−f+1)×(n−f+1)$；
- **Same 卷积**：进行填充，并使得卷积后结果大小与输入一致，这样 **$p = \frac{f-1}{2}$**。

在计算机视觉领域，f通常为奇数。原因包括 Same 卷积中 $p = \frac{f-1}{2}$ 能得到自然数结果，并且滤波器有一个便于表示其所在位置的中心点。

---

### 卷积步长

卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置**步长（Stride）**来压缩一部分信息。

步长表示滤波器在原始图片的水平方向和垂直方向上每次移动的距离。之前，步长被默认为 1。而如果我们设置步长为 2，则卷积过程如下图所示：

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/Stride.jpg)

设步长为 s，填充长度为 p，输入图片大小为 n×n，滤波器大小为 f×f，则卷积后图片的尺寸为：

$$\biggl\lfloor \frac{n+2p-f}{s}+1   \biggr\rfloor \times \biggl\lfloor \frac{n+2p-f}{s}+1 \biggr\rfloor$$

注意公式中有一个**向下取整**的符号，用于处理商不为整数的情况。向下取整反映着当取原始矩阵的图示蓝框完全包括在图像内部时，才对它进行运算。

目前为止我们学习的“卷积”实际上被称为**互相关**（cross-correlation），而非数学意义上的卷积。真正的卷积操作在做元素乘积求和之前，要将滤波器沿水平和垂直轴翻转（相当于旋转 180 度）。因为这种翻转对一般为水平或垂直对称的滤波器影响不大，按照机器学习的惯例，我们通常不进行翻转操作，在简化代码的同时使神经网络能够正常工作。


---

### 高维卷积
如果我们想要对三通道的 RGB 图片进行卷积运算，那么其对应的滤波器组也同样是三通道的。过程是将每个单通道（R，G，B）与对应的滤波器进行卷积运算求和，然后再将三个通道的和相加，将 27 个乘积的和作为输出图片的一个像素值。

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/Convolutions-on-RGB-image.png)

不同通道的滤波器可以不相同。例如只检测 R 通道的垂直边缘，G 通道和 B 通道不进行边缘检测，则 G 通道和 B 通道的滤波器全部置零。**当输入有特定的高、宽和通道数时，滤波器可以有不同的高和宽，<font color="red">但通道数必须和输入一致</font>**。

如果想同时检测垂直和水平边缘，或者更多的边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。设输入图片的尺寸为 $n×n×nc$（nc为通道数），滤波器尺寸为 $f×f×nc$，则卷积后的输出图片尺寸为 $(n−f+1)×(n−f+1)×n′c$，$n′c$为滤波器组的个数。

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/More-Filters.jpg)

<video id="video" width='621'  controls="" preload="none" poster="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/conv_kiank.png">
<source id="mp4"  src="https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/conv_kiank.mp4" type="video/mp4">
</video>

---

### 单层卷积网络

![](https://hexo-blog-wasim.oss-cn-shenzhen.aliyuncs.com/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E9%9B%86%E9%94%A6/One-Layer-of-a-Convolutional-Network.jpg)

与之前的卷积过程相比较，**卷积神经网络的单层结构多了激活函数和偏移量**；而与标准神经网络：

$$Z^{[l]} = W^{[l]}A^{[l-1]}+b$$
$$A^{[l]} = g^{[l]}(Z^{[l]})$$
相比，滤波器的数值对应着权重 $W[l]$，卷积运算对应着 $W[l]$与 $A[l−1]$的乘积运算，所选的激活函数变为 ReLU。

对于一个 3x3x3 的滤波器，包括偏移量 b在内共有 28 个参数。不论输入的图片有多大，用这一个滤波器来提取特征时，参数始终都是 28 个，固定不变。**<font color="red">即选定滤波器组后，参数的数目与输入图片的尺寸无关</font>**。因此，卷积神经网络的参数相较于标准神经网络来说要少得多。这是 CNN 的优点之一。


---

### 符号总结
设 $l$ 层为卷积层：

- $f^{[l]}$：滤波器的高（或宽）
- $p^{[l]}$：填充长度
- $s^{[l]}$：步长
- $n^{[l]}_c$：滤波器组的数量

**输入维度**：$n^{[l-1]}_H \times n^{[l-1]}_W \times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_H$表示输入图片的高，$n^{[l-1]}_W$表示输入图片的宽。之前的示例中输入图片的高和宽都相同，但是实际中也可能不同，因此加上下标予以区分。

**输出维度**：$n^{[l]}_H \times n^{[l]}_W \times n^{[l]}_c$ 。其中

$$n^{[l]}_H = \biggl\lfloor \frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \biggr\rfloor$$
$$n^{[l]}_W = \biggl\lfloor \frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \biggr\rfloor$$

**每个滤波器组的维度**：$f^{[l]} \times f^{[l]} \times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_c$ 为输入图片通道数（也称深度）。
**权重维度**：$f^{[l]} \times f^{[l]} \times n^{[l-1]}_c \times n^{[l]}_c$
**偏置维度**：$1 \times 1 \times 1 \times n^{[l]}_c$

由于深度学习的相关文献并未对卷积标示法达成一致，因此不同的资料关于高度、宽度和通道数的顺序可能不同。有些作者会将通道数放在首位，需要根据标示自行分辨。