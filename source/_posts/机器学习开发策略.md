---
title: 机器学习开发策略
tags:
  - 深度学习
  - 吴恩达
categories:
  - 深度学习
date: 2017-12-10 22:22:00
toc: true
mathjax: true

---

吴大大结构化机器学习项目总结，完善中...

### ML策略

假设你构建了一个喵咪分类器，训练之后准确率达到90%，但在测试集上还不够好。此时你可以想到的优化方法有哪些呢？总结后大致如下：
- 收集更多的数据
- 收集更多的多样化训练集，比如不同姿势的猫咪图片等
- 用梯度下降法训练更长时间
- 尝试Adam算法
- 尝试更大的网路
- 尝试小一点的网络
- 尝试dropout随机失活算法
- 加上L2正则项
- 改善网络结构，如变更激活函数，变更隐藏层节点数量

优化的方法虽然很多，但如果方向错误，可能白费几个月时间。
**那通过哪些策略可以减少错误发生的几率呢？怎么判断哪些方法可以尝试，哪些方法可以丢弃呢？**

<!-- more -->

---

### 正交化 Orthogonalization

优化前首先需要明白正交化。教科书式定义（可以直接略过 =。=）：正交化是一种系统设计属性，它确保修改指令或算法的组成部分不会对系统的其他组件产生或传播副作用。独立地验证某部分而不对其他部分产生影响，能有效减少测试和开发时间。

正交性很好理解，就像以前的老式黑白电视机，它有很多调节画面的旋钮。假设第一个旋钮控制上下方向，第二个旋钮控制左右方向。当我们需要调节画面时，调节上下方向不会影响左右方向。这样互不影响的设计能大大减少调节画面的时间。

同样，要弄好一个监督学习系统，我们也需要考虑系统的旋钮。
通常我们需要保证下面四个方面是正交的。
- 首先系统在训练集上表现良好
 - 如果拟合不好，那么尝试使用更大的神经网络或者切换更好的优化算法，比如Adam等
- 其次系统在开发集上表现良好
 - 如果拟合不好，尝试正则化或者更大的训练集
- 然后系统在测试集上表现良好
 - 如果拟合不好，尝试更大的开发集
- 最后系统在真实(生产)环境中表现良好
 - 如果表现不好，意味着开发测试集分布设置可能不对，或者损失函数不能有效反映算法在现实世界的表现

**我们需要使用正交化的思想去分析系统的瓶颈究竟出自哪一个方面，当系统表现不佳时，哪些旋钮是值得去尝试的。<font color='red'>尝试过程中需要训练网络，吴大大在这特意提到，他自己训练神经网络时通常不会提前停止网络训练，因为这会让问题的分析复杂化。比如提前停止训练集的训练，它在影响训练集的拟合的同时会改善开发集的表现，这样问题就不正交化了。<font>**


---

### 单一数字评估指标 Single number evaluation metric

**当我们知道模型在哪个集合拟合不好时，我们就有了优化的方向。无论是调整超参数，还是尝试更好的优化算法，为了更好更快的重新评估模型，我们都需要为问题设置一个单一的数字评估指标。**

下面是分别训练的两个分类器的 Precision(精准率)、Recall（召回率）以及F1 score。

![](1.png)

由上表可以看出，以 **Precision** 为指标，则分类器 A 的分类效果好；以 **Recall** 为指标，则分类器 B 的分类效果好。所以仅用以上判定指标，我们有时很难决定出 A 好还是 B 好。

这里以 Precision 和 Recall 为基础，构成一个综合指标 **F1 Score** ，那么我们利用F1 Score便可以更容易的评判出分类器A的效果更好。

**指标介绍：**

在二分类问题中，通过预测我们得到下面的真实值 y 和预测值 y^ 的表：

![](2.png)

- **Precision（精准率）： **
$Precision = \dfrac{True\ positive}{Number\ of\ predicted\ positive} \times 100\%= \dfrac{True\ positive}{True\ positive + False\ positive}$

假设在是否为猫的分类问题中，查准率代表：所有模型预测为猫的图片中，确实为猫的概率。

- **Recall（召回率）：** 
$Recall = \dfrac{True\ positive}{Number\ of\  actually\ positive} \times 100\%= \dfrac{True\ positive}{True\ positive + False\ negative}$

假设在是否为猫的分类问题中，查全率代表：真实为猫的图片中，预测正确的概率。
- **F1 Score： **
$vF1-Socre = \dfrac {2} {\dfrac{1}{p}+\dfrac{1}{r}}$

相当与精准率和召回率的一个特别形式的平均指标。

**示例：**下面是另外一个问题多种分类器在不同的国家中的分类错误率结果：

![](3.png)

模型在各个地区有不同的表现，这里用地区的平均值来对模型效果进行评估，转换为单一数字评估指标，就可以很容易的得出表现最好的模型。

---

### 满足和优化指标
**有时候把我们所有顾及的事情组成单一数字评估指标并不容易。这个时候，可以尝试把指标划分为满足指标和优化指标。**

比如现在有三个不同的分类器性能表现如下：

![](4.png)

假设我们对模型效果有一定的要求，不仅要求准确率，还要求运行时间在100 ms以内。那么我们可以以 Accuracy 为优化指标，以 Running time 为满足指标。一旦Running time达到要求，我们就可以把全部精力放在优化指标上。以这个思想，我们很快可以从中选出B是满足条件的最好的分类器。

**<font color='red'>一般的，如果要考虑N个指标，则选择一个指标为优化指标，其他N-1个指标都是满足指标：</font>**
$$
N_{metric}:\left\{ \begin{array}{l}
1\qquad \qquad \qquad Optimizing\ metric\\
N_{metric}-1\qquad Satisificing\ metric
\end{array} \right.
$$

---

### 训练/开发/测试集划分

**确立了评估指标，就是确定了靶心，团队拿到数据就可以快速迭代不断逼近指标。这个时候正确的数据集的划分就非常重要了，它直接关乎你的团队效率。**

我们知道训练集是用来训练模型的，开发集(也要交叉验证集)用来尝试迭代各种想法，用来优化性能，得到一个满意的损失结果后，最后用测试集评估。

现在假设有这样的一个数据集，需要将他们划分为开发集和测试集， 有些人可能会随机选择几个国家的作为开发集，剩下的作为测试集，就如下图所示。

![](9.png)

有些人可能觉得没问题，其实问题很大！因为开发集和测试集不服从同一分布，这就好像你在准备托福考试，你尽可能的得到了所有的考试技巧和其他资料，最后你的确得到了不错的成绩。但是后来因为工作需求需要你会说俄语，此时如果你用之前托福的资料来对付俄语考试则显然不对，这也就是为什么有时候开发集准确率高，但是测试集低。

**<font color='red'>所以数据要随机洗牌，然后放到训练、开发和测试集中。其次，这些随机洗牌的数据的来源也需要注意，即你选择的数据集，要能反应出你未来希望得到的数据，即模型数据要和未来数据相似，这样模型在生产环境中面对新的数据才能有好的表现。</font>**

---

### 开发集和测试集的大小

机器学习发展到现在，数据集的划分与传统稍有不同。具体详情见下图：

![](10.png)

**<font color='red'>开发集有时候需要足够大才能评估不同的想法，至于测试集的量，除非你对最终投产的系统有非常高的精准指标，否则一般情况下，1W的数据量足够。注意测试集的划分不再需要按照传统占据30%，以现在的数据量，30%很可能超过百万。</font>**

---

### 什么时候该改变开发集/测试集和指标

在针对某一问题我们设置开发集和评估指标后，这就像把目标定在某个位置，后面的过程就聚焦在该位置上。但有时候在这个项目的过程中，可能会发现目标的位置设置错了，所以要移动改变我们的目标。

example1

假设有两个猫的图片的分类器：
- 评估指标：分类错误率
- 算法A：3%错误率
- 算法B：5%错误率

这样来看，算法A的表现更好。但是在实际的测试中，算法A可能因为某些原因，将很多色情图片分类成了猫。所以当我们在线上部署的时候，算法A会给爱猫人士推送更多更准确的猫的图片（因为其误差率只有3%），但同时也会给用户推送一些色情图片，这是不能忍受的。所以，虽然算法A的错误率很低，但是它却不是一个好的算法。

这个时候我们就需要改变开发集、测试集或者评估指标。

假设开始我们的评估指标如下：
$Error = \dfrac{1}{m_{dev}}\sum\limits_{i=1}^{m_{dev}}I\{y^{(i)}_{pred}\neq y^{(i)}\}$

该评估指标对色情图片和非色情图片一视同仁，但是我们希望，分类器不会错误将色情图片标记为猫。

修改的方法，在其中加入权重$w^{(i)}$： 

$Error = \dfrac{1}{\sum w^{(i)}}\sum\limits_{i=1}^{m_{dev}} w^{(i)}I\{y^{(i)}_{pred}\neq y^{(i)}\}$
其中： 

$$
w^{(i)}=\left\{ \begin{array}{l}
1\qquad \qquad \qquad 如果x^{(i)}不是色情图片\\
10或100\qquad \qquad如果x^{(i)}是色情图片
\end{array} \right.
$$

这样通过设置权重，当算法将色情图片分类为猫时，误差项会快速变大。
总结来说就是：如果评估指标无法正确评估算法的排名，则需要重新定义一个新的评估指标。

example2

同样针对example1中的两个不同的猫图片的分类器A和B。

![](5.png)

但实际情况是对，我们一直使用的是网上下载的高质量的图片进行训练；而当部署到手机上时，由于图片的清晰度及拍照水平的原因，当实际测试算法时，会发现算法B的表现其实更好。

如果在训练开发测试的过程中得到的模型效果比较好，但是在实际应用中自己所真正关心的问题效果却不好的时候，就需要改变开发、测试集或者评估指标。

Guideline：
- 定义正确的评估指标来更好的给分类器的好坏进行排序；
- 优化评估指标。

---

### 为什么是人的表现

---

### 可避免偏差

假设针对两个问题分别具有相同的训练误差和交叉验证误差，如下所示：

![](6.png)

对于左边的问题，人类的误差为1%，对于右边的问题，人类的误差为7.5%。

对于某些任务如计算机视觉上，人类能够做到的水平和贝叶斯误差相差不远。（这里贝叶斯误差指最好的分类器的分类误差，也就是说没有分类器可以做到100%正确）。这里将人类水平误差近似为贝叶斯误差。

- 左边的例子：8%与1%差距较大 
主要着手减少偏差，即减少训练集误差和人类水平误差之间的差距，来提高模型性能。
- 右边的例子：8%与7.5%接近 
主要着手减少方差，即减少开发集误差和测试集误差之间的差距，来提高模型性能。

---

### 理解人的表现

如医学图像分类问题上，假设有下面几种分类的水平：

普通人：3% error
普通医生：1% error
专家：0.7% error
专家团队：0.5% error
在减小误诊率的背景下，人类水平误差在这种情形下应定义为：0.5% error；

如果在为了部署系统或者做研究分析的背景下，也许超过一名普通医生即可，即人类水平误差在这种情形下应定义为：1% error；

总结：

对人类水平误差有一个大概的估计，可以让我们去估计贝叶斯误差，这样可以让我们更快的做出决定：减少偏差还是减少方差。

而这个决策技巧通常都很有效果，直到系统的性能开始超越人类，那么我们对贝叶斯误差的估计就不再准确了，再从减少偏差和减少方差方面提升系统性能就会比较困难了。

![](7.png)

---

### 超过人的表现

---

### 改善你的模型表现
基本假设：

模型在训练集上有很好的表现；
模型推广到开发和测试集啥会给你也有很好的表现。
减少可避免偏差

训练更大的模型
训练更长时间、训练更好的优化算法（Momentum、RMSprop、Adam）
寻找更好的网络架构（RNN、CNN）、寻找更好的超参数
减少方差

收集更多的数据
正则化（L2、dropout、数据增强）
寻找更好的网络架构（RNN、CNN）、寻找更好的超参数
![](8.png)

---

### 误差分析

### 清除错误标记的样本

### 搭建系统

### 不同分布上的训练和测试

### 不同分布上的偏差和方差

### 解决数据分布不匹配问题

### 迁移学习

### 多任务学习

### 端到端深度学习