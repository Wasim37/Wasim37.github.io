---
title: DL笔记
categories:
  - 机器学习
date: 2018-1-1 21:18:00
toc: false
---

- [K-近邻算法](#K-近邻算法)
- [回归](#回归)
- [逻辑回归和梯度下降](#逻辑回归和梯度下降)
- [决策树](#决策树)
- [随机森林](#随机森林)
- [贝叶斯](#贝叶斯)
- [聚类](#聚类)
- [EM](#EM)
- [LDA主题模型](#LDA主题模型)
- [马尔可夫模型](#马尔可夫模型)
- [数据预处理](#数据预处理)
- [算法评估](#算法评估)
- [名词解释](#名词解释)
- [疑问](#疑问)

---

<!-- more -->

### <h2 id="K-近邻算法">K-近邻算法</h2>
思想：物以类聚，人以群分
不需要训练数据集，训练时间复杂度为0，即平时不干活，一来新数据，直接根据旧数据进行分类。
Knn的分类涉及距离计算，计算复杂度和数据大小正相关，数据越大越慢，分类效率低下
重点：如果样本某个类别数量占99%，那对分类结果影响太大，所以不同样本类别赋予不同权重
另外如果有些邻居很近，有些很远，分类的时候都按一票来投，不公平，所以近的邻居加权
【K近邻怎么做回归？和决策回归树一样，将最近邻居属性算平均赋值给样本，即得到该样本属性】

---

### <h2 id="回归">回归</h2>

[怎么理解最小二乘法](https://baike.baidu.com/item/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E8%A7%A3)

线性回归的线性是针对cta来说的

为什么先说回归
回归中参数有两个参数，一个是咱们要学习的参数，一个是要调的超参
对特征选择回归问题也可以很好的体现，因为其中有LASSO

正则化——对简单模型予以奖励而对复杂算法予以惩罚【正则化就是加了一个惩罚机制，相当于取消一些独立表达效果太好的参数】
- [初学者如何学习机器学习中的L1和L2正则化](https://baijia.baidu.com/s?id=1584829526467056227&wfr=pc&fr=app_lst)
- [L1正则和L2正则](http://www.jianshu.com/p/a47c46153326)
- [l1 相比于 l2 为什么容易获得稀疏解](https://www.zhihu.com/question/37096933?sort=created)
- [范数正则化L0、L1、L2-岭回归&Lasso回归](http://blog.csdn.net/sinat_26917383/article/details/52092040)

---

### <h2 id="逻辑回归和梯度下降">逻辑回归和梯度下降</h2>
sigmoid能把任意输入转化为0到1的值，默认阈值是0.5，我们可以认为大于0.5属于1，小于0.5等于0，阈值可以自己设置。这样相当间接的把回归的问题转换为分类问题（回归算法中算法的最终结果是一个连续的数据值）。所以逻辑回归做的不是回归问题，是分类问题，但是是二分类。

极大似然估计，就是求一套斯塔，使的斯塔和x组合，使求的结果和真实的y最接近，即误差最小。误差最小，用的最小二乘法。

梯度下降：回归中求最小误差，我们取对数后对斯塔求导，使函数为0，但是我们化简后发现还是找不出斯塔值，使函数等于0，这个时候引出梯度下降

目的：找全局最小值
简单方法：斯塔设置一个可能的范围，for循环代入函数，得出这个范围函数的最小值，但是可能是局部最小值。而且我们求的不是一个斯塔，而是一套斯塔，各种组合计算复杂，又不科学
那我们可以利用坡度的反方向（负梯度下降），坡度为负，坡度越大，下降越快（就是对各个点求导）。然后引入阿尔法，就是步长或学习率的意思。但是步长过大，很可能一步过大，直接跨过最优解。所以学习率一般设置的小些，然后不断的试，求出斯塔


在梯度下降的公式中中，斯塔和阿尔法都是已知的，斯塔可以初始化0，阿尔法设置一个教小数，
批量下降的思想中，斯塔迭代更新一次，是要计算全部数据一次的，耗时慢

逻辑回归的求解是求一套斯塔，即一个向量θn，softmax多分类求的却是一个二维矩阵θk*n

---

### <h2 id="决策树">决策树</h2>
- [决策树重点归纳](https://www.cnblogs.com/fuleying/p/4472540.html)
- [梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)

---

### <h2 id="随机森林">随机森林</h2>
做特征选择？【没看完，只知道过程中涉及特征选择】
也可以做多目标的分类，不需要做额外的计算

---

### <h2 id="贝叶斯">贝叶斯</h2>
- pd.Categorical().codes机器学习数据预处理常用，比如男女变01
one-hot编码一般是深度学习用的，比如0001.0010
- 后验概率和条件概率的区别：首先事件发生是有先后的，后验概率是我们要求的。比如已知各种类别P(A)的概率，已知各种类别A情况下的特征B，即P(B|A)，现在给定一系列特征B，求它是哪种类别，即求P(A|B)
- 朴素贝叶斯算法：要进行思想的转换，即用贝叶斯定理来进行分类
- 整个贝叶斯求的就是后验概率：通过先验概率和条件概率求后验概率
- 因为样本的分布有特征，比如x1概率是0.01，x2的是0.99，那么X1的影响极其小，我们不能简单的按比例计算概率，才按特征引入了高斯、伯努利和多项式朴素贝叶斯
- 贝叶斯在文本挖掘用的比较多
- 怎么判断特征是否独立：用cov(x,y) (不需要判断，只是假设。如果不独立，有其他办法?)

- 卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。

- 文本处理：用TF/DF的值来作为单词的权重，文档A中各个单词出现的频率TF(term/token)，文档A中的各个单词在所有文档中出现的频率DF(出现的文档/总的文档)

【贝叶斯网络】
LDA的文章-主题-词汇 就是一个三层贝叶斯网络
由于文章、主题和词汇不是独立分布的，它们之间有关联，所以不适用于朴素贝叶斯，然后引出了贝叶斯网络
贝叶斯网络有个重要知识点：P(a,b|c) = P(a|c)*P(b|c), 即在C给定的条件下，a和b被阻断独立的

---

### <h2 id="聚类">聚类</h2>
- k-means为什么对初始值敏感？
- 具体是怎么根据距离聚类的？


---

### <h2 id="EM">EM</h2>
- EM是算法，不是专门用来做回归和分类的，它是在其他算法中充当迭代的功能，用来聚类的
- GMM 我们知道男的高斯分布，知道女的高斯分布，他们在图形投影上有重合，现在算p(x,y),很难算，引入GMM

Nk总影响

聚类没有用到y,所以可以用来预测？y_hat,案例二。其实不分训练和测试集，都是训练集？
聚合的结果非常好，但是参数即方差很大，这是不合理的

极大似然函数就是用来求参数的

---

### <h2 id="LDA主题模型">LDA主题模型</h2>
LDA(Latent Dirichlet Allocation)是一个简单的模型，用来做主题分析（文本聚类），而不是用来生成文章的
LDA认为某一篇文章的生产过程是：
- 确定主题和词汇的分布
- 确定文章和主题的分布
- 随机确定该文章的词汇个数N
- 如果当前生成的词汇个数小于N执行第5步，否则执行第6步
- 由文档和主题分布随机生成一个主题，通过该主题由主题和词汇分布随机生成一个词，继续执行第4步
- 文章生成结束

上面的过程涉及的问题是：如何确定主题和词汇分布，还有文档与词汇的分布。LDA说：【主题和词汇的分布就是多项式分布！！！！文章和主题之间的分布也是符合多项式分布！！！】ps：反推的过程有点像贝叶斯后验

伯努利——二项分布——多项分布——贝塔分布——狄利克雷分布
- [LDA主题模型详解](LDA：http://blog.csdn.net/aws3217150/article/details/53840029)

- [贝塔分布](https://baike.baidu.com/item/狄利克雷分布/12728892)
- [狄利克雷分布](https://baike.baidu.com/item/%E8%B4%9D%E5%A1%94%E5%88%86%E5%B8%83/8994021)
- [贝叶斯学习及共轭先验](http://blog.csdn.net/acdreamers/article/details/45026459)

共轭：两个概率分布如果具有相同的形式，我们就说它们是共轭的

---

### <h2 id="马尔可夫模型">马尔可夫模型</h2>
统计模型，广泛应用在语音识别，词性自动标注，音字转换，概率文法等各个自然语言处理等应用领域

三个主要算法（后两个算法没懂）：
- 直接算法（暴力算法） 算出所有可能
- 前向算法
- 后向算法

隐马尔可夫模型的学习根据训练数据的丌同分为监督学习和非监督学习
- 若训练数据包括观测序列和对应的状态序列，则可迚行监督学习；
- 若训练数据仅有观测序列，那就要用非监督学习了

文章相关：
- [如何用简单易懂的例子解释隐马尔可夫模型？](https://www.zhihu.com/question/20962240)
- [HMM相关文章索引](http://url.cn/5E2hMHv)


---

### <h2 id="数据预处理">数据预处理</h2>
- 回归模型经常用stand  分类模型用minmax
- 先逻辑回归，再随机森林，还不行用提升算法算

逻辑回归看auc，一般不看得分
画图针对的是小数量级，大数量级一般直接只看最终结果，比如得分和auc等

---

### <h2 id="算法评估">算法评估</h2>
- [ROC和AUC介绍以及如何计算AUC](http://alexkong.net/2013/06/introduction-to-auc-and-roc/)

---

### <h2 id="名词解释">名词解释</h2>
- 协方差Cov与相关系数Pearson【https://www.zhihu.com/question/20852004】
- 稀疏解【只有很小的概率有值,大部分概率值都很小或为0】
- r方(也叫确定系数 coefficient of determination)：表示模型对现实数据拟合的程度】
- 残差：指实际观察值（真实值）与估计值（拟合值）之间的差
- 算法的收敛：如果迭代次数到一定数值时，解趋向于平稳，则可以认为算法已开始收敛
- 1-范数：即向量元素绝对值之和； 二范数：向量元素绝对值的平方和再开方

---

### <h2 id="疑问">疑问</h2>
- [机器学习各种算法怎么调参?](https://www.zhihu.com/question/34470160/answer/114305935)
- [SVM的核函数如何选取？](https://www.zhihu.com/question/21883548)
